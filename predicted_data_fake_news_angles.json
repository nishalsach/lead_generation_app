[
    {
        "arxiv_id":"2208.02056v1",
        "predicted_newsworthiness":0.8006555235,
        "title":"Fast or Accurate? Governing Conflicting Goals in Highly Autonomous Vehicles",
        "summary":"The tremendous excitement around the deployment of autonomous vehicles (AVs) comes from their purported promise. In addition to decreasing accidents, AVs are projected to usher in a new era of equity in human autonomy by providing affordable, accessible, and widespread mobility for disabled, elderly, and low-income populations. However, to realize this promise, it is necessary to ensure that AVs are safe for deployment, and to contend with the risks AV technology poses, which threaten to eclipse its benefits. In this Article, we focus on an aspect of AV engineering currently unexamined in the legal literature, but with critical implications for safety, accountability, liability, and power. Specifically, we explain how understanding the fundamental engineering trade-off between accuracy and speed in AVs is critical for policymakers to regulate the uncertainty and risk inherent in AV systems. We discuss how understanding the trade-off will help create tools that will enable policymakers to assess how the trade-off is being implemented. Such tools will facilitate opportunities for developing concrete, ex ante AV safety standards and conclusive mechanisms for ex post determination of accountability after accidents occur. This will shift the balance of power from manufacturers to the public by facilitating effective regulation, reducing barriers to tort recovery, and ensuring that public values like safety and accountability are appropriately balanced.",
        "published":1659533065000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02056v1",
        "arxiv_primary_category":"cs.cy",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.252143492,
        "technologyreview":0.4119373094,
        "newscientist":0.2494928565,
        "venturebeat":0.3531646938,
        "wired":0.3730778011,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.02187v1",
        "predicted_newsworthiness":0.7067764891,
        "title":"On the independence between phenomenal consciousness and computational intelligence",
        "summary":"Consciousness and intelligence are properties commonly understood as dependent by folk psychology and society in general. The term artificial intelligence and the kind of problems that it managed to solve in the recent years has been shown as an argument to establish that machines experience some sort of consciousness. Following the analogy of Russell, if a machine is able to do what a conscious human being does, the likelihood that the machine is conscious increases. However, the social implications of this analogy are catastrophic. Concretely, if rights are given to entities that can solve the kind of problems that a neurotypical person can, does the machine have potentially more rights that a person that has a disability? For example, the autistic syndrome disorder spectrum can make a person unable to solve the kind of problems that a machine solves. We believe that the obvious answer is no, as problem solving does not imply consciousness. Consequently, we will argue in this paper how phenomenal consciousness and, at least, computational intelligence are independent and why machines do not possess phenomenal consciousness, although they can potentially develop a higher computational intelligence that human beings. In order to do so, we try to formulate an objective measure of computational intelligence and study how it presents in human beings, animals and machines. Analogously, we study phenomenal consciousness as a dichotomous variable and how it is distributed in humans, animals and machines. As phenomenal consciousness and computational intelligence are independent, this fact has critical implications for society that we also analyze in this work.",
        "published":1659543431000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02187v1",
        "arxiv_primary_category":"cs.ai",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2595174929,
        "technologyreview":0.4074786229,
        "newscientist":0.3119430525,
        "venturebeat":0.3362680605,
        "wired":0.3267331472,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2208.02007v1",
        "predicted_newsworthiness":0.6389625458,
        "title":"Maintaining Performance with Less Data",
        "summary":"We propose a novel method for training a neural network for image classification to reduce input data dynamically, in order to reduce the costs of training a neural network model. As Deep Learning tasks become more popular, their computational complexity increases, leading to more intricate algorithms and models which have longer runtimes and require more input data. The result is a greater cost on time, hardware, and environmental resources. By using data reduction techniques, we reduce the amount of work performed, and therefore the environmental impact of AI techniques, and with dynamic data reduction we show that accuracy may be maintained while reducing runtime by up to 50%, and reducing carbon emission proportionally.",
        "published":1659529338000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02007v1",
        "arxiv_primary_category":"cs.lg",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1685615701,
        "technologyreview":0.3480001276,
        "newscientist":0.2288991013,
        "venturebeat":0.3227379419,
        "wired":0.2328671249,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02058v1",
        "predicted_newsworthiness":0.6126444709,
        "title":"Robots with Different Embodiments Can Express and Influence Carefulness in Object Manipulation",
        "summary":"Humans have an extraordinary ability to communicate and read the properties of objects by simply watching them being carried by someone else. This level of communicative skills and interpretation, available to humans, is essential for collaborative robots if they are to interact naturally and effectively. For example, suppose a robot is handing over a fragile object. In that case, the human who receives it should be informed of its fragility in advance, through an immediate and implicit message, i.e., by the direct modulation of the robot's action. This work investigates the perception of object manipulations performed with a communicative intent by two robots with different embodiments (an iCub humanoid robot and a Baxter robot). We designed the robots' movements to communicate carefulness or not during the transportation of objects. We found that not only this feature is correctly perceived by human observers, but it can elicit as well a form of motor adaptation in subsequent human object manipulations. In addition, we get an insight into which motion features may induce to manipulate an object more or less carefully.",
        "published":1659533212000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02058v1",
        "arxiv_primary_category":"cs.ro",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1602035381,
        "technologyreview":0.3147115834,
        "newscientist":0.2346764662,
        "venturebeat":0.2482707888,
        "wired":0.2416229545,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.02052v1",
        "predicted_newsworthiness":0.6093986306,
        "title":"Large scale analysis of gender bias and sexism in song lyrics",
        "summary":"We employ Natural Language Processing techniques to analyse 377808 English song lyrics from the \"Two Million Song Database\" corpus, focusing on the expression of sexism across five decades (1960-2010) and the measurement of gender biases. Using a sexism classifier, we identify sexist lyrics at a larger scale than previous studies using small samples of manually annotated popular songs. Furthermore, we reveal gender biases by measuring associations in word embeddings learned on song lyrics. We find sexist content to increase across time, especially from male artists and for popular songs appearing in Billboard charts. Songs are also shown to contain different language biases depending on the gender of the performer, with male solo artist songs containing more and stronger biases. This is the first large scale analysis of this type, giving insights into language usage in such an influential part of popular culture.",
        "published":1659532722000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02052v1",
        "arxiv_primary_category":"cs.cy",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2459197387,
        "technologyreview":0.2786261178,
        "newscientist":0.1857582784,
        "venturebeat":0.2484277722,
        "wired":0.2865302123,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.01901v1",
        "predicted_newsworthiness":0.58167722,
        "title":"Asynchronous Federated Learning for Edge-assisted Vehicular Networks",
        "summary":"Vehicular networks enable vehicles support real-time vehicular applications through training data. Due to the limited computing capability, vehicles usually transmit data to a road side unit (RSU) at the network edge to process data. However, vehicles are usually reluctant to share data with each other due to the privacy issue. For the traditional federated learning (FL), vehicles train the data locally to obtain a local model and then upload the local model to the RSU to update the global model, thus the data privacy can be protected through sharing model parameters instead of data. The traditional FL updates the global model synchronously, i.e., the RSU needs to wait for all vehicles to upload their models for the global model updating. However, vehicles may usually drive out of the coverage of the RSU before they obtain their local models through training, which reduces the accuracy of the global model. It is necessary to propose an asynchronous federated learning (AFL) to solve this problem, where the RSU updates the global model once it receives a local model from a vehicle. However, the amount of data, computing capability and vehicle mobility may affect the accuracy of the global model. In this paper, we jointly consider the amount of data, computing capability and vehicle mobility to design an AFL scheme to improve the accuracy of the global model. Extensive simulation experiments have demonstrated that our scheme outperforms the FL scheme",
        "published":1659513902000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01901v1",
        "arxiv_primary_category":"cs.lg",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1177417816,
        "technologyreview":0.2616295667,
        "newscientist":0.1359132424,
        "venturebeat":0.2686851282,
        "wired":0.2389197579,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01890v1",
        "predicted_newsworthiness":0.5749370546,
        "title":"High stable and accurate vehicle selection scheme based on federated edge learning in vehicular networks",
        "summary":"Federated edge learning (FEEL) technology for vehicular networks is considered as a promising technology to reduce the computation workload while keep the privacy of users. In the FEEL system, vehicles upload data to the edge servers, which train the vehicles' data to update local models and then return the result to vehicles to avoid sharing the original data. However, the cache queue in the edge is limited and the channel between edge server and each vehicle is a time varying wireless channel, which makes a challenge to select a suitable number of vehicles to upload data to keep a stable cache queue in edge server and maximize the learning accuracy. Moreover, selecting vehicles with different resource statuses to update data will affect the total amount of data involved in training, which further affects the model accuracy. In this paper, we propose a vehicle selection scheme, which maximizes the learning accuracy while ensuring the stability of the cache queue, where the statuses of all the vehicles in the coverage of edge server are taken into account. The performance of this scheme is evaluated through simulation experiments, which indicates that our proposed scheme can perform better than the known benchmark scheme.",
        "published":1659512377000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01890v1",
        "arxiv_primary_category":"cs.ni",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.106740813,
        "technologyreview":0.2585772897,
        "newscientist":0.1418873414,
        "venturebeat":0.2679240627,
        "wired":0.2408274963,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2208.01876v1",
        "predicted_newsworthiness":0.5644298105,
        "title":"Leveraging Smartphone Sensors for Detecting Abnormal Gait for Smart Wearable Mobile Technologies",
        "summary":"Walking is one of the most common modes of terrestrial locomotion for humans. Walking is essential for humans to perform most kinds of daily activities. When a person walks, there is a pattern in it, and it is known as gait. Gait analysis is used in sports and healthcare. We can analyze this gait in different ways, like using video captured by the surveillance cameras or depth image cameras in the lab environment. It also can be recognized by wearable sensors. e.g., accelerometer, force sensors, gyroscope, flexible goniometer, magneto resistive sensors, electromagnetic tracking system, force sensors, and electromyography (EMG). Analysis through these sensors required a lab condition, or users must wear these sensors. For detecting abnormality in gait action of a human, we need to incorporate the sensors separately. We can know about one's health condition by abnormal human gait after detecting it. Understanding a regular gait vs. abnormal gait may give insights to the health condition of the subject using the smart wearable technologies. Therefore, in this paper, we proposed a way to analyze abnormal human gait through smartphone sensors. Though smart devices like smartphones and smartwatches are used by most of the person nowadays. So, we can track down their gait using sensors of these intelligent wearable devices.",
        "published":1659510016000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01876v1",
        "arxiv_primary_category":"cs.hc",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1454689114,
        "technologyreview":0.2543879062,
        "newscientist":0.19776424,
        "venturebeat":0.250709025,
        "wired":0.2501261855,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.01844v1",
        "predicted_newsworthiness":0.5559257271,
        "title":"Multiclass ASMA vs Targeted PGD Attack in Image Segmentation",
        "summary":"Deep learning networks have demonstrated high performance in a large variety of applications, such as image classification, speech recognition, and natural language processing. However, there exists a major vulnerability exploited by the use of adversarial attacks. An adversarial attack imputes images by altering the input image very slightly, making it nearly undetectable to the naked eye, but results in a very different classification by the network. This paper explores the projected gradient descent (PGD) attack and the Adaptive Mask Segmentation Attack (ASMA) on the image segmentation DeepLabV3 model using two types of architectures: MobileNetV3 and ResNet50, It was found that PGD was very consistent in changing the segmentation to be its target while the generalization of ASMA to a multiclass target was not as effective. The existence of such attack however puts all of image classification deep learning networks in danger of exploitation.",
        "published":1659503130000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01844v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1098043665,
        "technologyreview":0.2773074599,
        "newscientist":0.1498093868,
        "venturebeat":0.2344209816,
        "wired":0.1788754198,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02043v1",
        "predicted_newsworthiness":0.5552532291,
        "title":"SmartControllerJS: A JavaScript library to turn smartphones into controllers for web-based interactive experiments",
        "summary":"We introduce SmartControllerJS, a new JavaScript library for fast, cost-effective designing of web applications controlled via everyday smartphones. At its core, SmartControllerJS establishes a connection between two webpages, one page running on a desktop browser and the other on the user's smartphone. The smartphone webpage loads a controller interface allowing users to control a web application running on their computer's browser. The SmartControllerJS framework enables fast iteration loops when designing interactive user experiments because it has minimal friction and allows for scaling, while having no running costs. We first describe how this library is built, how it can be used, and provide interactive examples. We then present two games designed for public screens along with results from user studies evaluating acceptability and ease of use. Finally, we implement a custom controller based on user feedback and introduce connection monitoring tools. We believe SmartControllerJS can accelerate the design of interactive experiments for researchers in Human-Computer Interaction, and be a useful tool for educational projects. To experience the various demos, we recommend reading this work on a desktop computer with your smartphone in hand. The library and the demos are available at https:\/\/github.com\/SmartControllerJS",
        "published":1659532302000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02043v1",
        "arxiv_primary_category":"cs.hc",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1927552918,
        "technologyreview":0.3185509387,
        "newscientist":0.2419836362,
        "venturebeat":0.3517254379,
        "wired":0.3485027441,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.02031v1",
        "predicted_newsworthiness":0.5463075475,
        "title":"Cross-lingual Approaches for the Detection of Adverse Drug Reactions in German from a Patient's Perspective",
        "summary":"In this work, we present the first corpus for German Adverse Drug Reaction (ADR) detection in patient-generated content. The data consists of 4,169 binary annotated documents from a German patient forum, where users talk about health issues and get advice from medical doctors. As is common in social media data in this domain, the class labels of the corpus are very imbalanced. This and a high topic imbalance make it a very challenging dataset, since often, the same symptom can have several causes and is not always related to a medication intake. We aim to encourage further multi-lingual efforts in the domain of ADR detection and provide preliminary experiments for binary classification using different methods of zero- and few-shot learning based on a multi-lingual model. When fine-tuning XLM-RoBERTa first on English patient forum data and then on the new German data, we achieve an F1-score of 37.52 for the positive class. We make the dataset and models publicly available for the community.",
        "published":1659531121000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02031v1",
        "arxiv_primary_category":"cs.cl",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1860993123,
        "technologyreview":0.2397600007,
        "newscientist":0.2017674958,
        "venturebeat":0.2264281508,
        "wired":0.1920507213,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.02121v1",
        "predicted_newsworthiness":0.5453956245,
        "title":"Pedestrian-Robot Interactions on Autonomous Crowd Navigation: Reactive Control Methods and Evaluation Metrics",
        "summary":"Autonomous navigation in highly populated areas remains a challenging task for robots because of the difficulty in guaranteeing safe interactions with pedestrians in unstructured situations. In this work, we present a crowd navigation control framework that delivers continuous obstacle avoidance and post-contact control evaluated on an autonomous personal mobility vehicle. We propose evaluation metrics for accounting efficiency, controller response and crowd interactions in natural crowds. We report the results of over 110 trials in different crowd types: sparse, flows, and mixed traffic, with low- (< 0.15 ppsm), mid- (< 0.65 ppsm), and high- (< 1 ppsm) pedestrian densities. We present comparative results between two low-level obstacle avoidance methods and a baseline of shared control. Results show a 10% drop in relative time to goal on the highest density tests, and no other efficiency metric decrease. Moreover, autonomous navigation showed to be comparable to shared-control navigation with a lower relative jerk and significantly higher fluency in commands indicating high compatibility with the crowd. We conclude that the reactive controller fulfils a necessary task of fast and continuous adaptation to crowd navigation, and it should be coupled with high-level planners for environmental and situational awareness.",
        "published":1659538563000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02121v1",
        "arxiv_primary_category":"cs.ro",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1412962903,
        "technologyreview":0.2556067526,
        "newscientist":0.1663028745,
        "venturebeat":0.2226286988,
        "wired":0.2225192948,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.02019v1",
        "predicted_newsworthiness":0.5263815389,
        "title":"YOLO-FaceV2: A Scale and Occlusion Aware Face Detector",
        "summary":"In recent years, face detection algorithms based on deep learning have made great progress. These algorithms can be generally divided into two categories, i.e. two-stage detector like Faster R-CNN and one-stage detector like YOLO. Because of the better balance between accuracy and speed, one-stage detectors have been widely used in many applications. In this paper, we propose a real-time face detector based on the one-stage detector YOLOv5, named YOLO-FaceV2. We design a Receptive Field Enhancement module called RFE to enhance receptive field of small face, and use NWD Loss to make up for the sensitivity of IoU to the location deviation of tiny objects. For face occlusion, we present an attention module named SEAM and introduce Repulsion Loss to solve it. Moreover, we use a weight function Slide to solve the imbalance between easy and hard samples and use the information of the effective receptive field to design the anchor. The experimental results on WiderFace dataset show that our face detector outperforms YOLO and its variants can be find in all easy, medium and hard subsets. Source code in https:\/\/github.com\/Krasjet-Yu\/YOLO-FaceV2",
        "published":1659530400000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02019v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1024859234,
        "technologyreview":0.2593807631,
        "newscientist":0.1590754814,
        "venturebeat":0.2355415642,
        "wired":0.1857638663,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01862v1",
        "predicted_newsworthiness":0.5198119342,
        "title":"MixNet: Structured Deep Neural Motion Prediction for Autonomous Racing",
        "summary":"Reliably predicting the motion of contestant vehicles surrounding an autonomous racecar is crucial for effective and performant planning. Although highly expressive, deep neural networks are black-box models, making their usage challenging in safety-critical applications, such as autonomous driving. In this paper, we introduce a structured way of forecasting the movement of opposing racecars with deep neural networks. The resulting set of possible output trajectories is constrained. Hence quality guarantees about the prediction can be given. We report the performance of the model by evaluating it together with an LSTM-based encoder-decoder architecture on data acquired from high-fidelity Hardware-in-the-Loop simulations. The proposed approach outperforms the baseline regarding the prediction accuracy but still fulfills the quality guarantees. Thus, a robust real-world application of the model is proven. The presented model was deployed on the racecar of the Technical University of Munich for the Indy Autonomous Challenge 2021. The code used in this research is available as open-source software at www.github.com\/TUMFTM\/MixNet.",
        "published":1659507368000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01862v1",
        "arxiv_primary_category":"cs.ro",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.111721078,
        "technologyreview":0.2788813997,
        "newscientist":0.1570461869,
        "venturebeat":0.2663964917,
        "wired":0.234473317,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01963v1",
        "predicted_newsworthiness":0.5125098253,
        "title":"Localization and Classification of Parasitic Eggs in Microscopic Images Using an EfficientDet Detector",
        "summary":"IPIs caused by protozoan and helminth parasites are among the most common infections in humans in LMICs. They are regarded as a severe public health concern, as they cause a wide array of potentially detrimental health conditions. Researchers have been developing pattern recognition techniques for the automatic identification of parasite eggs in microscopic images. Existing solutions still need improvements to reduce diagnostic errors and generate fast, efficient, and accurate results. Our paper addresses this and proposes a multi-modal learning detector to localize parasitic eggs and categorize them into 11 categories. The experiments were conducted on the novel Chula-ParasiteEgg-11 dataset that was used to train both EfficientDet model with EfficientNet-v2 backbone and EfficientNet-B7+SVM. The dataset has 11,000 microscopic training images from 11 categories. Our results show robust performance with an accuracy of 92%, and an F1 score of 93%. Additionally, the IOU distribution illustrates the high localization capability of the detector.",
        "published":1659522498000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01963v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1315140849,
        "technologyreview":0.1923283677,
        "newscientist":0.1829487416,
        "venturebeat":0.1772685626,
        "wired":0.1324564729,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02205v1",
        "predicted_newsworthiness":0.5105206768,
        "title":"DAHiTrA: Damage Assessment Using a Novel Hierarchical Transformer Architecture",
        "summary":"This paper presents DAHiTrA, a novel deep-learning model with hierarchical transformers to classify building damages based on satellite images in the aftermath of hurricanes. An automated building damage assessment provides critical information for decision making and resource allocation for rapid emergency response. Satellite imagery provides real-time, high-coverage information and offers opportunities to inform large-scale post-disaster building damage assessment. In addition, deep-learning methods have shown to be promising in classifying building damage. In this work, a novel transformer-based network is proposed for assessing building damage. This network leverages hierarchical spatial features of multiple resolutions and captures temporal difference in the feature domain after applying a transformer encoder on the spatial features. The proposed network achieves state-of-the-art-performance when tested on a large-scale disaster damage dataset (xBD) for building localization and damage classification, as well as on LEVIR-CD dataset for change detection tasks. In addition, we introduce a new high-resolution satellite imagery dataset, Ida-BD (related to the 2021 Hurricane Ida in Louisiana in 2021, for domain adaptation to further evaluate the capability of the model to be applied to newly damaged areas with scarce data. The domain adaptation results indicate that the proposed model can be adapted to a new event with only limited fine-tuning. Hence, the proposed model advances the current state of the art through better performance and domain adaptation. Also, Ida-BD provides a higher-resolution annotated dataset for future studies in this field.",
        "published":1659544899000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02205v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1801153966,
        "technologyreview":0.2602415796,
        "newscientist":0.1990865381,
        "venturebeat":0.2360261232,
        "wired":0.2186781725,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01910v1",
        "predicted_newsworthiness":0.5030101396,
        "title":"Multimodal Generation of Novel Action Appearances for Synthetic-to-Real Recognition of Activities of Daily Living",
        "summary":"Domain shifts, such as appearance changes, are a key challenge in real-world applications of activity recognition models, which range from assistive robotics and smart homes to driver observation in intelligent vehicles. For example, while simulations are an excellent way of economical data collection, a Synthetic-to-Real domain shift leads to a > 60% drop in accuracy when recognizing activities of Daily Living (ADLs). We tackle this challenge and introduce an activity domain generation framework which creates novel ADL appearances (novel domains) from different existing activity modalities (source domains) inferred from video training data. Our framework computes human poses, heatmaps of body joints, and optical flow maps and uses them alongside the original RGB videos to learn the essence of source domains in order to generate completely new ADL domains. The model is optimized by maximizing the distance between the existing source appearances and the generated novel appearances while ensuring that the semantics of an activity is preserved through an additional classification loss. While source data multimodality is an important concept in this design, our setup does not rely on multi-sensor setups, (i.e., all source modalities are inferred from a single video only.) The newly created activity domains are then integrated in the training of the ADL classification networks, resulting in models far less susceptible to changes in data distributions. Extensive experiments on the Synthetic-to-Real benchmark Sims4Action demonstrate the potential of the domain generation paradigm for cross-domain ADL recognition, setting new state-of-the-art results. Our code is publicly available at https:\/\/github.com\/Zrrr1997\/syn2real_DG",
        "published":1659515313000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01910v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1192711564,
        "technologyreview":0.2478732547,
        "newscientist":0.1749133119,
        "venturebeat":0.2339122665,
        "wired":0.2035922629,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01996v1",
        "predicted_newsworthiness":0.5010574692,
        "title":"Adaptive Domain Generalization via Online Disagreement Minimization",
        "summary":"Deep neural networks suffer from significant performance deterioration when there exists distribution shift between deployment and training. Domain Generalization (DG) aims to safely transfer a model to unseen target domains by only relying on a set of source domains. Although various DG approaches have been proposed, a recent study named DomainBed, reveals that most of them do not beat the simple Empirical Risk Minimization (ERM). To this end, we propose a general framework that is orthogonal to existing DG algorithms and could improve their performance consistently. Unlike previous DG works that stake on a static source model to be hopefully a universal one, our proposed AdaODM adaptively modifies the source model at test time for different target domains. Specifically, we create multiple domain-specific classifiers upon a shared domain-generic feature extractor. The feature extractor and classifiers are trained in an adversarial way, where the feature extractor embeds the input samples into a domain-invariant space, and the multiple classifiers capture the distinct decision boundaries that each of them relates to a specific source domain. During testing, distribution differences between target and source domains could be effectively measured by leveraging prediction disagreement among source classifiers. By fine-tuning source models to minimize the disagreement at test time, target domain features are well aligned to the invariant feature space. We verify AdaODM on two popular DG methods, namely ERM and CORAL, and four DG benchmarks, namely VLCS, PACS, OfficeHome, and TerraIncognita. The results show AdaODM stably improves the generalization capacity on unseen domains and achieves state-of-the-art performance.",
        "published":1659527471000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01996v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.102801448,
        "technologyreview":0.299848044,
        "newscientist":0.1469453541,
        "venturebeat":0.275583853,
        "wired":0.1874162424,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01956v1",
        "predicted_newsworthiness":0.491503405,
        "title":"Augmentation Learning for Semi-Supervised Classification",
        "summary":"Recently, a number of new Semi-Supervised Learning methods have emerged. As the accuracy for ImageNet and similar datasets increased over time, the performance on tasks beyond the classification of natural images is yet to be explored. Most Semi-Supervised Learning methods rely on a carefully manually designed data augmentation pipeline that is not transferable for learning on images of other domains. In this work, we propose a Semi-Supervised Learning method that automatically selects the most effective data augmentation policy for a particular dataset. We build upon the Fixmatch method and extend it with meta-learning of augmentations. The augmentation is learned in additional training before the classification training and makes use of bi-level optimization, to optimize the augmentation policy and maximize accuracy. We evaluate our approach on two domain-specific datasets, containing satellite images and hand-drawn sketches, and obtain state-of-the-art results. We further investigate in an ablation the different parameters relevant for learning augmentation policies and show how policy learning can be used to adapt augmentations to datasets beyond ImageNet.",
        "published":1659521211000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01956v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1066058673,
        "technologyreview":0.2754201256,
        "newscientist":0.1531939932,
        "venturebeat":0.2412231544,
        "wired":0.1809332833,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01897v1",
        "predicted_newsworthiness":0.4912771562,
        "title":"Combined CNN Transformer Encoder for Enhanced Fine-grained Human Action Recognition",
        "summary":"Fine-grained action recognition is a challenging task in computer vision. As fine-grained datasets have small inter-class variations in spatial and temporal space, fine-grained action recognition model requires good temporal reasoning and discrimination of attribute action semantics. Leveraging on CNN's ability in capturing high level spatial-temporal feature representations and Transformer's modeling efficiency in capturing latent semantics and global dependencies, we investigate two frameworks that combine CNN vision backbone and Transformer Encoder to enhance fine-grained action recognition: 1) a vision-based encoder to learn latent temporal semantics, and 2) a multi-modal video-text cross encoder to exploit additional text input and learn cross association between visual and text semantics. Our experimental results show that both our Transformer encoder frameworks effectively learn latent temporal semantics and cross-modality association, with improved recognition performance over CNN vision model. We achieve new state-of-the-art performance on the FineGym benchmark dataset for both proposed architectures.",
        "published":1659513715000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01897v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0822674475,
        "technologyreview":0.1738414677,
        "newscientist":0.105374508,
        "venturebeat":0.1627526414,
        "wired":0.1390969989,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02049v1",
        "predicted_newsworthiness":0.4776091941,
        "title":"AutoLaparo: A New Dataset of Integrated Multi-tasks for Image-guided Surgical Automation in Laparoscopic Hysterectomy",
        "summary":"Computer-assisted minimally invasive surgery has great potential in benefiting modern operating theatres. The video data streamed from the endoscope provides rich information to support context-awareness for next-generation intelligent surgical systems. To achieve accurate perception and automatic manipulation during the procedure, learning based technique is a promising way, which enables advanced image analysis and scene understanding in recent years. However, learning such models highly relies on large-scale, high-quality, and multi-task labelled data. This is currently a bottleneck for the topic, as available public dataset is still extremely limited in the field of CAI. In this paper, we present and release the first integrated dataset (named AutoLaparo) with multiple image-based perception tasks to facilitate learning-based automation in hysterectomy surgery. Our AutoLaparo dataset is developed based on full-length videos of entire hysterectomy procedures. Specifically, three different yet highly correlated tasks are formulated in the dataset, including surgical workflow recognition, laparoscope motion prediction, and instrument and key anatomy segmentation. In addition, we provide experimental results with state-of-the-art models as reference benchmarks for further model developments and evaluations on this dataset. The dataset is available at https:\/\/autolaparo.github.io.",
        "published":1659532643000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02049v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1055414595,
        "technologyreview":0.2746870219,
        "newscientist":0.1814825123,
        "venturebeat":0.2442248734,
        "wired":0.1975091109,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01916v1",
        "predicted_newsworthiness":0.4769157329,
        "title":"N-RPN: Hard Example Learning for Region Proposal Networks",
        "summary":"The region proposal task is to generate a set of candidate regions that contain an object. In this task, it is most important to propose as many candidates of ground-truth as possible in a fixed number of proposals. In a typical image, however, there are too few hard negative examples compared to the vast number of easy negatives, so region proposal networks struggle to train on hard negatives. Because of this problem, networks tend to propose hard negatives as candidates, while failing to propose ground-truth candidates, which leads to poor performance. In this paper, we propose a Negative Region Proposal Network(nRPN) to improve Region Proposal Network(RPN). The nRPN learns from the RPN's false positives and provide hard negative examples to the RPN. Our proposed nRPN leads to a reduction in false positives and better RPN performance. An RPN trained with an nRPN achieves performance improvements on the PASCAL VOC 2007 dataset.",
        "published":1659516513000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01916v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0779476324,
        "technologyreview":0.1992369115,
        "newscientist":0.1324496005,
        "venturebeat":0.1721466211,
        "wired":0.1315075428,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01951v1",
        "predicted_newsworthiness":0.4706576035,
        "title":"Exploration with Model Uncertainty at Extreme Scale in Real-Time Bidding",
        "summary":"In this work, we present a scalable and efficient system for exploring the supply landscape in real-time bidding. The system directs exploration based on the predictive uncertainty of models used for click-through rate prediction and works in a high-throughput, low-latency environment. Through online A\/B testing, we demonstrate that exploration with model uncertainty has a positive impact on model performance and business KPIs.",
        "published":1659520751000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01951v1",
        "arxiv_primary_category":"cs.lg",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1008293484,
        "technologyreview":0.2025071615,
        "newscientist":0.114586459,
        "venturebeat":0.2696244877,
        "wired":0.1746364553,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02150v1",
        "predicted_newsworthiness":0.4680999577,
        "title":"Empirical Study of Overfitting in Deep FNN Prediction Models for Breast Cancer Metastasis",
        "summary":"Overfitting is defined as the fact that the current model fits a specific data set perfectly, resulting in weakened generalization, and ultimately may affect the accuracy in predicting future data. In this research we used an EHR dataset concerning breast cancer metastasis to study overfitting of deep feedforward Neural Networks (FNNs) prediction models. We included 11 hyperparameters of the deep FNNs models and took an empirical approach to study how each of these hyperparameters was affecting both the prediction performance and overfitting when given a large range of values. We also studied how some of the interesting pairs of hyperparameters were interacting to influence the model performance and overfitting. The 11 hyperparameters we studied include activate function; weight initializer, number of hidden layers, learning rate, momentum, decay, dropout rate, batch size, epochs, L1, and L2. Our results show that most of the single hyperparameters are either negatively or positively corrected with model prediction performance and overfitting. In particular, we found that overfitting overall tends to negatively correlate with learning rate, decay, batch sides, and L2, but tends to positively correlate with momentum, epochs, and L1. According to our results, learning rate, decay, and batch size may have a more significant impact on both overfitting and prediction performance than most of the other hyperparameters, including L1, L2, and dropout rate, which were designed for minimizing overfitting. We also find some interesting interacting pairs of hyperparameters such as learning rate and momentum, learning rate and decay, and batch size and epochs. Keywords: Deep learning, overfitting, prediction, grid search, feedforward neural networks, breast cancer metastasis.",
        "published":1659540972000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02150v1",
        "arxiv_primary_category":"cs.lg",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1366810646,
        "technologyreview":0.2816503181,
        "newscientist":0.1816776759,
        "venturebeat":0.2436807685,
        "wired":0.179740084,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01899v1",
        "predicted_newsworthiness":0.4671425849,
        "title":"Understanding Adversarial Imitation Learning in Small Sample Regime: A Stage-coupled Analysis",
        "summary":"Imitation learning learns a policy from expert trajectories. While the expert data is believed to be crucial for imitation quality, it was found that a kind of imitation learning approach, adversarial imitation learning (AIL), can have exceptional performance. With as little as only one expert trajectory, AIL can match the expert performance even in a long horizon, on tasks such as locomotion control. There are two mysterious points in this phenomenon. First, why can AIL perform well with only a few expert trajectories? Second, why does AIL maintain good performance despite the length of the planning horizon? In this paper, we theoretically explore these two questions. For a total-variation-distance-based AIL (called TV-AIL), our analysis shows a horizon-free imitation gap $\\mathcal O(\\{\\min\\{1, \\sqrt{|\\mathcal S|\/N} \\})$ on a class of instances abstracted from locomotion control tasks. Here $|\\mathcal S|$ is the state space size for a tabular Markov decision process, and $N$ is the number of expert trajectories. We emphasize two important features of our bound. First, this bound is meaningful in both small and large sample regimes. Second, this bound suggests that the imitation gap of TV-AIL is at most 1 regardless of the planning horizon. Therefore, this bound can explain the empirical observation. Technically, we leverage the structure of multi-stage policy optimization in TV-AIL and present a new stage-coupled analysis via dynamic programming",
        "published":1659513813000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01899v1",
        "arxiv_primary_category":"cs.lg",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1122762413,
        "technologyreview":0.2924524383,
        "newscientist":0.1682314439,
        "venturebeat":0.2336545363,
        "wired":0.1967306477,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01841v1",
        "predicted_newsworthiness":0.4637240004,
        "title":"Robust Learning of Deep Time Series Anomaly Detection Models with Contaminated Training Data",
        "summary":"Time series anomaly detection (TSAD) is an important data mining task with numerous applications in the IoT era. In recent years, a large number of deep neural network-based methods have been proposed, demonstrating significantly better performance than conventional methods on addressing challenging TSAD problems in a variety of areas. Nevertheless, these deep TSAD methods typically rely on a clean training dataset that is not polluted by anomalies to learn the \"normal profile\" of the underlying dynamics. This requirement is nontrivial since a clean dataset can hardly be provided in practice. Moreover, without the awareness of their robustness, blindly applying deep TSAD methods with potentially contaminated training data can possibly incur significant performance degradation in the detection phase. In this work, to tackle this important challenge, we firstly investigate the robustness of commonly used deep TSAD methods with contaminated training data which provides a guideline for applying these methods when the provided training data are not guaranteed to be anomaly-free. Furthermore, we propose a model-agnostic method which can effectively improve the robustness of learning mainstream deep TSAD models with potentially contaminated data. Experiment results show that our method can consistently prevent or mitigate performance degradation of mainstream deep TSAD models on widely used benchmark datasets.",
        "published":1659502328000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01841v1",
        "arxiv_primary_category":"cs.lg",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1238175597,
        "technologyreview":0.2736194649,
        "newscientist":0.1766286495,
        "venturebeat":0.2684943184,
        "wired":0.2015923893,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02012v1",
        "predicted_newsworthiness":0.4635462995,
        "title":"Character Generation through Self-Supervised Vectorization",
        "summary":"The prevalent approach in self-supervised image generation is to operate on pixel level representations. While this approach can produce high quality images, it cannot benefit from the simplicity and innate quality of vectorization. Here we present a drawing agent that operates on stroke-level representation of images. At each time step, the agent first assesses the current canvas and decides whether to stop or keep drawing. When a 'draw' decision is made, the agent outputs a program indicating the stroke to be drawn. As a result, it produces a final raster image by drawing the strokes on a canvas, using a minimal number of strokes and dynamically deciding when to stop. We train our agent through reinforcement learning on MNIST and Omniglot datasets for unconditional generation and parsing (reconstruction) tasks. We utilize our parsing agent for exemplar generation and type conditioned concept generation in Omniglot challenge without any further training. We present successful results on all three generation tasks and the parsing task. Crucially, we do not need any stroke-level or vector supervision; we only use raster images for training.",
        "published":1659529915000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02012v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1127831679,
        "technologyreview":0.2662521935,
        "newscientist":0.1676803992,
        "venturebeat":0.2278467823,
        "wired":0.1888237854,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01889v1",
        "predicted_newsworthiness":0.461541219,
        "title":"Multi-Scale User Behavior Network for Entire Space Multi-Task Learning",
        "summary":"Modelling the user's multiple behaviors is an essential part of modern e-commerce, whose widely adopted application is to jointly optimize click-through rate (CTR) and conversion rate (CVR) predictions. Most of existing methods overlook the effect of two key characteristics of the user's behaviors: for each item list, (i) contextual dependence refers to that the user's behaviors on any item are not purely determinated by the item itself but also are influenced by the user's previous behaviors (e.g., clicks, purchases) on other items in the same sequence; (ii) multiple time scales means that users are likely to click frequently but purchase periodically. To this end, we develop a new multi-scale user behavior network named Hierarchical rEcurrent Ranking On the Entire Space (HEROES) which incorporates the contextual information to estimate the user multiple behaviors in a multi-scale fashion. Concretely, we introduce a hierarchical framework, where the lower layer models the user's engagement behaviors while the upper layer estimates the user's satisfaction behaviors. The proposed architecture can automatically learn a suitable time scale for each layer to capture the dynamic user's behavioral patterns. Besides the architecture, we also introduce the Hawkes process to form a novel recurrent unit which can not only encode the items' features in the context but also formulate the excitation or discouragement from the user's previous behaviors. We further show that HEROES can be extended to build unbiased ranking systems through combinations with the survival analysis technique. Extensive experiments over three large-scale industrial datasets demonstrate the superiority of our model compared with the state-of-the-art methods.",
        "published":1659512287000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01889v1",
        "arxiv_primary_category":"cs.ir",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1188172689,
        "technologyreview":0.2338967934,
        "newscientist":0.1483332776,
        "venturebeat":0.2655333078,
        "wired":0.2014041679,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.01997v1",
        "predicted_newsworthiness":0.4601303405,
        "title":"Convolutional Fine-Grained Classification with Self-Supervised Target Relation Regularization",
        "summary":"Fine-grained visual classification can be addressed by deep representation learning under supervision of manually pre-defined targets (e.g., one-hot or the Hadamard codes). Such target coding schemes are less flexible to model inter-class correlation and are sensitive to sparse and imbalanced data distribution as well. In light of this, this paper introduces a novel target coding scheme -- dynamic target relation graphs (DTRG), which, as an auxiliary feature regularization, is a self-generated structural output to be mapped from input images. Specifically, online computation of class-level feature centers is designed to generate cross-category distance in the representation space, which can thus be depicted by a dynamic graph in a non-parametric manner. Explicitly minimizing intra-class feature variations anchored on those class-level centers can encourage learning of discriminative features. Moreover, owing to exploiting inter-class dependency, the proposed target graphs can alleviate data sparsity and imbalanceness in representation learning. Inspired by recent success of the mixup style data augmentation, this paper introduces randomness into soft construction of dynamic target relation graphs to further explore relation diversity of target classes. Experimental results can demonstrate the effectiveness of our method on a number of diverse benchmarks of multiple visual classification tasks, especially achieving the state-of-the-art performance on popular fine-grained object benchmarks and superior robustness against sparse and imbalanced data. Source codes are made publicly available at https:\/\/github.com\/AkonLau\/DTRG.",
        "published":1659527513000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01997v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0924956764,
        "technologyreview":0.2363269645,
        "newscientist":0.136709723,
        "venturebeat":0.2034593918,
        "wired":0.1640191854,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02126v1",
        "predicted_newsworthiness":0.4598014048,
        "title":"Noise tolerance of learning to rank under class-conditional label noise",
        "summary":"Often, the data used to train ranking models is subject to label noise. For example, in web-search, labels created from clickstream data are noisy due to issues such as insufficient information in item descriptions on the SERP, query reformulation by the user, and erratic or unexpected user behavior. In practice, it is difficult to handle label noise without making strong assumptions about the label generation process. As a result, practitioners typically train their learning-to-rank (LtR) models directly on this noisy data without additional consideration of the label noise. Surprisingly, we often see strong performance from LtR models trained in this way. In this work, we describe a class of noise-tolerant LtR losses for which empirical risk minimization is a consistent procedure, even in the context of class-conditional label noise. We also develop noise-tolerant analogs of commonly used loss functions. The practical implications of our theoretical findings are further supported by experimental results.",
        "published":1659539088000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02126v1",
        "arxiv_primary_category":"cs.ir",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.089278288,
        "technologyreview":0.217224735,
        "newscientist":0.128620577,
        "venturebeat":0.2298847613,
        "wired":0.1649457277,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.02011v1",
        "predicted_newsworthiness":0.4592131919,
        "title":"Equivariant Disentangled Transformation for Domain Generalization under Combination Shift",
        "summary":"Machine learning systems may encounter unexpected problems when the data distribution changes in the deployment environment. A major reason is that certain combinations of domains and labels are not observed during training but appear in the test environment. Although various invariance-based algorithms can be applied, we find that the performance gain is often marginal. To formally analyze this issue, we provide a unique algebraic formulation of the combination shift problem based on the concepts of homomorphism, equivariance, and a refined definition of disentanglement. The algebraic requirements naturally derive a simple yet effective method, referred to as equivariant disentangled transformation (EDT), which augments the data based on the algebraic structures of labels and makes the transformation satisfy the equivariance and disentanglement requirements. Experimental results demonstrate that invariance may be insufficient, and it is important to exploit the equivariance structure in the combination shift problem.",
        "published":1659529891000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02011v1",
        "arxiv_primary_category":"cs.lg",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0768428469,
        "technologyreview":0.2203475835,
        "newscientist":0.1342607098,
        "venturebeat":0.1917185061,
        "wired":0.1374645968,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02225v1",
        "predicted_newsworthiness":0.4585022599,
        "title":"Sequence Model Imitation Learning with Unobserved Contexts",
        "summary":"We consider imitation learning problems where the expert has access to a per-episode context that is hidden from the learner, both in the demonstrations and at test-time. While the learner might not be able to accurately reproduce expert behavior early on in an episode, by considering the entire history of states and actions, they might be able to eventually identify the context and act as the expert would. We prove that on-policy imitation learning algorithms (with or without access to a queryable expert) are better equipped to handle these sorts of asymptotically realizable problems than off-policy methods and are able to avoid the latching behavior (naive repetition of past actions) that plagues the latter. We conduct experiments in a toy bandit domain that show that there exist sharp phase transitions of whether off-policy approaches are able to match expert performance asymptotically, in contrast to the uniformly good performance of on-policy approaches. We demonstrate that on several continuous control tasks, on-policy approaches are able to use history to identify the context while off-policy approaches actually perform worse when given access to history.",
        "published":1659547664000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02225v1",
        "arxiv_primary_category":"cs.lg",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0975556479,
        "technologyreview":0.2645389233,
        "newscientist":0.1697349534,
        "venturebeat":0.2218040821,
        "wired":0.1781781072,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02034v1",
        "predicted_newsworthiness":0.4569523193,
        "title":"SSformer: A Lightweight Transformer for Semantic Segmentation",
        "summary":"It is well believed that Transformer performs better in semantic segmentation compared to convolutional neural networks. Nevertheless, the original Vision Transformer may lack of inductive biases of local neighborhoods and possess a high time complexity. Recently, Swin Transformer sets a new record in various vision tasks by using hierarchical architecture and shifted windows while being more efficient. However, as Swin Transformer is specifically designed for image classification, it may achieve suboptimal performance on dense prediction-based segmentation task. Further, simply combing Swin Transformer with existing methods would lead to the boost of model size and parameters for the final segmentation model. In this paper, we rethink the Swin Transformer for semantic segmentation, and design a lightweight yet effective transformer model, called SSformer. In this model, considering the inherent hierarchical design of Swin Transformer, we propose a decoder to aggregate information from different layers, thus obtaining both local and global attentions. Experimental results show the proposed SSformer yields comparable mIoU performance with state-of-the-art models, while maintaining a smaller model size and lower compute.",
        "published":1659531420000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02034v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0789804173,
        "technologyreview":0.2209816892,
        "newscientist":0.1218016465,
        "venturebeat":0.2161283313,
        "wired":0.1592437219,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02063v1",
        "predicted_newsworthiness":0.455277714,
        "title":"Evaluation and comparison of eight popular Lidar and Visual SLAM algorithms",
        "summary":"In this paper, we evaluate eight popular and open-source 3D Lidar and visual SLAM (Simultaneous Localization and Mapping) algorithms, namely LOAM, Lego LOAM, LIO SAM, HDL Graph, ORB SLAM3, Basalt VIO, and SVO2. We have devised experiments both indoor and outdoor to investigate the effect of the following items: i) effect of mounting positions of the sensors, ii) effect of terrain type and vibration, iii) effect of motion (variation in linear and angular speed). We compare their performance in terms of relative and absolute pose error. We also provide comparison on their required computational resources. We thoroughly analyse and discuss the results and identify the best performing system for the environment cases with our multi-camera and multi-Lidar indoor and outdoor datasets. We hope our findings help one to choose a sensor and the corresponding SLAM algorithm combination suiting their needs, based on their target environment.",
        "published":1659533469000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02063v1",
        "arxiv_primary_category":"cs.ro",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0964493343,
        "technologyreview":0.1911235836,
        "newscientist":0.1465210621,
        "venturebeat":0.1890207288,
        "wired":0.1738099472,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01871v1",
        "predicted_newsworthiness":0.4524228119,
        "title":"A Deep Learning Approach to Detect Lean Blowout in Combustion Systems",
        "summary":"Lean combustion is environment friendly with low NOx emissions and also provides better fuel efficiency in a combustion system. However, approaching towards lean combustion can make engines more susceptible to lean blowout. Lean blowout (LBO) is an undesirable phenomenon that can cause sudden flame extinction leading to sudden loss of power. During the design stage, it is quite challenging for the scientists to accurately determine the optimal operating limits to avoid sudden LBO occurrence. Therefore, it is crucial to develop accurate and computationally tractable frameworks for online LBO detection in low NOx emission engines. To the best of our knowledge, for the first time, we propose a deep learning approach to detect lean blowout in combustion systems. In this work, we utilize a laboratory-scale combustor to collect data for different protocols. We start far from LBO for each protocol and gradually move towards the LBO regime, capturing a quasi-static time series dataset at each condition. Using one of the protocols in our dataset as the reference protocol and with conditions annotated by domain experts, we find a transition state metric for our trained deep learning model to detect LBO in the other test protocols. We find that our proposed approach is more accurate and computationally faster than other baseline models to detect the transitions to LBO. Therefore, we recommend this method for real-time performance monitoring in lean combustion engines.",
        "published":1659509310000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01871v1",
        "arxiv_primary_category":"cs.lg",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1196668316,
        "technologyreview":0.24016093,
        "newscientist":0.1695969477,
        "venturebeat":0.2345059822,
        "wired":0.1947280896,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02210v1",
        "predicted_newsworthiness":0.4510874532,
        "title":"Free-HeadGAN: Neural Talking Head Synthesis with Explicit Gaze Control",
        "summary":"We present Free-HeadGAN, a person-generic neural talking head synthesis system. We show that modeling faces with sparse 3D facial landmarks are sufficient for achieving state-of-the-art generative performance, without relying on strong statistical priors of the face, such as 3D Morphable Models. Apart from 3D pose and facial expressions, our method is capable of fully transferring the eye gaze, from a driving actor to a source identity. Our complete pipeline consists of three components: a canonical 3D key-point estimator that regresses 3D pose and expression-related deformations, a gaze estimation network and a generator that is built upon the architecture of HeadGAN. We further experiment with an extension of our generator to accommodate few-shot learning using an attention mechanism, in case more than one source images are available. Compared to the latest models for reenactment and motion transfer, our system achieves higher photo-realism combined with superior identity preservation, while offering explicit gaze control.",
        "published":1659545168000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02210v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0940956958,
        "technologyreview":0.2246716954,
        "newscientist":0.1392840751,
        "venturebeat":0.2324880499,
        "wired":0.1977174062,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02192v1",
        "predicted_newsworthiness":0.4465129358,
        "title":"RealPatch: A Statistical Matching Framework for Model Patching with Real Samples",
        "summary":"Machine learning classifiers are typically trained to minimise the average error across a dataset. Unfortunately, in practice, this process often exploits spurious correlations caused by subgroup imbalance within the training data, resulting in high average performance but highly variable performance across subgroups. Recent work to address this problem proposes model patching with CAMEL. This previous approach uses generative adversarial networks to perform intra-class inter-subgroup data augmentations, requiring (a) the training of a number of computationally expensive models and (b) sufficient quality of model's synthetic outputs for the given domain. In this work, we propose RealPatch, a framework for simpler, faster, and more data-efficient data augmentation based on statistical matching. Our framework performs model patching by augmenting a dataset with real samples, mitigating the need to train generative models for the target task. We demonstrate the effectiveness of RealPatch on three benchmark datasets, CelebA, Waterbirds and a subset of iWildCam, showing improvements in worst-case subgroup performance and in subgroup performance gap in binary classification. Furthermore, we conduct experiments with the imSitu dataset with 211 classes, a setting where generative model-based patching such as CAMEL is impractical. We show that RealPatch can successfully eliminate dataset leakage while reducing model leakage and maintaining high utility. The code for RealPatch can be found at https:\/\/github.com\/wearepal\/RealPatch.",
        "published":1659543750000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02192v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1309011555,
        "technologyreview":0.2979322507,
        "newscientist":0.1786494576,
        "venturebeat":0.2696549335,
        "wired":0.1915826979,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02029v1",
        "predicted_newsworthiness":0.4327611477,
        "title":"Supervised and Reinforcement Learning from Observations in Reconnaissance Blind Chess",
        "summary":"In this work, we adapt a training approach inspired by the original AlphaGo system to play the imperfect information game of Reconnaissance Blind Chess. Using only the observations instead of a full description of the game state, we first train a supervised agent on publicly available game records. Next, we increase the performance of the agent through self-play with the on-policy reinforcement learning algorithm Proximal Policy Optimization. We do not use any search to avoid problems caused by the partial observability of game states and only use the policy network to generate moves when playing. With this approach, we achieve an ELO of 1330 on the RBC leaderboard, which places our agent at position 27 at the time of this writing. We see that self-play significantly improves performance and that the agent plays acceptably well without search and without making assumptions about the true game state.",
        "published":1659531019000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02029v1",
        "arxiv_primary_category":"cs.ai",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1157130292,
        "technologyreview":0.2719974105,
        "newscientist":0.1638650606,
        "venturebeat":0.2433651112,
        "wired":0.1914815708,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2208.02169v1",
        "predicted_newsworthiness":0.4306996305,
        "title":"SpanDrop: Simple and Effective Counterfactual Learning for Long Sequences",
        "summary":"Distilling supervision signal from a long sequence to make predictions is a challenging task in machine learning, especially when not all elements in the input sequence contribute equally to the desired output. In this paper, we propose SpanDrop, a simple and effective data augmentation technique that helps models identify the true supervision signal in a long sequence with very few examples. By directly manipulating the input sequence, SpanDrop randomly ablates parts of the sequence at a time and ask the model to perform the same task to emulate counterfactual learning and achieve input attribution. Based on theoretical analysis of its properties, we also propose a variant of SpanDrop based on the beta-Bernoulli distribution, which yields diverse augmented sequences while providing a learning objective that is more consistent with the original dataset. We demonstrate the effectiveness of SpanDrop on a set of carefully designed toy tasks, as well as various natural language processing tasks that require reasoning over long sequences to arrive at the correct answer, and show that it helps models improve performance both when data is scarce and abundant.",
        "published":1659542317000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02169v1",
        "arxiv_primary_category":"cs.lg",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1290291148,
        "technologyreview":0.2968918275,
        "newscientist":0.1941656132,
        "venturebeat":0.2792197595,
        "wired":0.200232432,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02128v1",
        "predicted_newsworthiness":0.4264723584,
        "title":"Distributed On-Demand Routing for LEO Mega-Constellations: A Starlink Case Study",
        "summary":"The design and launch of large-scale satellite networks create an imminent demand for efficient and delay-minimising routing methods. With the rising number of satellites in such constellations, pre-computing all shortest routes between all satellites and for all times becomes more and more infeasible due to space and time limitations. Even though distributed on-demand routing methods were developed for specific LEO satellite network configurations, they are not suited for increasingly popular mega-constellations based on Walker Delta formations. The contributions of this paper are twofold. First, we introduce a formal model that mathematically captures the time-evolving locations of satellites in a Walker Delta constellation and use it to establish a formula to compute the minimum number of ISL hops between two given satellites. In the second part, we present an on-demand hop-count-based routing algorithm that approximates the optimal path while achieving superior performance compared to classical shortest-path algorithms like Dijkstra.",
        "published":1659539237000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02128v1",
        "arxiv_primary_category":"cs.ni",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0710032942,
        "technologyreview":0.1485922733,
        "newscientist":0.1246251256,
        "venturebeat":0.145626174,
        "wired":0.1354213292,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2208.01912v1",
        "predicted_newsworthiness":0.4248984754,
        "title":"Cross-Lingual Knowledge Transfer for Clinical Phenotyping",
        "summary":"Clinical phenotyping enables the automatic extraction of clinical conditions from patient records, which can be beneficial to doctors and clinics worldwide. However, current state-of-the-art models are mostly applicable to clinical notes written in English. We therefore investigate cross-lingual knowledge transfer strategies to execute this task for clinics that do not use the English language and have a small amount of in-domain data available. We evaluate these strategies for a Greek and a Spanish clinic leveraging clinical notes from different clinical domains such as cardiology, oncology and the ICU. Our results reveal two strategies that outperform the state-of-the-art: Translation-based methods in combination with domain-specific encoders and cross-lingual encoders plus adapters. We find that these strategies perform especially well for classifying rare phenotypes and we advise on which method to prefer in which situation. Our results show that using multilingual data overall improves clinical phenotyping models and can compensate for data sparseness.",
        "published":1659515601000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01912v1",
        "arxiv_primary_category":"cs.cl",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1563565803,
        "technologyreview":0.2274646887,
        "newscientist":0.1782544736,
        "venturebeat":0.2161839336,
        "wired":0.1486517506,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.01949v1",
        "predicted_newsworthiness":0.4239506533,
        "title":"Negative Frames Matter in Egocentric Visual Query 2D Localization",
        "summary":"The recently released Ego4D dataset and benchmark significantly scales and diversifies the first-person visual perception data. In Ego4D, the Visual Queries 2D Localization task aims to retrieve objects appeared in the past from the recording in the first-person view. This task requires a system to spatially and temporally localize the most recent appearance of a given object query, where query is registered by a single tight visual crop of the object in a different scene. Our study is based on the three-stage baseline introduced in the Episodic Memory benchmark. The baseline solves the problem by detection and tracking: detect the similar objects in all the frames, then run a tracker from the most confident detection result. In the VQ2D challenge, we identified two limitations of the current baseline. (1) The training configuration has redundant computation. Although the training set has millions of instances, most of them are repetitive and the number of unique object is only around 14.6k. The repeated gradient computation of the same object lead to an inefficient training; (2) The false positive rate is high on background frames. This is due to the distribution gap between training and evaluation. During training, the model is only able to see the clean, stable, and labeled frames, but the egocentric videos also have noisy, blurry, or unlabeled background frames. To this end, we developed a more efficient and effective solution. Concretely, we bring the training loop from ~15 days to less than 24 hours, and we achieve 0.17% spatial-temporal AP, which is 31% higher than the baseline. Our solution got the first ranking on the public leaderboard. Our code is publicly available at https:\/\/github.com\/facebookresearch\/vq2d_cvpr.",
        "published":1659520491000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01949v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0927149059,
        "technologyreview":0.216460453,
        "newscientist":0.1567937831,
        "venturebeat":0.2139503656,
        "wired":0.192729399,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01944v1",
        "predicted_newsworthiness":0.4216125166,
        "title":"PalQuant: Accelerating High-precision Networks on Low-precision Accelerators",
        "summary":"Recently low-precision deep learning accelerators (DLAs) have become popular due to their advantages in chip area and energy consumption, yet the low-precision quantized models on these DLAs bring in severe accuracy degradation. One way to achieve both high accuracy and efficient inference is to deploy high-precision neural networks on low-precision DLAs, which is rarely studied. In this paper, we propose the PArallel Low-precision Quantization (PalQuant) method that approximates high-precision computations via learning parallel low-precision representations from scratch. In addition, we present a novel cyclic shuffle module to boost the cross-group information communication between parallel low-precision groups. Extensive experiments demonstrate that PalQuant has superior performance to state-of-the-art quantization methods in both accuracy and inference speed, e.g., for ResNet-18 network quantization, PalQuant can obtain 0.52\\% higher accuracy and 1.78$\\times$ speedup simultaneously over their 4-bit counter-part on a state-of-the-art 2-bit accelerator. Code is available at \\url{https:\/\/github.com\/huqinghao\/PalQuant}.",
        "published":1659519853000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01944v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0758200757,
        "technologyreview":0.2156152435,
        "newscientist":0.1253321312,
        "venturebeat":0.2046499,
        "wired":0.1466256496,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01840v1",
        "predicted_newsworthiness":0.4214480437,
        "title":"'Labelling the Gaps': A Weakly Supervised Automatic Eye Gaze Estimation",
        "summary":"Over the past few years, there has been an increasing interest to interpret gaze direction in an unconstrained environment with limited supervision. Owing to data curation and annotation issues, replicating gaze estimation method to other platforms, such as unconstrained outdoor or AR\/VR, might lead to significant drop in performance due to insufficient availability of accurately annotated data for model training. In this paper, we explore an interesting yet challenging problem of gaze estimation method with a limited amount of labelled data. The proposed method distills knowledge from the labelled subset with visual features; including identity-specific appearance, gaze trajectory consistency and motion features. Given a gaze trajectory, the method utilizes label information of only the start and the end frames of a gaze sequence. An extension of the proposed method further reduces the requirement of labelled frames to only the start frame with a minor drop in the generated label's quality. We evaluate the proposed method on four benchmark datasets (CAVE, TabletGaze, MPII and Gaze360) as well as web-crawled YouTube videos. Our proposed method reduces the annotation effort to as low as 2.67%, with minimal impact on performance; indicating the potential of our model enabling gaze estimation 'in-the-wild' setup.",
        "published":1659502316000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01840v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1031583851,
        "technologyreview":0.2095866278,
        "newscientist":0.1448624376,
        "venturebeat":0.2124464921,
        "wired":0.18373431,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01954v1",
        "predicted_newsworthiness":0.4206258664,
        "title":"Dilated Context Integrated Network with Cross-Modal Consensus for Temporal Emotion Localization in Videos",
        "summary":"Understanding human emotions is a crucial ability for intelligent robots to provide better human-robot interactions. The existing works are limited to trimmed video-level emotion classification, failing to locate the temporal window corresponding to the emotion. In this paper, we introduce a new task, named Temporal Emotion Localization in videos~(TEL), which aims to detect human emotions and localize their corresponding temporal boundaries in untrimmed videos with aligned subtitles. TEL presents three unique challenges compared to temporal action localization: 1) The emotions have extremely varied temporal dynamics; 2) The emotion cues are embedded in both appearances and complex plots; 3) The fine-grained temporal annotations are complicated and labor-intensive. To address the first two challenges, we propose a novel dilated context integrated network with a coarse-fine two-stream architecture. The coarse stream captures varied temporal dynamics by modeling multi-granularity temporal contexts. The fine stream achieves complex plots understanding by reasoning the dependency between the multi-granularity temporal contexts from the coarse stream and adaptively integrates them into fine-grained video segment features. To address the third challenge, we introduce a cross-modal consensus learning paradigm, which leverages the inherent semantic consensus between the aligned video and subtitle to achieve weakly-supervised learning. We contribute a new testing set with 3,000 manually-annotated temporal boundaries so that future research on the TEL problem can be quantitatively evaluated. Extensive experiments show the effectiveness of our approach on temporal emotion localization. The repository of this work is at https:\/\/github.com\/YYJMJC\/Temporal-Emotion-Localization-in-Videos.",
        "published":1659520849000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01954v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1233947351,
        "technologyreview":0.2260409407,
        "newscientist":0.1464950711,
        "venturebeat":0.2136041888,
        "wired":0.1795874506,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01898v1",
        "predicted_newsworthiness":0.4185759583,
        "title":"XCon: Learning with Experts for Fine-grained Category Discovery",
        "summary":"We address the problem of generalized category discovery (GCD) in this paper, i.e. clustering the unlabeled images leveraging the information from a set of seen classes, where the unlabeled images could contain both seen classes and unseen classes. The seen classes can be seen as an implicit criterion of classes, which makes this setting different from unsupervised clustering where the cluster criteria may be ambiguous. We mainly concern the problem of discovering categories within a fine-grained dataset since it is one of the most direct applications of category discovery, i.e. helping experts discover novel concepts within an unlabeled dataset using the implicit criterion set forth by the seen classes. State-of-the-art methods for generalized category discovery leverage contrastive learning to learn the representations, but the large inter-class similarity and intra-class variance pose a challenge for the methods because the negative examples may contain irrelevant cues for recognizing a category so the algorithms may converge to a local-minima. We present a novel method called Expert-Contrastive Learning (XCon) to help the model to mine useful information from the images by first partitioning the dataset into sub-datasets using k-means clustering and then performing contrastive learning on each of the sub-datasets to learn fine-grained discriminative features. Experiments on fine-grained datasets show a clear improved performance over the previous best methods, indicating the effectiveness of our method.",
        "published":1659513792000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01898v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0883654631,
        "technologyreview":0.2249179608,
        "newscientist":0.1462417731,
        "venturebeat":0.2029494245,
        "wired":0.1481011055,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01913v1",
        "predicted_newsworthiness":0.4179530195,
        "title":"EgPDE-Net: Building Continuous Neural Networks for Time Series Prediction with Exogenous Variables",
        "summary":"While exogenous variables have a major impact on performance improvement in time series analysis, inter-series correlation and time dependence among them are rarely considered in the present continuous methods. The dynamical systems of multivariate time series could be modelled with complex unknown partial differential equations (PDEs) which play a prominent role in many disciplines of science and engineering. In this paper, we propose a continuous-time model for arbitrary-step prediction to learn an unknown PDE system in multivariate time series whose governing equations are parameterised by self-attention and gated recurrent neural networks. The proposed model, \\underline{E}xogenous-\\underline{g}uided \\underline{P}artial \\underline{D}ifferential \\underline{E}quation Network (EgPDE-Net), takes account of the relationships among the exogenous variables and their effects on the target series. Importantly, the model can be reduced into a regularised ordinary differential equation (ODE) problem with special designed regularisation guidance, which makes the PDE problem tractable to obtain numerical solutions and feasible to predict multiple future values of the target series at arbitrary time points. Extensive experiments demonstrate that our proposed model could achieve competitive accuracy over strong baselines: on average, it outperforms the best baseline by reducing $9.85\\%$ on RMSE and $13.98\\%$ on MAE for arbitrary-step prediction.",
        "published":1659515671000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01913v1",
        "arxiv_primary_category":"cs.lg",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.136130996,
        "technologyreview":0.2181575842,
        "newscientist":0.1560266705,
        "venturebeat":0.1988734697,
        "wired":0.1471353409,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01925v1",
        "predicted_newsworthiness":0.4166846683,
        "title":"SuperLine3D: Self-supervised Line Segmentation and Description for LiDAR Point Cloud",
        "summary":"Poles and building edges are frequently observable objects on urban roads, conveying reliable hints for various computer vision tasks. To repetitively extract them as features and perform association between discrete LiDAR frames for registration, we propose the first learning-based feature segmentation and description model for 3D lines in LiDAR point cloud. To train our model without the time consuming and tedious data labeling process, we first generate synthetic primitives for the basic appearance of target lines, and build an iterative line auto-labeling process to gradually refine line labels on real LiDAR scans. Our segmentation model can extract lines under arbitrary scale perturbations, and we use shared EdgeConv encoder layers to train the two segmentation and descriptor heads jointly. Base on the model, we can build a highly-available global registration module for point cloud registration, in conditions without initial transformation hints. Experiments have demonstrated that our line-based registration method is highly competitive to state-of-the-art point-based approaches. Our code is available at https:\/\/github.com\/zxrzju\/SuperLine3D.git.",
        "published":1659517574000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01925v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.098781424,
        "technologyreview":0.2006184156,
        "newscientist":0.1436493729,
        "venturebeat":0.195562683,
        "wired":0.1749955071,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01849v1",
        "predicted_newsworthiness":0.41152592,
        "title":"Coarse-to-Fine Knowledge-Enhanced Multi-Interest Learning Framework for Multi-Behavior Recommendation",
        "summary":"Multi-types of behaviors (e.g., clicking, adding to cart, purchasing, etc.) widely exist in most real-world recommendation scenarios, which are beneficial to learn users' multi-faceted preferences. As dependencies are explicitly exhibited by the multiple types of behaviors, effectively modeling complex behavior dependencies is crucial for multi-behavior prediction. The state-of-the-art multi-behavior models learn behavior dependencies indistinguishably with all historical interactions as input. However, different behaviors may reflect different aspects of user preference, which means that some irrelevant interactions may play as noises to the target behavior to be predicted. To address the aforementioned limitations, we introduce multi-interest learning to the multi-behavior recommendation. More specifically, we propose a novel Coarse-to-fine Knowledge-enhanced Multi-interest Learning (CKML) framework to learn shared and behavior-specific interests for different behaviors. CKML introduces two advanced modules, namely Coarse-grained Interest Extracting (CIE) and Fine-grained Behavioral Correlation (FBC), which work jointly to capture fine-grained behavioral dependencies. CIE uses knowledge-aware information to extract initial representations of each interest. FBC incorporates a dynamic routing scheme to further assign each behavior among interests. Additionally, we use the self-attention mechanism to correlate different behavioral information at the interest level. Empirical results on three real-world datasets verify the effectiveness and efficiency of our model in exploiting multi-behavior data. Further experiments demonstrate the effectiveness of each module and the robustness and superiority of the shared and specific modelling paradigm for multi-behavior data.",
        "published":1659504494000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01849v1",
        "arxiv_primary_category":"cs.ir",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1005348288,
        "technologyreview":0.2023189757,
        "newscientist":0.1257798148,
        "venturebeat":0.2256115402,
        "wired":0.1791951589,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.02070v1",
        "predicted_newsworthiness":0.4108409249,
        "title":"Efficient Fine-Tuning of Compressed Language Models with Learners",
        "summary":"Fine-tuning BERT-based models is resource-intensive in memory, computation, and time. While many prior works aim to improve inference efficiency via compression techniques, e.g., pruning, these works do not explicitly address the computational challenges of training to downstream tasks. We introduce Learner modules and priming, novel methods for fine-tuning that exploit the overparameterization of pre-trained language models to gain benefits in convergence speed and resource utilization. Learner modules navigate the double bind of 1) training efficiently by fine-tuning a subset of parameters, and 2) training effectively by ensuring quick convergence and high metric scores. Our results on DistilBERT demonstrate that learners perform on par with or surpass the baselines. Learners train 7x fewer parameters than state-of-the-art methods on GLUE. On CoLA, learners fine-tune 20% faster, and have significantly lower resource utilization.",
        "published":1659534150000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02070v1",
        "arxiv_primary_category":"cs.cl",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0792520185,
        "technologyreview":0.2088676946,
        "newscientist":0.1010379614,
        "venturebeat":0.2148511047,
        "wired":0.153755832,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.02140v1",
        "predicted_newsworthiness":0.4081847431,
        "title":"KPI-BERT: A Joint Named Entity Recognition and Relation Extraction Model for Financial Reports",
        "summary":"We present KPI-BERT, a system which employs novel methods of named entity recognition (NER) and relation extraction (RE) to extract and link key performance indicators (KPIs), e.g. \"revenue\" or \"interest expenses\", of companies from real-world German financial documents. Specifically, we introduce an end-to-end trainable architecture that is based on Bidirectional Encoder Representations from Transformers (BERT) combining a recurrent neural network (RNN) with conditional label masking to sequentially tag entities before it classifies their relations. Our model also introduces a learnable RNN-based pooling mechanism and incorporates domain expert knowledge by explicitly filtering impossible relations. We achieve a substantially higher prediction performance on a new practical dataset of German financial reports, outperforming several strong baselines including a competing state-of-the-art span-based entity tagging approach.",
        "published":1659540088000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02140v1",
        "arxiv_primary_category":"cs.cl",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1267715018,
        "technologyreview":0.2240183877,
        "newscientist":0.1234308806,
        "venturebeat":0.2604038278,
        "wired":0.1728975045,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.02178v1",
        "predicted_newsworthiness":0.4015270777,
        "title":"KD-SCFNet: Towards More Accurate and Efficient Salient Object Detection via Knowledge Distillation",
        "summary":"Most existing salient object detection (SOD) models are difficult to apply due to the complex and huge model structures. Although some lightweight models are proposed, the accuracy is barely satisfactory. In this paper, we design a novel semantics-guided contextual fusion network (SCFNet) that focuses on the interactive fusion of multi-level features for accurate and efficient salient object detection. Furthermore, we apply knowledge distillation to SOD task and provide a sizeable dataset KD-SOD80K. In detail, we transfer the rich knowledge from a seasoned teacher to the untrained SCFNet through unlabeled images, enabling SCFNet to learn a strong generalization ability to detect salient objects more accurately. The knowledge distillation based SCFNet (KDSCFNet) achieves comparable accuracy to the state-of-the-art heavyweight methods with less than 1M parameters and 174 FPS real-time detection speed. Extensive experiments demonstrate the robustness and effectiveness of the proposed distillation method and SOD framework. Code and data: https:\/\/github.com\/zhangjinCV\/KD-SCFNet.",
        "published":1659542591000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02178v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0905268512,
        "technologyreview":0.2200995211,
        "newscientist":0.1316846745,
        "venturebeat":0.2086478615,
        "wired":0.1723103863,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01960v1",
        "predicted_newsworthiness":0.3970552159,
        "title":"Learning Object Manipulation Skills from Video via Approximate Differentiable Physics",
        "summary":"We aim to teach robots to perform simple object manipulation tasks by watching a single video demonstration. Towards this goal, we propose an optimization approach that outputs a coarse and temporally evolving 3D scene to mimic the action demonstrated in the input video. Similar to previous work, a differentiable renderer ensures perceptual fidelity between the 3D scene and the 2D video. Our key novelty lies in the inclusion of a differentiable approach to solve a set of Ordinary Differential Equations (ODEs) that allows us to approximately model laws of physics such as gravity, friction, and hand-object or object-object interactions. This not only enables us to dramatically improve the quality of estimated hand and object states, but also produces physically admissible trajectories that can be directly translated to a robot without the need for costly reinforcement learning. We evaluate our approach on a 3D reconstruction task that consists of 54 video demonstrations sourced from 9 actions such as pull something from right to left or put something in front of something. Our approach improves over previous state-of-the-art by almost 30%, demonstrating superior quality on especially challenging actions involving physical interactions of two objects such as put something onto something. Finally, we showcase the learned skills on a Franka Emika Panda robot.",
        "published":1659522107000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01960v1",
        "arxiv_primary_category":"cs.ro",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.10104575,
        "technologyreview":0.2640400771,
        "newscientist":0.18039221,
        "venturebeat":0.2107544415,
        "wired":0.2060140402,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.02131v1",
        "predicted_newsworthiness":0.3967408686,
        "title":"Masked Vision and Language Modeling for Multi-modal Representation Learning",
        "summary":"In this paper, we study how to use masked signal modeling in vision and language (V+L) representation learning. Instead of developing masked language modeling (MLM) and masked image modeling (MIM) independently, we propose to build joint masked vision and language modeling, where the masked signal of one modality is reconstructed with the help from another modality. This is motivated by the nature of image-text paired data that both of the image and the text convey almost the same information but in different formats. The masked signal reconstruction of one modality conditioned on another modality can also implicitly learn cross-modal alignment between language tokens and image patches. Our experiments on various V+L tasks show that the proposed method not only achieves state-of-the-art performances by using a large amount of data, but also outperforms the other competitors by a significant margin in the regimes of limited training data.",
        "published":1659539461000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02131v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0934793156,
        "technologyreview":0.2126205898,
        "newscientist":0.1161827496,
        "venturebeat":0.1876771744,
        "wired":0.1517964408,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02148v1",
        "predicted_newsworthiness":0.3944647542,
        "title":"GPPF: A General Perception Pre-training Framework via Sparsely Activated Multi-Task Learning",
        "summary":"Pre-training over mixtured multi-task, multi-domain, and multi-modal data remains an open challenge in vision perception pre-training. In this paper, we propose GPPF, a General Perception Pre-training Framework, that pre-trains a task-level dynamic network, which is composed by knowledge \"legos\" in each layers, on labeled multi-task and multi-domain datasets. By inspecting humans' innate ability to learn in complex environment, we recognize and transfer three critical elements to deep networks: (1) simultaneous exposure to diverse cross-task and cross-domain information in each batch. (2) partitioned knowledge storage in separate lego units driven by knowledge sharing. (3) sparse activation of a subset of lego units for both pre-training and downstream tasks. Noteworthy, the joint training of disparate vision tasks is non-trivial due to their differences in input shapes, loss functions, output formats, data distributions, etc. Therefore, we innovatively develop a plug-and-play multi-task training algorithm, which supports Single Iteration Multiple Tasks (SIMT) concurrently training. SIMT lays the foundation of pre-training with large-scale multi-task multi-domain datasets and is proved essential for stable training in our GPPF experiments. Excitingly, the exhaustive experiments show that, our GPPF-R50 model achieves significant improvements of 2.5-5.8 over a strong baseline of the 8 pre-training tasks in GPPF-15M and harvests a range of SOTAs over the 22 downstream tasks with similar computation budgets. We also validate the generalization ability of GPPF to SOTA vision transformers with consistent improvements. These solid experimental results fully prove the effective knowledge learning, storing, sharing, and transfer provided by our novel GPPF framework.",
        "published":1659540875000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02148v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0826381618,
        "technologyreview":0.2544517837,
        "newscientist":0.144782502,
        "venturebeat":0.2217431646,
        "wired":0.1630958557,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01838v1",
        "predicted_newsworthiness":0.394072166,
        "title":"Re-Attention Transformer for Weakly Supervised Object Localization",
        "summary":"Weakly supervised object localization is a challenging task which aims to localize objects with coarse annotations such as image categories. Existing deep network approaches are mainly based on class activation map, which focuses on highlighting discriminative local region while ignoring the full object. In addition, the emerging transformer-based techniques constantly put a lot of emphasis on the backdrop that impedes the ability to identify complete objects. To address these issues, we present a re-attention mechanism termed token refinement transformer (TRT) that captures the object-level semantics to guide the localization well. Specifically, TRT introduces a novel module named token priority scoring module (TPSM) to suppress the effects of background noise while focusing on the target object. Then, we incorporate the class activation map as the semantically aware input to restrain the attention map to the target object. Extensive experiments on two benchmarks showcase the superiority of our proposed method against existing methods with image category annotations. Source code is available in \\url{https:\/\/github.com\/su-hui-zz\/ReAttentionTransformer}.",
        "published":1659501268000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01838v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0785223012,
        "technologyreview":0.1935879876,
        "newscientist":0.1279217506,
        "venturebeat":0.1727274488,
        "wired":0.1385411272,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02080v1",
        "predicted_newsworthiness":0.3930670134,
        "title":"A Feature-space Multimodal Data Augmentation Technique for Text-video Retrieval",
        "summary":"Every hour, huge amounts of visual contents are posted on social media and user-generated content platforms. To find relevant videos by means of a natural language query, text-video retrieval methods have received increased attention over the past few years. Data augmentation techniques were introduced to increase the performance on unseen test examples by creating new training samples with the application of semantics-preserving techniques, such as color space or geometric transformations on images. Yet, these techniques are usually applied on raw data, leading to more resource-demanding solutions and also requiring the shareability of the raw data, which may not always be true, e.g. copyright issues with clips from movies or TV series. To address this shortcoming, we propose a multimodal data augmentation technique which works in the feature space and creates new videos and captions by mixing semantically similar samples. We experiment our solution on a large scale public dataset, EPIC-Kitchens-100, and achieve considerable improvements over a baseline method, improved state-of-the-art performance, while at the same time performing multiple ablation studies. We release code and pretrained models on Github at https:\/\/github.com\/aranciokov\/FSMMDA_VideoRetrieval.",
        "published":1659535520000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02080v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1427228149,
        "technologyreview":0.2546599831,
        "newscientist":0.1559325538,
        "venturebeat":0.2439153648,
        "wired":0.2351630819,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02129v1",
        "predicted_newsworthiness":0.3916521333,
        "title":"SC6D: Symmetry-agnostic and Correspondence-free 6D Object Pose Estimation",
        "summary":"This paper presents an efficient symmetry-agnostic and correspondence-free framework, referred to as SC6D, for 6D object pose estimation from a single monocular RGB image. SC6D requires neither the 3D CAD model of the object nor any prior knowledge of the symmetries. The pose estimation is decomposed into three sub-tasks: a) object 3D rotation representation learning and matching; b) estimation of the 2D location of the object center; and c) scale-invariant distance estimation (the translation along the z-axis) via classification. SC6D is evaluated on three benchmark datasets, T-LESS, YCB-V, and ITODD, and results in state-of-the-art performance on the T-LESS dataset. Moreover, SC6D is computationally much more efficient than the previous state-of-the-art method SurfEmb. The implementation and pre-trained models are publicly available at https:\/\/github.com\/dingdingcai\/SC6D-pose.",
        "published":1659539307000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02129v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0630250269,
        "technologyreview":0.1640670126,
        "newscientist":0.1046962026,
        "venturebeat":0.1762283419,
        "wired":0.140395235,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01874v1",
        "predicted_newsworthiness":0.3890062243,
        "title":"Exploring Generative Neural Temporal Point Process",
        "summary":"Temporal point process (TPP) is commonly used to model the asynchronous event sequence featuring occurrence timestamps and revealed by probabilistic models conditioned on historical impacts. While lots of previous works have focused on `goodness-of-fit' of TPP models by maximizing the likelihood, their predictive performance is unsatisfactory, which means the timestamps generated by models are far apart from true observations. Recently, deep generative models such as denoising diffusion and score matching models have achieved great progress in image generating tasks by demonstrating their capability of generating samples of high quality. However, there are no complete and unified works exploring and studying the potential of generative models in the context of event occurence modeling for TPP. In this work, we try to fill the gap by designing a unified \\textbf{g}enerative framework for \\textbf{n}eural \\textbf{t}emporal \\textbf{p}oint \\textbf{p}rocess (\\textsc{GNTPP}) model to explore their feasibility and effectiveness, and further improve models' predictive performance. Besides, in terms of measuring the historical impacts, we revise the attentive models which summarize influence from historical events with an adaptive reweighting term considering events' type relation and time intervals. Extensive experiments have been conducted to illustrate the improved predictive capability of \\textsc{GNTPP} with a line of generative probabilistic decoders, and performance gain from the revised attention. To the best of our knowledge, this is the first work that adapts generative models in a complete unified framework and studies their effectiveness in the context of TPP. Our codebase including all the methods given in Section.5.1.1 is open in \\url{https:\/\/github.com\/BIRD-TAO\/GNTPP}. We hope the code framework can facilitate future research in Neural TPPs.",
        "published":1659509788000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01874v1",
        "arxiv_primary_category":"cs.lg",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1097165144,
        "technologyreview":0.2080530382,
        "newscientist":0.1526839862,
        "venturebeat":0.1949016245,
        "wired":0.1757508835,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02108v1",
        "predicted_newsworthiness":0.387600893,
        "title":"MTGFlow: Unsupervised Multivariate Time Series Anomaly Detection via Dynamic Graph and Entity-aware Normalizing Flow",
        "summary":"Multivariate time series anomaly detection has been extensively studied under the semi-supervised setting, where a training dataset with all normal instances is required. However, preparing such a dataset is very laborious since each single data instance should be fully guaranteed to be normal. It is, therefore, desired to explore multivariate time series anomaly detection methods based on the dataset without any label knowledge. In this paper, we propose MTGFlow, an unsupervised anomaly detection approach for Multivariate Time series anomaly detection via dynamic Graph and entity-aware normalizing Flow, leaning only on a widely accepted hypothesis that abnormal instances exhibit sparse densities than the normal. However, the complex interdependencies among entities and the diverse inherent characteristics of each entity pose significant challenges on the density estimation, let alone to detect anomalies based on the estimated possibility distribution. To tackle these problems, we propose to learn the mutual and dynamic relations among entities via a graph structure learning model, which helps to model accurate distribution of multivariate time series. Moreover, taking account of distinct characteristics of the individual entities, an entity-aware normalizing flow is developed to describe each entity into a parameterized normal distribution, thereby producing fine-grained density estimation. Incorporating these two strategies, MTGFlowachieves superior anomaly detection performance. Experiments on the real-world datasets are conducted, demonstrating that MTGFlow outperforms the state-of-the-art (SOTA) by 5.0% and 1.6% AUROC for SWaT and WADI datasets respectively. Also, through the anomaly scores contributed by individual entities, MTGFlow can provide explanation information for the detection results.",
        "published":1659537499000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02108v1",
        "arxiv_primary_category":"cs.lg",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1171995432,
        "technologyreview":0.2081518495,
        "newscientist":0.1450674457,
        "venturebeat":0.2211659323,
        "wired":0.172729505,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01875v1",
        "predicted_newsworthiness":0.3867962738,
        "title":"Introducing BEREL: BERT Embeddings for Rabbinic-Encoded Language",
        "summary":"We present a new pre-trained language model (PLM) for Rabbinic Hebrew, termed Berel (BERT Embeddings for Rabbinic-Encoded Language). Whilst other PLMs exist for processing Hebrew texts (e.g., HeBERT, AlephBert), they are all trained on modern Hebrew texts, which diverges substantially from Rabbinic Hebrew in terms of its lexicographical, morphological, syntactic and orthographic norms. We demonstrate the superiority of Berel on Rabbinic texts via a challenge set of Hebrew homographs. We release the new model and homograph challenge set for unrestricted use.",
        "published":1659509944000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01875v1",
        "arxiv_primary_category":"cs.cl",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0951381764,
        "technologyreview":0.1464265774,
        "newscientist":0.0942586542,
        "venturebeat":0.154953771,
        "wired":0.1321668549,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.01909v1",
        "predicted_newsworthiness":0.3864222461,
        "title":"Rethinking the Evaluation of Unbiased Scene Graph Generation",
        "summary":"Since the severe imbalanced predicate distributions in common subject-object relations, current Scene Graph Generation (SGG) methods tend to predict frequent predicate categories and fail to recognize rare ones. To improve the robustness of SGG models on different predicate categories, recent research has focused on unbiased SGG and adopted mean Recall@K (mR@K) as the main evaluation metric. However, we discovered two overlooked issues about this de facto standard metric mR@K, which makes current unbiased SGG evaluation vulnerable and unfair: 1) mR@K neglects the correlations among predicates and unintentionally breaks category independence when ranking all the triplet predictions together regardless of the predicate categories, leading to the performance of some predicates being underestimated. 2) mR@K neglects the compositional diversity of different predicates and assigns excessively high weights to some oversimple category samples with limited composable relation triplet types. It totally conflicts with the goal of SGG task which encourages models to detect more types of visual relationship triplets. In addition, we investigate the under-explored correlation between objects and predicates, which can serve as a simple but strong baseline for unbiased SGG. In this paper, we refine mR@K and propose two complementary evaluation metrics for unbiased SGG: Independent Mean Recall (IMR) and weighted IMR (wIMR). These two metrics are designed by considering the category independence and diversity of composable relation triplets, respectively. We compare the proposed metrics with the de facto standard metrics through extensive experiments and discuss the solutions to evaluate unbiased SGG in a more trustworthy way.",
        "published":1659515031000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01909v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1096793629,
        "technologyreview":0.2285235917,
        "newscientist":0.1437873263,
        "venturebeat":0.20574366,
        "wired":0.1775629026,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01837v1",
        "predicted_newsworthiness":0.3851682313,
        "title":"Learning Prior Feature and Attention Enhanced Image Inpainting",
        "summary":"Many recent inpainting works have achieved impressive results by leveraging Deep Neural Networks (DNNs) to model various prior information for image restoration. Unfortunately, the performance of these methods is largely limited by the representation ability of vanilla Convolutional Neural Networks (CNNs) backbones.On the other hand, Vision Transformers (ViT) with self-supervised pre-training have shown great potential for many visual recognition and object detection tasks. A natural question is whether the inpainting task can be greatly benefited from the ViT backbone? However, it is nontrivial to directly replace the new backbones in inpainting networks, as the inpainting is an inverse problem fundamentally different from the recognition tasks. To this end, this paper incorporates the pre-training based Masked AutoEncoder (MAE) into the inpainting model, which enjoys richer informative priors to enhance the inpainting process. Moreover, we propose to use attention priors from MAE to make the inpainting model learn more long-distance dependencies between masked and unmasked regions. Sufficient ablations have been discussed about the inpainting and the self-supervised pre-training models in this paper. Besides, experiments on both Places2 and FFHQ demonstrate the effectiveness of our proposed model. Codes and pre-trained models are released in https:\/\/github.com\/ewrfcas\/MAE-FAR.",
        "published":1659501173000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01837v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0830885646,
        "technologyreview":0.2030327011,
        "newscientist":0.1149590207,
        "venturebeat":0.1640828771,
        "wired":0.134166748,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02005v1",
        "predicted_newsworthiness":0.3849112818,
        "title":"Gradient-based Uncertainty for Monocular Depth Estimation",
        "summary":"In monocular depth estimation, disturbances in the image context, like moving objects or reflecting materials, can easily lead to erroneous predictions. For that reason, uncertainty estimates for each pixel are necessary, in particular for safety-critical applications such as automated driving. We propose a post hoc uncertainty estimation approach for an already trained and thus fixed depth estimation model, represented by a deep neural network. The uncertainty is estimated with the gradients which are extracted with an auxiliary loss function. To avoid relying on ground-truth information for the loss definition, we present an auxiliary loss function based on the correspondence of the depth prediction for an image and its horizontally flipped counterpart. Our approach achieves state-of-the-art uncertainty estimation results on the KITTI and NYU Depth V2 benchmarks without the need to retrain the neural network. Models and code are publicly available at https:\/\/github.com\/jhornauer\/GrUMoDepth.",
        "published":1659529262000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02005v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0815080513,
        "technologyreview":0.1974544424,
        "newscientist":0.1209017819,
        "venturebeat":0.2009589062,
        "wired":0.1597865117,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01924v1",
        "predicted_newsworthiness":0.3808066748,
        "title":"Per-Clip Video Object Segmentation",
        "summary":"Recently, memory-based approaches show promising results on semi-supervised video object segmentation. These methods predict object masks frame-by-frame with the help of frequently updated memory of the previous mask. Different from this per-frame inference, we investigate an alternative perspective by treating video object segmentation as clip-wise mask propagation. In this per-clip inference scheme, we update the memory with an interval and simultaneously process a set of consecutive frames (i.e. clip) between the memory updates. The scheme provides two potential benefits: accuracy gain by clip-level optimization and efficiency gain by parallel computation of multiple frames. To this end, we propose a new method tailored for the per-clip inference. Specifically, we first introduce a clip-wise operation to refine the features based on intra-clip correlation. In addition, we employ a progressive matching mechanism for efficient information-passing within a clip. With the synergy of two modules and a newly proposed per-clip based training, our network achieves state-of-the-art performance on Youtube-VOS 2018\/2019 val (84.6% and 84.6%) and DAVIS 2016\/2017 val (91.9% and 86.1%). Furthermore, our model shows a great speed-accuracy trade-off with varying memory update intervals, which leads to huge flexibility.",
        "published":1659517349000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01924v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0830657746,
        "technologyreview":0.1627405599,
        "newscientist":0.1008181477,
        "venturebeat":0.1580972125,
        "wired":0.1369990365,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02035v1",
        "predicted_newsworthiness":0.3780631794,
        "title":"Template matching with white balance adjustment under multiple illuminants",
        "summary":"In this paper, we propose a novel template matching method with a white balancing adjustment, called N-white balancing, which was proposed for multi-illuminant scenes. To reduce the influence of lighting effects, N-white balancing is applied to images for multi-illumination color constancy, and then a template matching method is carried out by using adjusted images. In experiments, the effectiveness of the proposed method is demonstrated to be effective in object detection tasks under various illumination conditions.",
        "published":1659531438000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02035v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0576375715,
        "technologyreview":0.1346256479,
        "newscientist":0.1089036175,
        "venturebeat":0.1314417431,
        "wired":0.1032464423,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02245v1",
        "predicted_newsworthiness":0.3773772596,
        "title":"MinVIS: A Minimal Video Instance Segmentation Framework without Video-based Training",
        "summary":"We propose MinVIS, a minimal video instance segmentation (VIS) framework that achieves state-of-the-art VIS performance with neither video-based architectures nor training procedures. By only training a query-based image instance segmentation model, MinVIS outperforms the previous best result on the challenging Occluded VIS dataset by over 10% AP. Since MinVIS treats frames in training videos as independent images, we can drastically sub-sample the annotated frames in training videos without any modifications. With only 1% of labeled frames, MinVIS outperforms or is comparable to fully-supervised state-of-the-art approaches on YouTube-VIS 2019\/2021. Our key observation is that queries trained to be discriminative between intra-frame object instances are temporally consistent and can be used to track instances without any manually designed heuristics. MinVIS thus has the following inference pipeline: we first apply the trained query-based image instance segmentation to video frames independently. The segmented instances are then tracked by bipartite matching of the corresponding queries. This inference is done in an online fashion and does not need to process the whole video at once. MinVIS thus has the practical advantages of reducing both the labeling costs and the memory requirements, while not sacrificing the VIS performance. Code is available at: https:\/\/github.com\/NVlabs\/MinVIS",
        "published":1659549042000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02245v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0808601459,
        "technologyreview":0.1837086572,
        "newscientist":0.1262410705,
        "venturebeat":0.189572027,
        "wired":0.1657778675,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02105v1",
        "predicted_newsworthiness":0.3755530782,
        "title":"Edge-Based Self-Supervision for Semi-Supervised Few-Shot Microscopy Image Cell Segmentation",
        "summary":"Deep neural networks currently deliver promising results for microscopy image cell segmentation, but they require large-scale labelled databases, which is a costly and time-consuming process. In this work, we relax the labelling requirement by combining self-supervised with semi-supervised learning. We propose the prediction of edge-based maps for self-supervising the training of the unlabelled images, which is combined with the supervised training of a small number of labelled images for learning the segmentation task. In our experiments, we evaluate on a few-shot microscopy image cell segmentation benchmark and show that only a small number of annotated images, e.g. 10% of the original training set, is enough for our approach to reach similar performance as with the fully annotated databases on 1- to 10-shots. Our code and trained models is made publicly available",
        "published":1659537300000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02105v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0757842894,
        "technologyreview":0.2167144187,
        "newscientist":0.1476054042,
        "venturebeat":0.1840557115,
        "wired":0.1463289693,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01923v1",
        "predicted_newsworthiness":0.3727120763,
        "title":"Graph Regularized Nonnegative Latent Factor Analysis Model for Temporal Link Prediction in Cryptocurrency Transaction Networks",
        "summary":"With the development of blockchain technology, the cryptocurrency based on blockchain technology is becoming more and more popular. This gave birth to a huge cryptocurrency transaction network has received widespread attention. Link prediction learning structure of network is helpful to understand the mechanism of network, so it is also widely studied in cryptocurrency network. However, the dynamics of cryptocurrency transaction networks have been neglected in the past researches. We use graph regularized method to link past transaction records with future transactions. Based on this, we propose a single latent factor-dependent, non-negative, multiplicative and graph regularized-incorporated update (SLF-NMGRU) algorithm and further propose graph regularized nonnegative latent factor analysis (GrNLFA) model. Finally, experiments on a real cryptocurrency transaction network show that the proposed method improves both the accuracy and the computational efficiency",
        "published":1659517139000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01923v1",
        "arxiv_primary_category":"cs.lg",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1242390751,
        "technologyreview":0.2337356736,
        "newscientist":0.142334321,
        "venturebeat":0.222629718,
        "wired":0.174994252,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01903v1",
        "predicted_newsworthiness":0.3596469547,
        "title":"Neural Dynamic Movement Primitives -- a survey",
        "summary":"One of the most important challenges in robotics is producing accurate trajectories and controlling their dynamic parameters so that the robots can perform different tasks. The ability to provide such motion control is closely related to how such movements are encoded. Advances on deep learning have had a strong repercussion in the development of novel approaches for Dynamic Movement Primitives. In this work, we survey scientific literature related to Neural Dynamic Movement Primitives, to complement existing surveys on Dynamic Movement Primitives.",
        "published":1659514268000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01903v1",
        "arxiv_primary_category":"cs.ro",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0758154885,
        "technologyreview":0.235246634,
        "newscientist":0.1479030492,
        "venturebeat":0.1880206228,
        "wired":0.1595990571,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01957v1",
        "predicted_newsworthiness":0.3565579169,
        "title":"PolarMOT: How Far Can Geometric Relations Take Us in 3D Multi-Object Tracking?",
        "summary":"Most (3D) multi-object tracking methods rely on appearance-based cues for data association. By contrast, we investigate how far we can get by only encoding geometric relationships between objects in 3D space as cues for data-driven data association. We encode 3D detections as nodes in a graph, where spatial and temporal pairwise relations among objects are encoded via localized polar coordinates on graph edges. This representation makes our geometric relations invariant to global transformations and smooth trajectory changes, especially under non-holonomic motion. This allows our graph neural network to learn to effectively encode temporal and spatial interactions and fully leverage contextual and motion cues to obtain final scene interpretation by posing data association as edge classification. We establish a new state-of-the-art on nuScenes dataset and, more importantly, show that our method, PolarMOT, generalizes remarkably well across different locations (Boston, Singapore, Karlsruhe) and datasets (nuScenes and KITTI).",
        "published":1659521216000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01957v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0967687339,
        "technologyreview":0.2042216435,
        "newscientist":0.1444727915,
        "venturebeat":0.1930561288,
        "wired":0.1682121755,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01853v1",
        "predicted_newsworthiness":0.3524509227,
        "title":"Robust Graph Neural Networks using Weighted Graph Laplacian",
        "summary":"Graph neural network (GNN) is achieving remarkable performances in a variety of application domains. However, GNN is vulnerable to noise and adversarial attacks in input data. Making GNN robust against noises and adversarial attacks is an important problem. The existing defense methods for GNNs are computationally demanding and are not scalable. In this paper, we propose a generic framework for robustifying GNN known as Weighted Laplacian GNN (RWL-GNN). The method combines Weighted Graph Laplacian learning with the GNN implementation. The proposed method benefits from the positive semi-definiteness property of Laplacian matrix, feature smoothness, and latent features via formulating a unified optimization framework, which ensures the adversarial\/noisy edges are discarded and connections in the graph are appropriately weighted. For demonstration, the experiments are conducted with Graph convolutional neural network(GCNN) architecture, however, the proposed framework is easily amenable to any existing GNN architecture. The simulation results with benchmark dataset establish the efficacy of the proposed method, both in accuracy and computational efficiency. Code can be accessed at https:\/\/github.com\/Bharat-Runwal\/RWL-GNN.",
        "published":1659504995000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01853v1",
        "arxiv_primary_category":"cs.lg",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1134920787,
        "technologyreview":0.2620418381,
        "newscientist":0.1586123107,
        "venturebeat":0.226932091,
        "wired":0.1856513077,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02207v1",
        "predicted_newsworthiness":0.3476554498,
        "title":"Robot Learning from Demonstration Using Elastic Maps",
        "summary":"Learning from Demonstration (LfD) is a popular method of reproducing and generalizing robot skills from human-provided demonstrations. In this paper, we propose a novel optimization-based LfD method that encodes demonstrations as elastic maps. An elastic map is a graph of nodes connected through a mesh of springs. We build a skill model by fitting an elastic map to the set of demonstrations. The formulated optimization problem in our approach includes three objectives with natural and physical interpretations. The main term rewards the mean squared error in the Cartesian coordinate. The second term penalizes the non-equidistant distribution of points resulting in the optimum total length of the trajectory. The third term rewards smoothness while penalizing nonlinearity. These quadratic objectives form a convex problem that can be solved efficiently with local optimizers. We examine nine methods for constructing and weighting the elastic maps and study their performance in robotic tasks. We also evaluate the proposed method in several simulated and real-world experiments using a UR5e manipulator arm, and compare it to other LfD approaches to demonstrate its benefits and flexibility across a variety of metrics.",
        "published":1659544927000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02207v1",
        "arxiv_primary_category":"cs.ro",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.085801498,
        "technologyreview":0.2238564683,
        "newscientist":0.1439052218,
        "venturebeat":0.1619443434,
        "wired":0.1712011951,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01905v1",
        "predicted_newsworthiness":0.3283014519,
        "title":"Graph Signal Processing for Heterogeneous Change Detection Part II: Spectral Domain Analysis",
        "summary":"This is the second part of the paper that provides a new strategy for the heterogeneous change detection (HCD) problem, that is, solving HCD from the perspective of graph signal processing (GSP). We construct a graph to represent the structure of each image, and treat each image as a graph signal defined on the graph. In this way, we can convert the HCD problem into a comparison of responses of signals on systems defined on the graphs. In the part I, the changes are measured by comparing the structure difference between the graphs from the vertex domain. In this part II, we analyze the GSP for HCD from the spectral domain. We first analyze the spectral properties of the different images on the same graph, and show that their spectra exhibit commonalities and dissimilarities. Specially, it is the change that leads to the dissimilarities of their spectra. Then, we propose a regression model for the HCD, which decomposes the source signal into the regressed signal and changed signal, and requires the regressed signal have the same spectral property as the target signal on the same graph. With the help of graph spectral analysis, the proposed regression model is flexible and scalable. Experiments conducted on seven real data sets show the effectiveness of the proposed method.",
        "published":1659514284000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01905v1",
        "arxiv_primary_category":"cs.cv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0785999461,
        "technologyreview":0.144470767,
        "newscientist":0.1270750412,
        "venturebeat":0.1224674014,
        "wired":0.1086575168,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02068v1",
        "predicted_newsworthiness":0.2984509217,
        "title":"HybridGNN: Learning Hybrid Representation in Multiplex Heterogeneous Networks",
        "summary":"Recently, graph neural networks have shown the superiority of modeling the complex topological structures in heterogeneous network-based recommender systems. Due to the diverse interactions among nodes and abundant semantics emerging from diverse types of nodes and edges, there is a bursting research interest in learning expressive node representations in multiplex heterogeneous networks. One of the most important tasks in recommender systems is to predict the potential connection between two nodes under a specific edge type (i.e., relationship). Although existing studies utilize explicit metapaths to aggregate neighbors, practically they only consider intra-relationship metapaths and thus fail to leverage the potential uplift by inter-relationship information. Moreover, it is not always straightforward to exploit inter-relationship metapaths comprehensively under diverse relationships, especially with the increasing number of node and edge types. In addition, contributions of different relationships between two nodes are difficult to measure. To address the challenges, we propose HybridGNN, an end-to-end GNN model with hybrid aggregation flows and hierarchical attentions to fully utilize the heterogeneity in the multiplex scenarios. Specifically, HybridGNN applies a randomized inter-relationship exploration module to exploit the multiplexity property among different relationships. Then, our model leverages hybrid aggregation flows under intra-relationship metapaths and randomized exploration to learn the rich semantics. To explore the importance of different aggregation flow and take advantage of the multiplexity property, we bring forward a novel hierarchical attention module which leverages both metapath-level attention and relationship-level attention. Extensive experimental results suggest that HybridGNN achieves the best performance compared to several state-of-the-art baselines.",
        "published":1659533987000,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02068v1",
        "arxiv_primary_category":"cs.lg",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0892372968,
        "technologyreview":0.1859152502,
        "newscientist":0.1103041615,
        "venturebeat":0.1918329778,
        "wired":0.1511510165,
        "outlet_relevance":"N\/A",
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    }
]