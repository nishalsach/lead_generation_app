[
    {
        "arxiv_id":"2206.08029v1",
        "predicted_newsworthiness":0.6469029816,
        "title":"DIALOG-22 RuATD Generated Text Detection",
        "summary":"Text Generation Models (TGMs) succeed in creating text that matches human language style reasonably well. Detectors that can distinguish between TGM-generated text and human-written ones play an important role in preventing abuse of TGM. In this paper, we describe our pipeline for the two DIALOG-22 RuATD tasks: detecting generated text (binary task) and classification of which model was used to generate text (multiclass task). We achieved 1st place on the binary classification task with an accuracy score of 0.82995 on the private test set and 4th place on the multiclass classification task with an accuracy score of 0.62856 on the private test set. We proposed an ensemble method of different pre-trained models based on the attention mechanism.",
        "completion1":"DIALOG-22 RuATD Generated Text Detection: 1st Place in Binary Classification",
        "completion2":"DIALOG-22 RuATD Generated Text Detection: 4th Place in Multiclass Classification",
        "completion3":"DIALOG-22 RuATD Generated Text Detection: AttentionMechanism Ensemble Method",
        "technologyreview":0.2572706903,
        "venturebeat":0.2307540792,
        "wired":0.0738431339,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.08029v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1655372006000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2202.13203v1",
        "predicted_newsworthiness":0.5394960454,
        "title":"Dropout can Simulate Exponential Number of Models for Sample Selection Techniques",
        "summary":"Following Coteaching, generally in the literature, two models are used in sample selection based approaches for training with noisy labels. Meanwhile, it is also well known that Dropout when present in a network trains an ensemble of sub-networks. We show how to leverage this property of Dropout to train an exponential number of shared models, by training a single model with Dropout. We show how we can modify existing two model-based sample selection methodologies to use an exponential number of shared models. Not only is it more convenient to use a single model with Dropout, but this approach also combines the natural benefits of Dropout with that of training an exponential number of models, leading to improved results.",
        "completion1":"Dropout can simulate exponential number of models for sample selection techniques.",
        "completion2":"Dropout leads to improved results for training with noisy labels.",
        "completion3":"Dropout enables single model to be trained with multiple benefits.",
        "technologyreview":0.210467517,
        "venturebeat":0.1693057009,
        "wired":0.0429589459,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.13203v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1645898006000,
        "code_mentioned":0,
        "readability":0.87
    },
    {
        "arxiv_id":"2205.00949v1",
        "predicted_newsworthiness":0.4646049831,
        "title":"Answer-Me: Multi-Task Open-Vocabulary Visual Question Answering",
        "summary":"We present Answer-Me, a task-aware multi-task framework which unifies a variety of question answering tasks, such as, visual question answering, visual entailment, visual reasoning. In contrast to previous works using contrastive or generative captioning training, we propose a novel and simple recipe to pre-train a vision-language joint model, which is multi-task as well. The pre-training uses only noisy image captioning data, and is formulated to use the entire architecture end-to-end with both a strong language encoder and decoder. Our results show state-of-the-art performance, zero-shot generalization, robustness to forgetting, and competitive single-task results across a variety of question answering tasks. Our multi-task mixture training learns from tasks of various question intents and thus generalizes better, including on zero-shot vision-language tasks. We conduct experiments in the challenging multi-task and open-vocabulary settings and across a variety of datasets and tasks, such as VQA2.0, SNLI-VE, NLVR2, GQA, VizWiz. We observe that the proposed approach is able to generalize to unseen tasks and that more diverse mixtures lead to higher accuracy in both known and novel tasks.",
        "completion1":"Answer-Me: Multi-Task Open-Vocabulary Visual Question Answering",
        "completion2":"Multi-Task Open-Vocabulary Visual Question Answering with Answer-Me",
        "completion3":"Open-Vocabulary Visual Question Answering with Answer-Me Multi-Tasked",
        "technologyreview":0.2174348525,
        "venturebeat":0.1825992214,
        "wired":0.0483824669,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.00949v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1651503193000,
        "code_mentioned":0,
        "readability":0.83
    },
    {
        "arxiv_id":"2203.02146v3",
        "predicted_newsworthiness":0.4518698611,
        "title":"Attention Concatenation Volume for Accurate and Efficient Stereo Matching",
        "summary":"Stereo matching is a fundamental building block for many vision and robotics applications. An informative and concise cost volume representation is vital for stereo matching of high accuracy and efficiency. In this paper, we present a novel cost volume construction method which generates attention weights from correlation clues to suppress redundant information and enhance matching-related information in the concatenation volume. To generate reliable attention weights, we propose multi-level adaptive patch matching to improve the distinctiveness of the matching cost at different disparities even for textureless regions. The proposed cost volume is named attention concatenation volume (ACV) which can be seamlessly embedded into most stereo matching networks, the resulting networks can use a more lightweight aggregation network and meanwhile achieve higher accuracy, e.g. using only 1\/25 parameters of the aggregation network can achieve higher accuracy for GwcNet. Furthermore, we design a highly accurate network (ACVNet) based on our ACV, which achieves state-of-the-art performance on several benchmarks.",
        "completion1":"A novel cost volume construction method which generates attention weights from correlation clues to suppress redundant information and enhance matching-related information in the concatenation volume.",
        "completion2":"To generate reliable attention weights, we propose multi-level adaptive patch matching to improve the distinctiveness of the matching cost at different disparities even for textureless regions.",
        "completion3":"The proposed cost volume is named attention concatenation volume  which can be seamlessly embedded into most stereo matching networks, the resulting networks can use a more lightweight aggregation network and meanwhile achieve higher accuracy, e.g. using only 1\/25 parameters of the aggregation network can achieve higher accuracy for GwcNet.",
        "technologyreview":0.2225523751,
        "venturebeat":0.1856294358,
        "wired":0.0382158042,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.02146v3",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1646375338000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2206.06238v1",
        "predicted_newsworthiness":0.5299916773,
        "title":"Indian Legal Text Summarization: A Text Normalisation-based Approach",
        "summary":"In the Indian court system, pending cases have long been a problem. There are more than 4 crore cases outstanding. Manually summarising hundreds of documents is a time-consuming and tedious task for legal stakeholders. Many state-of-the-art models for text summarization have emerged as machine learning has progressed. Domain-independent models don't do well with legal texts, and fine-tuning those models for the Indian Legal System is problematic due to a lack of publicly available datasets. To improve the performance of domain-independent models, the authors have proposed a methodology for normalising legal texts in the Indian context. The authors experimented with two state-of-the-art domain-independent models for legal text summarization, namely BART and PEGASUS. BART and PEGASUS are put through their paces in terms of extractive and abstractive summarization to understand the effectiveness of the text normalisation approach. Summarised texts are evaluated by domain experts on multiple parameters and using ROUGE metrics. It shows the proposed text normalisation approach is effective in legal texts with domain-independent models.",
        "completion1":"State-of-the-art models for text summarization have emerged as machine learning has progressed.",
        "completion2":"Domain-independent models don't do well with legal texts, and fine-tuning those models for the Indian Legal System is problematic due to a lack of publicly available datasets.",
        "completion3":"To improve the performance of domain-independent models, the authors have proposed a methodology for normalising legal texts in the Indian context.",
        "technologyreview":0.1959390965,
        "venturebeat":0.1819687245,
        "wired":0.0416124784,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.06238v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1655133410000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2207.06387v1",
        "predicted_newsworthiness":0.4814276199,
        "title":"A General Framework for Partial to Full Image Registration",
        "summary":"Image registration is a research field in which images must be compared and aligned independently of the point of view or camera characteristics. In some applications (such as forensic biometrics, satellite photography or outdoor scene identification) classical image registration systems fail due to one of the images compared represents a tiny piece of the other image. For instance, in forensics palmprint recognition, it is usual to find only a small piece of the palmprint, but in the database, the whole palmprint has been enrolled. The main reason of the poor behaviour of classical image registration methods is the gap between the amounts of salient points of both images, which is related to the number of points to be considered as outliers. Usually, the difficulty of finding a good match increases when the image that represents the tiny part of the scene has been drastically rotated. Again, in the case of palmprint forensics, it is difficult to decide a priori the orientation of the found tiny palmprint image. We present a rotation invariant registration method that explicitly considers that the image to be matched is a small piece of a larger image. We have experimentally validated our method in two different scenarios; palmprint identification and outdoor image registration.",
        "completion1":"A General Framework for Partial to Full Image Registration Developed",
        "completion2":"Researchers Find New Method to Improve Image Registration Accuracy",
        "completion3":"Study Shows Improved Image Registration can Assist in Forensic Investigations",
        "technologyreview":0.1710348217,
        "venturebeat":0.1284907987,
        "wired":0.0121497162,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.06387v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1657734289000,
        "code_mentioned":0,
        "readability":0.88
    },
    {
        "arxiv_id":"2201.10522v1",
        "predicted_newsworthiness":0.4350033651,
        "title":"Blind Image Deblurring: a Review",
        "summary":"This is a review on blind image deblurring. First, we formulate the blind image deblurring problem and explain why it is challenging. Next, we bring some psychological and cognitive studies on the way our human vision system deblurs. Then, relying on several previous reviews, we discuss the topic of metrics and datasets, which is non-trivial to blind deblurring. Finally, we introduce some typical optimization-based methods and learning-based methods.",
        "completion1":"Formulating the Blind Image Deblurring Problem",
        "completion2":"Why Blind Image Deblurring is Challenging",
        "completion3":"Introducing Some Optimization-Based and Learning-Based Methods",
        "technologyreview":0.1971103301,
        "venturebeat":0.1240061194,
        "wired":0.0157313721,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.10522v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1642839672000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2205.05306v1",
        "predicted_newsworthiness":0.882645745,
        "title":"The Conflict Between Explainable and Accountable Decision-Making Algorithms",
        "summary":"Decision-making algorithms are being used in important decisions, such as who should be enrolled in health care programs and be hired. Even though these systems are currently deployed in high-stakes scenarios, many of them cannot explain their decisions. This limitation has prompted the Explainable Artificial Intelligence (XAI) initiative, which aims to make algorithms explainable to comply with legal requirements, promote trust, and maintain accountability. This paper questions whether and to what extent explainability can help solve the responsibility issues posed by autonomous AI systems. We suggest that XAI systems that provide post-hoc explanations could be seen as blameworthy agents, obscuring the responsibility of developers in the decision-making process. Furthermore, we argue that XAI could result in incorrect attributions of responsibility to vulnerable stakeholders, such as those who are subjected to algorithmic decisions (i.e., patients), due to a misguided perception that they have control over explainable algorithms. This conflict between explainability and accountability can be exacerbated if designers choose to use algorithms and patients as moral and legal scapegoats. We conclude with a set of recommendations for how to approach this tension in the socio-technical process of algorithmic decision-making and a defense of hard regulation to prevent designers from escaping responsibility.",
        "completion1":"The Conflict Between Explainable and Accountable Decision-Making Algorithms",
        "completion2":"Decision-making algorithms cannot explain their decisions",
        "completion3":"XAI systems that provide post-hoc explanations could be seen as blameworthy agents",
        "technologyreview":0.4495898718,
        "venturebeat":0.3696960332,
        "wired":0.1526819709,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.05306v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy",
            "cs.ai"
        ],
        "published":1652253568000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2204.07000v1",
        "predicted_newsworthiness":0.4143877894,
        "title":"Solving AC Power Flow with Graph Neural Networks under Realistic Constraints",
        "summary":"In this paper we propose a graph neural network architecture solving the AC power flow problem under realistic constraints. While the energy transition is changing the energy industry to a digitalized and decentralized energy system, the challenges are increasingly shifting to the distribution grid level to integrate new loads and generation technologies. To ensure a save and resilient operation of distribution grids, AC power flow calculations are the means of choice to determine grid operating limits or analyze grid asset utilization in planning procedures. In our approach we demonstrate the development of a framework which makes use of graph neural networks to learn the physical constraints of the power flow. We present our model architecture on which we perform unsupervised training to learn a general solution of the AC power flow formulation that is independent of the specific topologies and supply tasks used for training. Finally, we demonstrate, validate and discuss our results on medium voltage benchmark grids.",
        "completion1":"Solving AC Power Flow with Graph Neural Networks under Realistic Constraints",
        "completion2":"AC power flow problem under realistic constraints",
        "completion3":"Graph neural network architecture solving the AC power flow problem",
        "technologyreview":0.215974477,
        "venturebeat":0.1859662788,
        "wired":0.041563471,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.07000v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1649947774000,
        "code_mentioned":0,
        "readability":0.87
    },
    {
        "arxiv_id":"2204.07069v1",
        "predicted_newsworthiness":0.5634391359,
        "title":"Panoptic Segmentation using Synthetic and Real Data",
        "summary":"Being able to understand the relations between the user and the surrounding environment is instrumental to assist users in a worksite. For instance, understanding which objects a user is interacting with from images and video collected through a wearable device can be useful to inform the worker on the usage of specific objects in order to improve productivity and prevent accidents. Despite modern vision systems can rely on advanced algorithms for object detection, semantic and panoptic segmentation, these methods still require large quantities of domain-specific labeled data, which can be difficult to obtain in industrial scenarios. Motivated by this observation, we propose a pipeline which allows to generate synthetic images from 3D models of real environments and real objects. The generated images are automatically labeled and hence effortless to obtain. Exploiting the proposed pipeline, we generate a dataset comprising synthetic images automatically labeled for panoptic segmentation. This set is complemented by a small number of manually labeled real images for fine-tuning. Experiments show that the use of synthetic images allows to drastically reduce the number of real images needed to obtain reasonable panoptic segmentation performance.",
        "completion1":"Panoptic Segmentation: A New Way to Understand Your Surroundings",
        "completion2":"Panoptic Segmentation using Synthetic and Real Data: Improving Productivity and Preventing Accidents",
        "completion3":"Panoptic Segmentation: A Pipeline for Generating Synthetic Images",
        "technologyreview":0.2830194112,
        "venturebeat":0.2468306054,
        "wired":0.0739075482,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.07069v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1649952108000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2201.04989v2",
        "predicted_newsworthiness":0.4076114628,
        "title":"Observability Analysis and Keyframe-Based Filtering for Visual Inertial Odometry with Full Self-Calibration",
        "summary":"Camera-IMU (Inertial Measurement Unit) sensor fusion has been extensively studied in recent decades. Numerous observability analysis and fusion schemes for motion estimation with self-calibration have been presented. However, it has been uncertain whether both camera and IMU intrinsic parameters are observable under general motion. To answer this question, by using the Lie derivatives, we first prove that for a rolling shutter (RS) camera-IMU system, all intrinsic and extrinsic parameters, camera time offset, and readout time of the RS camera, are observable with an unknown landmark. To our knowledge, we are the first to present such a proof. Next, to validate this analysis and to solve the drift issue of a structureless filter during standstills, we develop a Keyframe-based Sliding Window Filter (KSWF) for odometry and self-calibration, which works with a monocular RS camera or stereo RS cameras. Though the keyframe concept is widely used in vision-based sensor fusion, to our knowledge, KSWF is the first of its kind to support self-calibration. Our simulation and real data tests have validated that it is possible to fully calibrate the camera-IMU system using observations of opportunistic landmarks under diverse motion. Real data tests confirmed previous allusions that keeping landmarks in the state vector can remedy the drift in standstill, and showed that the keyframe-based scheme is an alternative solution.",
        "completion1":"Rolling Shutter Camera-IMU System Proven Observable for Motion Estimation with Self-Calibration",
        "completion2":"Keyframe-based Sliding Window Filter Developed for Odometry and Self-Calibration",
        "completion3":"Simulation and Real Data Tests Validate Full Calibration of Camera-IMU System",
        "technologyreview":0.1206542672,
        "venturebeat":0.1274810496,
        "wired":-0.0098670582,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.04989v2",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1642082290000,
        "code_mentioned":0,
        "readability":0.8
    },
    {
        "arxiv_id":"2204.09162v1",
        "predicted_newsworthiness":0.4704662362,
        "title":"Computational Adaptation of XR Interfaces Through Interaction Simulation",
        "summary":"Adaptive and intelligent user interfaces have been proposed as a critical component of a successful extended reality (XR) system. In particular, a predictive system can make inferences about a user and provide them with task-relevant recommendations or adaptations. However, we believe such adaptive interfaces should carefully consider the overall \\emph{cost} of interactions to better address uncertainty of predictions. In this position paper, we discuss a computational approach to adapt XR interfaces, with the goal of improving user experience and performance. Our novel model, applied to menu selection tasks, simulates user interactions by considering both cognitive and motor costs. In contrast to greedy algorithms that adapt based on predictions alone, our model holistically accounts for costs and benefits of adaptations towards adapting the interface and providing optimal recommendations to the user.",
        "completion1":"Computational Adaptation of XR Interfaces Through Interaction Simulation.",
        "completion2":"A predictive system can make inferences about a user and provide them with task-relevant recommendations or adaptations.",
        "completion3":"In contrast to greedy algorithms that adapt based on predictions alone, our model holistically accounts for costs and benefits of adaptations towards adapting the interface and providing optimal recommendations to the user.",
        "technologyreview":0.2551525989,
        "venturebeat":0.2629883818,
        "wired":0.0837221908,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.09162v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ai"
        ],
        "published":1650411427000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2203.09505v2",
        "predicted_newsworthiness":0.4969312721,
        "title":"Visualizing Global Explanations of Point Cloud DNNs",
        "summary":"In the field of autonomous driving and robotics, point clouds are showing their excellent real-time performance as raw data from most of the mainstream 3D sensors. Therefore, point cloud neural networks have become a popular research direction in recent years. So far, however, there has been little discussion about the explainability of deep neural networks for point clouds. In this paper, we propose a point cloud-applicable explainability approach based on a local surrogate model-based method to show which components contribute to the classification. Moreover, we propose quantitative fidelity validations for generated explanations that enhance the persuasive power of explainability and compare the plausibility of different existing point cloud-applicable explainability methods. Our new explainability approach provides a fairly accurate, more semantically coherent and widely applicable explanation for point cloud classification tasks. Our code is available at https:\/\/github.com\/Explain3D\/LIME-3D",
        "completion1":"New method provides accurate, semantically coherent explanations for point cloud classification tasks.",
        "completion2":"Quantitative fidelity validations for generated explanations enhance persuasive power of explainability.",
        "completion3":"Comparison of plausibility of different existing point cloud-applicable explainability methods.",
        "technologyreview":0.2903818946,
        "venturebeat":0.2435036839,
        "wired":0.0862783355,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.09505v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1647539591000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2204.04621v1",
        "predicted_newsworthiness":0.4032613314,
        "title":"Unsupervised Manga Character Re-identification via Face-body and Spatial-temporal Associated Clustering",
        "summary":"In the past few years, there has been a dramatic growth in e-manga (electronic Japanese-style comics). Faced with the booming demand for manga research and the large amount of unlabeled manga data, we raised a new task, called unsupervised manga character re-identification. However, the artistic expression and stylistic limitations of manga pose many challenges to the re-identification problem. Inspired by the idea that some content-related features may help clustering, we propose a Face-body and Spatial-temporal Associated Clustering method (FSAC). In the face-body combination module, a face-body graph is constructed to solve problems such as exaggeration and deformation in artistic creation by using the integrity of the image. In the spatial-temporal relationship correction module, we analyze the appearance features of characters and design a temporal-spatial-related triplet loss to fine-tune the clustering. Extensive experiments on a manga book dataset with 109 volumes validate the superiority of our method in unsupervised manga character re-identification.",
        "completion1":"Unsupervised Manga Character Re-identification via Face-body and Spatial-temporal Associated Clustering",
        "completion2":"A New Method For Unsupervised Manga Character Re-identification",
        "completion3":"Face-body and Spatial-temporal Associated Clustering: A Novel Method For Unsupervised Manga Character Re",
        "technologyreview":0.1739288013,
        "venturebeat":0.140352173,
        "wired":0.0421862682,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.04621v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1649575721000,
        "code_mentioned":0,
        "readability":0.8
    },
    {
        "arxiv_id":"2205.05424v2",
        "predicted_newsworthiness":0.7472433485,
        "title":"\"If it didn't happen, why would I change my decision?\": How Judges Respond to Counterfactual Explanations for the Public Safety Assessment",
        "summary":"Many researchers and policymakers have expressed excitement about how algorithmic explanations may enable more fair and responsible decision-making. However, recent experimental studies have found that explanations do not always improve human use of algorithmic advice. In this study, we shed light on how people interpret and respond to counterfactual explanations (CFEs) -- an explanation that shows how a model's output changes with marginal changes to an input -- in the context of pretrial risk assessment instruments (PRAIs). We ran think-aloud trials with eight sitting US state court judges, providing them with recommendations from the PRAI as well as CFEs. At first, judges misinterpreted the counterfactuals as real -- rather than hypothetical -- changes to defendants. Once judges understood what the counterfactuals meant, they ignored them, stating they must make decisions based only on the actual defendant in question. They also expressed a mix of reasons for ignoring or following the advice of the PRAI. These results add to the literature on how people use algorithms and explanations in unexpected ways and the challenges associated with creating effective human-algorithm collaboration.",
        "completion1":"Judges ignore algorithmic explanations of defendants' risk levels.",
        "completion2":"8 state court judges were studied in think-aloud trials.",
        "completion3":"The results add to the literature on how people use algorithms and explanations in unexpected ways.",
        "technologyreview":0.3382740905,
        "venturebeat":0.263468642,
        "wired":0.1247935171,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.05424v2",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1652269567000,
        "code_mentioned":0,
        "readability":0.88
    },
    {
        "arxiv_id":"2202.04996v2",
        "predicted_newsworthiness":0.474826629,
        "title":"AA-TransUNet: Attention Augmented TransUNet For Nowcasting Tasks",
        "summary":"Data driven modeling based approaches have recently gained a lot of attention in many challenging meteorological applications including weather element forecasting. This paper introduces a novel data-driven predictive model based on TransUNet for precipitation nowcasting task. The TransUNet model which combines the Transformer and U-Net models has been previously successfully applied in medical segmentation tasks. Here, TransUNet is used as a core model and is further equipped with Convolutional Block Attention Modules (CBAM) and Depthwise-separable Convolution (DSC). The proposed Attention Augmented TransUNet (AA-TransUNet) model is evaluated on two distinct datasets: the Dutch precipitation map dataset and the French cloud cover dataset. The obtained results show that the proposed model outperforms other examined models on both tested datasets. Furthermore, the uncertainty analysis of the proposed AA-TransUNet is provided to give additional insights on its predictions.",
        "completion1":"AA-TransUNet model outperforms other examined models on Dutch precipitation map dataset and the French cloud cover dataset",
        "completion2":"The proposed Attention Augmented TransUNet (AA-TransUNet) model is evaluated on two distinct datasets",
        "completion3":"The uncertainty analysis of the proposed AA-TransUNet is provided to give additional insights on its predictions.",
        "technologyreview":0.2320031283,
        "venturebeat":0.1952087472,
        "wired":0.045806141,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.04996v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1644497330000,
        "code_mentioned":1,
        "readability":0.76
    },
    {
        "arxiv_id":"2202.12365v1",
        "predicted_newsworthiness":0.4230432085,
        "title":"Alternative Metrics to Select Motors for Quasi-Direct Drive Actuators",
        "summary":"Robotic systems for legged locomotion -- including legged robots, exoskeletons, and prosthetics -- require actuators with low inertia and high output torque. Traditionally, motors have been selected for these applications by maximizing the motor gap radius. We present alternative metrics for motor selection that are invariant to transmission ratio. The proposed metrics reward minimizing the motor inertia while maximizing the torque and motor constants without special consideration for gap radius, providing a better balance of properties for legged locomotion applications. We rigorously characterize the T-Motor RI50 and demonstrate the use of the metrics by comparing the RI50 to the widely-used T-Motor U8 as a case study.",
        "completion1":"Alternative Metrics for Selecting Motors Used in Quasi-Direct Drive Actuators",
        "completion2":"Robotic Systems for Legged Locomotion Require Low Inertia and High Output Torque",
        "completion3":"T-Motor RI50 is Rigorously Characterized and Comparable to T-Motor U8",
        "technologyreview":0.1589360721,
        "venturebeat":0.1059707474,
        "wired":0.0175422033,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.12365v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1645737180000,
        "code_mentioned":0,
        "readability":0.8
    },
    {
        "arxiv_id":"2207.01794v1",
        "predicted_newsworthiness":0.3688722045,
        "title":"Manipulator Differential Kinematics Part II: Acceleration and Advanced Applications",
        "summary":"This is the second and final article on the tutorial on manipulator differential kinematics. In Part I, we described a method of modelling kinematics using the elementary transform sequence (ETS), before formulating forward kinematics and the manipulator Jacobian. We then described some basic applications of the manipulator Jacobian including resolved-rate motion control (RRMC), inverse kinematics (IK), and some manipulator performance measures. In this article, we formulate the second-order differential kinematics, leading to a definition of manipulator Hessian. We then describe the differential kinematics' analytical forms, which are essential to dynamics applications. Subsequently, we provide a general formula for higher-order derivatives. The first application we consider is advanced velocity control. In this section, we extend resolved-rate motion control to perform sub-tasks while still achieving the goal before redefining the algorithm as a quadratic program to enable greater flexibility and additional constraints. We then take another look at numerical inverse kinematics with an emphasis on adding constraints. Finally, we analyse how the manipulator Hessian can help to escape singularities. We have provided Jupyter Notebooks to accompany each section within this tutorial. The Notebooks are written in Python code and use the Robotics Toolbox for Python, and the Swift Simulator to provide examples and implementations of algorithms. While not absolutely essential, for the most engaging and informative experience, we recommend working through the Jupyter Notebooks while reading this article. The Notebooks and setup instructions can be accessed at https:\/\/github.com\/jhavl\/dkt.",
        "completion1":"Manipulator Differential Kinematics Part II: Acceleration and Advanced Applications",
        "completion2":"How the Manipulator Hessian Can Help to Escape Singularities",
        "completion3":"Advanced Velocity Control Using the Manipulator Hessian",
        "technologyreview":0.138587849,
        "venturebeat":0.0980337037,
        "wired":0.0112979209,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.01794v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1656992939000,
        "code_mentioned":1,
        "readability":0.79
    },
    {
        "arxiv_id":"2201.13073v1",
        "predicted_newsworthiness":0.3007449843,
        "title":"Learning Representations of Entities and Relations",
        "summary":"Encoding facts as representations of entities and binary relationships between them, as learned by knowledge graph representation models, is useful for various tasks, including predicting new facts, question answering, fact checking and information retrieval. The focus of this thesis is on (i) improving knowledge graph representation with the aim of tackling the link prediction task; and (ii) devising a theory on how semantics can be captured in the geometry of relation representations. Most knowledge graphs are very incomplete and manually adding new information is costly, which drives the development of methods which can automatically infer missing facts. The first contribution of this thesis is HypER, a convolutional model which simplifies and improves upon the link prediction performance of the existing convolutional state-of-the-art model ConvE and can be mathematically explained in terms of constrained tensor factorisation. The second contribution is TuckER, a relatively straightforward linear model, which, at the time of its introduction, obtained state-of-the-art link prediction performance across standard datasets. The third contribution is MuRP, first multi-relational graph representation model embedded in hyperbolic space. MuRP outperforms all existing models and its Euclidean counterpart MuRE in link prediction on hierarchical knowledge graph relations whilst requiring far fewer dimensions. Despite the development of a large number of knowledge graph representation models with gradually increasing predictive performance, relatively little is known of the latent structure they learn. We generalise recent theoretical understanding of how semantic relations of similarity, paraphrase and analogy are encoded in the geometric interactions of word embeddings to how more general relations, as found in knowledge graphs, can be encoded in their representations.",
        "completion1":"Knowledge graph representation models learn to encode facts as representations of entities and binary relationships between them, which is useful for various tasks such as predicting new facts, question answering, fact checking and information retrieval.",
        "completion2":"The focus of this thesis is on (i) improving knowledge graph representation with the aim of tackling the link prediction task; and (ii) devising a theory on how semantics can be captured in the geometry of relation representations.",
        "completion3":"The first contribution of this thesis is HypER, a convolutional model which simplifies and improves upon the link prediction performance of the existing convolutional state-of-the-art model ConvE and can be mathematically explained in terms of constrained tensor factorisation.",
        "technologyreview":0.1916108773,
        "venturebeat":0.1464624434,
        "wired":0.0389279226,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.13073v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1643621083000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2205.00126v1",
        "predicted_newsworthiness":0.4640302519,
        "title":"SciEv: Finding Scientific Evidence Papers for Scientific News",
        "summary":"In the past decade, many scientific news media that report scientific breakthroughs and discoveries emerged, bringing science and technology closer to the general public. However, not all scientific news article cites proper sources, such as original scientific papers. A portion of scientific news articles contain misinterpreted, exaggerated, or distorted information that deviates from facts asserted in the original papers. Manually identifying proper citations is laborious and costly. Therefore, it is necessary to automatically search for pertinent scientific papers that could be used as evidence for a given piece of scientific news. We propose a system called SciEv that searches for scientific evidence papers given a scientific news article. The system employs a 2-stage query paradigm with the first stage retrieving candidate papers and the second stage reranking them. The key feature of SciEv is it uses domain knowledge entities (DKEs) to find candidates in the first stage, which proved to be more effective than regular keyphrases. In the reranking stage, we explore different document representations for news articles and candidate papers. To evaluate our system, we compiled a pilot dataset consisting of 100 manually curated (news,paper) pairs from ScienceAlert and similar websites. To our best knowledge, this is the first dataset of this kind. Our experiments indicate that the transformer model performs the best for DKE extraction. The system achieves a P@1=50%, P@5=71%, and P@10=74% when it uses a TFIDF-based text representation. The transformer-based re-ranker achieves a comparable performance but costs twice as much time. We will collect more data and test the system for user experience.",
        "completion1":"SciEv: Finding Scientific Evidence Papers for Scientific News",
        "completion2":"A system called SciEv that searches for scientific evidence papers given a scientific news article",
        "completion3":"The key feature of SciEv is it uses domain knowledge entities (DKEs) to find candidates in the first stage, which proved to be more effective than regular keyphrases.",
        "technologyreview":0.266549941,
        "venturebeat":0.2308372644,
        "wired":0.1017884265,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.00126v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.ai"
        ],
        "published":1651283003000,
        "code_mentioned":1,
        "readability":0.87
    },
    {
        "arxiv_id":"2206.04129v1",
        "predicted_newsworthiness":0.4638474569,
        "title":"Receding Moving Object Segmentation in 3D LiDAR Data Using Sparse 4D Convolutions",
        "summary":"A key challenge for autonomous vehicles is to navigate in unseen dynamic environments. Separating moving objects from static ones is essential for navigation, pose estimation, and understanding how other traffic participants are likely to move in the near future. In this work, we tackle the problem of distinguishing 3D LiDAR points that belong to currently moving objects, like walking pedestrians or driving cars, from points that are obtained from non-moving objects, like walls but also parked cars. Our approach takes a sequence of observed LiDAR scans and turns them into a voxelized sparse 4D point cloud. We apply computationally efficient sparse 4D convolutions to jointly extract spatial and temporal features and predict moving object confidence scores for all points in the sequence. We develop a receding horizon strategy that allows us to predict moving objects online and to refine predictions on the go based on new observations. We use a binary Bayes filter to recursively integrate new predictions of a scan resulting in more robust estimation. We evaluate our approach on the SemanticKITTI moving object segmentation challenge and show more accurate predictions than existing methods. Since our approach only operates on the geometric information of point clouds over time, it generalizes well to new, unseen environments, which we evaluate on the Apollo dataset.",
        "completion1":"A new approach to distinguishing moving objects from static ones in 3D LiDAR data is presented.",
        "completion2":"The new approach, called receding moving object segmentation, uses sparse 4D convolutions to jointly extract spatial and temporal features and predict moving object confidence scores for all points in a sequence.",
        "completion3":"The receding moving object segmentation approach is evaluated on the SemanticKITTI moving object segmentation challenge and the Apollo dataset, and shows more accurate predictions than existing methods.",
        "technologyreview":0.2352365684,
        "venturebeat":0.1996875965,
        "wired":0.0657191168,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.04129v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv"
        ],
        "published":1654714274000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2206.13389v2",
        "predicted_newsworthiness":0.4567099961,
        "title":"UI Layers Merger: Merging UI layers via Visual Learning and Boundary Prior",
        "summary":"With the fast-growing GUI development workload in the Internet industry, some work on intelligent methods attempted to generate maintainable front-end code from UI screenshots. It can be more suitable for utilizing UI design drafts that contain UI metadata. However, fragmented layers inevitably appear in the UI design drafts which greatly reduces the quality of code generation. None of the existing GUI automated techniques detects and merges the fragmented layers to improve the accessibility of generated code. In this paper, we propose UI Layers Merger (UILM), a vision-based method, which can automatically detect and merge fragmented layers into UI components. Our UILM contains Merging Area Detector (MAD) and a layers merging algorithm. MAD incorporates the boundary prior knowledge to accurately detect the boundaries of UI components. Then, the layers merging algorithm can search out the associated layers within the components' boundaries and merge them into a whole part. We present a dynamic data augmentation approach to boost the performance of MAD. We also construct a large-scale UI dataset for training the MAD and testing the performance of UILM. The experiment shows that the proposed method outperforms the best baseline regarding merging area detection and achieves a decent accuracy regarding layers merging.",
        "completion1":"UI Layers Merger: A vision-based method to automatically detect and merge fragmented layers into UI components.",
        "completion2":"The proposed UI Layers Merger outperforms the best baseline regarding merging area detection and achieves a decent accuracy regarding layers merging.",
        "completion3":"A large-scale UI dataset is used to train the MAD and test the performance of UILM.",
        "technologyreview":0.2201182628,
        "venturebeat":0.2122050624,
        "wired":0.054997575,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.13389v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1655568568000,
        "code_mentioned":1,
        "readability":0.78
    },
    {
        "arxiv_id":"2203.07260v2",
        "predicted_newsworthiness":0.4101070333,
        "title":"Graph-Survival: A Survival Analysis Framework for Machine Learning on Temporal Networks",
        "summary":"Continuous time temporal networks are attracting increasing attention due their omnipresence in real-world datasets and they manifold applications. While static network models have been successful in capturing static topological regularities, they often fail to model effects coming from the causal nature that explain the generation of networks. Exploiting the temporal aspect of networks has thus been the focus of various studies in the last decades. We propose a framework for designing generative models for continuous time temporal networks. Assuming a first order Markov assumption on the edge-specific temporal point processes enables us to flexibly apply survival analysis models directly on the waiting time between events, while using time-varying history-based features as covariates for these predictions. This approach links the well-documented field of temporal networks analysis through multivariate point processes, with methodological tools adapted from survival analysis. We propose a fitting method for models within this framework, and an algorithm for simulating new temporal networks having desired properties. We evaluate our method on a downstream future link prediction task, and provide a qualitative assessment of the network simulations.",
        "completion1":"Graph-Survival: A Survival Analysis Framework for Machine Learning on Temporal Networks",
        "completion2":"Continuous time temporal networks are attracting increasing attention due their omnipresence in real-world datasets and they manifold applications.",
        "completion3":"We propose a framework for designing generative models for continuous time temporal networks.",
        "technologyreview":0.195463007,
        "venturebeat":0.1739163017,
        "wired":0.067767091,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.07260v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1647276057000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2202.01340v2",
        "predicted_newsworthiness":0.5687590332,
        "title":"An Artificial Intelligence Dataset for Solar Energy Locations in India",
        "summary":"Rapid development of renewable energy sources, particularly solar photovoltaics (PV), is critical to mitigate climate change. As a result, India has set ambitious goals to install 500 gigawatts of solar energy capacity by 2030. Given the large footprint projected to meet renewables energy targets, the potential for land use conflicts over environmental values is high. To expedite development of solar energy, land use planners will need access to up-to-date and accurate geo-spatial information of PV infrastructure. In this work, we developed a spatially explicit machine learning model to map utility-scale solar projects across India using freely available satellite imagery with a mean accuracy of 92%. Our model predictions were validated by human experts to obtain a dataset of 1363 solar PV farms. Using this dataset, we measure the solar footprint across India and quantified the degree of landcover modification associated with the development of PV infrastructure. Our analysis indicates that over 74% of solar development In India was built on landcover types that have natural ecosystem preservation, or agricultural value.",
        "completion1":"India sets ambitious goal to install 500 gigawatts of solar energy capacity by 2030",
        "completion2":"Spatially explicit machine learning model maps utility-scale solar projects across India with mean accuracy of 92%",
        "completion3":"over 74% of solar development In India was built on landcover types that have natural ecosystem preservation, or agricultural value",
        "technologyreview":0.3299780794,
        "venturebeat":0.2601370525,
        "wired":0.081310674,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.01340v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1643673199000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2201.11379v1",
        "predicted_newsworthiness":0.3596675808,
        "title":"Deep Confidence Guided Distance for 3D Partial Shape Registration",
        "summary":"We present a novel non-iterative learnable method for partial-to-partial 3D shape registration. The partial alignment task is extremely complex, as it jointly tries to match between points and identify which points do not appear in the corresponding shape, causing the solution to be non-unique and ill-posed in most cases. Until now, two principal methodologies have been suggested to solve this problem: sample a subset of points that are likely to have correspondences or perform soft alignment between the point clouds and try to avoid a match to an occluded part. These heuristics work when the partiality is mild or when the transformation is small but fails for severe occlusions or when outliers are present. We present a unique approach named Confidence Guided Distance Network (CGD-net), where we fuse learnable similarity between point embeddings and spatial distance between point clouds, inducing an optimized solution for the overlapping points while ignoring parts that only appear in one of the shapes. The point feature generation is done by a self-supervised architecture that repels far points to have different embeddings, therefore succeeds to align partial views of shapes, even with excessive internal symmetries or acute rotations. We compare our network to recently presented learning-based and axiomatic methods and report a fundamental boost in performance.",
        "completion1":"New AI-based method could revolutionize 3D shape registration",
        "completion2":"CGD-net outperforms existing methods for partial 3D shape registration",
        "completion3":"Self-supervised architecture enables accurate alignment of partial views of shapes",
        "technologyreview":0.1561637103,
        "venturebeat":0.1242828676,
        "wired":0.0155924219,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.11379v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1643272805000,
        "code_mentioned":0,
        "readability":0.85
    },
    {
        "arxiv_id":"2202.04737v1",
        "predicted_newsworthiness":0.7624859502,
        "title":"Telegram Monitor: Monitoring Brazilian Political Groups and Channels on Telegram",
        "summary":"Instant messaging platforms such as Telegram became one of the main means of communication used by people all over the world. Most of them are home of several groups and channels that connect thousands of people focused on political topics. However, they have suffered with misinformation campaigns with a direct impact on electoral processes around the world. While some platforms, such as WhatsApp, took restrictive policies and measures to attenuate the issues arising from the abuse of their systems, others have emerged as alternatives, presenting little or no restrictions on content moderation or actions in combating misinformation. Telegram is one of those systems, which has been attracting more users and gaining popularity. In this work, we present the \"Telegram Monitor\", a web-based system that monitors the political debate in this environment and enables the analysis of the most shared content in multiple channels and public groups. Our system aims to allow journalists, researchers, and fact-checking agencies to identify trending conspiracy theories, misinformation campaigns, or simply to monitor the political debate in this space along the 2022 Brazilian elections. We hope our system can assist the combat of misinformation spreading through Telegram in Brazil.",
        "completion1":"Telegram Monitor\" Aims to Help Combat Misinformation in Brazil",
        "completion2":"New Web-Based System Monitors Political Telegram Groups and Channels",
        "completion3":"Telegram Monitor Could Help Fact-Checkers During Brazilian Elections",
        "technologyreview":0.4076719596,
        "venturebeat":0.3079479408,
        "wired":0.1267770016,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.04737v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1644442433000,
        "code_mentioned":0,
        "readability":0.93
    },
    {
        "arxiv_id":"2202.13101v1",
        "predicted_newsworthiness":0.6409665167,
        "title":"Sustainability using Renewable Electricity (SuRE) towards NetZero Emissions",
        "summary":"Demand for energy has increased significantly across the globe due to increase in population and economic growth. Growth in energy demand poses serious threat to the environment since majority of the energy sources are non-renewable and based on fossil fuels, which leads to emission of harmful greenhouse gases. Organizations across the world are facing challenges in transitioning from fossil fuels-based sources to greener sources to reduce their carbon footprint. As a step towards achieving Net-Zero emission target, we present a scalable AI based solution that can be used by organizations to increase their overall renewable electricity share in total energy consumption. Our solution provides facilities with accurate energy demand forecast, recommendation for procurement of renewable electricity to optimize cost and carbon offset recommendations to compensate for Greenhouse Gas (GHG) emissions. This solution has been used in production for more than a year for four facilities and has increased their renewable electricity share significantly.",
        "completion1":"Global Warming Solution Found?",
        "completion2":"New AI Technology Averts Climate Change",
        "completion3":"Sustainability Using Renewable Electricity (SuRE) Towards NetZero Emissions",
        "technologyreview":0.3836588342,
        "venturebeat":0.3339650743,
        "wired":0.0850165554,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.13101v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1645869866000,
        "code_mentioned":1,
        "readability":0.91
    },
    {
        "arxiv_id":"2207.00349v1",
        "predicted_newsworthiness":0.4636484891,
        "title":"Vers la compr\u00e9hension automatique de la parole bout-en-bout \u00e0 moindre effort",
        "summary":"Recent advances in spoken language understanding benefited from Self-Supervised models trained on large speech corpora. For French, the LeBenchmark project has made such models available and has led to impressive progress on several tasks including spoken language understanding. These advances have a non-negligible cost in terms of computation time and energy consumption. In this paper, we compare several learning strategies aiming at reducing such cost while keeping competitive performances. The experiments are performed on the MEDIA corpus, and show that it is possible to reduce the learning cost while maintaining state-of-the-art performances.",
        "completion1":"Spoken language understanding made easier with Self-Supervised models",
        "completion2":"French LeBenchmark project impressive results in spoken language understanding",
        "completion3":"Learning strategies to reduce computation time and energy consumption",
        "technologyreview":0.2269691973,
        "venturebeat":0.2063775078,
        "wired":0.0759438768,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.00349v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1656674999000,
        "code_mentioned":1,
        "readability":0.89
    },
    {
        "arxiv_id":"2203.03397v3",
        "predicted_newsworthiness":0.5204237495,
        "title":"OverlapTransformer: An Efficient and Rotation-Invariant Transformer Network for LiDAR-Based Place Recognition",
        "summary":"Place recognition is an important capability for autonomously navigating vehicles operating in complex environments and under changing conditions. It is a key component for tasks such as loop closing in SLAM or global localization. In this paper, we address the problem of place recognition based on 3D LiDAR scans recorded by an autonomous vehicle. We propose a novel lightweight neural network exploiting the range image representation of LiDAR sensors to achieve fast execution with less than 2 ms per frame. We design a yaw-angle-invariant architecture exploiting a transformer network, which boosts the place recognition performance of our method. We evaluate our approach on the KITTI and Ford Campus datasets. The experimental results show that our method can effectively detect loop closures compared to the state-of-the-art methods and generalizes well across different environments. To evaluate long-term place recognition performance, we provide a novel dataset containing LiDAR sequences recorded by a mobile robot in repetitive places at different times. The implementation of our method and dataset are released here: https:\/\/github.com\/haomo-ai\/OverlapTransformer",
        "completion1":"A novel, lightweight neural network for efficient place recognition in 3D LiDAR scans is proposed.",
        "completion2":"The transformer network-based architecture is yaw-angle invariant and boosts performance.",
        "completion3":"Experimental results on multiple datasets show the effectiveness of the proposed method.",
        "technologyreview":0.2710549139,
        "venturebeat":0.2228852666,
        "wired":0.0678422258,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.03397v3",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1646660949000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2206.00856v1",
        "predicted_newsworthiness":0.5319490229,
        "title":"MentSum: A Resource for Exploring Summarization of Mental Health Online Posts",
        "summary":"Mental health remains a significant challenge of public health worldwide. With increasing popularity of online platforms, many use the platforms to share their mental health conditions, express their feelings, and seek help from the community and counselors. Some of these platforms, such as Reachout, are dedicated forums where the users register to seek help. Others such as Reddit provide subreddits where the users publicly but anonymously post their mental health distress. Although posts are of varying length, it is beneficial to provide a short, but informative summary for fast processing by the counselors. To facilitate research in summarization of mental health online posts, we introduce Mental Health Summarization dataset, MentSum, containing over 24k carefully selected user posts from Reddit, along with their short user-written summary (called TLDR) in English from 43 mental health subreddits. This domain-specific dataset could be of interest not only for generating short summaries on Reddit, but also for generating summaries of posts on the dedicated mental health forums such as Reachout. We further evaluate both extractive and abstractive state-of-the-art summarization baselines in terms of Rouge scores, and finally conduct an in-depth human evaluation study of both user-written and system-generated summaries, highlighting challenges in this research.",
        "completion1":"MentSum: A Resource for Exploring Summarization of Mental Health Online Posts.",
        "completion2":"With increasing popularity of online platforms, many use the platforms to share their mental health conditions, express their feelings, and seek help from the community and counselors.",
        "completion3":"To facilitate research in summarization of mental health online posts, we introduce Mental Health Summarization dataset, MentSum, containing over 24k carefully selected user posts from Reddit, along with their short user-written summary (called TLDR) in English from 43 mental health subreddits.",
        "technologyreview":0.2638872709,
        "venturebeat":0.2165814451,
        "wired":0.0845241788,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.00856v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1654139314000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2204.04148v1",
        "predicted_newsworthiness":0.4749034839,
        "title":"Process Mining on Uncertain Event Data",
        "summary":"With the widespread adoption of process mining in organizations, the field of process science is seeing an increase in the demand for ad-hoc analysis techniques of non-standard event data. An example of such data are uncertain event data: events characterized by a described and quantified attribute imprecision. This paper outlines a research project aimed at developing process mining techniques able to extract insights from uncertain data. We set the basis for this research topic, recapitulate the available literature, and define a future outlook.",
        "completion1":"Process Mining on Uncertain Event Data: Setting the Basis",
        "completion2":"Recapitulating the Available Literature",
        "completion3":"Defining a Future Outlook",
        "technologyreview":0.1443147477,
        "venturebeat":0.1618192133,
        "wired":0.0218101966,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.04148v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1649433360000,
        "code_mentioned":1,
        "readability":0.88
    },
    {
        "arxiv_id":"2207.00874v1",
        "predicted_newsworthiness":0.5240045215,
        "title":"Neural Networks for Path Planning",
        "summary":"The scientific community is able to present a new set of solutions to practical problems that substantially improve the performance of modern technology in terms of efficiency and speed of computation due to the advancement in neural networks architectures. We present the latest works considering the utilization of neural networks in robot path planning. Our survey shows the contrast between different formulations of the problems that consider different inputs, outputs, and environments and how different neural networks architectures are able to provide solutions to all of the presented problems.",
        "completion1":"Neural Networks Could Help Robots Get Around More Quickly",
        "completion2":"New Solutions to Path Planning Problem Show Promise",
        "completion3":"Neural Networks Advancement Could Speed Up Computation",
        "technologyreview":0.2596196753,
        "venturebeat":0.1970435376,
        "wired":0.0224487392,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.00874v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.ro"
        ],
        "published":1656778393000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2202.08465v1",
        "predicted_newsworthiness":0.4113669643,
        "title":"End-to-End Training of Both Translation Models in the Back-Translation Framework",
        "summary":"Semi-supervised learning algorithms in neural machine translation (NMT) have significantly improved translation quality compared to the supervised learning algorithms by using additional monolingual corpora. Among them, back-translation is a theoretically well-structured and cutting-edge method. Given two pre-trained NMT models between source and target languages, one translates a monolingual sentence as a latent sentence, and the other reconstructs the monolingual input sentence given the latent sentence. Therefore, previous works tried to apply the variational auto-encoder's (VAE) training framework to the back-translation framework. However, the discrete property of the latent sentence made it impossible to use backpropagation in the framework. This paper proposes a categorical reparameterization trick that generates a differentiable sentence, with which we practically implement the VAE's training framework for the back-translation and train it by end-to-end backpropagation. In addition, we propose several regularization techniques that are especially advantageous to this framework. In our experiments, we demonstrate that our method makes backpropagation available through the latent sentences and improves the BLEU scores on the datasets of the WMT18 translation task.",
        "completion1":"End-to-End Training of Both Translation Models in the Back-Translation Framework",
        "completion2":"Semi-supervised learning algorithms in neural machine translation improve translation quality",
        "completion3":"New method makes backpropagation available through latent sentences",
        "technologyreview":0.2223790683,
        "venturebeat":0.1678265257,
        "wired":0.0447645604,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.08465v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1645079463000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2203.09416v2",
        "predicted_newsworthiness":0.3908245107,
        "title":"Bi-directional Object-context Prioritization Learning for Saliency Ranking",
        "summary":"The saliency ranking task is recently proposed to study the visual behavior that humans would typically shift their attention over different objects of a scene based on their degrees of saliency. Existing approaches focus on learning either object-object or object-scene relations. Such a strategy follows the idea of object-based attention in Psychology, but it tends to favor those objects with strong semantics (e.g., humans), resulting in unrealistic saliency ranking. We observe that spatial attention works concurrently with object-based attention in the human visual recognition system. During the recognition process, the human spatial attention mechanism would move, engage, and disengage from region to region (i.e., context to context). This inspires us to model the region-level interactions, in addition to the object-level reasoning, for saliency ranking. To this end, we propose a novel bi-directional method to unify spatial attention and object-based attention for saliency ranking. Our model includes two novel modules: (1) a selective object saliency (SOS) module that models objectbased attention via inferring the semantic representation of the salient object, and (2) an object-context-object relation (OCOR) module that allocates saliency ranks to objects by jointly modeling the object-context and context-object interactions of the salient objects. Extensive experiments show that our approach outperforms existing state-of-theart methods. Our code and pretrained model are available at https:\/\/github.com\/GrassBro\/OCOR.",
        "completion1":"Researchers develop a new bi-directional object-context prioritization learning method for saliency ranking that outperforms existing state-of-the-art methods.",
        "completion2":"This novel approach unifies spatial attention and object-based attention for improved saliency ranking.",
        "completion3":"The new model includes a selective object saliency module and an object-context-object relation module for better object allocation.",
        "technologyreview":0.1835174401,
        "venturebeat":0.1477453192,
        "wired":0.0441962342,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.09416v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1647533763000,
        "code_mentioned":1,
        "readability":0.79
    },
    {
        "arxiv_id":"2202.00718v2",
        "predicted_newsworthiness":0.3747515098,
        "title":"Personalized Federated Learning via Convex Clustering",
        "summary":"We propose a parametric family of algorithms for personalized federated learning with locally convex user costs. The proposed framework is based on a generalization of convex clustering in which the differences between different users' models are penalized via a sum-of-norms penalty, weighted by a penalty parameter $\\lambda$. The proposed approach enables \"automatic\" model clustering, without prior knowledge of the hidden cluster structure, nor the number of clusters. Analytical bounds on the weight parameter, that lead to simultaneous personalization, generalization and automatic model clustering are provided. The solution to the formulated problem enables personalization, by providing different models across different clusters, and generalization, by providing models different than the per-user models computed in isolation. We then provide an efficient algorithm based on the Parallel Direction Method of Multipliers (PDMM) to solve the proposed formulation in a federated server-users setting. Numerical experiments corroborate our findings. As an interesting byproduct, our results provide several generalizations to convex clustering.",
        "completion1":"A new parametric family of algorithms for personalized federated learning with locally convex user costs",
        "completion2":"The proposed framework is based on a generalization of convex clustering",
        "completion3":"The solution to the formulated problem enables personalization, by providing different models across different clusters",
        "technologyreview":0.1831458625,
        "venturebeat":0.1864009921,
        "wired":0.0287541701,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.00718v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1643743531000,
        "code_mentioned":0,
        "readability":0.76
    },
    {
        "arxiv_id":"2207.01377v5",
        "predicted_newsworthiness":0.5072076108,
        "title":"Detection of ADHD based on Eye Movements during Natural Viewing",
        "summary":"Attention-deficit\/hyperactivity disorder (ADHD) is a neurodevelopmental disorder that is highly prevalent and requires clinical specialists to diagnose. It is known that an individual's viewing behavior, reflected in their eye movements, is directly related to attentional mechanisms and higher-order cognitive processes. We therefore explore whether ADHD can be detected based on recorded eye movements together with information about the video stimulus in a free-viewing task. To this end, we develop an end-to-end deep learning-based sequence model which we pre-train on a related task for which more data are available. We find that the method is in fact able to detect ADHD and outperforms relevant baselines. We investigate the relevance of the input features in an ablation study. Interestingly, we find that the model's performance is closely related to the content of the video, which provides insights for future experimental designs.",
        "completion1":"ADHD can be detected based on recorded eye movements, according to new study.",
        "completion2":"Study finds that deep learning-based sequence models can detect ADHD with high accuracy.",
        "completion3":"Ablation study reveals that model performance is closely related to content of video stimulus.",
        "technologyreview":0.2782782473,
        "venturebeat":0.2270079388,
        "wired":0.0582405171,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.01377v5",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1656939364000,
        "code_mentioned":1,
        "readability":0.87
    },
    {
        "arxiv_id":"2204.13574v2",
        "predicted_newsworthiness":0.6016125405,
        "title":"An Explainable Regression Framework for Predicting Remaining Useful Life of Machines",
        "summary":"Prediction of a machine's Remaining Useful Life (RUL) is one of the key tasks in predictive maintenance. The task is treated as a regression problem where Machine Learning (ML) algorithms are used to predict the RUL of machine components. These ML algorithms are generally used as a black box with a total focus on the performance without identifying the potential causes behind the algorithms' decisions and their working mechanism. We believe, the performance (in terms of Mean Squared Error (MSE), etc.,) alone is not enough to build the trust of the stakeholders in ML prediction rather more insights on the causes behind the predictions are needed. To this aim, in this paper, we explore the potential of Explainable AI (XAI) techniques by proposing an explainable regression framework for the prediction of machines' RUL. We also evaluate several ML algorithms including classical and Neural Networks (NNs) based solutions for the task. For the explanations, we rely on two model agnostic XAI methods namely Local Interpretable Model-Agnostic Explanations (LIME) and Shapley Additive Explanations (SHAP). We believe, this work will provide a baseline for future research in the domain.",
        "completion1":"An Explainable Regression Framework for Predicting Remaining Useful Life of Machines",
        "completion2":"Prediction of a machine's Remaining Useful Life is one of the key tasks in predictive maintenance",
        "completion3":"The task is treated as a regression problem where Machine Learning algorithms are used to predict the RUL of machine components",
        "technologyreview":0.3211712303,
        "venturebeat":0.2769970111,
        "wired":0.0744269846,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.13574v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1651160652000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2207.01112v1",
        "predicted_newsworthiness":0.4838352846,
        "title":"Augment to Detect Anomalies with Continuous Labelling",
        "summary":"Anomaly detection is to recognize samples that differ in some respect from the training observations. These samples which do not conform to the distribution of normal data are called outliers or anomalies. In real-world anomaly detection problems, the outliers are absent, not well defined, or have a very limited number of instances. Recent state-of-the-art deep learning-based anomaly detection methods suffer from high computational cost, complexity, unstable training procedures, and non-trivial implementation, making them difficult to deploy in real-world applications. To combat this problem, we leverage a simple learning procedure that trains a lightweight convolutional neural network, reaching state-of-the-art performance in anomaly detection. In this paper, we propose to solve anomaly detection as a supervised regression problem. We label normal and anomalous data using two separable distributions of continuous values. To compensate for the unavailability of anomalous samples during training time, we utilize straightforward image augmentation techniques to create a distinct set of samples as anomalies. The distribution of the augmented set is similar but slightly deviated from the normal data, whereas real anomalies are expected to have an even further distribution. Therefore, training a regressor on these augmented samples will result in more separable distributions of labels for normal and real anomalous data points. Anomaly detection experiments on image and video datasets show the superiority of the proposed method over the state-of-the-art approaches.",
        "completion1":"New AI detects anomalies with continuous labelling",
        "completion2":"State-of-the-art deep learning-based anomaly detection method suffers from high computational cost, complexity, unstable training procedures, and non-trivial implementation, making them difficult to deploy in real-world applications.",
        "completion3":"To combat this problem, we leverage a simple learning procedure that trains a lightweight convolutional neural network, reaching state-of-the-art performance in anomaly detection.",
        "technologyreview":0.2823275879,
        "venturebeat":0.236057072,
        "wired":0.0711583717,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.01112v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1656879111000,
        "code_mentioned":0,
        "readability":0.82
    },
    {
        "arxiv_id":"2202.06315v2",
        "predicted_newsworthiness":0.5619646667,
        "title":"Towards Decentralised Cloud Storage with IPFS: Opportunities, Challenges, and Future Directions",
        "summary":"The InterPlanetary File System (IPFS) is a novel decentralised storage architecture, which attempts to provide decentralised cloud storage by building on founding principles of P2P networking and content addressing. IPFS is used by more than 230k peers per week and serves tens of millions of requests per day, which makes it an interesting large-scale operational network to study. While it is used as a building block in several projects and studies, its inner workings, properties, and implications have only been marginally explored in research. Thus, we provide an overview of the IPFS design and its core features, along with the opportunities that it opens as well as the challenges that it faces because of its properties. Overall, IPFS presents an interesting set of characteristics and offers lessons which can help building decentralised systems of the future.",
        "completion1":"Design and overview of IPFS.",
        "completion2":"Opportunities that IPFS offers for decentralised systems.",
        "completion3":"Challenges that IPFS faces because of its properties.",
        "technologyreview":0.2255073356,
        "venturebeat":0.234311765,
        "wired":0.0858980811,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.06315v2",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1644760489000,
        "code_mentioned":0,
        "readability":0.89
    },
    {
        "arxiv_id":"2207.02518v1",
        "predicted_newsworthiness":0.3788283895,
        "title":"Compositional Generalization in Grounded Language Learning via Induced Model Sparsity",
        "summary":"We provide a study of how induced model sparsity can help achieve compositional generalization and better sample efficiency in grounded language learning problems. We consider simple language-conditioned navigation problems in a grid world environment with disentangled observations. We show that standard neural architectures do not always yield compositional generalization. To address this, we design an agent that contains a goal identification module that encourages sparse correlations between words in the instruction and attributes of objects, composing them together to find the goal. The output of the goal identification module is the input to a value iteration network planner. Our agent maintains a high level of performance on goals containing novel combinations of properties even when learning from a handful of demonstrations. We examine the internal representations of our agent and find the correct correspondences between words in its dictionary and attributes in the environment.",
        "completion1":"Induced model sparsity encourages compositional generalization in grounded language learning.",
        "completion2":"Standard neural architectures often fail to produce compositional generalization.",
        "completion3":"Our agent maintains high performance on goals containing novel combinations of properties.",
        "technologyreview":0.2612379922,
        "venturebeat":0.1923151048,
        "wired":0.0441826086,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.02518v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1657097187000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2202.09348v1",
        "predicted_newsworthiness":0.5154190023,
        "title":"A Machine Learning Paradigm for Studying Pictorial Realism: Are Constable's Clouds More Real than His Contemporaries?",
        "summary":"European artists have sought to create life-like images since the Renaissance. The techniques used by artists to impart realism to their paintings often rely on approaches based in mathematics, like linear perspective; yet the means used to assess the verisimilitude of realist paintings have remained subjective, even intuitive. An exploration of alternative and relatively objective methods for evaluating pictorial realism could enhance existing art historical research. We propose a machine-learning-based paradigm for studying pictorial realism in an explainable way. Unlike subjective evaluations made by art historians or computer-based painting analysis exploiting inexplicable learned features, our framework assesses realism by measuring the similarity between clouds painted by exceptionally skillful 19th-century landscape painters like John Constable and photographs of clouds. The experimental results of cloud classification show that Constable approximates more consistently than his contemporaries the formal features of actual clouds in his paintings. Our analyses suggest that artists working in the decades leading up to the invention of photography worked in a mode that anticipated some of the stylistic features of photography. The study is a springboard for deeper analyses of pictorial realism using computer vision and machine learning.",
        "completion1":"Constable's Clouds More Real Than His Contemporaries, Say Researchers",
        "completion2":"Machine Learning Paradigm Could Enhance Art Historical Research",
        "completion3":"Study Suggests Artists of the pre-Photography Era Worked in a Mode That Anticipated Some Features of Photography",
        "technologyreview":0.2974505512,
        "venturebeat":0.2139043981,
        "wired":0.076982956,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.09348v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1645209361000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2204.03827v1",
        "predicted_newsworthiness":0.3790239352,
        "title":"IA-GCN: Interactive Graph Convolutional Network for Recommendation",
        "summary":"Recently, Graph Convolutional Network (GCN) has become a novel state-of-art for Collaborative Filtering (CF) based Recommender Systems (RS). It is a common practice to learn informative user and item representations by performing embedding propagation on a user-item bipartite graph, and then provide the users with personalized item suggestions based on the representations. Despite effectiveness, existing algorithms neglect precious interactive features between user-item pairs in the embedding process. When predicting a user's preference for different items, they still aggregate the user tree in the same way, without emphasizing target-related information in the user neighborhood. Such a uniform aggregation scheme easily leads to suboptimal user and item representations, limiting the model expressiveness to some extent. In this work, we address this problem by building bilateral interactive guidance between each user-item pair and proposing a new model named IA-GCN (short for InterActive GCN). Specifically, when learning the user representation from its neighborhood, we assign higher attention weights to those neighbors similar to the target item. Correspondingly, when learning the item representation, we pay more attention to those neighbors resembling the target user. This leads to interactive and interpretable features, effectively distilling target-specific information through each graph convolutional operation. Our model is built on top of LightGCN, a state-of-the-art GCN model for CF, and can be combined with various GCN-based CF architectures in an end-to-end fashion. Extensive experiments on three benchmark datasets demonstrate the effectiveness and robustness of IA-GCN.",
        "completion1":"IA-GCN: Interactive Graph Convolutional Network for Recommendation",
        "completion2":"Graph Convolutional Network becomes state-of-art for Collaborative Filtering based Recommender Systems",
        "completion3":"IA-GCN addresses problem of suboptimal user and item representations by building bilateral interactive guidance between each user-item pair",
        "technologyreview":0.2027131505,
        "venturebeat":0.188731203,
        "wired":0.0591978412,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.03827v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.ai"
        ],
        "published":1649389089000,
        "code_mentioned":0,
        "readability":0.78
    },
    {
        "arxiv_id":"2203.03903v1",
        "predicted_newsworthiness":0.4646654653,
        "title":"InstructionNER: A Multi-Task Instruction-Based Generative Framework for Few-shot NER",
        "summary":"Recently, prompt-based methods have achieved significant performance in few-shot learning scenarios by bridging the gap between language model pre-training and fine-tuning for downstream tasks. However, existing prompt templates are mostly designed for sentence-level tasks and are inappropriate for sequence labeling objectives. To address the above issue, we propose a multi-task instruction-based generative framework, named InstructionNER, for low-resource named entity recognition. Specifically, we reformulate the NER task as a generation problem, which enriches source sentences with task-specific instructions and answer options, then inferences the entities and types in natural language. We further propose two auxiliary tasks, including entity extraction and entity typing, which enable the model to capture more boundary information of entities and deepen the understanding of entity type semantics, respectively. Experimental results show that our method consistently outperforms other baselines on five datasets in few-shot settings.",
        "completion1":"InstructionNER: A Multi-Task Instruction-Based Generative Framework for Few-shot NER",
        "completion2":"New Framework Achieves Significant Performance in Few-shot Learning Scenarios",
        "completion3":"InstructionNER Outperforms Other Baselines on Five Datasets in Few-shot Settings",
        "technologyreview":0.1916883754,
        "venturebeat":0.1775832203,
        "wired":0.0609195691,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.03903v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1646726196000,
        "code_mentioned":0,
        "readability":0.81
    },
    {
        "arxiv_id":"2207.06827v1",
        "predicted_newsworthiness":0.4047470516,
        "title":"Point-to-Box Network for Accurate Object Detection via Single Point Supervision",
        "summary":"Object detection using single point supervision has received increasing attention over the years. In this paper, we attribute such a large performance gap to the failure of generating high-quality proposal bags which are crucial for multiple instance learning (MIL). To address this problem, we introduce a lightweight alternative to the off-the-shelf proposal (OTSP) method and thereby create the Point-to-Box Network (P2BNet), which can construct an inter-objects balanced proposal bag by generating proposals in an anchor-like way. By fully investigating the accurate position information, P2BNet further constructs an instance-level bag, avoiding the mixture of multiple objects. Finally, a coarse-to-fine policy in a cascade fashion is utilized to improve the IoU between proposals and ground-truth (GT). Benefiting from these strategies, P2BNet is able to produce high-quality instance-level bags for object detection. P2BNet improves the mean average precision (AP) by more than 50% relative to the previous best PSOD method on the MS COCO dataset. It also demonstrates the great potential to bridge the performance gap between point supervised and bounding-box supervised detectors. The code will be released at github.com\/ucas-vg\/P2BNet.",
        "completion1":"Point-to-Box Network for Accurate Object Detection via Single Point Supervision",
        "completion2":"Object detection using single point supervision has received increasing attention over the years",
        "completion3":"P2BNet improves the mean average precision by more than 50% relative to the previous best PSOD method on the MS COCO dataset.",
        "technologyreview":0.2097453859,
        "venturebeat":0.1680566075,
        "wired":0.0416966484,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.06827v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1657798320000,
        "code_mentioned":1,
        "readability":0.87
    },
    {
        "arxiv_id":"2203.09116v2",
        "predicted_newsworthiness":0.4145434483,
        "title":"MotionAug: Augmentation with Physical Correction for Human Motion Prediction",
        "summary":"This paper presents a motion data augmentation scheme incorporating motion synthesis encouraging diversity and motion correction imposing physical plausibility. This motion synthesis consists of our modified Variational AutoEncoder (VAE) and Inverse Kinematics (IK). In this VAE, our proposed sampling-near-samples method generates various valid motions even with insufficient training motion data. Our IK-based motion synthesis method allows us to generate a variety of motions semi-automatically. Since these two schemes generate unrealistic artifacts in the synthesized motions, our motion correction rectifies them. This motion correction scheme consists of imitation learning with physics simulation and subsequent motion debiasing. For this imitation learning, we propose the PD-residual force that significantly accelerates the training process. Furthermore, our motion debiasing successfully offsets the motion bias induced by imitation learning to maximize the effect of augmentation. As a result, our method outperforms previous noise-based motion augmentation methods by a large margin on both Recurrent Neural Network-based and Graph Convolutional Network-based human motion prediction models. The code is available at https:\/\/github.com\/meaten\/MotionAug.",
        "completion1":"New motion data augmentation scheme encourages diversity and imposes physical plausibility",
        "completion2":"Modified Variational AutoEncoder generates various valid motions even with insufficient training motion data",
        "completion3":"Inverse Kinematics-based motion synthesis method allows us to generate a variety of motions semi-automatically",
        "technologyreview":0.2317127233,
        "venturebeat":0.1939853068,
        "wired":0.0574210393,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.09116v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1647499995000,
        "code_mentioned":1,
        "readability":0.76
    },
    {
        "arxiv_id":"2206.05314v1",
        "predicted_newsworthiness":0.4823394465,
        "title":"Large-Scale Retrieval for Reinforcement Learning",
        "summary":"Effective decision making involves flexibly relating past experiences and relevant contextual information to a novel situation. In deep reinforcement learning, the dominant paradigm is for an agent to amortise information that helps decision-making into its network weights via gradient descent on training losses. Here, we pursue an alternative approach in which agents can utilise large-scale context-sensitive database lookups to support their parametric computations. This allows agents to directly learn in an end-to-end manner to utilise relevant information to inform their outputs. In addition, new information can be attended to by the agent, without retraining, by simply augmenting the retrieval dataset. We study this approach in Go, a challenging game for which the vast combinatorial state space privileges generalisation over direct matching to past experiences. We leverage fast, approximate nearest neighbor techniques in order to retrieve relevant data from a set of tens of millions of expert demonstration states. Attending to this information provides a significant boost to prediction accuracy and game-play performance over simply using these demonstrations as training trajectories, providing a compelling demonstration of the value of large-scale retrieval in reinforcement learning agents.",
        "completion1":"Large-Scale Retrieval for Reinforcement Learning Yields Compelling Performance Boost",
        "completion2":"Agent Learns to Use Relevant Information from Large-Scale Database to Improve Decision Making",
        "completion3":"Neural Network Trained on Expert Demonstrations Can Use Fast Approximate Nearest Neighbor Methods for Large-Scale Retrieval",
        "technologyreview":0.3171672428,
        "venturebeat":0.25811726,
        "wired":0.06795111,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.05314v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1654885530000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2203.05402v1",
        "predicted_newsworthiness":0.3769505424,
        "title":"Representation Compensation Networks for Continual Semantic Segmentation",
        "summary":"In this work, we study the continual semantic segmentation problem, where the deep neural networks are required to incorporate new classes continually without catastrophic forgetting. We propose to use a structural re-parameterization mechanism, named representation compensation (RC) module, to decouple the representation learning of both old and new knowledge. The RC module consists of two dynamically evolved branches with one frozen and one trainable. Besides, we design a pooled cube knowledge distillation strategy on both spatial and channel dimensions to further enhance the plasticity and stability of the model. We conduct experiments on two challenging continual semantic segmentation scenarios, continual class segmentation and continual domain segmentation. Without any extra computational overhead and parameters during inference, our method outperforms state-of-the-art performance. The code is available at \\url{https:\/\/github.com\/zhangchbin\/RCIL}.",
        "completion1":"Representation Compensation Networks for Continual Semantic Segmentation",
        "completion2":"RC module consists of two dynamically evolved branches with one frozen and one trainable",
        "completion3":"We conduct experiments on two challenging continual semantic segmentation scenarios, continual class segmentation and continual domain segmentation",
        "technologyreview":0.1799854125,
        "venturebeat":0.1327799692,
        "wired":0.0189483988,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.05402v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1646923721000,
        "code_mentioned":1,
        "readability":0.75
    },
    {
        "arxiv_id":"2203.06918v2",
        "predicted_newsworthiness":0.4424522029,
        "title":"Uncertainty-Aware Text-to-Program for Question Answering on Structured Electronic Health Records",
        "summary":"Question Answering on Electronic Health Records (EHR-QA) has a significant impact on the healthcare domain, and it is being actively studied. Previous research on structured EHR-QA focuses on converting natural language queries into query language such as SQL or SPARQL (NLQ2Query), so the problem scope is limited to pre-defined data types by the specific query language. In order to expand the EHR-QA task beyond this limitation to handle multi-modal medical data and solve complex inference in the future, more primitive systemic language is needed. In this paper, we design the program-based model (NLQ2Program) for EHR-QA as the first step towards the future direction. We tackle MIMICSPARQL*, the graph-based EHR-QA dataset, via a program-based approach in a semi-supervised manner in order to overcome the absence of gold programs. Without the gold program, our proposed model shows comparable performance to the previous state-of-the-art model, which is an NLQ2Query model (0.9% gain). In addition, for a reliable EHR-QA model, we apply the uncertainty decomposition method to measure the ambiguity in the input question. We empirically confirmed data uncertainty is most indicative of the ambiguity in the input question.",
        "completion1":"New model for question answering on structured electronic health records",
        "completion2":"Uncertainty-aware text-to-program for question answering on structured electronic health records",
        "completion3":"Program-based approach to question answering on structured electronic health records",
        "technologyreview":0.2404541389,
        "venturebeat":0.2159761362,
        "wired":0.0392906983,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.06918v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai",
            "cs.lg"
        ],
        "published":1647245536000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2202.06027v1",
        "predicted_newsworthiness":0.4114972944,
        "title":"End-to-end Reinforcement Learning of Robotic Manipulation with Robust Keypoints Representation",
        "summary":"We present an end-to-end Reinforcement Learning(RL) framework for robotic manipulation tasks, using a robust and efficient keypoints representation. The proposed method learns keypoints from camera images as the state representation, through a self-supervised autoencoder architecture. The keypoints encode the geometric information, as well as the relationship of the tool and target in a compact representation to ensure efficient and robust learning. After keypoints learning, the RL step then learns the robot motion from the extracted keypoints state representation. The keypoints and RL learning processes are entirely done in the simulated environment. We demonstrate the effectiveness of the proposed method on robotic manipulation tasks including grasping and pushing, in different scenarios. We also investigate the generalization capability of the trained model. In addition to the robust keypoints representation, we further apply domain randomization and adversarial training examples to achieve zero-shot sim-to-real transfer in real-world robotic manipulation tasks.",
        "completion1":"End-to-end Reinforcement Learning of Robotic Manipulation with Robust Keypoints Representation",
        "completion2":"Proposed Method Learns Keypoints from Camera Images as State Representation",
        "completion3":"RL Step Then Learns Robot Motion from Extracted Keypoints State Representation",
        "technologyreview":0.2647182032,
        "venturebeat":0.2059358796,
        "wired":0.047183583,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.06027v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv"
        ],
        "published":1644659889000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2202.13986v1",
        "predicted_newsworthiness":0.3609128732,
        "title":"Contact-Implicit Trajectory Optimization with Hydroelastic Contact and iLQR",
        "summary":"Contact-implicit trajectory optimization offers an appealing method of automatically generating complex and contact-rich behaviors for robot manipulation and locomotion. The scalability of such techniques has been limited, however, by the challenge of ensuring both numerical reliability and physical realism. In this paper, we present preliminary results suggesting that the Iterative Linear Quadratic Regulator (iLQR) algorithm together with the recently proposed pressure-field-based hydroelastic contact model enables reliable and physically realistic trajectory optimization through contact. We use this approach synthesize contact-rich behaviors like quadruped locomotion and whole-arm manipulation. Furthermore, open-loop playback on a Kinova Gen3 robot arm demonstrates the physical accuracy of the whole-arm manipulation trajectories. Code is available at https:\/\/bit.ly\/ilqr_hc and videos can be found at https:\/\/youtu.be\/IqxJKbM8_ms .",
        "completion1":"Contact-Implicit Trajectory Optimization offers method of automatically generating complex behaviors for robot manipulation and locomotion",
        "completion2":"The scalability of such techniques has been limited by the challenge of ensuring both numerical reliability and physical realism",
        "completion3":"In this paper, we present preliminary results suggesting that the Iterative Linear Quadratic Regulator (iLQR) algorithm together with the recently proposed pressure-field-based hydroelastic contact model enables reliable and physically realistic trajectory optimization through contact.",
        "technologyreview":0.1608973117,
        "venturebeat":0.1190676334,
        "wired":0.0174508883,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.13986v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1646070133000,
        "code_mentioned":1,
        "readability":0.78
    },
    {
        "arxiv_id":"2207.07920v1",
        "predicted_newsworthiness":0.3979169395,
        "title":"Physics Embedded Neural Network Vehicle Model and Applications in Risk-Aware Autonomous Driving Using Latent Features",
        "summary":"Non-holonomic vehicle motion has been studied extensively using physics-based models. Common approaches when using these models interpret the wheel\/ground interactions using a linear tire model and thus may not fully capture the nonlinear and complex dynamics under various environments. On the other hand, neural network models have been widely employed in this domain, demonstrating powerful function approximation capabilities. However, these black-box learning strategies completely abandon the existing knowledge of well-known physics. In this paper, we seamlessly combine deep learning with a fully differentiable physics model to endow the neural network with available prior knowledge. The proposed model shows better generalization performance than the vanilla neural network model by a large margin. We also show that the latent features of our model can accurately represent lateral tire forces without the need for any additional training. Lastly, We develop a risk-aware model predictive controller using proprioceptive information derived from the latent features. We validate our idea in two autonomous driving tasks under unknown friction, outperforming the baseline control framework.",
        "completion1":"New AI-based model offers improved accuracy and performance for autonomous vehicles.",
        "completion2":"Researchers develop risk-aware autonomous driving system using physics and neural networks.",
        "completion3":"New controller offers safer driving in unknown conditions for autonomous vehicles.",
        "technologyreview":0.2534207651,
        "venturebeat":0.1839478172,
        "wired":0.051303382,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.07920v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai"
        ],
        "published":1657973215000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2206.01417v1",
        "predicted_newsworthiness":0.4234587169,
        "title":"Learning an Adaptation Function to Assess Image Visual Similarities",
        "summary":"Human perception is routinely assessing the similarity between images, both for decision making and creative thinking. But the underlying cognitive process is not really well understood yet, hence difficult to be mimicked by computer vision systems. State-of-the-art approaches using deep architectures are often based on the comparison of images described as feature vectors learned for image categorization task. As a consequence, such features are powerful to compare semantically related images but not really efficient to compare images visually similar but semantically unrelated. Inspired by previous works on neural features adaptation to psycho-cognitive representations, we focus here on the specific task of learning visual image similarities when analogy matters. We propose to compare different supervised, semi-supervised and self-supervised networks, pre-trained on distinct scales and contents datasets (such as ImageNet-21k, ImageNet-1K or VGGFace2) to conclude which model may be the best to approximate the visual cortex and learn only an adaptation function corresponding to the approximation of the the primate IT cortex through the metric learning framework. Our experiments conducted on the Totally Looks Like image dataset highlight the interest of our method, by increasing the retrieval scores of the best model @1 by 2.25x. This research work was recently accepted for publication at the ICIP 2021 international conference [1]. In this new article, we expand on this previous work by using and comparing new pre-trained feature extractors on other datasets.",
        "completion1":"Researchers Develop AI That Can Learn to Assess Visual Similarities Between Images",
        "completion2":"New AI Can Help Computers Understand Human Perception",
        "completion3":"Study Shows How AI Can Be Used to Improve Computer Vision Systems",
        "technologyreview":0.258057725,
        "venturebeat":0.1863469713,
        "wired":0.0506058657,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.01417v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1654240500000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2204.01694v2",
        "predicted_newsworthiness":0.4191507273,
        "title":"\"This is my unicorn, Fluffy\": Personalizing frozen vision-language representations",
        "summary":"Large Vision & Language models pretrained on web-scale data provide representations that are invaluable for numerous V&L problems. However, it is unclear how they can be used for reasoning about user-specific visual concepts in unstructured language. This problem arises in multiple domains, from personalized image retrieval to personalized interaction with smart devices. We introduce a new learning setup called Personalized Vision & Language (PerVL) with two new benchmark datasets for retrieving and segmenting user-specific \"personalized\" concepts \"in the wild\". In PerVL, one should learn personalized concepts (1) independently of the downstream task (2) allowing a pretrained model to reason about them with free language, and (3) does not require personalized negative examples. We propose an architecture for solving PerVL that operates by extending the input vocabulary of a pretrained model with new word embeddings for the new personalized concepts. The model can then reason about them by simply using them in a sentence. We demonstrate that our approach learns personalized visual concepts from a few examples and can effectively apply them in image retrieval and semantic segmentation using rich textual queries.",
        "completion1":"New learning setup called PerVL for Pretrained Vision & Language models to reasoning about user-specific visual concepts.",
        "completion2":"New benchmark datasets for retrieving and segmenting user-specific \"personalized\" concepts proposed in PerVL.",
        "completion3":"An architecture that operates by extending the input vocabulary of a pretrained model with new word embeddings for the new personalized concepts proposed in PerVL.",
        "technologyreview":0.2916058322,
        "venturebeat":0.2510346633,
        "wired":0.078059,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.01694v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1649095091000,
        "code_mentioned":1,
        "readability":0.79
    },
    {
        "arxiv_id":"2204.08304v1",
        "predicted_newsworthiness":0.6682088558,
        "title":"UMass PCL at SemEval-2022 Task 4: Pre-trained Language Model Ensembles for Detecting Patronizing and Condescending Language",
        "summary":"Patronizing and condescending language (PCL) is everywhere, but rarely is the focus on its use by media towards vulnerable communities. Accurately detecting PCL of this form is a difficult task due to limited labeled data and how subtle it can be. In this paper, we describe our system for detecting such language which was submitted to SemEval 2022 Task 4: Patronizing and Condescending Language Detection. Our approach uses an ensemble of pre-trained language models, data augmentation, and optimizing the threshold for detection. Experimental results on the evaluation dataset released by the competition hosts show that our work is reliably able to detect PCL, achieving an F1 score of 55.47% on the binary classification task and a macro F1 score of 36.25% on the fine-grained, multi-label detection task.",
        "completion1":"UMass achieves high scores in detecting patronizing and condescending language.",
        "completion2":"Pre-trained language models show promise in detecting PCL.",
        "completion3":"Data augmentation and threshold optimization improve results.",
        "technologyreview":0.3120578165,
        "venturebeat":0.2440934415,
        "wired":0.0980179063,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.08304v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1650288130000,
        "code_mentioned":0,
        "readability":0.8
    },
    {
        "arxiv_id":"2201.12723v3",
        "predicted_newsworthiness":0.4755315987,
        "title":"A Frustratingly Simple Approach for End-to-End Image Captioning",
        "summary":"Image Captioning is a fundamental task to join vision and language, concerning about cross-modal understanding and text generation. Recent years witness the emerging attention on image captioning. Most of existing works follow a traditional two-stage training paradigm. Before training the captioning models, an extra object detector is utilized to recognize the objects in the image at first. However, they require sizeable datasets with fine-grained object annotation for training the object detector, which is a daunting task. In addition, the errors of the object detectors are easy to propagate to the following captioning models, degenerating models' performance. To alleviate such defects, we propose a frustratingly simple but highly effective end-to-end image captioning framework, Visual Conditioned GPT (VC-GPT), by connecting the pre-trained visual encoder (CLIP-ViT) and language decoder (GPT2). Different from the vanilla connection method that directly inserts the cross-attention modules into GPT2, we come up with a self-ensemble cross-modal fusion mechanism that comprehensively considers both the single- and cross-modal knowledge. As a result, we do not need extra object detectors for model training. Experimental results conducted on three popular image captioning benchmarks (MSCOCO, Flickr30k and NoCaps) demonstrate that our VC-GPT achieves either the best or the second-best performance across all evaluation metrics over extensive baseline systems.",
        "completion1":"A new approach for end-to-end image captioning that does not require an extra object detector.",
        "completion2":"A self-ensemble cross-modal fusion mechanism for image captioning.",
        "completion3":"VC-GPT achieves either the best or the second-best performance across all evaluation metrics over extensive baseline systems.",
        "technologyreview":0.208998835,
        "venturebeat":0.1754091246,
        "wired":0.0633076776,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.12723v3",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.cl"
        ],
        "published":1643517894000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2205.02536v1",
        "predicted_newsworthiness":0.4496643349,
        "title":"YOLOPose: Transformer-based Multi-Object 6D Pose Estimation using Keypoint Regression",
        "summary":"6D object pose estimation is a crucial prerequisite for autonomous robot manipulation applications. The state-of-the-art models for pose estimation are convolutional neural network (CNN)-based. Lately, Transformers, an architecture originally proposed for natural language processing, is achieving state-of-the-art results in many computer vision tasks as well. Equipped with the multi-head self-attention mechanism, Transformers enable simple single-stage end-to-end architectures for learning object detection and 6D object pose estimation jointly. In this work, we propose YOLOPose (short form for You Only Look Once Pose estimation), a Transformer-based multi-object 6D pose estimation method based on keypoint regression. In contrast to the standard heatmaps for predicting keypoints in an image, we directly regress the keypoints. Additionally, we employ a learnable orientation estimation module to predict the orientation from the keypoints. Along with a separate translation estimation module, our model is end-to-end differentiable. Our method is suitable for real-time applications and achieves results comparable to state-of-the-art methods.",
        "completion1":"You Only Look Once Pose Estimation: A Transformer-based Approach for Multi-object 6D Pose Estimation",
        "completion2":"YOLOPose: Learning 6D Object Poses from Keypoint Regressions",
        "completion3":"End-to-End Differentiable 6D Object Pose Estimation with the YOLOPose Method",
        "technologyreview":0.244228317,
        "venturebeat":0.2079244603,
        "wired":0.0553483206,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.02536v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1651744299000,
        "code_mentioned":0,
        "readability":0.77
    },
    {
        "arxiv_id":"2201.03353v1",
        "predicted_newsworthiness":0.5174046312,
        "title":"GMFIM: A Generative Mask-guided Facial Image Manipulation Model for Privacy Preservation",
        "summary":"The use of social media websites and applications has become very popular and people share their photos on these networks. Automatic recognition and tagging of people's photos on these networks has raised privacy preservation issues and users seek methods for hiding their identities from these algorithms. Generative adversarial networks (GANs) are shown to be very powerful in generating face images in high diversity and also in editing face images. In this paper, we propose a Generative Mask-guided Face Image Manipulation (GMFIM) model based on GANs to apply imperceptible editing to the input face image to preserve the privacy of the person in the image. Our model consists of three main components: a) the face mask module to cut the face area out of the input image and omit the background, b) the GAN-based optimization module for manipulating the face image and hiding the identity and, c) the merge module for combining the background of the input image and the manipulated de-identified face image. Different criteria are considered in the loss function of the optimization step to produce high-quality images that are as similar as possible to the input image while they cannot be recognized by AFR systems. The results of the experiments on different datasets show that our model can achieve better performance against automated face recognition systems in comparison to the state-of-the-art methods and it catches a higher attack success rate in most experiments from a total of 18. Moreover, the generated images of our proposed model have the highest quality and are more pleasing to human eyes.",
        "completion1":"GMFIM: A Generative Mask-guided Facial Image Manipulation Model for Privacy Preservation",
        "completion2":"GAN-based optimization module for manipulating the face image and hiding the identity",
        "completion3":"The results of the experiments on different datasets show that our model can achieve better performance against automated face recognition systems in comparison to the state-of-the-art methods",
        "technologyreview":0.2692997291,
        "venturebeat":0.1997985766,
        "wired":0.0620076921,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.03353v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1641823754000,
        "code_mentioned":1,
        "readability":0.88
    },
    {
        "arxiv_id":"2207.04344v1",
        "predicted_newsworthiness":0.6437752084,
        "title":"Freedom to Choose: Understanding Input Modality Preferences of People with Upper-body Motor Impairments for Activities of Daily Living",
        "summary":"Many people with upper-body motor impairments encounter challenges while performing Activities of Daily Living (ADLs) and Instrumental Activities of Daily Living (IADLs), such as toileting, grooming, and managing finances, which have impacts on their Quality of Life (QOL). Although existing assistive technologies enable people with upper-body motor impairments to use different input modalities to interact with computing devices independently (e.g., using voice to interact with a computer), many people still require Personal Care Assistants (PCAs) to perform ADLs. Multimodal input has the potential to enable users to perform ADLs without human assistance. We conducted 12 semi-structured interviews with people who have upper-body motor impairments to capture their existing practices and challenges of performing ADLs, identify opportunities to expand the input possibilities for assistive devices, and understand user preferences for multimodal interaction during everyday tasks. Finally, we discuss implications for the design and use of multimodal input solutions to support user independence and collaborative experiences when performing daily living tasks.",
        "completion1":"New study finds that many people with upper-body motor impairments still require Personal Care Assistants to perform Activities of Daily Living.",
        "completion2":"Multimodal input has the potential to enable users to perform Activities of Daily Living without human assistance.",
        "completion3":" Interviews with people who have upper-body motor impairments reveal user preferences for multimodal interaction during everyday tasks.",
        "technologyreview":0.3016483027,
        "venturebeat":0.2747686523,
        "wired":0.1013883914,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.04344v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1657405082000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2202.12443v1",
        "predicted_newsworthiness":0.7410400957,
        "title":"Towards an Accountable and Reproducible Federated Learning: A FactSheets Approach",
        "summary":"Federated Learning (FL) is a novel paradigm for the shared training of models based on decentralized and private data. With respect to ethical guidelines, FL is promising regarding privacy, but needs to excel vis-\\`a-vis transparency and trustworthiness. In particular, FL has to address the accountability of the parties involved and their adherence to rules, law and principles. We introduce AF^2 Framework, where we instrument FL with accountability by fusing verifiable claims with tamper-evident facts, into reproducible arguments. We build on AI FactSheets for instilling transparency and trustworthiness into the AI lifecycle and expand it to incorporate dynamic and nested facts, as well as complex model compositions in FL. Based on our approach, an auditor can validate, reproduce and certify a FL process. This can be directly applied in practice to address the challenges of AI engineering and ethics.",
        "completion1":"Federated Learning is a novel paradigm for the shared training of models based on decentralized and private data.",
        "completion2":"With respect to ethical guidelines, FL is promising regarding privacy, but needs to excel vis-\\`a-vis transparency and trustworthiness.",
        "completion3":"In particular, FL has to address the accountability of the parties involved and their adherence to rules, law and principles.",
        "technologyreview":0.3892830154,
        "venturebeat":0.3339252669,
        "wired":0.0903193296,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.12443v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.lg"
        ],
        "published":1645749254000,
        "code_mentioned":0,
        "readability":0.8
    },
    {
        "arxiv_id":"2205.08689v3",
        "predicted_newsworthiness":0.4309445994,
        "title":"Spatial-Temporal Interactive Dynamic Graph Convolution Network for Traffic Forecasting",
        "summary":"Accurate traffic forecasting is essential for smart cities to achieve traffic control, route planning, and flow detection. Although many spatial-temporal methods are currently proposed, these methods are deficient in capturing the spatial-temporal dependence of traffic data synchronously. In addition, most of the methods ignore the dynamically changing correlations between road network nodes that arise as traffic data changes. We propose a neural network-based Spatial-Temporal Interactive Dynamic Graph Convolutional Network (STIDGCN) to address the above challenges for traffic forecasting. Specifically, we propose an interactive dynamic graph convolution structure, which divides the sequences at intervals and synchronously captures the traffic data's spatial-temporal dependence through an interactive learning strategy. The interactive learning strategy makes STIDGCN effective for long-term prediction. We also propose a novel dynamic graph convolution module to capture the dynamically changing correlations in the traffic network, consisting of a graph generator and fusion graph convolution. The dynamic graph convolution module can use the input traffic data and pre-defined graph structure to generate a graph structure. It is then fused with the defined adaptive adjacency matrix to generate a dynamic adjacency matrix, which fills the pre-defined graph structure and simulates the generation of dynamic associations between nodes in the road network. Extensive experiments on four real-world traffic flow datasets demonstrate that STIDGCN outperforms the state-of-the-art baseline.",
        "completion1":"Spatial-Temporal Interactive Dynamic Graph Convolution Network for Traffic Forecasting",
        "completion2":"STIDGCN outperforms the state-of-the-art baseline",
        "completion3":"Spatial-Temporal Methods are Deficient in Capturing the Spatial-Temporal Dependence of Traffic Data",
        "technologyreview":0.246541809,
        "venturebeat":0.2209087591,
        "wired":0.0691764135,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.08689v3",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1652839170000,
        "code_mentioned":0,
        "readability":0.75
    },
    {
        "arxiv_id":"2205.09284v1",
        "predicted_newsworthiness":0.3650448097,
        "title":"Towards Applicable Reinforcement Learning: Improving the Generalization and Sample Efficiency with Policy Ensemble",
        "summary":"It is challenging for reinforcement learning (RL) algorithms to succeed in real-world applications like financial trading and logistic system due to the noisy observation and environment shifting between training and evaluation. Thus, it requires both high sample efficiency and generalization for resolving real-world tasks. However, directly applying typical RL algorithms can lead to poor performance in such scenarios. Considering the great performance of ensemble methods on both accuracy and generalization in supervised learning (SL), we design a robust and applicable method named Ensemble Proximal Policy Optimization (EPPO), which learns ensemble policies in an end-to-end manner. Notably, EPPO combines each policy and the policy ensemble organically and optimizes both simultaneously. In addition, EPPO adopts a diversity enhancement regularization over the policy space which helps to generalize to unseen states and promotes exploration. We theoretically prove EPPO increases exploration efficacy, and through comprehensive experimental evaluations on various tasks, we demonstrate that EPPO achieves higher efficiency and is robust for real-world applications compared with vanilla policy optimization algorithms and other ensemble methods. Code and supplemental materials are available at https:\/\/seqml.github.io\/eppo.",
        "completion1":"Towards Applicable Reinforcement Learning: Improving the Generalization and Sample Efficiency with Policy Ensemble",
        "completion2":"EPPO combines each policy and the policy ensemble organically and optimizes both simultaneously",
        "completion3":"EPPO adopts a diversity enhancement regularization over the policy space which helps to generalize to unseen states and promotes exploration",
        "technologyreview":0.2348978546,
        "venturebeat":0.2095454917,
        "wired":0.0361133911,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.09284v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1652927132000,
        "code_mentioned":1,
        "readability":0.79
    },
    {
        "arxiv_id":"2201.08598v1",
        "predicted_newsworthiness":0.3925175697,
        "title":"Taxonomy Enrichment with Text and Graph Vector Representations",
        "summary":"Knowledge graphs such as DBpedia, Freebase or Wikidata always contain a taxonomic backbone that allows the arrangement and structuring of various concepts in accordance with the hypo-hypernym (\"class-subclass\") relationship. With the rapid growth of lexical resources for specific domains, the problem of automatic extension of the existing knowledge bases with new words is becoming more and more widespread. In this paper, we address the problem of taxonomy enrichment which aims at adding new words to the existing taxonomy. We present a new method that allows achieving high results on this task with little effort. It uses the resources which exist for the majority of languages, making the method universal. We extend our method by incorporating deep representations of graph structures like node2vec, Poincar\\'e embeddings, GCN etc. that have recently demonstrated promising results on various NLP tasks. Furthermore, combining these representations with word embeddings allows us to beat the state of the art. We conduct a comprehensive study of the existing approaches to taxonomy enrichment based on word and graph vector representations and their fusion approaches. We also explore the ways of using deep learning architectures to extend the taxonomic backbones of knowledge graphs. We create a number of datasets for taxonomy extension for English and Russian. We achieve state-of-the-art results across different datasets and provide an in-depth error analysis of mistakes.",
        "completion1":"Taxonomy Enrichment with Text and Graph Vector Representations Beats State of the Art",
        "completion2":"New Method for Taxonomy Enrichment Uses Existing Resources for Majority of Languages",
        "completion3":"Deep Learning Architectures Extend Taxonomic Backbones of Knowledge Graphs",
        "technologyreview":0.1941425068,
        "venturebeat":0.1632334643,
        "wired":0.05109001,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.08598v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1642755672000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2201.02658v1",
        "predicted_newsworthiness":0.5308913353,
        "title":"Fair and efficient contribution valuation for vertical federated learning",
        "summary":"Federated learning is a popular technology for training machine learning models on distributed data sources without sharing data. Vertical federated learning or feature-based federated learning applies to the cases that different data sources share the same sample ID space but differ in feature space. To ensure the data owners' long-term engagement, it is critical to objectively assess the contribution from each data source and recompense them accordingly. The Shapley value (SV) is a provably fair contribution valuation metric originated from cooperative game theory. However, computing the SV requires extensively retraining the model on each subset of data sources, which causes prohibitively high communication costs in federated learning. We propose a contribution valuation metric called vertical federated Shapley value (VerFedSV) based on SV. We show that VerFedSV not only satisfies many desirable properties for fairness but is also efficient to compute, and can be adapted to both synchronous and asynchronous vertical federated learning algorithms. Both theoretical analysis and extensive experimental results verify the fairness, efficiency, and adaptability of VerFedSV.",
        "completion1":"VerFedSV: A Fair and Efficient Contribution Valuation Metric for Vertical Federated Learning",
        "completion2":"VerFedSV Satisfies Many Desirable Properties for Fairness, Efficiency, and Adaptability",
        "completion3":"VerFedSV Enables Engagement of Data Owners in Federated Learning",
        "technologyreview":0.2923604468,
        "venturebeat":0.2733727191,
        "wired":0.0796749944,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.02658v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1641585435000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2206.02394v1",
        "predicted_newsworthiness":0.6112033397,
        "title":"An Estimation Framework for Passerby Engagement Interacting with Social Robots",
        "summary":"Social robots are expected to be a human labor support technology, and one application of them is an advertising medium in public spaces. When social robots provide information, such as recommended shops, adaptive communication according to the user's state is desired. User engagement, which is also defined as the level of interest in the robot, is likely to play an important role in adaptive communication. Therefore, in this paper, we propose a new framework to estimate user engagement. The proposed method focuses on four unsolved open problems: multi-party interactions, process of state change in engagement, difficulty in annotating engagement, and interaction dataset in the real world. The accuracy of the proposed method for estimating engagement was evaluated using interaction duration. The results show that the interaction duration can be accurately estimated by considering the influence of the behaviors of other people; this also implies that the proposed model accurately estimates the level of engagement during interaction with the robot.",
        "completion1":"Social Robots to Provide Information in Public Spaces",
        "completion2":"New Framework to Estimate User Engagement with Social Robots",
        "completion3":"Accuracy of New Framework Evaluated Using Interaction Duration",
        "technologyreview":0.3175899739,
        "venturebeat":0.2950005295,
        "wired":0.1002241342,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.02394v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1654498508000,
        "code_mentioned":0,
        "readability":0.89
    },
    {
        "arxiv_id":"2202.02776v1",
        "predicted_newsworthiness":0.8067640865,
        "title":"Human rights, democracy, and the rule of law assurance framework for AI systems: A proposal",
        "summary":"Following on from the publication of its Feasibility Study in December 2020, the Council of Europe's Ad Hoc Committee on Artificial Intelligence (CAHAI) and its subgroups initiated efforts to formulate and draft its Possible Elements of a Legal Framework on Artificial Intelligence, based on the Council of Europe's standards on human rights, democracy, and the rule of law. This document was ultimately adopted by the CAHAI plenary in December 2021. To support this effort, The Alan Turing Institute undertook a programme of research that explored the governance processes and practical tools needed to operationalise the integration of human right due diligence with the assurance of trustworthy AI innovation practices. The resulting framework was completed and submitted to the Council of Europe in September 2021. It presents an end-to-end approach to the assurance of AI project lifecycles that integrates context-based risk analysis and appropriate stakeholder engagement with comprehensive impact assessment, and transparent risk management, impact mitigation, and innovation assurance practices. Taken together, these interlocking processes constitute a Human Rights, Democracy and the Rule of Law Assurance Framework (HUDERAF). The HUDERAF combines the procedural requirements for principles-based human rights due diligence with the governance mechanisms needed to set up technical and socio-technical guardrails for responsible and trustworthy AI innovation practices. Its purpose is to provide an accessible and user-friendly set of mechanisms for facilitating compliance with a binding legal framework on artificial intelligence, based on the Council of Europe's standards on human rights, democracy, and the rule of law, and to ensure that AI innovation projects are carried out with appropriate levels of public accountability, transparency, and democratic governance.",
        "completion1":"The HUDERAF: A proposal for a human rights, democracy, and the rule of law assurance framework for AI systems",
        "completion2":"The Alan Turing Institute completes research on governance processes and practical tools needed to operationalise the integration of human rights due diligence with trustworthy AI innovation practices",
        "completion3":"Council of Europe's Ad Hoc Committee on Artificial Intelligence adopts possible elements of legal framework on artificial intelligence",
        "technologyreview":0.4680370173,
        "venturebeat":0.386458408,
        "wired":0.1408631975,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.02776v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.cy",
            "cs.hc",
            "cs.lg"
        ],
        "published":1644155786000,
        "code_mentioned":1,
        "readability":0.9
    },
    {
        "arxiv_id":"2204.05421v1",
        "predicted_newsworthiness":0.7250352056,
        "title":"A Survey on Crowdsourcing Applications in Smart Cities",
        "summary":"With the emergence of the Internet of things (IoT), human life is now progressing towards smartification faster than ever before. Thus, smart cities become automated in different aspects such as business, education, economy, medicine, and urban areas. Since smartification requires a variety of dynamic information in different urban dimensions, mobile crowdsourcing has gained importance in smart cities. This chapter systematically reviews the related applications of smart cities that use mobile crowdsourcing for data acquisition. For this purpose, the applications are classified as environmental, urban life, and transportation categories and then investigated in detail. This survey helps in understanding the current situation of smart cities from the viewpoint of crowdsourcing and discusses the future research directions in this field.",
        "completion1":"Mobile Crowdsourcing Gains Importance in Smart Cities",
        "completion2":"Different Applications of Mobile Crowdsourcing in Smart Cities",
        "completion3":"Future Directions for Mobile Crowdsourcing in Smart Cities",
        "technologyreview":0.3176658113,
        "venturebeat":0.2804043027,
        "wired":0.091894138,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.05421v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1649360258000,
        "code_mentioned":0,
        "readability":0.89
    },
    {
        "arxiv_id":"2201.07734v1",
        "predicted_newsworthiness":0.4775071827,
        "title":"Towards holistic scene understanding: Semantic segmentation and beyond",
        "summary":"This dissertation addresses visual scene understanding and enhances segmentation performance and generalization, training efficiency of networks, and holistic understanding. First, we investigate semantic segmentation in the context of street scenes and train semantic segmentation networks on combinations of various datasets. In Chapter 2 we design a framework of hierarchical classifiers over a single convolutional backbone, and train it end-to-end on a combination of pixel-labeled datasets, improving generalizability and the number of recognizable semantic concepts. Chapter 3 focuses on enriching semantic segmentation with weak supervision and proposes a weakly-supervised algorithm for training with bounding box-level and image-level supervision instead of only with per-pixel supervision. The memory and computational load challenges that arise from simultaneous training on multiple datasets are addressed in Chapter 4. We propose two methodologies for selecting informative and diverse samples from datasets with weak supervision to reduce our networks' ecological footprint without sacrificing performance. Motivated by memory and computation efficiency requirements, in Chapter 5, we rethink simultaneous training on heterogeneous datasets and propose a universal semantic segmentation framework. This framework achieves consistent increases in performance metrics and semantic knowledgeability by exploiting various scene understanding datasets. Chapter 6 introduces the novel task of part-aware panoptic segmentation, which extends our reasoning towards holistic scene understanding. This task combines scene and parts-level semantics with instance-level object detection. In conclusion, our contributions span over convolutional network architectures, weakly-supervised learning, part and panoptic segmentation, paving the way towards a holistic, rich, and sustainable visual scene understanding.",
        "completion1":"A new framework for holistic scene understanding",
        "completion2":"Improved performance and generalization in semantic segmentation networks",
        "completion3":"A novel approach to part-aware panoptic segmentation",
        "technologyreview":0.2088688068,
        "venturebeat":0.1513273655,
        "wired":0.0417825989,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.07734v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1642360691000,
        "code_mentioned":0,
        "readability":0.73
    },
    {
        "arxiv_id":"2201.02149v1",
        "predicted_newsworthiness":0.4245656654,
        "title":"Bio-inspired Min-Nets Improve the Performance and Robustness of Deep Networks",
        "summary":"Min-Nets are inspired by end-stopped cortical cells with units that output the minimum of two learned filters. We insert such Min-units into state-of-the-art deep networks, such as the popular ResNet and DenseNet, and show that the resulting Min-Nets perform better on the Cifar-10 benchmark. Moreover, we show that Min-Nets are more robust against JPEG compression artifacts. We argue that the minimum operation is the simplest way of implementing an AND operation on pairs of filters and that such AND operations introduce a bias that is appropriate given the statistics of natural images.",
        "completion1":"Bio-inspired Min-Nets Improve the Performance of Deep Networks.",
        "completion2":"Min-Nets are more robust against JPEG compression artifacts.",
        "completion3":"The minimum operation is the simplest way of implementing an AND operation on pairs of filters and that such AND operations introduce a bias that is appropriate given the statistics of natural images.",
        "technologyreview":0.2722934744,
        "venturebeat":0.1973384848,
        "wired":0.0581698231,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.02149v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1641490281000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2207.04447v1",
        "predicted_newsworthiness":0.6754414422,
        "title":"Human-Centric Research for NLP: Towards a Definition and Guiding Questions",
        "summary":"With Human-Centric Research (HCR) we can steer research activities so that the research outcome is beneficial for human stakeholders, such as end users. But what exactly makes research human-centric? We address this question by providing a working definition and define how a research pipeline can be split into different stages in which human-centric components can be added. Additionally, we discuss existing NLP with HCR components and define a series of guiding questions, which can serve as starting points for researchers interested in exploring human-centric research approaches. We hope that this work would inspire researchers to refine the proposed definition and to pose other questions that might be meaningful for achieving HCR.",
        "completion1":"What is Human-Centric Research?",
        "completion2":"ow can we make research more beneficial for human stakeholders?",
        "completion3":"hat are the existing components of NLP with HCR?",
        "technologyreview":0.3182749406,
        "venturebeat":0.2817983333,
        "wired":0.1001070554,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.04447v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1657454547000,
        "code_mentioned":0,
        "readability":0.88
    },
    {
        "arxiv_id":"2206.04688v1",
        "predicted_newsworthiness":0.5871917933,
        "title":"NNTrainer: Light-Weight On-Device Training Framework",
        "summary":"Modern consumer electronic devices have adopted deep learning-based intelligence services for their key features. Vendors have recently started to execute intelligence services on devices to preserve personal data in devices, reduce network and cloud costs. We find such a trend as the opportunity to personalize intelligence services by updating neural networks with user data without exposing the data out of devices: on-device training. For example, we may add a new class, my dog, Alpha, for robotic vacuums, adapt speech recognition for the users accent, let text-to-speech speak as if the user speaks. However, the resource limitations of target devices incur significant difficulties. We propose NNTrainer, a light-weight on-device training framework. We describe optimization techniques for neural networks implemented by NNTrainer, which are evaluated along with the conventional. The evaluations show that NNTrainer can reduce memory consumption down to 1\/28 without deteriorating accuracy or training time and effectively personalizes applications on devices. NNTrainer is cross-platform and practical open source software, which is being deployed to millions of devices in the authors affiliation.",
        "completion1":"NNTrainer: Light-Weight On-Device Training Framework",
        "completion2":"On-device training promises to personalize intelligence services",
        "completion3":"Cross-platform and practical open source software NNTrainer being deployed to millions of devices",
        "technologyreview":0.3727411451,
        "venturebeat":0.3538445024,
        "wired":0.1035683857,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.04688v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1654763279000,
        "code_mentioned":1,
        "readability":0.89
    },
    {
        "arxiv_id":"2203.02459v3",
        "predicted_newsworthiness":0.4003754048,
        "title":"From Simultaneous to Streaming Machine Translation by Leveraging Streaming History",
        "summary":"Simultaneous Machine Translation is the task of incrementally translating an input sentence before it is fully available. Currently, simultaneous translation is carried out by translating each sentence independently of the previously translated text. More generally, Streaming MT can be understood as an extension of Simultaneous MT to the incremental translation of a continuous input text stream. In this work, a state-of-the-art simultaneous sentence-level MT system is extended to the streaming setup by leveraging the streaming history. Extensive empirical results are reported on IWSLT Translation Tasks, showing that leveraging the streaming history leads to significant quality gains. In particular, the proposed system proves to compare favorably to the best performing systems.",
        "completion1":"Researchers develop new streaming machine translation method that significantly improves quality.",
        "completion2":"New system outperforms existing methods for simultaneous machine translation.",
        "completion3":"Researchers leverage streaming history to improve machine translation quality.",
        "technologyreview":0.1681274235,
        "venturebeat":0.1694648621,
        "wired":0.0595877848,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.02459v3",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1646415705000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2205.06750v1",
        "predicted_newsworthiness":0.5143416548,
        "title":"Provably Safe Reinforcement Learning: A Theoretical and Experimental Comparison",
        "summary":"Ensuring safety of reinforcement learning (RL) algorithms is crucial for many real-world tasks. However, vanilla RL does not guarantee safety for an agent. In recent years, several methods have been proposed to provide safety guarantees for RL. To the best of our knowledge, there is no comprehensive comparison of these provably safe RL methods. We therefore introduce a categorization for existing provably safe RL methods, and present the theoretical foundations for both continuous and discrete action spaces. Additionally, we evaluate provably safe RL on an inverted pendulum. In the experiments, it is shown that indeed only provably safe RL methods guarantee safety.",
        "completion1":"New research compares several methods for provably safe reinforcement learning.",
        "completion2":"Foundation for provably safe RL provided for both continuous and discrete action spaces.",
        "completion3":"Evaluation of provably safe RL on inverted pendulum shows that only these methods guarantee safety.",
        "technologyreview":0.2290731762,
        "venturebeat":0.1646287737,
        "wired":0.0352469835,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.06750v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1652459676000,
        "code_mentioned":0,
        "readability":0.8
    },
    {
        "arxiv_id":"2204.01969v1",
        "predicted_newsworthiness":0.4318829683,
        "title":"Region Rebalance for Long-Tailed Semantic Segmentation",
        "summary":"In this paper, we study the problem of class imbalance in semantic segmentation. We first investigate and identify the main challenges of addressing this issue through pixel rebalance. Then a simple and yet effective region rebalance scheme is derived based on our analysis. In our solution, pixel features belonging to the same class are grouped into region features, and a rebalanced region classifier is applied via an auxiliary region rebalance branch during training. To verify the flexibility and effectiveness of our method, we apply the region rebalance module into various semantic segmentation methods, such as Deeplabv3+, OCRNet, and Swin. Our strategy achieves consistent improvement on the challenging ADE20K and COCO-Stuff benchmark. In particular, with the proposed region rebalance scheme, state-of-the-art BEiT receives +0.7% gain in terms of mIoU on the ADE20K val set.",
        "completion1":"Region Rebalance for Long-Tailed Semantic Segmentation",
        "completion2":"Auxiliary region rebalance branch during training",
        "completion3":"State-of-the-art BEiT receiving +0.7% gain in terms of mIoU on the ADE20K val set.",
        "technologyreview":0.1890760594,
        "venturebeat":0.1536414835,
        "wired":0.0309791943,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.01969v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1649130467000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2205.09443v1",
        "predicted_newsworthiness":0.3739544704,
        "title":"PYSKL: Towards Good Practices for Skeleton Action Recognition",
        "summary":"We present PYSKL: an open-source toolbox for skeleton-based action recognition based on PyTorch. The toolbox supports a wide variety of skeleton action recognition algorithms, including approaches based on GCN and CNN. In contrast to existing open-source skeleton action recognition projects that include only one or two algorithms, PYSKL implements six different algorithms under a unified framework with both the latest and original good practices to ease the comparison of efficacy and efficiency. We also provide an original GCN-based skeleton action recognition model named ST-GCN++, which achieves competitive recognition performance without any complicated attention schemes, serving as a strong baseline. Meanwhile, PYSKL supports the training and testing of nine skeleton-based action recognition benchmarks and achieves state-of-the-art recognition performance on eight of them. To facilitate future research on skeleton action recognition, we also provide a large number of trained models and detailed benchmark results to give some insights. PYSKL is released at https:\/\/github.com\/kennymckormick\/pyskl and is actively maintained. We will update this report when we add new features or benchmarks. The current version corresponds to PYSKL v0.2.",
        "completion1":"PYSKL: New Open-Source Toolbox for Skeleton-Based Action Recognition",
        "completion2":"PYSKL Achieves State-of-the-Art Results on Eight Benchmarks",
        "completion3":"PYSKL Provides Trained Models and Detailed Benchmark Results",
        "technologyreview":0.1705127543,
        "venturebeat":0.1507989119,
        "wired":0.0462507362,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.09443v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1652954312000,
        "code_mentioned":1,
        "readability":0.87
    },
    {
        "arxiv_id":"2207.09721v1",
        "predicted_newsworthiness":0.3700393913,
        "title":"Feature Representation Learning for Unsupervised Cross-domain Image Retrieval",
        "summary":"Current supervised cross-domain image retrieval methods can achieve excellent performance. However, the cost of data collection and labeling imposes an intractable barrier to practical deployment in real applications. In this paper, we investigate the unsupervised cross-domain image retrieval task, where class labels and pairing annotations are no longer a prerequisite for training. This is an extremely challenging task because there is no supervision for both in-domain feature representation learning and cross-domain alignment. We address both challenges by introducing: 1) a new cluster-wise contrastive learning mechanism to help extract class semantic-aware features, and 2) a novel distance-of-distance loss to effectively measure and minimize the domain discrepancy without any external supervision. Experiments on the Office-Home and DomainNet datasets consistently show the superior image retrieval accuracies of our framework over state-of-the-art approaches. Our source code can be found at https:\/\/github.com\/conghuihu\/UCDIR.",
        "completion1":"New unsupervised cross-domain image retrieval method outperforms current supervised methods",
        "completion2":"Cluster-wise contrastive learning helps extract class semantic-aware features",
        "completion3":"Distance-of-distance loss effectively measures and minimizes domain discrepancy",
        "technologyreview":0.1805683693,
        "venturebeat":0.1546568425,
        "wired":0.0415689416,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.09721v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658303534000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2205.03854v1",
        "predicted_newsworthiness":0.5609825514,
        "title":"Introduction to Soar",
        "summary":"This paper is the recommended initial reading for a functional overview of Soar, version 9.6. It includes an abstract overview of the architectural structure of Soar including its processing, memories, learning modules, their interfaces, and the representations of knowledge used by those modules. From there it describes the processing supported by those modules, including decision making, impasses and substates, procedure learning via chunking, reinforcement learning, semantic memory, episodic memory, and spatial-visual reasoning. It then reviews the levels of decision making and variety of learning in Soar, and analysis of Soar as an architecture supporting general human-level AI. Following the references is an appendix that contains short descriptions of recent Soar agents and a glossary of the terminology we use in describing Soar.",
        "completion1":"Soar: An Architecture for Human-Level AI",
        "completion2":"Soar 9.6: A Functional Overview",
        "completion3":"Soar: Recent Agents and Glossary",
        "technologyreview":0.3712156707,
        "venturebeat":0.3134772586,
        "wired":0.0817780421,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.03854v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1652013891000,
        "code_mentioned":0,
        "readability":0.82
    },
    {
        "arxiv_id":"2203.07345v1",
        "predicted_newsworthiness":0.4315096038,
        "title":"Federated Cycling (FedCy): Semi-supervised Federated Learning of Surgical Phases",
        "summary":"Recent advancements in deep learning methods bring computer-assistance a step closer to fulfilling promises of safer surgical procedures. However, the generalizability of such methods is often dependent on training on diverse datasets from multiple medical institutions, which is a restrictive requirement considering the sensitive nature of medical data. Recently proposed collaborative learning methods such as Federated Learning (FL) allow for training on remote datasets without the need to explicitly share data. Even so, data annotation still represents a bottleneck, particularly in medicine and surgery where clinical expertise is often required. With these constraints in mind, we propose FedCy, a federated semi-supervised learning (FSSL) method that combines FL and self-supervised learning to exploit a decentralized dataset of both labeled and unlabeled videos, thereby improving performance on the task of surgical phase recognition. By leveraging temporal patterns in the labeled data, FedCy helps guide unsupervised training on unlabeled data towards learning task-specific features for phase recognition. We demonstrate significant performance gains over state-of-the-art FSSL methods on the task of automatic recognition of surgical phases using a newly collected multi-institutional dataset of laparoscopic cholecystectomy videos. Furthermore, we demonstrate that our approach also learns more generalizable features when tested on data from an unseen domain.",
        "completion1":"FedCy Aims to Improve Surgical Procedures with Federated Learning",
        "completion2":"FedCy Uses Federated Learning to Teach AI Surgical Phases",
        "completion3":"FedCy's Semi-Supervised Learning Method Could Help AI Assist in Surgeries",
        "technologyreview":0.2571057613,
        "venturebeat":0.2055066571,
        "wired":0.0563254556,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.07345v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1647279893000,
        "code_mentioned":0,
        "readability":0.82
    },
    {
        "arxiv_id":"2206.07382v1",
        "predicted_newsworthiness":0.3663194401,
        "title":"Sparse Structure Search for Parameter-Efficient Tuning",
        "summary":"Adapting large pre-trained models (PTMs) through fine-tuning imposes prohibitive computational and storage burdens. Recent studies of parameter-efficient tuning (PET) find that only optimizing a small portion of parameters conditioned on PTMs could yield on-par performance compared to conventional fine-tuning. Generally, PET methods exquisitely design parameter-efficient modules (PET modules) which could be applied to arbitrary fine-grained positions inside PTMs. However, the effectiveness of these fine-grained positions largely relies on sophisticated manual designation, thereby usually producing sub-optimal results. In contrast to the manual designation, we explore constructing PET modules in an automatic manner. We automatically \\textbf{S}earch for the \\textbf{S}parse \\textbf{S}tructure of \\textbf{P}arameter-\\textbf{E}fficient \\textbf{T}uning (S$^3$PET). Based on a unified framework of various PET methods, S$^3$PET conducts the differentiable PET structure search through bi-level optimization and proposes shifted global sigmoid method to explicitly control the number of trainable parameters. Extensive experiments show that S$^3$PET surpasses manual and random structures with less trainable parameters. The searched structures preserve more than 99\\% fine-tuning performance with 0.01\\% trainable parameters. Moreover, the advantage of S$^3$PET is amplified with extremely low trainable parameters budgets (0.0009\\%$\\sim$0.01\\%). The searched structures are transferable and explainable, providing suggestions and guidance for the future design of PET methods.",
        "completion1":"Sparse Structure Search for Parameter-Efficient Tuning",
        "completion2":"Method Finds Only Optimizing a Small Portion of Parameters Conditioned on Pre-Trained Models Could Yield On-Par Performance",
        "completion3":"Automatically Constructing Parameter-Efficient Modules in an Optimal Manner",
        "technologyreview":0.1330139662,
        "venturebeat":0.0718096059,
        "wired":-0.0100932565,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.07382v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1655282721000,
        "code_mentioned":0,
        "readability":0.74
    },
    {
        "arxiv_id":"2207.07180v1",
        "predicted_newsworthiness":0.4233988777,
        "title":"Contrastive Adapters for Foundation Model Group Robustness",
        "summary":"While large pretrained foundation models (FMs) have shown remarkable zero-shot classification robustness to dataset-level distribution shifts, their robustness to subpopulation or group shifts is relatively underexplored. We study this problem, and find that FMs such as CLIP may not be robust to various group shifts. Across 9 robustness benchmarks, zero-shot classification with their embeddings results in gaps of up to 80.7 percentage points (pp) between average and worst-group accuracy. Unfortunately, existing methods to improve robustness require retraining, which can be prohibitively expensive on large foundation models. We also find that efficient ways to improve model inference (e.g., via adapters, lightweight networks with FM embeddings as inputs) do not consistently improve and can sometimes hurt group robustness compared to zero-shot (e.g., increasing the accuracy gap by 50.1 pp on CelebA). We thus develop an adapter training strategy to effectively and efficiently improve FM group robustness. Our motivating observation is that while poor robustness results from groups in the same class being embedded far apart in the foundation model \"embedding space,\" standard adapter training may not bring these points closer together. We thus propose contrastive adapting, which trains adapters with contrastive learning to bring sample embeddings close to both their ground-truth class embeddings and other sample embeddings in the same class. Across the 9 benchmarks, our approach consistently improves group robustness, raising worst-group accuracy by 8.5 to 56.0 pp over zero-shot. Our approach is also efficient, doing so without any FM finetuning and only a fixed set of frozen FM embeddings. On benchmarks such as Waterbirds and CelebA, this leads to worst-group accuracy comparable to state-of-the-art methods that retrain entire models, while only training $\\leq$1% of the model parameters.",
        "completion1":"CLIP May Not Be Robust to Various Group Shifts",
        "completion2":"Zero-Shot Classification With Their Embeddings Results in Gaps of Up to 80.7 Percentage Points (pp) Between Average and Worst-Group Accuracy",
        "completion3":"Contrastive Adapters for Foundation Model Group Robustness",
        "technologyreview":0.2560683062,
        "venturebeat":0.1993022058,
        "wired":0.064596993,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.07180v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1657827655000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2202.00134v1",
        "predicted_newsworthiness":0.6604237066,
        "title":"Using Transition Learning to Enhance Mobile-Controlled Handoff In Decentralized Future Networks",
        "summary":"Traditionally, resource management and capacity allocation has been controlled network-side in cellular deployment. As autonomicity has been added to network design, machine learning technologies have largely followed this paradigm, benefiting from the higher compute capacity and informational context available at the network core. However, when these network services are disaggregated or decentralized, models that rely on assumed levels of network or information availability may no longer function reliably. This paper presents an inverted view of the resource management paradigm; one in which the client device executes a learning algorithm and manages its own mobility under a scenario where the networks and their corresponding data underneath are not being centrally managed.",
        "completion1":"Inverted View of Resource Management Paradigm Could Enhance Mobile-Controlled Handoff In Decentralized Future Networks",
        "completion2":"Client Devices Could Manage Their Own Mobility Under A Scenario Where The Networks And Their Corresponding Data Are Not Being Centrally Managed",
        "completion3":"Machine Learning Technologies May Benefit From Disaggregation Or Decentralization Of Network Services",
        "technologyreview":0.2620079227,
        "venturebeat":0.2556713389,
        "wired":0.0581040391,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.00134v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1643668925000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2206.03715v2",
        "predicted_newsworthiness":0.4484729603,
        "title":"Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning",
        "summary":"Commonsense reasoning systems should be able to generalize to diverse reasoning cases. However, most state-of-the-art approaches depend on expensive data annotations and overfit to a specific benchmark without learning how to perform general semantic reasoning. To overcome these drawbacks, zero-shot QA systems have shown promise as a robust learning scheme by transforming a commonsense knowledge graph (KG) into synthetic QA-form samples for model training. Considering the increasing type of different commonsense KGs, this paper aims to extend the zero-shot transfer learning scenario into multiple-source settings, where different KGs can be utilized synergetically. Towards this goal, we propose to mitigate the loss of knowledge from the interference among the different knowledge sources, by developing a modular variant of the knowledge aggregation as a new zero-shot commonsense reasoning framework. Results on five commonsense reasoning benchmarks demonstrate the efficacy of our framework, improving the performance with multiple KGs.",
        "completion1":"Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning",
        "completion2":"Zero-shot QA systems have shown promise as a robust learning scheme by transforming a commonsense knowledge graph into synthetic QA-form samples for model training",
        "completion3":"We propose to mitigate the loss of knowledge from the interference among the different knowledge sources, by developing a modular variant of the knowledge aggregation as a new zero-shot commonsense reasoning framework.",
        "technologyreview":0.1946606053,
        "venturebeat":0.1641559094,
        "wired":0.0387388343,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.03715v2",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.cl",
            "cs.lg"
        ],
        "published":1654673791000,
        "code_mentioned":0,
        "readability":0.81
    },
    {
        "arxiv_id":"2204.10419v1",
        "predicted_newsworthiness":0.4162854472,
        "title":"Learning Sequential Latent Variable Models from Multimodal Time Series Data",
        "summary":"Sequential modelling of high-dimensional data is an important problem that appears in many domains including model-based reinforcement learning and dynamics identification for control. Latent variable models applied to sequential data (i.e., latent dynamics models) have been shown to be a particularly effective probabilistic approach to solve this problem, especially when dealing with images. However, in many application areas (e.g., robotics), information from multiple sensing modalities is available -- existing latent dynamics methods have not yet been extended to effectively make use of such multimodal sequential data. Multimodal sensor streams can be correlated in a useful manner and often contain complementary information across modalities. In this work, we present a self-supervised generative modelling framework to jointly learn a probabilistic latent state representation of multimodal data and the respective dynamics. Using synthetic and real-world datasets from a multimodal robotic planar pushing task, we demonstrate that our approach leads to significant improvements in prediction and representation quality. Furthermore, we compare to the common learning baseline of concatenating each modality in the latent space and show that our principled probabilistic formulation performs better. Finally, despite being fully self-supervised, we demonstrate that our method is nearly as effective as an existing supervised approach that relies on ground truth labels.",
        "completion1":"New Framework Learns Latent State Representation of Multimodal Data",
        "completion2":"New Approach Shows Promise for Learning from Multiple Sensor Modalities",
        "completion3":"Self-Supervised Generative Modeling Framework outperforms Concurrent Baseline",
        "technologyreview":0.219620804,
        "venturebeat":0.1767368528,
        "wired":0.0518357141,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.10419v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cv",
            "cs.ro"
        ],
        "published":1650578364000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2202.09177v2",
        "predicted_newsworthiness":0.4299908833,
        "title":"Space4HGNN: A Novel, Modularized and Reproducible Platform to Evaluate Heterogeneous Graph Neural Network",
        "summary":"Heterogeneous Graph Neural Network (HGNN) has been successfully employed in various tasks, but we cannot accurately know the importance of different design dimensions of HGNNs due to diverse architectures and applied scenarios. Besides, in the research community of HGNNs, implementing and evaluating various tasks still need much human effort. To mitigate these issues, we first propose a unified framework covering most HGNNs, consisting of three components: heterogeneous linear transformation, heterogeneous graph transformation, and heterogeneous message passing layer. Then we build a platform Space4HGNN by defining a design space for HGNNs based on the unified framework, which offers modularized components, reproducible implementations, and standardized evaluation for HGNNs. Finally, we conduct experiments to analyze the effect of different designs. With the insights found, we distill a condensed design space and verify its effectiveness.",
        "completion1":"Space4HGNN: A Novel, Modularized and Reproducible Platform to Evaluate Heterogeneous Graph Neural Network",
        "completion2":"How Space4HGNN Can Help Advance the Field of Heterogeneous Graph Neural Networks",
        "completion3":"Researchers Find New Way to Improve Evaluation of Heterogeneous Graph Neural Networks",
        "technologyreview":0.2473728773,
        "venturebeat":0.2105154961,
        "wired":0.0625821506,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.09177v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1645189895000,
        "code_mentioned":1,
        "readability":0.73
    },
    {
        "arxiv_id":"2202.01862v2",
        "predicted_newsworthiness":0.4441874772,
        "title":"Practical Imitation Learning in the Real World via Task Consistency Loss",
        "summary":"Recent work in visual end-to-end learning for robotics has shown the promise of imitation learning across a variety of tasks. Such approaches are expensive both because they require large amounts of real world training demonstrations and because identifying the best model to deploy in the real world requires time-consuming real-world evaluations. These challenges can be mitigated by simulation: by supplementing real world data with simulated demonstrations and using simulated evaluations to identify high performing policies. However, this introduces the well-known \"reality gap\" problem, where simulator inaccuracies decorrelate performance in simulation from that of reality. In this paper, we build on top of prior work in GAN-based domain adaptation and introduce the notion of a Task Consistency Loss (TCL), a self-supervised loss that encourages sim and real alignment both at the feature and action-prediction levels. We demonstrate the effectiveness of our approach by teaching a mobile manipulator to autonomously approach a door, turn the handle to open the door, and enter the room. The policy performs control from RGB and depth images and generalizes to doors not encountered in training data. We achieve 72% success across sixteen seen and unseen scenes using only ~16.2 hours of teleoperated demonstrations in sim and real. To the best of our knowledge, this is the first work to tackle latched door opening from a purely end-to-end learning approach, where the task of navigation and manipulation are jointly modeled by a single neural network.",
        "completion1":"New approach to imitation learning offers hope for more efficient real-world training.",
        "completion2":"Task Consistency Loss may be key to successful end-to-end imitation learning.",
        "completion3":"Researchers achieve 72% success rate in door opening task with new method.",
        "technologyreview":0.2671507714,
        "venturebeat":0.1967664588,
        "wired":0.0412338856,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.01862v2",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.lg"
        ],
        "published":1643924586000,
        "code_mentioned":0,
        "readability":0.88
    },
    {
        "arxiv_id":"2207.07781v1",
        "predicted_newsworthiness":0.3868618387,
        "title":"Subgroup Discovery in Unstructured Data",
        "summary":"Subgroup discovery is a descriptive and exploratory data mining technique to identify subgroups in a population that exhibit interesting behavior with respect to a variable of interest. Subgroup discovery has numerous applications in knowledge discovery and hypothesis generation, yet it remains inapplicable for unstructured, high-dimensional data such as images. This is because subgroup discovery algorithms rely on defining descriptive rules based on (attribute, value) pairs, however, in unstructured data, an attribute is not well defined. Even in cases where the notion of attribute intuitively exists in the data, such as a pixel in an image, due to the high dimensionality of the data, these attributes are not informative enough to be used in a rule. In this paper, we introduce the subgroup-aware variational autoencoder, a novel variational autoencoder that learns a representation of unstructured data which leads to subgroups with higher quality. Our experimental results demonstrate the effectiveness of the method at learning subgroups with high quality while supporting the interpretability of the concepts.",
        "completion1":"Subgroup Discovery in Unstructured Data",
        "completion2":"Subgroup-Aware Variational Autoencoder",
        "completion3":"Effective Learning of Subgroups with High Quality",
        "technologyreview":0.210441887,
        "venturebeat":0.1765444537,
        "wired":0.0454237675,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.07781v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1657926834000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2203.05848v1",
        "predicted_newsworthiness":0.6171215796,
        "title":"Pardon? An Overview of the Current State and Requirements of Voice User Interfaces for Blind and Visually Impaired Users",
        "summary":"People with special needs like blind and visually impaired (BVI) people can particularly benefit from using voice assistants providing spoken information input and output in everyday life. However, it is crucial to understand their needs and to include these in the development of accessible and useful assistance systems. By conducting an online survey with 145 BVI people, this paper revealed that common voice assistants like Apple's Siri or Amazon's Alexa are used by a majority of BVI people and are also considered helpful. In particular, features in the context of audio entertainment, internet access and everyday life practical things like weather queries, time-related information (e.g. setting an alarm clock), checking calendar entries and taking notes are particularly often used and appreciated. The participants also indicated that the integration of smart home devices, the optimization of existing functionalities and voice input are important, but also potentially negative aspects such as data privacy and data security are relevant. Therefore, it seems particularly interesting to implement an online data processing as far as possible. Our results contribute to this development by providing an overview of empirically collected requirements for functions and implementation-related aspects.",
        "completion1":"Voice assistants becoming more popular among blind and visually impaired people.",
        "completion2":"Online survey reveals what features are most used and appreciated by BVI people.",
        "completion3":"Results contribute to the development of accessible and useful voice assistants.",
        "technologyreview":0.3479178931,
        "venturebeat":0.3391893851,
        "wired":0.1187347174,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.05848v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1646995890000,
        "code_mentioned":0,
        "readability":0.89
    },
    {
        "arxiv_id":"2202.13474v1",
        "predicted_newsworthiness":0.488790865,
        "title":"Interpretable Concept-based Prototypical Networks for Few-Shot Learning",
        "summary":"Few-shot learning aims at recognizing new instances from classes with limited samples. This challenging task is usually alleviated by performing meta-learning on similar tasks. However, the resulting models are black-boxes. There has been growing concerns about deploying black-box machine learning models and FSL is not an exception in this regard. In this paper, we propose a method for FSL based on a set of human-interpretable concepts. It constructs a set of metric spaces associated with the concepts and classifies samples of novel classes by aggregating concept-specific decisions. The proposed method does not require concept annotations for query samples. This interpretable method achieved results on a par with six previously state-of-the-art black-box FSL methods on the CUB fine-grained bird classification dataset.",
        "completion1":"Interpretable Concept-based Prototypical Networks for Few-Shot Learning",
        "completion2":"Few-shot learning aims at recognizing new instances from classes with limited samples",
        "completion3":"The proposed method does not require concept annotations for query samples",
        "technologyreview":0.2705308131,
        "venturebeat":0.2201437639,
        "wired":0.0682816488,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.13474v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1646003593000,
        "code_mentioned":0,
        "readability":0.83
    },
    {
        "arxiv_id":"2201.04419v1",
        "predicted_newsworthiness":0.3859190003,
        "title":"Topic Modeling on Podcast Short-Text Metadata",
        "summary":"Podcasts have emerged as a massively consumed online content, notably due to wider accessibility of production means and scaled distribution through large streaming platforms. Categorization systems and information access technologies typically use topics as the primary way to organize or navigate podcast collections. However, annotating podcasts with topics is still quite problematic because the assigned editorial genres are broad, heterogeneous or misleading, or because of data challenges (e.g. short metadata text, noisy transcripts). Here, we assess the feasibility to discover relevant topics from podcast metadata, titles and descriptions, using topic modeling techniques for short text. We also propose a new strategy to leverage named entities (NEs), often present in podcast metadata, in a Non-negative Matrix Factorization (NMF) topic modeling framework. Our experiments on two existing datasets from Spotify and iTunes and Deezer, a new dataset from an online service providing a catalog of podcasts, show that our proposed document representation, NEiCE, leads to improved topic coherence over the baselines. We release the code for experimental reproducibility of the results.",
        "completion1":"Podcasts have emerged as a massively consumed online content, notably due to wider accessibility of production means and scaled distribution through large streaming platforms.",
        "completion2":"Categorization systems and information access technologies typically use topics as the primary way to organize or navigate podcast collections.",
        "completion3":"However, annotating podcasts with topics is still quite problematic because the assigned editorial genres are broad, heterogeneous or misleading, or because of data challenges (e.g. short metadata text, noisy transcripts).",
        "technologyreview":0.1997559581,
        "venturebeat":0.1961732305,
        "wired":0.0874470674,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.04419v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.cl"
        ],
        "published":1641985625000,
        "code_mentioned":1,
        "readability":0.78
    },
    {
        "arxiv_id":"2203.14963v2",
        "predicted_newsworthiness":0.6474224125,
        "title":"Deep Learning and Artificial General Intelligence: Still a Long Way to Go",
        "summary":"In recent years, deep learning using neural network architecture, i.e. deep neural networks, has been on the frontier of computer science research. It has even lead to superhuman performance in some problems, e.g., in computer vision, games and biology, and as a result the term deep learning revolution was coined. The undisputed success and rapid growth of deep learning suggests that, in future, it might become an enabler for Artificial General Intelligence (AGI). In this article, we approach this statement critically showing five major reasons of why deep neural networks, as of the current state, are not ready to be the technique of choice for reaching AGI.",
        "completion1":"Deep Learning and Artificial General Intelligence: Still a Long Way to Go",
        "completion2":"Deep Learning Revolution Coined",
        "completion3":"Deep Neural Networks Not Yet Ready for Artificial General Intelligence",
        "technologyreview":0.4553333715,
        "venturebeat":0.3570195282,
        "wired":0.0977627127,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.14963v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1648251377000,
        "code_mentioned":0,
        "readability":0.92
    },
    {
        "arxiv_id":"2205.15305v1",
        "predicted_newsworthiness":0.5185434628,
        "title":"A Design Space for Explainable Ranking and Ranking Models",
        "summary":"Item ranking systems support users in multi-criteria decision-making tasks. Users need to trust rankings and ranking algorithms to reflect user preferences nicely while avoiding systematic errors and biases. However, today only few approaches help end users, model developers, and analysts to explain rankings. We report on the study of explanation approaches from the perspectives of recommender systems, explainable AI, and visualization research and propose the first cross-domain design space for explainers of item rankings. In addition, we leverage the descriptive power of the design space to characterize a) existing explainers and b) three main user groups involved in ranking explanation tasks. The generative power of the design space is a means for future designers and developers to create more target-oriented solutions in this only weakly exploited space.",
        "completion1":"Design Space for Explainable Ranking and Ranking Models Could Help End Users Trust Algorithms",
        "completion2":"Study Proposes Cross-Domain Design Space for Explainers of Item Rankings",
        "completion3":" researchers propose the first cross-domain design space for explainers of item rankings in order to help end users trust ranking algorithms.",
        "technologyreview":0.2814692594,
        "venturebeat":0.2721666958,
        "wired":0.1114691189,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.15305v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ir"
        ],
        "published":1653638171000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2202.00481v1",
        "predicted_newsworthiness":0.5576714457,
        "title":"RabindraNet, Creating Literary Works in the Style of Rabindranath Tagore",
        "summary":"Bengali literature has a rich history of hundreds of years with luminary figures such as Rabindranath Tagore and Kazi Nazrul Islam. However, analytical works involving the most recent advancements in NLP have barely scratched the surface utilizing the enormous volume of the collected works from the writers of the language. In order to bring attention to the analytical study involving the works of Bengali writers and spearhead the text generation endeavours in the style of existing literature, we are introducing RabindraNet, a character level RNN model with stacked-LSTM layers trained on the works of Rabindranath Tagore to produce literary works in his style for multiple genres. We created an extensive dataset as well by compiling the digitized works of Rabindranath Tagore from authentic online sources and published as open source dataset on data science platform Kaggle.",
        "completion1":"Bengali literature gets a boost with new AI project",
        "completion2":"RabindraNet to bring attention to analytical study of Bengali writers",
        "completion3":"New AI project generates literary works in style of Rabindranath Tagore",
        "technologyreview":0.2304225657,
        "venturebeat":0.1817907394,
        "wired":0.0798444195,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.00481v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1641399817000,
        "code_mentioned":0,
        "readability":0.8
    },
    {
        "arxiv_id":"2204.06945v1",
        "predicted_newsworthiness":0.5346510006,
        "title":"The Vision of a Human-Centered Piano",
        "summary":"For around 300 years, humans have been learning to play the modern piano either with a teacher or on their own. In recent years teaching and learning have been enhanced using augmented technologies that support novices. Other technologies have also tried to improve different use cases with the piano, such as composing and performing. Researchers and practitioners have showcased several forms of augmentation, from hardware improvements, sound quality, rendering projected visualizations to gesture-based and immersive technologies. Today, the landscape of piano augmentations is very diverse, and it is unclear how to describe the ideal piano and its features. In this work, we discuss how the human-centered piano -- the piano that has been designed with humans in the center of the design process and that effectively supports tasks performed on it -- can support pianists. In detail, we present the three tasks of learning, composing, and improvising in which a human-centered piano would be beneficial for the pianist.",
        "completion1":"The Vision of a Human-Centered Piano",
        "completion2":"The Benefits of a Human-Centered Piano",
        "completion3":"The Future of the Piano Augmentation Landscape",
        "technologyreview":0.2941966045,
        "venturebeat":0.2920656825,
        "wired":0.1006282688,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.06945v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1649942206000,
        "code_mentioned":1,
        "readability":0.87
    },
    {
        "arxiv_id":"2205.08249v1",
        "predicted_newsworthiness":0.3749067878,
        "title":"Learnable Optimal Sequential Grouping for Video Scene Detection",
        "summary":"Video scene detection is the task of dividing videos into temporal semantic chapters. This is an important preliminary step before attempting to analyze heterogeneous video content. Recently, Optimal Sequential Grouping (OSG) was proposed as a powerful unsupervised solution to solve a formulation of the video scene detection problem. In this work, we extend the capabilities of OSG to the learning regime. By giving the capability to both learn from examples and leverage a robust optimization formulation, we can boost performance and enhance the versatility of the technology. We present a comprehensive analysis of incorporating OSG into deep learning neural networks under various configurations. These configurations include learning an embedding in a straight-forward manner, a tailored loss designed to guide the solution of OSG, and an integrated model where the learning is performed through the OSG pipeline. With thorough evaluation and analysis, we assess the benefits and behavior of the various configurations, and show that our learnable OSG approach exhibits desirable behavior and enhanced performance compared to the state of the art.",
        "completion1":"Optimal Sequential Grouping: A powerful unsupervised solution to the video scene detection problem",
        "completion2":"Optimal Sequential Grouping for Video Scene Detection: A Comprehensive Analysis",
        "completion3":"Learnable Optimal Sequential Grouping for Video Scene Detection: Enhanced Performance and Desirable Behavior",
        "technologyreview":0.1918065351,
        "venturebeat":0.1526511647,
        "wired":0.0432676472,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.08249v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1652787903000,
        "code_mentioned":0,
        "readability":0.8
    },
    {
        "arxiv_id":"2201.12021v1",
        "predicted_newsworthiness":0.5688538185,
        "title":"Network Selection schemes in Heterogeneous Wireless Networks",
        "summary":"Heterogeneous Wireless Networks HWNs are combined networks made of different Radio Access Technologies RAT. Next-Generation Networks NGN will provide high bandwidth connectivity and high data throughput with smooth support for the user's QoS requirements, in this context, users with multi-interface terminals will be able to connect to different wireless technologies such as 802.16, 802.11 families and cellular families UMTS, HSPA and LTE in the same time. The idea of NGN is that users will not be tied with a contract with one single operator but, users will be able to choose the Radio Access Network RAT considering the user's QoS requested. This paper focuses on the network selection strategies and the inter technologies Handoff, we will present a description of the existed methods of network selection, we will discuss the merits and the weakness of such method and we will give our point of view.",
        "completion1":"Multi-Interface terminals will be able to connect to different wireless technologies in the same time.",
        "completion2":"he idea of NGN is that users will not be tied with a contract with one single operator but, users will be able to choose the Radio Access Network RAT considering the user's QoS requested.",
        "completion3":"n this paper we will present a description of the existed methods of network selection and we will discuss the merits and weaknesses of such methods.",
        "technologyreview":0.1463238873,
        "venturebeat":0.163544289,
        "wired":0.0295640073,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.12021v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1643364162000,
        "code_mentioned":0,
        "readability":0.85
    },
    {
        "arxiv_id":"2205.03860v4",
        "predicted_newsworthiness":0.4124944655,
        "title":"Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-Language Framework",
        "summary":"Vision-language pre-training (VLP) on large-scale datasets has shown premier performance on various downstream tasks. A complete and fair benchmark (i.e., including large-scale pre-training datasets and diverse downstream tasks) is essential for VLP. While there are plenty of benchmarks with English corpus, building a rich benchmark for VLP with other languages, such as Chinese, remains a critical problem. To this end, we build a large-scale Chinese cross-modal benchmark called Zero for the research community to fairly compare VLP models. We release two pre-training datasets and five fine-tuning datasets for downstream tasks. Alongside, we propose a novel pre-training framework of pre-Ranking + Ranking for cross-modal learning. Specifically, we apply global contrastive pre-ranking to learn the individual representations of images and texts, respectively. We then fuse the representations in a fine-grained ranking manner via an image-text cross encoder and a text-image cross encoder. To further enhance the capability of the model, we propose a two-way distillation strategy consisting of target-guided Distillation and feature-guided Distillation. For brevity, we name our model R2D2. We achieve state-of-the-art performance on four public cross-modal datasets and the proposed five downstream datasets. When conducting zero-shot tasks on Flickr30k-CN, COCO-CN, and MUGE, R2D2 pre-trained on a 250 million dataset achieves significant improvements of 4.7%, 5.4%, and 6.3% in mean recall compared to the state-of-the-art. The datasets, models, and codes are available at https:\/\/github.com\/yuxie11\/R2D2",
        "completion1":"Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark",
        "completion2":"Zero and R2D2: A Vision-Language Framework",
        "completion3":"Zero and R2D2: State-of-the-art Performance",
        "technologyreview":0.2213562202,
        "venturebeat":0.1671022497,
        "wired":0.0593565524,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.03860v4",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1652015963000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2201.10633v3",
        "predicted_newsworthiness":0.4372882451,
        "title":"Informative Path Planning to Estimate Quantiles for Environmental Analysis",
        "summary":"Scientists interested in studying natural phenomena often take physical specimens from locations in the environment for later analysis. These analysis locations are typically specified by expert heuristics. Instead, we propose to choose locations for scientific analysis by using a robot to perform an informative path planning survey. The survey results in a list of locations that correspond to the quantile values of the phenomenon of interest. We develop a robot planner using novel objective functions to improve the estimates of the quantile values over time and an approach to find locations which correspond to the quantile values. We test our approach in four different environments using previously collected aquatic data and validate it in a field trial. Our proposed approach to estimate quantiles has a 10.2% mean reduction in median error when compared to a baseline approach which attempts to maximize spatial coverage. Additionally, when localizing these values in the environment, we see a 15.7% mean reduction in median error when using cross-entropy with our loss function compared to a baseline.",
        "completion1":"Scientists Develop Robot to Find Optimal Locations for Environmental Analysis",
        "completion2":"Robot Planner Uses Novel Objective Functions to Improve Estimates of Quantile Values",
        "completion3":"Field Trial Shows Proposed Approach to Estimate Quantiles is Highly Accurate",
        "technologyreview":0.229391798,
        "venturebeat":0.1644348502,
        "wired":0.0289137136,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.10633v3",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1643145665000,
        "code_mentioned":0,
        "readability":0.87
    },
    {
        "arxiv_id":"2203.01369v2",
        "predicted_newsworthiness":0.3818905539,
        "title":"ePA*SE: Edge-based Parallel A* for Slow Evaluations",
        "summary":"Parallel search algorithms harness the multithreading capability of modern processors to achieve faster planning. One such algorithm is PA*SE (Parallel A* for Slow Expansions), which parallelizes state expansions to achieve faster planning in domains where state expansions are slow. In this work, we propose ePA*SE (Edge-based Parallel A* for Slow Evaluations) that improves on PA*SE by parallelizing edge evaluations instead of state expansions. This makes ePA*SE more efficient in domains where edge evaluations are expensive and need varying amounts of computational effort, which is often the case in robotics. On the theoretical front, we show that ePA*SE provides rigorous optimality guarantees. In addition, ePA*SE can be trivially extended to handle an inflation weight on the heuristic resulting in a bounded suboptimal algorithm w-ePA*SE (Weighted ePA*SE) that trades off optimality for faster planning. On the experimental front, we validate the proposed algorithm in two different planning domains: 1) motion planning for 3D humanoid navigation and 2) task and motion planning for a dual-arm robotic assembly task. We show that ePA*SE can be significantly more efficient than PA*SE and other alternatives. The open-source code for ePA*SE along with the baselines is available here: https:\/\/github.com\/shohinm\/parallel_search",
        "completion1":"ePA*SE: Edge-based Parallel A* for Slow Evaluations",
        "completion2":"ePA*SE more efficient in domains where edge evaluations are expensive",
        "completion3":"ePA*SE provides rigorous optimality guarantees",
        "technologyreview":0.2188738351,
        "venturebeat":0.171667215,
        "wired":0.0244562695,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.01369v2",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1646249598000,
        "code_mentioned":1,
        "readability":0.77
    },
    {
        "arxiv_id":"2201.07124v2",
        "predicted_newsworthiness":0.4802812549,
        "title":"Attentional Feature Refinement and Alignment Network for Aircraft Detection in SAR Imagery",
        "summary":"Aircraft detection in Synthetic Aperture Radar (SAR) imagery is a challenging task in SAR Automatic Target Recognition (SAR ATR) areas due to aircraft's extremely discrete appearance, obvious intraclass variation, small size and serious background's interference. In this paper, a single-shot detector namely Attentional Feature Refinement and Alignment Network (AFRAN) is proposed for detecting aircraft in SAR images with competitive accuracy and speed. Specifically, three significant components including Attention Feature Fusion Module (AFFM), Deformable Lateral Connection Module (DLCM) and Anchor-guided Detection Module (ADM), are carefully designed in our method for refining and aligning informative characteristics of aircraft. To represent characteristics of aircraft with less interference, low-level textural and high-level semantic features of aircraft are fused and refined in AFFM throughly. The alignment between aircraft's discrete back-scatting points and convolutional sampling spots is promoted in DLCM. Eventually, the locations of aircraft are predicted precisely in ADM based on aligned features revised by refined anchors. To evaluate the performance of our method, a self-built SAR aircraft sliced dataset and a large scene SAR image are collected. Extensive quantitative and qualitative experiments with detailed analysis illustrate the effectiveness of the three proposed components. Furthermore, the topmost detection accuracy and competitive speed are achieved by our method compared with other domain-specific,e.g., DAPN, PADN, and general CNN-based methods,e.g., FPN, Cascade R-CNN, SSD, RefineDet and RPDet.",
        "completion1":"New AI can detect aircraft in SAR images with high accuracy",
        "completion2":"Alignment Network for Aircraft Detection in SAR Imagery",
        "completion3":"Deformable Lateral Connection Module  and Anchor-guided Detection Module",
        "technologyreview":0.2179468799,
        "venturebeat":0.1670793827,
        "wired":0.0446820411,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.07124v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1642524889000,
        "code_mentioned":0,
        "readability":0.75
    },
    {
        "arxiv_id":"2205.01416v2",
        "predicted_newsworthiness":0.3777227553,
        "title":"Exact Paired-Permutation Testing for Structured Test Statistics",
        "summary":"Significance testing -- especially the paired-permutation test -- has played a vital role in developing NLP systems to provide confidence that the difference in performance between two systems (i.e., the test statistic) is not due to luck. However, practitioners rely on Monte Carlo approximation to perform this test due to a lack of a suitable exact algorithm. In this paper, we provide an efficient exact algorithm for the paired-permutation test for a family of structured test statistics. Our algorithm runs in $\\mathcal{O}(GN (\\log GN )(\\log N ))$ time where $N$ is the dataset size and $G$ is the range of the test statistic. We found that our exact algorithm was $10$x faster than the Monte Carlo approximation with $20000$ samples on a common dataset.",
        "completion1":"NLP Researchers Develop Exact Paired-Permutation Testing",
        "completion2":"New Algorithm Runs in  Samples on Common Dataset",
        "completion3":"Exact Paired-Permutation Test Provides Confidence for NLP Systems",
        "technologyreview":0.159448715,
        "venturebeat":0.1379337062,
        "wired":0.0221020745,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.01416v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1651575659000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2201.12888v1",
        "predicted_newsworthiness":0.4519268369,
        "title":"A Dataset for Medical Instructional Video Classification and Question Answering",
        "summary":"This paper introduces a new challenge and datasets to foster research toward designing systems that can understand medical videos and provide visual answers to natural language questions. We believe medical videos may provide the best possible answers to many first aids, medical emergency, and medical education questions. Toward this, we created the MedVidCL and MedVidQA datasets and introduce the tasks of Medical Video Classification (MVC) and Medical Visual Answer Localization (MVAL), two tasks that focus on cross-modal (medical language and medical video) understanding. The proposed tasks and datasets have the potential to support the development of sophisticated downstream applications that can benefit the public and medical practitioners. Our datasets consist of 6,117 annotated videos for the MVC task and 3,010 annotated questions and answers timestamps from 899 videos for the MVAL task. These datasets have been verified and corrected by medical informatics experts. We have also benchmarked each task with the created MedVidCL and MedVidQA datasets and proposed the multimodal learning methods that set competitive baselines for future research.",
        "completion1":"A new dataset for medical instructional video classification and question answering has been introduced to foster research toward designing systems that can understand medical videos and provide visual answers to natural language questions.",
        "completion2":"The proposed tasks and datasets have the potential to support the development of sophisticated downstream applications that can benefit the public and medical practitioners.",
        "completion3":"The MedVidCL and MedVidQA datasets have been verified and corrected by medical informatics experts, and benchmarked each task with the created MedVidCL and MedVidQA datasets.",
        "technologyreview":0.2526605611,
        "venturebeat":0.2135245053,
        "wired":0.0717710882,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.12888v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.cl"
        ],
        "published":1643565991000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2207.04969v1",
        "predicted_newsworthiness":0.7447139107,
        "title":"From Correlation to Causation: Formalizing Interpretable Machine Learning as a Statistical Process",
        "summary":"Explainable AI (XAI) is a necessity in safety-critical systems such as in clinical diagnostics due to a high risk for fatal decisions. Currently, however, XAI resembles a loose collection of methods rather than a well-defined process. In this work, we elaborate on conceptual similarities between the largest subgroup of XAI, interpretable machine learning (IML), and classical statistics. Based on these similarities, we present a formalization of IML along the lines of a statistical process. Adopting this statistical view allows us to interpret machine learning models and IML methods as sophisticated statistical tools. Based on this interpretation, we infer three key questions, which we identify as crucial for the success and adoption of IML in safety-critical settings. By formulating these questions, we further aim to spark a discussion about what distinguishes IML from classical statistics and what our perspective implies for the future of the field.",
        "completion1":"Formalizing Interpretable Machine Learning as a Statistical Process",
        "completion2":"Explainable AI is a necessity in safety-critical systems",
        "completion3":"IML methods as sophisticated statistical tools",
        "technologyreview":0.3731880338,
        "venturebeat":0.3036291103,
        "wired":0.0822337876,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.04969v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1657554554000,
        "code_mentioned":0,
        "readability":0.82
    },
    {
        "arxiv_id":"2203.03296v1",
        "predicted_newsworthiness":0.4538864962,
        "title":"Whole-Body Control for Velocity-Controlled Mobile Collaborative Robots Using Coupling Dynamic Movement Primitives",
        "summary":"In this paper, we propose a unified whole-body control framework for velocity-controlled mobile collaborative robots which can distribute task motion into the base and arm according to specific task requirements by adjusting weighting factors. Our framework focuses on addressing two challenging issues in whole-body coordination: 1) different dynamic characteristics of base and arm; 2) avoidance of violating constraints of both safety and configuration. In addition, our controller involves Coupling Dynamic Movement Primitive to enable the essential compliance capabilities for collaboration and interaction applications, such as obstacle avoidance, hand guiding, and force control. Based on these, we design a motion mode of intuitive physical human-robot interaction through an adjusting strategy of the weighting factors. The proposed controller is in closed-form and thus quite computationally efficient against the-state-of-art optimization-based methods. Experimental results of a real mobile collaborative robot validate the effectiveness of the proposed controller. The documented video of the experiments is available at https:\/\/youtu.be\/x2Z593dV9C8.",
        "completion1":"New Controller for Mobile Collaborative Robots",
        "completion2":"Improved Efficiency and Accuracy for Mobile Robots",
        "completion3":"Increased Capabilities for Mobile Robots",
        "technologyreview":0.1791022971,
        "venturebeat":0.1332249558,
        "wired":0.0234733057,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.03296v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1646651840000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2204.04518v1",
        "predicted_newsworthiness":0.3884556329,
        "title":"Attention U-Net as a surrogate model for groundwater prediction",
        "summary":"Numerical simulations of groundwater flow are used to analyze and predict the response of an aquifer system to its change in state by approximating the solution of the fundamental groundwater physical equations. The most used and classical methodologies, such as Finite Difference (FD) and Finite Element (FE) Methods, use iterative solvers which are associated with high computational cost. This study proposes a physics-based convolutional encoder-decoder neural network as a surrogate model to quickly calculate the response of the groundwater system. Holding strong promise in cross-domain mappings, encoder-decoder networks are applicable for learning complex input-output mappings of physical systems. This manuscript presents an Attention U-Net model that attempts to capture the fundamental input-output relations of the groundwater system and generates solutions of hydraulic head in the whole domain given a set of physical parameters and boundary conditions. The model accurately predicts the steady state response of a highly heterogeneous groundwater system given the locations and piezometric head of up to 3 wells as input. The network learns to pay attention only in the relevant parts of the domain and the generated hydraulic head field corresponds to the target samples in great detail. Even relative to coarse finite difference approximations the proposed model is shown to be significantly faster than a comparative state-of-the-art numerical solver, thus providing a base for further development of the presented networks as surrogate models for groundwater prediction.",
        "completion1":"Proposed new neural network predicts groundwater better than classical methods.",
        "completion2":"New Attention U-Net model accurately captures input-output relations of groundwater system.",
        "completion3":"Model significantly faster than comparative state-of-the-art numerical solver, providing a base for further development.",
        "technologyreview":0.2062050062,
        "venturebeat":0.1408310371,
        "wired":0.0315150471,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.04518v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1649526384000,
        "code_mentioned":0,
        "readability":0.83
    },
    {
        "arxiv_id":"2203.03078v1",
        "predicted_newsworthiness":0.4058558422,
        "title":"Fast and Data Efficient Reinforcement Learning from Pixels via Non-Parametric Value Approximation",
        "summary":"We present Nonparametric Approximation of Inter-Trace returns (NAIT), a Reinforcement Learning algorithm for discrete action, pixel-based environments that is both highly sample and computation efficient. NAIT is a lazy-learning approach with an update that is equivalent to episodic Monte-Carlo on episode completion, but that allows the stable incorporation of rewards while an episode is ongoing. We make use of a fixed domain-agnostic representation, simple distance based exploration and a proximity graph-based lookup to facilitate extremely fast execution. We empirically evaluate NAIT on both the 26 and 57 game variants of ATARI100k where, despite its simplicity, it achieves competitive performance in the online setting with greater than 100x speedup in wall-time.",
        "completion1":"Reinforcement Learning algorithm achieves competitive performance in online setting with greater than 100x speedup in wall-time",
        "completion2":"Fixed domain-agnostic representation, simple distance based exploration and proximity graph-based lookup facilitate extremely fast execution",
        "completion3":"Lazy-learning approach with an update that is equivalent to episodic Monte-Carlo on episode completion, but that allows the stable incorporation of rewards while an episode is ongoing",
        "technologyreview":0.2458175609,
        "venturebeat":0.2381038126,
        "wired":0.0743524056,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.03078v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1646613091000,
        "code_mentioned":1,
        "readability":0.78
    },
    {
        "arxiv_id":"2203.02083v1",
        "predicted_newsworthiness":0.4896470908,
        "title":"TransMUSE: Transferable Traffic Prediction in MUlti-Service EdgeNetworks",
        "summary":"The Covid-19 pandemic has forced the workforce to switch to working from home, which has put significant burdens on the management of broadband networks and called for intelligent service-by-service resource optimization at the network edge. In this context, network traffic prediction is crucial for operators to provide reliable connectivity across large geographic regions. Although recent advances in neural network design have demonstrated potential to effectively tackle forecasting, in this work we reveal based on real-world measurements that network traffic across different regions differs widely. As a result, models trained on historical traffic data observed in one region can hardly serve in making accurate predictions in other areas. Training bespoke models for different regions is tempting, but that approach bears significant measurement overhead, is computationally expensive, and does not scale. Therefore, in this paper we propose TransMUSE, a novel deep learning framework that clusters similar services, groups edge-nodes into cohorts by traffic feature similarity, and employs a Transformer-based Multi-service Traffic Prediction Network (TMTPN), which can be directly transferred within a cohort without any customization. We demonstrate that TransMUSE exhibits imperceptible performance degradation in terms of mean absolute error (MAE) when forecasting traffic, compared with settings where a model is trained for each individual edge node. Moreover, our proposed TMTPN architecture outperforms the state-of-the-art, achieving up to 43.21% lower MAE in the multi-service traffic prediction task. To the best of our knowledge, this is the first work that jointly employs model transfer and multi-service traffic prediction to reduce measurement overhead, while providing fine-grained accurate demand forecasts for edge services provisioning.",
        "completion1":"TransMUSE: Transferable Traffic Prediction in MUlti-Service EdgeNetworks.",
        "completion2":"The Covid-19 pandemic has forced the workforce to switch to working from home, which has put significant burdens on the management of broadband networks and called for intelligent service-by-service resource optimization at the network edge.",
        "completion3":"In this context, network traffic prediction is crucial for operators to provide reliable connectivity across large geographic regions.",
        "technologyreview":0.2636981574,
        "venturebeat":0.265947415,
        "wired":0.0687816025,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.02083v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1646356566000,
        "code_mentioned":1,
        "readability":0.88
    },
    {
        "arxiv_id":"2205.09363v1",
        "predicted_newsworthiness":0.3387985493,
        "title":"Plane Geometry Diagram Parsing",
        "summary":"Geometry diagram parsing plays a key role in geometry problem solving, wherein the primitive extraction and relation parsing remain challenging due to the complex layout and between-primitive relationship. In this paper, we propose a powerful diagram parser based on deep learning and graph reasoning. Specifically, a modified instance segmentation method is proposed to extract geometric primitives, and the graph neural network (GNN) is leveraged to realize relation parsing and primitive classification incorporating geometric features and prior knowledge. All the modules are integrated into an end-to-end model called PGDPNet to perform all the sub-tasks simultaneously. In addition, we build a new large-scale geometry diagram dataset named PGDP5K with primitive level annotations. Experiments on PGDP5K and an existing dataset IMP-Geometry3K show that our model outperforms state-of-the-art methods in four sub-tasks remarkably. Our code, dataset and appendix material are available at https:\/\/github.com\/mingliangzhang2018\/PGDP.",
        "completion1":"A Powerful Diagram Parser Based on Deep Learning and Graph Reasoning",
        "completion2":"Deep Learning and Graph Reasoning Key to Plane Geometry Problem Solving",
        "completion3":"New Large-Scale Geometry Dataset Named PGDP5K Shows Promise for AI Plane Geometry Understanding",
        "technologyreview":0.1820256041,
        "venturebeat":0.1521411659,
        "wired":0.0222834575,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.09363v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1652946421000,
        "code_mentioned":1,
        "readability":0.73
    },
    {
        "arxiv_id":"2204.08465v1",
        "predicted_newsworthiness":0.5561088361,
        "title":"Intelligent Spatial Interpolation-based Frost Prediction Methodology using Artificial Neural Networks with Limited Local Data",
        "summary":"The weather phenomenon of frost poses great threats to agriculture. Since it damages the crops and plants from upstream of the supply chain, the potential impact of frosts is significant for agriculture-related industries. As recent frost prediction methods are based on on-site historical data and sensors, extra development and deployment time are required for data collection in any new site. The aim of this article is to eliminate the dependency on on-site historical data and sensors for frost prediction methods. In this article, a frost prediction method based on spatial interpolation is proposed. The models use climate data from existing weather stations, digital elevation models surveys, and normalized difference vegetation index data to estimate a target site's next hour minimum temperature. The proposed method utilizes ensemble learning to increase the model accuracy. Ensemble methods include averaging and weighted averaging. Climate datasets are obtained from 75 weather stations across New South Wales and Australian Capital Territory areas of Australia. The models are constructed with five-fold validation, splitting the weather stations into five testing dataset folds. For each fold, the other stations act as training datasets. After the models are constructed, three experiments are conducted. The first experiment compares the results generated by models between different folds. Then, the second experiment compares the accuracy of different methods. The final experiment reveals the effect of available stations on the proposed models. The results show that the proposed method reached a detection rate up to 92.55%. This method could be implemented as an alternative solution when on-site historical datasets are scarce.",
        "completion1":"New method for predicting frost doesn't require on-site historical data",
        "completion2":"Proposed frost prediction method is 92.55% accurate",
        "completion3":"New method for predicting frost could be implemented as an alternative when on-site historical data is scarce",
        "technologyreview":0.223847809,
        "venturebeat":0.1671738899,
        "wired":0.0221767954,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.08465v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1650057247000,
        "code_mentioned":1,
        "readability":0.87
    },
    {
        "arxiv_id":"2203.10297v2",
        "predicted_newsworthiness":0.4016758323,
        "title":"Incremental Few-Shot Learning via Implanting and Compressing",
        "summary":"This work focuses on tackling the challenging but realistic visual task of Incremental Few-Shot Learning (IFSL), which requires a model to continually learn novel classes from only a few examples while not forgetting the base classes on which it was pre-trained. Our study reveals that the challenges of IFSL lie in both inter-class separation and novel-class representation. Dur to intra-class variation, a novel class may implicitly leverage the knowledge from multiple base classes to construct its feature representation. Hence, simply reusing the pre-trained embedding space could lead to a scattered feature distribution and result in category confusion. To address such issues, we propose a two-step learning strategy referred to as \\textbf{Im}planting and \\textbf{Co}mpressing (\\textbf{IMCO}), which optimizes both feature space partition and novel class reconstruction in a systematic manner. Specifically, in the \\textbf{Implanting} step, we propose to mimic the data distribution of novel classes with the assistance of data-abundant base set, so that a model could learn semantically-rich features that are beneficial for discriminating between the base and other unseen classes. In the \\textbf{Compressing} step, we adapt the feature extractor to precisely represent each novel class for enhancing intra-class compactness, together with a regularized parameter updating rule for preventing aggressive model updating. Finally, we demonstrate that IMCO outperforms competing baselines with a significant margin, both in image classification task and more challenging object detection task.",
        "completion1":"Incremental Few-Shot Learning: A New Strategy to Learn Novel Classes",
        "completion2":"Two-Step Learning Strategy Outperforms Competing Baselines in Object Detection Task",
        "completion3":"Enhancing Intra-Class Compactness with Regularized Parameter Updating",
        "technologyreview":0.2253220221,
        "venturebeat":0.1783565028,
        "wired":0.057199839,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.10297v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1647687883000,
        "code_mentioned":0,
        "readability":0.85
    },
    {
        "arxiv_id":"2202.07962v1",
        "predicted_newsworthiness":0.4031192998,
        "title":"Revisiting Parameter-Efficient Tuning: Are We Really There Yet?",
        "summary":"Parameter-efficient tuning (PETuning) methods have been deemed by many as the new paradigm for using pretrained language models (PLMs). By tuning just a fraction amount of parameters comparing to full model finetuning, PETuning methods claim to have achieved performance on par with or even better than finetuning. In this work, we take a step back and re-examine these PETuning methods by conducting the first comprehensive investigation into the training and evaluation of PETuning methods. We found the problematic validation and testing practice in current studies, when accompanied by the instability nature of PETuning methods, has led to unreliable conclusions. When being compared under a truly fair evaluation protocol, PETuning cannot yield consistently competitive performance while finetuning remains to be the best-performing method in medium- and high-resource settings. We delve deeper into the cause of the instability and observed that model size does not explain the phenomenon but training iteration positively correlates with the stability.",
        "completion1":" Parameter-Efficient Tuning Methods Ineffective, According to New Study",
        "completion2":" Study Finds That PETuning Methods Are Not as Effective as Previously Thought",
        "completion3":" New Research Shows That Parameter-Efficient Tuning Methods May Not Be the Best Option",
        "technologyreview":0.203468044,
        "venturebeat":0.1769330788,
        "wired":0.0526529709,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.07962v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1645006279000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2204.04842v1",
        "predicted_newsworthiness":0.4500759273,
        "title":"Towards Homogeneous Modality Learning and Multi-Granularity Information Exploration for Visible-Infrared Person Re-Identification",
        "summary":"Visible-infrared person re-identification (VI-ReID) is a challenging and essential task, which aims to retrieve a set of person images over visible and infrared camera views. In order to mitigate the impact of large modality discrepancy existing in heterogeneous images, previous methods attempt to apply generative adversarial network (GAN) to generate the modality-consisitent data. However, due to severe color variations between the visible domain and infrared domain, the generated fake cross-modality samples often fail to possess good qualities to fill the modality gap between synthesized scenarios and target real ones, which leads to sub-optimal feature representations. In this work, we address cross-modality matching problem with Aligned Grayscale Modality (AGM), an unified dark-line spectrum that reformulates visible-infrared dual-mode learning as a gray-gray single-mode learning problem. Specifically, we generate the grasycale modality from the homogeneous visible images. Then, we train a style tranfer model to transfer infrared images into homogeneous grayscale images. In this way, the modality discrepancy is significantly reduced in the image space. In order to reduce the remaining appearance discrepancy, we further introduce a multi-granularity feature extraction network to conduct feature-level alignment. Rather than relying on the global information, we propose to exploit local (head-shoulder) features to assist person Re-ID, which complements each other to form a stronger feature descriptor. Comprehensive experiments implemented on the mainstream evaluation datasets include SYSU-MM01 and RegDB indicate that our method can significantly boost cross-modality retrieval performance against the state of the art methods.",
        "completion1":"Towards Homogeneous Modality Learning and Multi-Granularity Information Exploration for Visible-Infrared Person Re-Identification",
        "completion2":"Visible-infrared person re-identification (VI-ReID) is a challenging and essential task, which aims to retrieve a set of person images over visible and infrared camera views.",
        "completion3":"In order to mitigate the impact of large modality discrepancy existing in heterogeneous images, previous methods attempt to apply generative adversarial network  to generate the modality-consisitent data.",
        "technologyreview":0.2110406712,
        "venturebeat":0.150078198,
        "wired":0.0440855379,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.04842v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1649646199000,
        "code_mentioned":0,
        "readability":0.81
    },
    {
        "arxiv_id":"2205.15223v2",
        "predicted_newsworthiness":0.4892555872,
        "title":"Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained Models",
        "summary":"Pre-trained masked language models successfully perform few-shot learning by formulating downstream tasks as text infilling. However, as a strong alternative in full-shot settings, discriminative pre-trained models like ELECTRA do not fit into the paradigm. In this work, we adapt prompt-based few-shot learning to ELECTRA and show that it outperforms masked language models in a wide range of tasks. ELECTRA is pre-trained to distinguish if a token is generated or original. We naturally extend that to prompt-based few-shot learning by training to score the originality of the target options without introducing new parameters. Our method can be easily adapted to tasks involving multi-token predictions without extra computation overhead. Analysis shows that ELECTRA learns distributions that align better with downstream tasks.",
        "completion1":"ELECTRA: A new pre-trained model that outperforms masked language models in a wide range of tasks.",
        "completion2":"Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained Models.",
        "completion3":"ELECTRA: A New Pre-Trained Model That Outperforms Masked Language Models.",
        "technologyreview":0.2381536544,
        "venturebeat":0.2077324184,
        "wired":0.0671974233,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.15223v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1653928350000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2205.00824v1",
        "predicted_newsworthiness":0.4375693796,
        "title":"Exploration in Deep Reinforcement Learning: A Survey",
        "summary":"This paper reviews exploration techniques in deep reinforcement learning. Exploration techniques are of primary importance when solving sparse reward problems. In sparse reward problems, the reward is rare, which means that the agent will not find the reward often by acting randomly. In such a scenario, it is challenging for reinforcement learning to learn rewards and actions association. Thus more sophisticated exploration methods need to be devised. This review provides a comprehensive overview of existing exploration approaches, which are categorized based on the key contributions as follows reward novel states, reward diverse behaviours, goal-based methods, probabilistic methods, imitation-based methods, safe exploration and random-based methods. Then, the unsolved challenges are discussed to provide valuable future research directions. Finally, the approaches of different categories are compared in terms of complexity, computational effort and overall performance.",
        "completion1":"Deep reinforcement learning survey reveals key challenges and future directions.",
        "completion2":"Exploration in deep reinforcement learning: A review of existing approaches.",
        "completion3":"A comprehensive overview of exploration techniques in deep reinforcement learning.",
        "technologyreview":0.2213811806,
        "venturebeat":0.1843893963,
        "wired":0.0326649853,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.00824v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1651493024000,
        "code_mentioned":0,
        "readability":0.87
    },
    {
        "arxiv_id":"2205.04017v2",
        "predicted_newsworthiness":0.6409783376,
        "title":"What Do You Get from Turning on Your Video? Effects of Videoconferencing Affordances on Remote Class Experience During COVID-19",
        "summary":"The outbreak of COVID-19 forced schools to swiftly transition from in-person classes to online or remote offerings, making educators and learners alike rely on online videoconferencing platforms. Platforms like Zoom offer audio-visual channels of communication and include features that are designed to approximate the classroom experience. However, it is not clear how students' learning experiences are affected by affordances of the videoconferencing platforms or what underlying factors explain the differential effects of these affordances on class experiences of engagement, interaction, and satisfaction. In order to find out, we conducted two online survey studies: Study 1 (N = 176) investigated the effects of three types of videoconferencing affordances (i.e., modality, interactivity, and agency affordances) on class experience during the first two months after the transition to online learning. Results showed that usage of the three kinds of affordances was positively correlated with students' class engagement, interaction, and satisfaction. Perceived anonymity, nonverbal cues, and comfort level were found to be the key mediators. In addition, students' usage of video cameras in class was influenced by their classmates. Study 2 (N = 256) tested the proposed relationships at a later stage of the pandemic and found similar results, thus serving as a constructive replication. This paper focuses on reporting the results of Study 1 since it captures the timely reactions from students when they first went online, and the second study plays a supplementary role in verifying Study 1 and thereby extending its external validity. Together, the two studies provide insights for instructors on how to leverage different videoconferencing affordances to enhance the virtual learning experience. Design implications for digital tools in online education are also discussed.",
        "completion1":"The outbreak of COVID-19 forces schools to rely on online videoconferencing platforms.",
        "completion2":"Online videoconferencing platforms offer audio-visual channels of communication and include features that are designed to approximate the classroom experience.",
        "completion3":"It is not clear how students' learning experiences are affected by affordances of the videoconferencing platforms or what underlying factors explain the differential effects of these affordances on class experiences of engagement, interaction, and satisfaction.",
        "technologyreview":0.3192123709,
        "venturebeat":0.2952255373,
        "wired":0.1475522731,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.04017v2",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1652067117000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2206.02252v1",
        "predicted_newsworthiness":0.5084976031,
        "title":"Exploring Cross-lingual Textual Style Transfer with Large Multilingual Language Models",
        "summary":"Detoxification is a task of generating text in polite style while preserving meaning and fluency of the original toxic text. Existing detoxification methods are designed to work in one exact language. This work investigates multilingual and cross-lingual detoxification and the behavior of large multilingual models like in this setting. Unlike previous works we aim to make large language models able to perform detoxification without direct fine-tuning in given language. Experiments show that multilingual models are capable of performing multilingual style transfer. However, models are not able to perform cross-lingual detoxification and direct fine-tuning on exact language is inevitable.",
        "completion1":"Exploring Cross-lingual Textual Style Transfer with Large Multilingual Language Models",
        "completion2":"Detoxification is a task of generating text in polite style while preserving meaning and fluency of the original toxic text",
        "completion3":"Existing detoxification methods are designed to work in one exact language.",
        "technologyreview":0.2230704491,
        "venturebeat":0.1677238443,
        "wired":0.0662999617,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.02252v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1654459350000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2201.08958v1",
        "predicted_newsworthiness":0.5419620564,
        "title":"Learning Efficient Representations for Enhanced Object Detection on Large-scene SAR Images",
        "summary":"It is a challenging problem to detect and recognize targets on complex large-scene Synthetic Aperture Radar (SAR) images. Recently developed deep learning algorithms can automatically learn the intrinsic features of SAR images, but still have much room for improvement on large-scene SAR images with limited data. In this paper, based on learning representations and multi-scale features of SAR images, we propose an efficient and robust deep learning based target detection method. Especially, by leveraging the effectiveness of adversarial autoencoder (AAE) which influences the distribution of the investigated data explicitly, the raw SAR dataset is augmented into an enhanced version with a large quantity and diversity. Besides, an auto-labeling scheme is proposed to improve labeling efficiency. Finally, with jointly training small target chips and large-scene images, an integrated YOLO network combining non-maximum suppression on sub-images is used to realize multiple targets detection of high resolution images. The numerical experimental results on the MSTAR dataset show that our method can realize target detection and recognition on large-scene images accurately and efficiently. The superior anti-noise performance is also confirmed by experiments.",
        "completion1":"New deep learning method helps improve target detection on large-scene SAR images",
        "completion2":"Adversarial autoencoder used to augment raw SAR data",
        "completion3":"Joint training of small target chips and large-scene images improves accuracy",
        "technologyreview":0.2405274103,
        "venturebeat":0.1831793946,
        "wired":0.0538480869,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.08958v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1642821924000,
        "code_mentioned":0,
        "readability":0.85
    },
    {
        "arxiv_id":"2202.05306v2",
        "predicted_newsworthiness":0.5213508355,
        "title":"Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks",
        "summary":"We hypothesize that due to the greedy nature of learning in multi-modal deep neural networks, these models tend to rely on just one modality while under-fitting the other modalities. Such behavior is counter-intuitive and hurts the models' generalization, as we observe empirically. To estimate the model's dependence on each modality, we compute the gain on the accuracy when the model has access to it in addition to another modality. We refer to this gain as the conditional utilization rate. In the experiments, we consistently observe an imbalance in conditional utilization rates between modalities, across multiple tasks and architectures. Since conditional utilization rate cannot be computed efficiently during training, we introduce a proxy for it based on the pace at which the model learns from each modality, which we refer to as the conditional learning speed. We propose an algorithm to balance the conditional learning speeds between modalities during training and demonstrate that it indeed addresses the issue of greedy learning. The proposed algorithm improves the model's generalization on three datasets: Colored MNIST, ModelNet40, and NVIDIA Dynamic Hand Gesture.",
        "completion1":"DNNs found to be 'greedy' when learning from multiple modalities",
        "completion2":"Researchers develop algorithm to overcome greedy nature of DNNs",
        "completion3":"Algorithm results in improved generalization for DNNs on three datasets",
        "technologyreview":0.2728012145,
        "venturebeat":0.2030549895,
        "wired":0.0638968503,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.05306v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1644523881000,
        "code_mentioned":1,
        "readability":0.78
    },
    {
        "arxiv_id":"2207.00473v3",
        "predicted_newsworthiness":0.3467175219,
        "title":"Assessing the Effects of Hyperparameters on Knowledge Graph Embedding Quality",
        "summary":"Embedding knowledge graphs into low-dimensional spaces is a popular method for applying approaches, such as link prediction or node classification, to these databases. This embedding process is very costly in terms of both computational time and space. Part of the reason for this is the optimisation of hyperparameters, which involves repeatedly sampling, by random, guided, or brute-force selection, from a large hyperparameter space and testing the resulting embeddings for their quality. However, not all hyperparameters in this search space will be equally important. In fact, with prior knowledge of the relative importance of the hyperparameters, some could be eliminated from the search altogether without significantly impacting the overall quality of the outputted embeddings. To this end, we ran a Sobol sensitivity analysis to evaluate the effects of tuning different hyperparameters on the variance of embedding quality. This was achieved by performing thousands of embedding trials, each time measuring the quality of embeddings produced by different hyperparameter configurations. We regressed the embedding quality on those hyperparameter configurations, using this model to generate Sobol sensitivity indices for each of the hyperparameters. By evaluating the correlation between Sobol indices, we find substantial variability in the hyperparameter sensitivities between knowledge graphs, with differing dataset characteristics being the probable cause of these inconsistencies. As an additional contribution of this work we identify several relations in the UMLS knowledge graph that may cause data leakage via inverse relations, and derive and present UMLS-43, a leakage-robust variant of that graph.",
        "completion1":"Sensitivity Analysis of Hyperparameters in Knowledge Graph Embeddings",
        "completion2":"Hyperparameter Optimization for Knowledge Graph Embeddings",
        "completion3":"Dataset Characteristics Impact Hyperparameter Sensitivity in Knowledge Graphs",
        "technologyreview":0.233266961,
        "venturebeat":0.2043160159,
        "wired":0.0538050454,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.00473v3",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.cl",
            "cs.si"
        ],
        "published":1656687196000,
        "code_mentioned":1,
        "readability":0.79
    },
    {
        "arxiv_id":"2204.11167v2",
        "predicted_newsworthiness":0.4691558924,
        "title":"RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning",
        "summary":"Reasoning about visual relationships is central to how humans interpret the visual world. This task remains challenging for current deep learning algorithms since it requires addressing three key technical problems jointly: 1) identifying object entities and their properties, 2) inferring semantic relations between pairs of entities, and 3) generalizing to novel object-relation combinations, i.e., systematic generalization. In this work, we use vision transformers (ViTs) as our base model for visual reasoning and make better use of concepts defined as object entities and their relations to improve the reasoning ability of ViTs. Specifically, we introduce a novel concept-feature dictionary to allow flexible image feature retrieval at training time with concept keys. This dictionary enables two new concept-guided auxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a local task for facilitating semantic object-centric correspondence learning. To examine the systematic generalization of visual reasoning models, we introduce systematic splits for the standard HICO and GQA benchmarks. We show the resulting model, Concept-guided Vision Transformer (or RelViT for short) significantly outperforms prior approaches on HICO and GQA by 16% and 13% in the original split, and by 43% and 18% in the systematic split. Our ablation analyses also reveal our model's compatibility with multiple ViT variants and robustness to hyper-parameters.",
        "completion1":"ViTs for Visual Relational Reasoning",
        "completion2":"A New Approach to Systematic Generalization of Visual Reasoning Models",
        "completion3":"Concept-guided Vision Transformer (RelViT) Outperforms Prior Approaches",
        "technologyreview":0.253214436,
        "venturebeat":0.1965435993,
        "wired":0.0476167979,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.11167v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1650768403000,
        "code_mentioned":0,
        "readability":0.78
    },
    {
        "arxiv_id":"2207.06694v2",
        "predicted_newsworthiness":0.4388681574,
        "title":"Dynamic Low-Resolution Distillation for Cost-Efficient End-to-End Text Spotting",
        "summary":"End-to-end text spotting has attached great attention recently due to its benefits on global optimization and high maintainability for real applications. However, the input scale has always been a tough trade-off since recognizing a small text instance usually requires enlarging the whole image, which brings high computational costs. In this paper, to address this problem, we propose a novel cost-efficient Dynamic Low-resolution Distillation (DLD) text spotting framework, which aims to infer images in different small but recognizable resolutions and achieve a better balance between accuracy and efficiency. Concretely, we adopt a resolution selector to dynamically decide the input resolutions for different images, which is constraint by both inference accuracy and computational cost. Another sequential knowledge distillation strategy is conducted on the text recognition branch, making the low-res input obtains comparable performance to a high-res image. The proposed method can be optimized end-to-end and adopted in any current text spotting framework to improve the practicability. Extensive experiments on several text spotting benchmarks show that the proposed method vastly improves the usability of low-res models. The code is available at https:\/\/github.com\/hikopensource\/DAVAR-Lab-OCR\/.",
        "completion1":"A new text spotting framework that is more efficient and easier to use.",
        "completion2":"A framework that improves accuracy while maintaining efficiency for text spotting.",
        "completion3":"A cost-effective solution for end-to-end text spotting.",
        "technologyreview":0.2143573427,
        "venturebeat":0.1814802498,
        "wired":0.0524317048,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.06694v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1657781399000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2206.04317v2",
        "predicted_newsworthiness":0.4618493964,
        "title":"Topic-Aware Evaluation and Transformer Methods for Topic-Controllable Summarization",
        "summary":"Topic-controllable summarization is an emerging research area with a wide range of potential applications. However, existing approaches suffer from significant limitations. First, there is currently no established evaluation metric for this task. Furthermore, existing methods built upon recurrent architectures, which can significantly limit their performance compared to more recent Transformer-based architectures, while they also require modifications to the model's architecture for controlling the topic. In this work, we propose a new topic-oriented evaluation measure to automatically evaluate the generated summaries based on the topic affinity between the generated summary and the desired topic. We also conducted a user study that validates the reliability of this measure. Finally, we propose simple, yet powerful methods for topic-controllable summarization either incorporating topic embeddings into the model's architecture or employing control tokens to guide the summary generation. Experimental results show that control tokens can achieve better performance compared to more complicated embedding-based approaches while being at the same time significantly faster.",
        "completion1":"A new evaluation metric for topic-controllable summarization",
        "completion2":"Simple, yet powerful methods for topic-controllable summarization",
        "completion3":"User study validates reliability of new evaluation metric",
        "technologyreview":0.1990556643,
        "venturebeat":0.172103663,
        "wired":0.0646489596,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.04317v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1654759696000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2204.10305v3",
        "predicted_newsworthiness":0.7773783199,
        "title":"People are not coins. Morally distinct types of predictions necessitate different fairness constraints",
        "summary":"A recent paper (Hedden 2021) has argued that most of the group fairness constraints discussed in the machine learning literature are not necessary conditions for the fairness of predictions, and hence that there are no genuine fairness metrics. This is proven by discussing a special case of a fair prediction. In our paper, we show that Hedden 's argument does not hold for the most common kind of predictions used in data science, which are about people and based on data from similar people; we call these human-group-based practices. We argue that there is a morally salient distinction between human-group-based practices and those that are based on data of only one person, which we call human-individual-based practices. Thus, what may be a necessary condition for the fairness of human-group-based practices may not be a necessary condition for the fairness of human-individual-based practices, on which Hedden 's argument is based. Accordingly, the group fairness metrics discussed in the machine learning literature may still be relevant for most applications of prediction-based decision making.",
        "completion1":"People are not coins: study says there is no such thing as a 'genuine fairness metric",
        "completion2":"Morally distinct types of predictions necessitate different fairness constraints",
        "completion3":"Most group fairness constraints in machine learning literature are unnecessary, paper argues",
        "technologyreview":0.3432431385,
        "venturebeat":0.268433366,
        "wired":0.0873307604,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.10305v3",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1650562780000,
        "code_mentioned":0,
        "readability":0.92
    },
    {
        "arxiv_id":"2204.09992v1",
        "predicted_newsworthiness":0.4770976186,
        "title":"Arbitrary Bit-width Network: A Joint Layer-Wise Quantization and Adaptive Inference Approach",
        "summary":"Conventional model quantization methods use a fixed quantization scheme to different data samples, which ignores the inherent \"recognition difficulty\" differences between various samples. We propose to feed different data samples with varying quantization schemes to achieve a data-dependent dynamic inference, at a fine-grained layer level. However, enabling this adaptive inference with changeable layer-wise quantization schemes is challenging because the combination of bit-widths and layers is growing exponentially, making it extremely difficult to train a single model in such a vast searching space and use it in practice. To solve this problem, we present the Arbitrary Bit-width Network (ABN), where the bit-widths of a single deep network can change at runtime for different data samples, with a layer-wise granularity. Specifically, first we build a weight-shared layer-wise quantizable \"super-network\" in which each layer can be allocated with multiple bit-widths and thus quantized differently on demand. The super-network provides a considerably large number of combinations of bit-widths and layers, each of which can be used during inference without retraining or storing myriad models. Second, based on the well-trained super-network, each layer's runtime bit-width selection decision is modeled as a Markov Decision Process (MDP) and solved by an adaptive inference strategy accordingly. Experiments show that the super-network can be built without accuracy degradation, and the bit-widths allocation of each layer can be adjusted to deal with various inputs on the fly. On ImageNet classification, we achieve 1.1% top1 accuracy improvement while saving 36.2% BitOps.",
        "completion1":"Researchers Propose New 'Arbitrary Bit-width Network' Approach to Quantization",
        "completion2":"Study Shows 'Arbitrary Bit-width Network' Can Improve Accuracy and Save Resources",
        "completion3":"New Research Could Change How Deep Learning Networks are Quantized",
        "technologyreview":0.2388830864,
        "venturebeat":0.1812626655,
        "wired":0.0481304523,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.09992v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1650533803000,
        "code_mentioned":0,
        "readability":0.82
    },
    {
        "arxiv_id":"2201.10665v2",
        "predicted_newsworthiness":0.6411709901,
        "title":"Writer Recognition Using Off-line Handwritten Single Block Characters",
        "summary":"Block characters are often used when filling paper forms for a variety of purposes. We investigate if there is biometric information contained within individual digits of handwritten text. In particular, we use personal identity numbers consisting of the six digits of the date of birth, DoB. We evaluate two recognition approaches, one based on handcrafted features that compute contour directional measurements, and another based on deep features from a ResNet50 model. We use a self-captured database of 317 individuals and 4920 written DoBs in total. Results show the presence of identity-related information in a piece of handwritten information as small as six digits with the DoB. We also analyze the impact of the amount of enrolment samples, varying its number between one and ten. Results with such small amount of data are promising. With ten enrolment samples, the Top-1 accuracy with deep features is around 94%, and reaches nearly 100% by Top-10. The verification accuracy is more modest, with EER>20%with any given feature and enrolment set size, showing that there is still room for improvement.",
        "completion1":"Writer Recognition Using Off-line Handwritten Single Block Characters",
        "completion2":"Block characters often used when filling paper forms",
        "completion3":"We investigate if there is biometric information contained within individual digits of handwritten text",
        "technologyreview":0.2927527692,
        "venturebeat":0.2429212523,
        "wired":0.0791509474,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.10665v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1643151850000,
        "code_mentioned":0,
        "readability":0.88
    },
    {
        "arxiv_id":"2206.05971v1",
        "predicted_newsworthiness":0.4211309389,
        "title":"Biologically Inspired Neural Path Finding",
        "summary":"The human brain can be considered to be a graphical structure comprising of tens of billions of biological neurons connected by synapses. It has the remarkable ability to automatically re-route information flow through alternate paths in case some neurons are damaged. Moreover, the brain is capable of retaining information and applying it to similar but completely unseen scenarios. In this paper, we take inspiration from these attributes of the brain, to develop a computational framework to find the optimal low cost path between a source node and a destination node in a generalized graph. We show that our framework is capable of handling unseen graphs at test time. Moreover, it can find alternate optimal paths, when nodes are arbitrarily added or removed during inference, while maintaining a fixed prediction time. Code is available here: https:\/\/github.com\/hangligit\/pathfinding",
        "completion1":"Biologically Inspired Neural Path Finding Could Help Robots Find Their Way.",
        "completion2":"This Brain-Inspired Framework Could Help Computers Navigate More Like We Do.",
        "completion3":"Researchers Find New, Brain-Inspired Way to Help Computers Find Their Way.",
        "technologyreview":0.2889102497,
        "venturebeat":0.2148842854,
        "wired":0.0670839419,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.05971v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1655109202000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2201.09207v2",
        "predicted_newsworthiness":0.4215746778,
        "title":"Visual Object Tracking on Multi-modal RGB-D Videos: A Review",
        "summary":"The development of visual object tracking has continued for decades. Recent years, as the wide accessibility of the low-cost RGBD sensors, the task of visual object tracking on RGB-D videos has drawn much attention. Compared to conventional RGB-only tracking, the RGB-D videos can provide more information that facilitates objecting tracking in some complicated scenarios. The goal of this review is to summarize the relative knowledge of the research filed of RGB-D tracking. To be specific, we will generalize the related RGB-D tracking benchmarking datasets as well as the corresponding performance measurements. Besides, the existing RGB-D tracking methods are summarized in the paper. Moreover, we discuss the possible future direction in the field of RGB-D tracking.",
        "completion1":"Visual Object Tracking on Multi-modal RGB-D Videos: A Review",
        "completion2":"The development of visual object tracking has continued for decades",
        "completion3":"Recent years, as the wide accessibility of the low-cost RGBD sensors, the task of visual object tracking on RGB-D videos has drawn much attention.",
        "technologyreview":0.1616978653,
        "venturebeat":0.1516653812,
        "wired":0.0204587603,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.09207v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1642924969000,
        "code_mentioned":0,
        "readability":0.79
    },
    {
        "arxiv_id":"2204.02557v2",
        "predicted_newsworthiness":0.4315295253,
        "title":"MixFormer: Mixing Features across Windows and Dimensions",
        "summary":"While local-window self-attention performs notably in vision tasks, it suffers from limited receptive field and weak modeling capability issues. This is mainly because it performs self-attention within non-overlapped windows and shares weights on the channel dimension. We propose MixFormer to find a solution. First, we combine local-window self-attention with depth-wise convolution in a parallel design, modeling cross-window connections to enlarge the receptive fields. Second, we propose bi-directional interactions across branches to provide complementary clues in the channel and spatial dimensions. These two designs are integrated to achieve efficient feature mixing among windows and dimensions. Our MixFormer provides competitive results on image classification with EfficientNet and shows better results than RegNet and Swin Transformer. Performance in downstream tasks outperforms its alternatives by significant margins with less computational costs in 5 dense prediction tasks on MS COCO, ADE20k, and LVIS. Code is available at \\url{https:\/\/github.com\/PaddlePaddle\/PaddleClas}.",
        "completion1":"MixFormer: A new approach to image classification outperforms EfficientNet and RegNet",
        "completion2":"MixFormer: Bi-directional interactions across branches provide complementary clues in the channel and spatial dimensions",
        "completion3":"MixFormer: New approach to image classification shows better results than RegNet and Swin Transformer",
        "technologyreview":0.240176186,
        "venturebeat":0.2039231259,
        "wired":0.0577502264,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.02557v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1649214830000,
        "code_mentioned":1,
        "readability":0.79
    },
    {
        "arxiv_id":"2205.01811v1",
        "predicted_newsworthiness":0.5667952194,
        "title":"Assessing Dataset Bias in Computer Vision",
        "summary":"A biased dataset is a dataset that generally has attributes with an uneven class distribution. These biases have the tendency to propagate to the models that train on them, often leading to a poor performance in the minority class. In this project, we will explore the extent to which various data augmentation methods alleviate intrinsic biases within the dataset. We will apply several augmentation techniques on a sample of the UTKFace dataset, such as undersampling, geometric transformations, variational autoencoders (VAEs), and generative adversarial networks (GANs). We then trained a classifier for each of the augmented datasets and evaluated their performance on the native test set and on external facial recognition datasets. We have also compared their performance to the state-of-the-art attribute classifier trained on the FairFace dataset. Through experimentation, we were able to find that training the model on StarGAN-generated images led to the best overall performance. We also found that training on geometrically transformed images lead to a similar performance with a much quicker training time. Additionally, the best performing models also exhibit a uniform performance across the classes within each attribute. This signifies that the model was also able to mitigate the biases present in the baseline model that was trained on the original training set. Finally, we were able to show that our model has a better overall performance and consistency on age and ethnicity classification on multiple datasets when compared with the FairFace model. Our final model has an accuracy on the UTKFace test set of 91.75%, 91.30%, and 87.20% for the gender, age, and ethnicity attribute respectively, with a standard deviation of less than 0.1 between the accuracies of the classes of each attribute.",
        "completion1":"Project Aims to Assess Dataset Bias in Computer Vision",
        "completion2":"Study Finds Various Data Augmentation Methods Alleviate Intrinsic biases Within Datasets",
        "completion3":"New Model Has Better Overall Performance and Consistency on Age and Ethnicity Classification",
        "technologyreview":0.3409661978,
        "venturebeat":0.262521484,
        "wired":0.0886187471,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.01811v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1651617949000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2204.03191v1",
        "predicted_newsworthiness":0.43681361,
        "title":"Efficient Community Detection in Large-Scale Dynamic Networks Using Topological Data Analysis",
        "summary":"In this paper, we propose a method that extends the persistence-based topological data analysis (TDA) that is typically used for characterizing shapes to general networks. We introduce the concept of the community tree, a tree structure established based on clique communities from the clique percolation method, to summarize the topological structures in a network from a persistence perspective. Furthermore, we develop efficient algorithms to construct and update community trees by maintaining a series of clique graphs in the form of spanning forests, in which each spanning tree is built on an underlying Euler Tour tree. With the information revealed by community trees and the corresponding persistence diagrams, our proposed approach is able to detect clique communities and keep track of the major structural changes during their evolution given a stability threshold. The results demonstrate its effectiveness in extracting useful structural insights for time-varying social networks.",
        "completion1":"Efficient Community Detection in Large-Scale Dynamic Networks Using Topological Data Analysis",
        "completion2":"In this paper, we propose a method that extends the persistence-based topological data analysis",
        "completion3":"We introduce the concept of the community tree, a tree structure established based on clique communities from the clique percolation method, to summarize the topological structures in a network from a persistence perspective.",
        "technologyreview":0.1798450316,
        "venturebeat":0.1514946794,
        "wired":0.0481749014,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.03191v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1649302942000,
        "code_mentioned":0,
        "readability":0.85
    },
    {
        "arxiv_id":"2205.13799v2",
        "predicted_newsworthiness":0.3528282931,
        "title":"Generalization Bounds for Gradient Methods via Discrete and Continuous Prior",
        "summary":"Proving algorithm-dependent generalization error bounds for gradient-type optimization methods has attracted significant attention recently in learning theory. However, most existing trajectory-based analyses require either restrictive assumptions on the learning rate (e.g., fast decreasing learning rate), or continuous injected noise (such as the Gaussian noise in Langevin dynamics). In this paper, we introduce a new discrete data-dependent prior to the PAC-Bayesian framework, and prove a high probability generalization bound of order $O(\\frac{1}{n}\\cdot \\sum_{t=1}^T(\\gamma_t\/\\varepsilon_t)^2\\left\\|{\\mathbf{g}_t}\\right\\|^2)$ for Floored GD (i.e. a version of gradient descent with precision level $\\varepsilon_t$), where $n$ is the number of training samples, $\\gamma_t$ is the learning rate at step $t$, $\\mathbf{g}_t$ is roughly the difference of the gradient computed using all samples and that using only prior samples. $\\left\\|{\\mathbf{g}_t}\\right\\|$ is upper bounded by and and typical much smaller than the gradient norm $\\left\\|{\\nabla f(W_t)}\\right\\|$. We remark that our bound holds for nonconvex and nonsmooth scenarios. Moreover, our theoretical results provide numerically favorable upper bounds of testing errors (e.g., $0.037$ on MNIST). Using a similar technique, we can also obtain new generalization bounds for certain variants of SGD. Furthermore, we study the generalization bounds for gradient Langevin Dynamics (GLD). Using the same framework with a carefully constructed continuous prior, we show a new high probability generalization bound of order $O(\\frac{1}{n} + \\frac{L^2}{n^2}\\sum_{t=1}^T(\\gamma_t\/\\sigma_t)^2)$ for GLD. The new $1\/n^2$ rate is due to the concentration of the difference between the gradient of training samples and that of the prior.",
        "completion1":"Generalization Bounds for Gradient Methods Proven",
        "completion2":"Discrete and Continuous Prior Used in new Analysis",
        "completion3":"New Algorithm Dependent Generalization Error Bounds",
        "technologyreview":0.2026490223,
        "venturebeat":0.149185238,
        "wired":0.0267556723,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.13799v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1653636181000,
        "code_mentioned":0,
        "readability":0.82
    },
    {
        "arxiv_id":"2202.07716v3",
        "predicted_newsworthiness":0.3974388229,
        "title":"Learning Model Predictive Control for Quadrotors",
        "summary":"Aerial robots can enhance their safe and agile navigation in complex and cluttered environments by efficiently exploiting the information collected during a given task. In this paper, we address the learning model predictive control problem for quadrotors. We design a learning receding--horizon nonlinear control strategy directly formulated on the system nonlinear manifold configuration space SO(3)xR^3. The proposed approach exploits past successful task iterations to improve the system performance over time while respecting system dynamics and actuator constraints. We further relax its computational complexity making it compatible with real-time quadrotor control requirements. We show the effectiveness of the proposed approach in learning a minimum time control task, respecting dynamics, actuators, and environment constraints. Several experiments in simulation and real-world set-up validate the proposed approach.",
        "completion1":"Aerial Robots Get Brains Upgrade With Learning Model Predictive Control",
        "completion2":"Quadrotors Receive Safe and Agile Navigation Boost From Learning Algorithms",
        "completion3":"Smart' Quadrotors Learn to Navigate Complex Environments More Efficiently",
        "technologyreview":0.1767487134,
        "venturebeat":0.1376643804,
        "wired":0.0233179956,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.07716v3",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1644957112000,
        "code_mentioned":0,
        "readability":0.79
    },
    {
        "arxiv_id":"2206.03401v1",
        "predicted_newsworthiness":0.453871793,
        "title":"MIX-MAB: Reinforcement Learning-based Resource Allocation Algorithm for LoRaWAN",
        "summary":"This paper focuses on improving the resource allocation algorithm in terms of packet delivery ratio (PDR), i.e., the number of successfully received packets sent by end devices (EDs) in a long-range wide-area network (LoRaWAN). Setting the transmission parameters significantly affects the PDR. Employing reinforcement learning (RL), we propose a resource allocation algorithm that enables the EDs to configure their transmission parameters in a distributed manner. We model the resource allocation problem as a multi-armed bandit (MAB) and then address it by proposing a two-phase algorithm named MIX-MAB, which consists of the exponential weights for exploration and exploitation (EXP3) and successive elimination (SE) algorithms. We evaluate the MIX-MAB performance through simulation results and compare it with other existing approaches. Numerical results show that the proposed solution performs better than the existing schemes in terms of convergence time and PDR.",
        "completion1":"MIX-MAB: Reinforcement Learning-based Resource Allocation Algorithm for LoRaWAN",
        "completion2":"Reinforcement Learning Algorithm outperforms existing schemes in terms of convergence time and PDR",
        "completion3":"Two-phase algorithm' MIX-MAB proposed to enable EDs to configure their transmission parameters in a distributed manner",
        "technologyreview":0.1649020201,
        "venturebeat":0.1592252997,
        "wired":0.0183729485,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.03401v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni",
            "cs.ai"
        ],
        "published":1654617005000,
        "code_mentioned":1,
        "readability":0.78
    },
    {
        "arxiv_id":"2203.01714v3",
        "predicted_newsworthiness":0.3960237495,
        "title":"Weakly Supervised Object Localization as Domain Adaption",
        "summary":"Weakly supervised object localization (WSOL) focuses on localizing objects only with the supervision of image-level classification masks. Most previous WSOL methods follow the classification activation map (CAM) that localizes objects based on the classification structure with the multi-instance learning (MIL) mechanism. However, the MIL mechanism makes CAM only activate discriminative object parts rather than the whole object, weakening its performance for localizing objects. To avoid this problem, this work provides a novel perspective that models WSOL as a domain adaption (DA) task, where the score estimator trained on the source\/image domain is tested on the target\/pixel domain to locate objects. Under this perspective, a DA-WSOL pipeline is designed to better engage DA approaches into WSOL to enhance localization performance. It utilizes a proposed target sampling strategy to select different types of target samples. Based on these types of target samples, domain adaption localization (DAL) loss is elaborated. It aligns the feature distribution between the two domains by DA and makes the estimator perceive target domain cues by Universum regularization. Experiments show that our pipeline outperforms SOTA methods on multi benchmarks. Code are released at \\url{https:\/\/github.com\/zh460045050\/DA-WSOL_CVPR2022}.",
        "completion1":"Weakly Supervised Object Localization as Domain Adaption",
        "completion2":"DA-WSOL Pipeline Outperforms SOTA Methods",
        "completion3":"Universum Regularization Aligns Feature Distribution",
        "technologyreview":0.1824477657,
        "venturebeat":0.1431138781,
        "wired":0.0248677071,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.01714v3",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1646315422000,
        "code_mentioned":1,
        "readability":0.75
    },
    {
        "arxiv_id":"2207.00814v1",
        "predicted_newsworthiness":0.4877690102,
        "title":"Customized Conversational Recommender Systems",
        "summary":"Conversational recommender systems (CRS) aim to capture user's current intentions and provide recommendations through real-time multi-turn conversational interactions. As a human-machine interactive system, it is essential for CRS to improve the user experience. However, most CRS methods neglect the importance of user experience. In this paper, we propose two key points for CRS to improve the user experience: (1) Speaking like a human, human can speak with different styles according to the current dialogue context. (2) Identifying fine-grained intentions, even for the same utterance, different users have diverse finegrained intentions, which are related to users' inherent preference. Based on the observations, we propose a novel CRS model, coined Customized Conversational Recommender System (CCRS), which customizes CRS model for users from three perspectives. For human-like dialogue services, we propose multi-style dialogue response generator which selects context-aware speaking style for utterance generation. To provide personalized recommendations, we extract user's current fine-grained intentions from dialogue context with the guidance of user's inherent preferences. Finally, to customize the model parameters for each user, we train the model from the meta-learning perspective. Extensive experiments and a series of analyses have shown the superiority of our CCRS on both the recommendation and dialogue services.",
        "completion1":"Study shows how to make recommender systems more user-friendly",
        "completion2":"New research finds ways to improve user experience for recommender systems",
        "completion3":"How to make your recommender system stand out from the rest",
        "technologyreview":0.2579626524,
        "venturebeat":0.2592281551,
        "wired":0.0658358104,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.00814v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.ai"
        ],
        "published":1656582336000,
        "code_mentioned":0,
        "readability":0.82
    },
    {
        "arxiv_id":"2206.05895v3",
        "predicted_newsworthiness":0.4226250857,
        "title":"Latent Diffusion Energy-Based Model for Interpretable Text Modeling",
        "summary":"Latent space Energy-Based Models (EBMs), also known as energy-based priors, have drawn growing interests in generative modeling. Fueled by its flexibility in the formulation and strong modeling power of the latent space, recent works built upon it have made interesting attempts aiming at the interpretability of text modeling. However, latent space EBMs also inherit some flaws from EBMs in data space; the degenerate MCMC sampling quality in practice can lead to poor generation quality and instability in training, especially on data with complex latent structures. Inspired by the recent efforts that leverage diffusion recovery likelihood learning as a cure for the sampling issue, we introduce a novel symbiosis between the diffusion models and latent space EBMs in a variational learning framework, coined as the latent diffusion energy-based model. We develop a geometric clustering-based regularization jointly with the information bottleneck to further improve the quality of the learned latent space. Experiments on several challenging tasks demonstrate the superior performance of our model on interpretable text modeling over strong counterparts.",
        "completion1":"New model offers interpretability for text modeling.",
        "completion2":"Latent diffusion energy-based model offers superior performance.",
        "completion3":"Model overcomes flaws of latent space EBMs.",
        "technologyreview":0.1749976427,
        "venturebeat":0.1288015081,
        "wired":0.056742786,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.05895v3",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cl"
        ],
        "published":1655091691000,
        "code_mentioned":1,
        "readability":0.77
    },
    {
        "arxiv_id":"2206.01067v1",
        "predicted_newsworthiness":0.4676144707,
        "title":"Practical Adversarial Multivalid Conformal Prediction",
        "summary":"We give a simple, generic conformal prediction method for sequential prediction that achieves target empirical coverage guarantees against adversarially chosen data. It is computationally lightweight -- comparable to split conformal prediction -- but does not require having a held-out validation set, and so all data can be used for training models from which to derive a conformal score. It gives stronger than marginal coverage guarantees in two ways. First, it gives threshold calibrated prediction sets that have correct empirical coverage even conditional on the threshold used to form the prediction set from the conformal score. Second, the user can specify an arbitrary collection of subsets of the feature space -- possibly intersecting -- and the coverage guarantees also hold conditional on membership in each of these subsets. We call our algorithm MVP, short for MultiValid Prediction. We give both theory and an extensive set of empirical evaluations.",
        "completion1":"A new conformal prediction method does not require a held-out validation set and is computationally lightweight.",
        "completion2":"The algorithm gives correct empirical coverage even conditional on the threshold used to form the prediction set from the conformal score.",
        "completion3":"The user can specify an arbitrary collection of subsets of the feature space -- possibly intersecting -- and the coverage guarantees also hold conditional on membership in each of these subsets.",
        "technologyreview":0.2609525884,
        "venturebeat":0.2096320766,
        "wired":0.0611444345,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.01067v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1654180380000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2204.07749v1",
        "predicted_newsworthiness":0.6420532421,
        "title":"From Parental Control to Joint Family Oversight: Can Parents and Teens Manage Mobile Online Safety and Privacy as Equals?",
        "summary":"Our research aims to highlight and alleviate the complex tensions around online safety, privacy, and smartphone usage in families so that parents and teens can work together to better manage mobile privacy and security-related risks. We developed a mobile application (\"app\") for Community Oversight of Privacy and Security (\"CO-oPS\") and had parents and teens assess whether it would be applicable for use with their families. CO-oPS is an Android app that allows a group of users to co-monitor the apps installed on one another's devices and the privacy permissions granted to those apps. We conducted a study with 19 parent-teen (ages 13-17) pairs to understand how they currently managed mobile safety and app privacy within their family and then had them install, use, and evaluate the CO-oPS app. We found that both parents and teens gave little consideration to online safety and privacy before installing new apps or granting privacy permissions. When using CO-oPS, participants liked how the app increased transparency into one another's devices in a way that facilitated communication, but were less inclined to use features for in-app messaging or to hide apps from one another. Key themes related to power imbalances between parents and teens surfaced that made co-management challenging. Parents were more open to collaborative oversight than teens, who felt that it was not their place to monitor their parents, even though both often believed parents lacked the technological expertise to monitor themselves. Our study sheds light on why collaborative practices for managing online safety and privacy within families may be beneficial but also quite difficult to implement in practice. We provide recommendations for overcoming these challenges based on the insights gained from our study.",
        "completion1":"From Parental Control to Joint Family Oversight: Can Parents and Teens Manage Mobile Online Safety and Privacy as Equals?",
        "completion2":"Our research aims to highlight and alleviate the complex tensions around online safety, privacy, and smartphone usage in families so that parents and teens can work together to better manage mobile privacy and security-related risks.",
        "completion3":"We developed a mobile application ('app') for Community Oversight of Privacy and Security ('CO-oPS') and had parents and teens assess whether it would be applicable for use with their families.",
        "technologyreview":0.3437353366,
        "venturebeat":0.289964655,
        "wired":0.1165590075,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.07749v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1650097808000,
        "code_mentioned":0,
        "readability":0.91
    },
    {
        "arxiv_id":"2206.03324v1",
        "predicted_newsworthiness":0.4948542177,
        "title":"Efficient decentralized multi-agent learning in asymmetric queuing systems",
        "summary":"We study decentralized multi-agent learning in bipartite queuing systems, a standard model for service systems. In particular, $N$ agents request service from $K$ servers in a fully decentralized way, i.e, by running the same algorithm without communication. Previous decentralized algorithms are restricted to symmetric systems, have performance that is degrading exponentially in the number of servers, require communication through shared randomness and unique agent identities, and are computationally demanding. In contrast, we provide a simple learning algorithm that, when run decentrally by each agent, leads the queuing system to have efficient performance in general asymmetric bipartite queuing systems while also having additional robustness properties. Along the way, we provide the first UCB-based algorithm for the centralized case of the problem, which resolves an open question by Krishnasamy et al. (2016,2021).",
        "completion1":"Decentralized Learning Algorithm Leads to Efficient Performance in Queueing Systems",
        "completion2":"New Algorithm Provides Robustness in Asymmetric Queuing Systems",
        "completion3":"UCB-based Algorithm Resolves Open Question by Krishnasamy et al.",
        "technologyreview":0.1871301548,
        "venturebeat":0.1846323686,
        "wired":0.0348625869,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.03324v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1654449749000,
        "code_mentioned":0,
        "readability":0.82
    },
    {
        "arxiv_id":"2204.03839v1",
        "predicted_newsworthiness":0.6063181902,
        "title":"Infusing Knowledge from Wikipedia to Enhance Stance Detection",
        "summary":"Stance detection infers a text author's attitude towards a target. This is challenging when the model lacks background knowledge about the target. Here, we show how background knowledge from Wikipedia can help enhance the performance on stance detection. We introduce Wikipedia Stance Detection BERT (WS-BERT) that infuses the knowledge into stance encoding. Extensive results on three benchmark datasets covering social media discussions and online debates indicate that our model significantly outperforms the state-of-the-art methods on target-specific stance detection, cross-target stance detection, and zero\/few-shot stance detection.",
        "completion1":" Infusing Knowledge from Wikipedia to Enhance Stance Detection",
        "completion2":"WS-BERT significantly outperforms state-of-the-art methods on target-specific stance detection",
        "completion3":"Cross-target stance detection and zero\/few-shot stance detection also benefits from WS-BERT model",
        "technologyreview":0.2250566011,
        "venturebeat":0.1666185756,
        "wired":0.0536842261,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.03839v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1649393395000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2207.04415v1",
        "predicted_newsworthiness":0.4295031987,
        "title":"SFNet: Faster, Accurate, and Domain Agnostic Semantic Segmentation via Semantic Flow",
        "summary":"In this paper, we focus on exploring effective methods for faster, accurate, and domain agnostic semantic segmentation. Inspired by the Optical Flow for motion alignment between adjacent video frames, we propose a Flow Alignment Module (FAM) to learn \\textit{Semantic Flow} between feature maps of adjacent levels, and broadcast high-level features to high resolution features effectively and efficiently. Furthermore, integrating our FAM to a common feature pyramid structure exhibits superior performance over other real-time methods even on light-weight backbone networks, such as ResNet-18 and DFNet. Then to further speed up the inference procedure, we also present a novel Gated Dual Flow Alignment Module to directly align high resolution feature maps and low resolution feature maps where we term improved version network as SFNet-Lite. Extensive experiments are conducted on several challenging datasets, where results show the effectiveness of both SFNet and SFNet-Lite. In particular, the proposed SFNet-Lite series achieve 80.1 mIoU while running at 60 FPS using ResNet-18 backbone and 78.8 mIoU while running at 120 FPS using STDC backbone on RTX-3090. Moreover, we unify four challenging driving datasets (i.e., Cityscapes, Mapillary, IDD and BDD) into one large dataset, which we named Unified Driving Segmentation (UDS) dataset. It contains diverse domain and style information. We benchmark several representative works on UDS. Both SFNet and SFNet-Lite still achieve the best speed and accuracy trade-off on UDS which serves as a strong baseline in such a new challenging setting. All the code and models are publicly available at https:\/\/github.com\/lxtGH\/SFSegNets.",
        "completion1":"SFNet: A New Method for Faster, Accurate, and Domain Agnostic Semantic Segmentation",
        "completion2":"FNet-Lite: A Novel Gated Dual Flow Alignment Module for Fast and Accurate Semantic Segmentation",
        "completion3":"FNet on Unified Driving Segmentation Dataset: The Best Speed and Accuracy Trade-Off",
        "technologyreview":0.2126339271,
        "venturebeat":0.1857505501,
        "wired":0.0440697773,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.04415v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1657441547000,
        "code_mentioned":1,
        "readability":0.77
    },
    {
        "arxiv_id":"2202.03059v1",
        "predicted_newsworthiness":0.5956919453,
        "title":"Evaluation of Runtime Monitoring for UAV Emergency Landing",
        "summary":"To certify UAV operations in populated areas, risk mitigation strategies -- such as Emergency Landing (EL) -- must be in place to account for potential failures. EL aims at reducing ground risk by finding safe landing areas using on-board sensors. The first contribution of this paper is to present a new EL approach, in line with safety requirements introduced in recent research. In particular, the proposed EL pipeline includes mechanisms to monitor learning based components during execution. This way, another contribution is to study the behavior of Machine Learning Runtime Monitoring (MLRM) approaches within the context of a real-world critical system. A new evaluation methodology is introduced, and applied to assess the practical safety benefits of three MLRM mechanisms. The proposed approach is compared to a default mitigation strategy (open a parachute when a failure is detected), and appears to be much safer.",
        "completion1":"New Emergency Landing Approach for UAVs is Presented",
        "completion2":"Study Shows Machine Learning Runtime Monitoring Approaches are Safe for Critical Systems",
        "completion3":"Evaluation of Runtime Monitoring for UAV Emergency Landing Proves Effective",
        "technologyreview":0.3181446559,
        "venturebeat":0.2881472261,
        "wired":0.0766969395,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.03059v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai",
            "cs.cv",
            "cs.lg"
        ],
        "published":1644231083000,
        "code_mentioned":1,
        "readability":0.91
    },
    {
        "arxiv_id":"2206.06213v1",
        "predicted_newsworthiness":0.4427974405,
        "title":"Symbolic Regression for Space Applications: Differentiable Cartesian Genetic Programming Powered by Multi-objective Memetic Algorithms",
        "summary":"Interpretable regression models are important for many application domains, as they allow experts to understand relations between variables from sparse data. Symbolic regression addresses this issue by searching the space of all possible free form equations that can be constructed from elementary algebraic functions. While explicit mathematical functions can be rediscovered this way, the determination of unknown numerical constants during search has been an often neglected issue. We propose a new multi-objective memetic algorithm that exploits a differentiable Cartesian Genetic Programming encoding to learn constants during evolutionary loops. We show that this approach is competitive or outperforms machine learned black box regression models or hand-engineered fits for two applications from space: the Mars express thermal power estimation and the determination of the age of stars by gyrochronology.",
        "completion1":"Scientists develop new 'symbolic regression' to better understand space data",
        "completion2":"New algorithm outperforms machine learning for two space applications",
        "completion3":"Symbolic regression' could help unlock mysteries of space",
        "technologyreview":0.2088078995,
        "venturebeat":0.1656153666,
        "wired":0.0389140729,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.06213v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1655131455000,
        "code_mentioned":0,
        "readability":0.77
    },
    {
        "arxiv_id":"2202.09728v1",
        "predicted_newsworthiness":0.5869565299,
        "title":"The Effects of System Initiative during Conversational Collaborative Search",
        "summary":"Our research in this paper lies at the intersection of collaborative and conversational search. We report on a Wizard of Oz lab study in which 27 pairs of participants collaborated on search tasks over the Slack messaging platform. To complete tasks, pairs of collaborators interacted with a so-called \\emph{searchbot} with conversational capabilities. The role of the searchbot was played by a reference librarian. It is widely accepted that conversational search systems should be able to engage in \\emph{mixed-initiative interaction} -- take and relinquish control of a multi-agent conversation as appropriate. Research in discourse analysis differentiates between dialog- and task-level initiative. Taking \\emph{dialog-level} initiative involves leading a conversation for the sole purpose of establishing mutual belief between agents. Conversely, taking \\emph{task-level} initiative involves leading a conversation with the intent to influence the goals of the other agent(s). Participants in our study experienced three \\emph{searchbot conditions}, which varied based on the level of initiative the human searchbot was able to take: (1) no initiative, (2) only dialog-level initiative, and (3) both dialog- and task-level initiative. We investigate the effects of the searchbot condition on six different types of outcomes: (RQ1) perceptions of the searchbot's utility, (RQ2) perceptions of workload, (RQ3) perceptions of the collaboration, (RQ4) patterns of communication and collaboration, and perceived (RQ5) benefits and (RQ6) challenges from engaging with the searchbot.",
        "completion1":"Colloquial search systems found to be more effective when taking initiative in conversations.",
        "completion2":"Study shows that leading a conversation can positively impact task goals.",
        "completion3":"Searchbots with mixed-initiative capabilities improve user satisfaction and efficiency.",
        "technologyreview":0.3287334439,
        "venturebeat":0.307022456,
        "wired":0.1021576328,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.09728v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ir"
        ],
        "published":1645330592000,
        "code_mentioned":0,
        "readability":0.83
    },
    {
        "arxiv_id":"2203.16027v3",
        "predicted_newsworthiness":0.4442377734,
        "title":"Clozer: Adaptable Data Augmentation for Cloze-style Reading Comprehension",
        "summary":"Task-adaptive pre-training (TAPT) alleviates the lack of labelled data and provides performance lift by adapting unlabelled data to downstream task. Unfortunately, existing adaptations mainly involve deterministic rules that cannot generalize well. Here, we propose Clozer, a sequence-tagging based cloze answer extraction method used in TAPT that is extendable for adaptation on any cloze-style machine reading comprehension (MRC) downstream tasks. We experiment on multiple-choice cloze-style MRC tasks, and show that Clozer performs significantly better compared to the oracle and state-of-the-art in escalating TAPT effectiveness in lifting model performance, and prove that Clozer is able to recognize the gold answers independently of any heuristics.",
        "completion1":"Clozer\" outperforms existing methods for data augmentation in cloze-style reading comprehension tasks.",
        "completion2":"Clozer\" is a sequence-tagging method that is more effective than existing methods for data augmentation in cloze-style reading comprehension tasks.",
        "completion3":"Clozer\" is a sequence-tagging method that is extendable for adaptation to any cloze-style machine reading comprehension downstream tasks.",
        "technologyreview":0.204853633,
        "venturebeat":0.187277511,
        "wired":0.046482946,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.16027v3",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1648610495000,
        "code_mentioned":0,
        "readability":0.75
    },
    {
        "arxiv_id":"2203.05973v1",
        "predicted_newsworthiness":0.4247029227,
        "title":"Imitation and Adaptation Based on Consistency: A Quadruped Robot Imitates Animals from Videos Using Deep Reinforcement Learning",
        "summary":"The essence of quadrupeds' movements is the movement of the center of gravity, which has a pattern in the action of quadrupeds. However, the gait motion planning of the quadruped robot is time-consuming. Animals in nature can provide a large amount of gait information for robots to learn and imitate. Common methods learn animal posture with a motion capture system or numerous motion data points. In this paper, we propose a video imitation adaptation network (VIAN) that can imitate the action of animals and adapt it to the robot from a few seconds of video. The deep learning model extracts key points during animal motion from videos. The VIAN eliminates noise and extracts key information of motion with a motion adaptor, and then applies the extracted movements function as the motion pattern into deep reinforcement learning (DRL). To ensure similarity between the learning result and the animal motion in the video, we introduce rewards that are based on the consistency of the motion. DRL explores and learns to maintain balance from movement patterns from videos, imitates the action of animals, and eventually, allows the model to learn the gait or skills from short motion videos of different animals and to transfer the motion pattern to the real robot.",
        "completion1":"New deep learning algorithm allows robots to learn from videos of animals",
        "completion2":"Robots can now imitate the movements of animals",
        "completion3":"New system enables robots to adapt motions from videos",
        "technologyreview":0.2586013585,
        "venturebeat":0.1909981138,
        "wired":0.0369135576,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.05973v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.lg"
        ],
        "published":1646234830000,
        "code_mentioned":0,
        "readability":0.88
    },
    {
        "arxiv_id":"2205.01300v1",
        "predicted_newsworthiness":0.4945125736,
        "title":"Towards an Ensemble Regressor Model for Anomalous ISP Traffic Prediction",
        "summary":"Prediction of network traffic behavior is significant for the effective management of modern telecommunication networks. However, the intuitive approach of predicting network traffic using administrative experience and market analysis data is inadequate for an efficient forecast framework. As a result, many different mathematical models have been studied to capture the general trend of the network traffic and predict accordingly. But the comprehensive performance analysis of varying regression models and their ensemble has not been studied before for analyzing real-world anomalous traffic. In this paper, several regression models such as Extra Gradient Boost (XGBoost), Light Gradient Boosting Machine (LightGBM), Stochastic Gradient Descent (SGD), Gradient Boosting Regressor (GBR), and CatBoost Regressor were analyzed to predict real traffic without and with outliers and show the significance of outlier detection in real-world traffic prediction. Also, we showed the outperformance of the ensemble regression model over the individual prediction model. We compared the performance of different regression models based on five different feature sets of lengths 6, 9, 12, 15, and 18. Our ensemble regression model achieved the minimum average gap of 5.04% between actual and predicted traffic with nine outlier-adjusted inputs. In general, our experimental results indicate that the outliers in the data can significantly impact the quality of the prediction. Thus, outlier detection and mitigation assist the regression model in learning the general trend and making better predictions.",
        "completion1":"Toward an Ensemble Regressor Model for Anomalous ISP Traffic Prediction",
        "completion2":"Prediction of network traffic behavior is significant for the effective management of modern telecommunication networks",
        "completion3":"But the comprehensive performance analysis of varying regression models and their ensemble has not been studied before for analyzing real-world anomalous traffic.",
        "technologyreview":0.2405938599,
        "venturebeat":0.247088726,
        "wired":0.0605811404,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.01300v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ni"
        ],
        "published":1651552657000,
        "code_mentioned":0,
        "readability":0.83
    },
    {
        "arxiv_id":"2202.07036v1",
        "predicted_newsworthiness":0.5436165926,
        "title":"Benchmarking Online Sequence-to-Sequence and Character-based Handwriting Recognition from IMU-Enhanced Pens",
        "summary":"Handwriting is one of the most frequently occurring patterns in everyday life and with it come challenging applications such as handwriting recognition (HWR), writer identification, and signature verification. In contrast to offline HWR that only uses spatial information (i.e., images), online HWR (OnHWR) uses richer spatio-temporal information (i.e., trajectory data or inertial data). While there exist many offline HWR datasets, there is only little data available for the development of OnHWR methods as it requires hardware-integrated pens. This paper presents data and benchmark models for real-time sequence-to-sequence (seq2seq) learning and single character-based recognition. Our data is recorded by a sensor-enhanced ballpoint pen, yielding sensor data streams from triaxial accelerometers, a gyroscope, a magnetometer and a force sensor at 100Hz. We propose a variety of datasets including equations and words for both the writer-dependent and writer-independent tasks. We provide an evaluation benchmark for seq2seq and single character-based HWR using recurrent and temporal convolutional networks and Transformers combined with a connectionist temporal classification (CTC) loss and cross entropy losses. Our methods do not resort to language or lexicon models.",
        "completion1":"Benchmarking Online Sequence-to-Sequence and Character-based Handwriting Recognition from IMU-Enhanced Pens.",
        "completion2":"IMU-Enhanced Pens Make Handwriting Recognition More Accurate.",
        "completion3":"New Sensor-Enhanced Ballpoint Pen Can Improve Handwriting Recognition.",
        "technologyreview":0.2505527567,
        "venturebeat":0.2252225003,
        "wired":0.0839487984,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.07036v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1644872133000,
        "code_mentioned":1,
        "readability":0.75
    },
    {
        "arxiv_id":"2207.09161v1",
        "predicted_newsworthiness":0.4373305937,
        "title":"Single Stage Virtual Try-on via Deformable Attention Flows",
        "summary":"Virtual try-on aims to generate a photo-realistic fitting result given an in-shop garment and a reference person image. Existing methods usually build up multi-stage frameworks to deal with clothes warping and body blending respectively, or rely heavily on intermediate parser-based labels which may be noisy or even inaccurate. To solve the above challenges, we propose a single-stage try-on framework by developing a novel Deformable Attention Flow (DAFlow), which applies the deformable attention scheme to multi-flow estimation. With pose keypoints as the guidance only, the self- and cross-deformable attention flows are estimated for the reference person and the garment images, respectively. By sampling multiple flow fields, the feature-level and pixel-level information from different semantic areas are simultaneously extracted and merged through the attention mechanism. It enables clothes warping and body synthesizing at the same time which leads to photo-realistic results in an end-to-end manner. Extensive experiments on two try-on datasets demonstrate that our proposed method achieves state-of-the-art performance both qualitatively and quantitatively. Furthermore, additional experiments on the other two image editing tasks illustrate the versatility of our method for multi-view synthesis and image animation.",
        "completion1":"Debut of 'Single Stage Virtual Try-on via Deformable Attention Flows",
        "completion2":"How the new Deformable Attention Flow technology works",
        "completion3":"State-of-the-art performance seen in new virtual try-on method",
        "technologyreview":0.1774214808,
        "venturebeat":0.1500973675,
        "wired":0.0348393021,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.09161v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658224891000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2203.06215v1",
        "predicted_newsworthiness":0.4520703857,
        "title":"Can I see an Example? Active Learning the Long Tail of Attributes and Relations",
        "summary":"There has been significant progress in creating machine learning models that identify objects in scenes along with their associated attributes and relationships; however, there is a large gap between the best models and human capabilities. One of the major reasons for this gap is the difficulty in collecting sufficient amounts of annotated relations and attributes for training these systems. While some attributes and relations are abundant, the distribution in the natural world and existing datasets is long tailed. In this paper, we address this problem by introducing a novel incremental active learning framework that asks for attributes and relations in visual scenes. While conventional active learning methods ask for labels of specific examples, we flip this framing to allow agents to ask for examples from specific categories. Using this framing, we introduce an active sampling method that asks for examples from the tail of the data distribution and show that it outperforms classical active learning methods on Visual Genome.",
        "completion1":"Machine learning: Active Learning the Long Tail of Attributes and Relations.",
        "completion2":"Progress in machine learning models: Large gap between best models and human capabilities.",
        "completion3":"Collecting annotated relations and attributes: Difficult task for training these systems.",
        "technologyreview":0.3294375982,
        "venturebeat":0.2796538081,
        "wired":0.0714738446,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.06215v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1647026899000,
        "code_mentioned":0,
        "readability":0.92
    },
    {
        "arxiv_id":"2207.00251v1",
        "predicted_newsworthiness":0.5247911436,
        "title":"Computer-aided Tuberculosis Diagnosis with Attribute Reasoning Assistance",
        "summary":"Although deep learning algorithms have been intensively developed for computer-aided tuberculosis diagnosis (CTD), they mainly depend on carefully annotated datasets, leading to much time and resource consumption. Weakly supervised learning (WSL), which leverages coarse-grained labels to accomplish fine-grained tasks, has the potential to solve this problem. In this paper, we first propose a new large-scale tuberculosis (TB) chest X-ray dataset, namely the tuberculosis chest X-ray attribute dataset (TBX-Att), and then establish an attribute-assisted weakly-supervised framework to classify and localize TB by leveraging the attribute information to overcome the insufficiency of supervision in WSL scenarios. Specifically, first, the TBX-Att dataset contains 2000 X-ray images with seven kinds of attributes for TB relational reasoning, which are annotated by experienced radiologists. It also includes the public TBX11K dataset with 11200 X-ray images to facilitate weakly supervised detection. Second, we exploit a multi-scale feature interaction model for TB area classification and detection with attribute relational reasoning. The proposed model is evaluated on the TBX-Att dataset and will serve as a solid baseline for future research. The code and data will be available at https:\/\/github.com\/GangmingZhao\/tb-attribute-weak-localization.",
        "completion1":"A new large-scale tuberculosis chest X-ray dataset has been created to help with diagnosis.",
        "completion2":"A weakly supervised learning framework has been established to overcome the lack of supervision in WSL scenarios.",
        "completion3":"The proposed model has been evaluated on the TBX-Att dataset and will serve as a solid baseline for future research.",
        "technologyreview":0.2368958265,
        "venturebeat":0.1891789087,
        "wired":0.0308877181,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.00251v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1656661835000,
        "code_mentioned":1,
        "readability":0.76
    },
    {
        "arxiv_id":"2206.02915v1",
        "predicted_newsworthiness":0.5394641485,
        "title":"8-bit Numerical Formats for Deep Neural Networks",
        "summary":"Given the current trend of increasing size and complexity of machine learning architectures, it has become of critical importance to identify new approaches to improve the computational efficiency of model training. In this context, we address the advantages of floating-point over fixed-point representation, and present an in-depth study on the use of 8-bit floating-point number formats for activations, weights, and gradients for both training and inference. We explore the effect of different bit-widths for exponents and significands and different exponent biases. The experimental results demonstrate that a suitable choice of these low-precision formats enables faster training and reduced power consumption without any degradation in accuracy for a range of deep learning models for image classification and language processing.",
        "completion1":"8-bit Numerical Formats for Deep Neural Networks offers Faster Training and Reduced Power Consumption",
        "completion2":"8-bit Numerical Formats found to Maintain Accuracy for Image Classification and Language Processing",
        "completion3":"Study Shows 8-bit Numerical Formats Improve Computational Efficiency of Model Training",
        "technologyreview":0.2413604293,
        "venturebeat":0.2013487599,
        "wired":0.0510816738,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.02915v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1654551092000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2205.09536v1",
        "predicted_newsworthiness":0.3725331345,
        "title":"A Simple yet Effective Relation Information Guided Approach for Few-Shot Relation Extraction",
        "summary":"Few-Shot Relation Extraction aims at predicting the relation for a pair of entities in a sentence by training with a few labelled examples in each relation. Some recent works have introduced relation information (i.e., relation labels or descriptions) to assist model learning based on Prototype Network. However, most of them constrain the prototypes of each relation class implicitly with relation information, generally through designing complex network structures, like generating hybrid features, combining with contrastive learning or attention networks. We argue that relation information can be introduced more explicitly and effectively into the model. Thus, this paper proposes a direct addition approach to introduce relation information. Specifically, for each relation class, the relation representation is first generated by concatenating two views of relations (i.e., [CLS] token embedding and the mean value of embeddings of all tokens) and then directly added to the original prototype for both train and prediction. Experimental results on the benchmark dataset FewRel 1.0 show significant improvements and achieve comparable results to the state-of-the-art, which demonstrates the effectiveness of our proposed approach. Besides, further analyses verify that the direct addition is a much more effective way to integrate the relation representations and the original prototypes.",
        "completion1":"Simple and Effective Relation Information Guided Approach for Few-Shot Relation Extraction",
        "completion2":"Improved Few-Shot Relation Extraction with Direct Addition of Relation Representations",
        "completion3":"Simple Method Outperforms State-of-the-Art for Few-Shot Relation Extraction",
        "technologyreview":0.194715148,
        "venturebeat":0.1603537531,
        "wired":0.0467158572,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.09536v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1652965381000,
        "code_mentioned":1,
        "readability":0.88
    },
    {
        "arxiv_id":"2201.10257v1",
        "predicted_newsworthiness":0.5033369459,
        "title":"PREVIS -- A Combined Machine Learning and Visual Interpolation Approach for Interactive Reverse Engineering in Assembly Quality Control",
        "summary":"We present PREVIS, a visual analytics tool, enhancing machine learning performance analysis in engineering applications. The presented toolchain allows for a direct comparison of regression models. In addition, we provide a methodology to visualize the impact of regression errors on the underlying field of interest in the original domain, the part geometry, via exploiting standard interpolation methods. Further, we allow a real-time preview of user-driven parameter changes in the displacement field via visual interpolation. This allows for fast and accountable online change management. We demonstrate the effectiveness with an ex-ante optimization of an automotive engine hood.",
        "completion1":"PREVIS: A Combined Machine Learning and Visual Interpolation Approach for Interactive Reverse Engineering in Assembly Quality Control",
        "completion2":"PREVIS performance analysis toolchain enhances machine learning in engineering applications",
        "completion3":"PREVIS visual analytics tool allows for direct comparison of regression models",
        "technologyreview":0.2649539969,
        "venturebeat":0.2415651395,
        "wired":0.0564633027,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.10257v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.lg"
        ],
        "published":1643111787000,
        "code_mentioned":0,
        "readability":0.82
    },
    {
        "arxiv_id":"2206.06501v1",
        "predicted_newsworthiness":0.4392421037,
        "title":"Optimal Clipping and Magnitude-aware Differentiation for Improved Quantization-aware Training",
        "summary":"Data clipping is crucial in reducing noise in quantization operations and improving the achievable accuracy of quantization-aware training (QAT). Current practices rely on heuristics to set clipping threshold scalars and cannot be shown to be optimal. We propose Optimally Clipped Tensors And Vectors (OCTAV), a recursive algorithm to determine MSE-optimal clipping scalars. Derived from the fast Newton-Raphson method, OCTAV finds optimal clipping scalars on the fly, for every tensor, at every iteration of the QAT routine. Thus, the QAT algorithm is formulated with provably minimum quantization noise at each step. In addition, we reveal limitations in common gradient estimation techniques in QAT and propose magnitude-aware differentiation as a remedy to further improve accuracy. Experimentally, OCTAV-enabled QAT achieves state-of-the-art accuracy on multiple tasks. These include training-from-scratch and retraining ResNets and MobileNets on ImageNet, and Squad fine-tuning using BERT models, where OCTAV-enabled QAT consistently preserves accuracy at low precision (4-to-6-bits). Our results require no modifications to the baseline training recipe, except for the insertion of quantization operations where appropriate.",
        "completion1":"Optimal Clipping and Magnitude-aware Differentiation for Improved Quantization-aware Training",
        "completion2":"Data clipping is crucial in reducing noise in quantization operations and improving the achievable accuracy of quantization-aware training",
        "completion3":"OCTAV-enabled QAT achieves state-of-the-art accuracy on multiple tasks",
        "technologyreview":0.2525598657,
        "venturebeat":0.1997228323,
        "wired":0.0519371274,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.06501v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1655158521000,
        "code_mentioned":1,
        "readability":0.71
    },
    {
        "arxiv_id":"2204.10390v1",
        "predicted_newsworthiness":0.3660207426,
        "title":"SoftEdge: Regularizing Graph Classification with Random Soft Edges",
        "summary":"Graph data augmentation plays a vital role in regularizing Graph Neural Networks (GNNs), which leverage information exchange along edges in graphs, in the form of message passing, for learning. Due to their effectiveness, simple edge and node manipulations (e.g., addition and deletion) have been widely used in graph augmentation. In this paper, we identify a limitation in such a common augmentation technique. That is, simple edge and node manipulations can create graphs with an identical structure or indistinguishable structures to message passing GNNs but of conflict labels, leading to the sample collision issue and thus the degradation of model performance. To address this problem, we propose SoftEdge, which assigns random weights to a portion of the edges of a given graph to construct dynamic neighborhoods over the graph. We prove that SoftEdge creates collision-free augmented graphs. We also show that this simple method obtains superior accuracy to popular node and edge manipulation approaches and notable resilience to the accuracy degradation with the GNN depth.",
        "completion1":"SoftEdge: Regularizing Graph Classification with Random Soft Edges",
        "completion2":"Graph Data Augmentation Plays Vital Role in Regularizing Graph Neural Networks",
        "completion3":"Simple Edge and Node Manipulations can Create Graphs with Identical Structure",
        "technologyreview":0.2604596672,
        "venturebeat":0.2134566151,
        "wired":0.060880575,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.10390v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1650571956000,
        "code_mentioned":1,
        "readability":0.74
    },
    {
        "arxiv_id":"2202.12488v1",
        "predicted_newsworthiness":0.4342642799,
        "title":"Learn From the Past: Experience Ensemble Knowledge Distillation",
        "summary":"Traditional knowledge distillation transfers \"dark knowledge\" of a pre-trained teacher network to a student network, and ignores the knowledge in the training process of the teacher, which we call teacher's experience. However, in realistic educational scenarios, learning experience is often more important than learning results. In this work, we propose a novel knowledge distillation method by integrating the teacher's experience for knowledge transfer, named experience ensemble knowledge distillation (EEKD). We save a moderate number of intermediate models from the training process of the teacher model uniformly, and then integrate the knowledge of these intermediate models by ensemble technique. A self-attention module is used to adaptively assign weights to different intermediate models in the process of knowledge transfer. Three principles of constructing EEKD on the quality, weights and number of intermediate models are explored. A surprising conclusion is found that strong ensemble teachers do not necessarily produce strong students. The experimental results on CIFAR-100 and ImageNet show that EEKD outperforms the mainstream knowledge distillation methods and achieves the state-of-the-art. In particular, EEKD even surpasses the standard ensemble distillation on the premise of saving training cost.",
        "completion1":"A novel knowledge distillation method by integrating the teacher's experience for knowledge transfer, named experience ensemble knowledge distillation.",
        "completion2":"We save a moderate number of intermediate models from the training process of the teacher model uniformly, and then integrate the knowledge of these intermediate models by ensemble technique.",
        "completion3":"A self-attention module is used to adaptively assign weights to different intermediate models in the process of knowledge transfer.",
        "technologyreview":0.2651165873,
        "venturebeat":0.2009753887,
        "wired":0.0435404402,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.12488v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1645761909000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2202.03103v1",
        "predicted_newsworthiness":0.6100148946,
        "title":"Combining Deep Learning and Reasoning for Address Detection in Unstructured Text Documents",
        "summary":"Extracting information from unstructured text documents is a demanding task, since these documents can have a broad variety of different layouts and a non-trivial reading order, like it is the case for multi-column documents or nested tables. Additionally, many business documents are received in paper form, meaning that the textual contents need to be digitized before further analysis. Nonetheless, automatic detection and capturing of crucial document information like the sender address would boost many companies' processing efficiency. In this work we propose a hybrid approach that combines deep learning with reasoning for finding and extracting addresses from unstructured text documents. We use a visual deep learning model to detect the boundaries of possible address regions on the scanned document images and validate these results by analyzing the containing text using domain knowledge represented as a rule based system.",
        "completion1":"New Approach Combines Deep Learning and Reasoning for Address Detection in Unstructured Text Documents",
        "completion2":"Hybrid Approach Achieves More Accurate Results Than Either Method Alone",
        "completion3":"New Technique Could Help Companies Boost Processing Efficiency",
        "technologyreview":0.2769366445,
        "venturebeat":0.2571500408,
        "wired":0.0593495087,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.03103v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.ir",
            "cs.lg"
        ],
        "published":1644237120000,
        "code_mentioned":1,
        "readability":0.87
    },
    {
        "arxiv_id":"2201.12538v1",
        "predicted_newsworthiness":0.372061094,
        "title":"Incorporating Commonsense Knowledge into Story Ending Generation via Heterogeneous Graph Networks",
        "summary":"Story ending generation is an interesting and challenging task, which aims to generate a coherent and reasonable ending given a story context. The key challenges of the task lie in how to comprehend the story context sufficiently and handle the implicit knowledge behind story clues effectively, which are still under-explored by previous work. In this paper, we propose a Story Heterogeneous Graph Network (SHGN) to explicitly model both the information of story context at different granularity levels and the multi-grained interactive relations among them. In detail, we consider commonsense knowledge, words and sentences as three types of nodes. To aggregate non-local information, a global node is also introduced. Given this heterogeneous graph network, the node representations are updated through graph propagation, which adequately utilizes commonsense knowledge to facilitate story comprehension. Moreover, we design two auxiliary tasks to implicitly capture the sentiment trend and key events lie in the context. The auxiliary tasks are jointly optimized with the primary story ending generation task in a multi-task learning strategy. Extensive experiments on the ROCStories Corpus show that the developed model achieves new state-of-the-art performances. Human study further demonstrates that our model generates more reasonable story endings.",
        "completion1":"Heterogeneous graph networks help computers generate more reasonable story endings.",
        "completion2":"Commonsense knowledge is key to understanding story context, say researchers.",
        "completion3":"New algorithm for story ending generation outperforms previous methods.",
        "technologyreview":0.2185026387,
        "venturebeat":0.183179573,
        "wired":0.0759523203,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.12538v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1643448791000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2204.00645v1",
        "predicted_newsworthiness":0.4798422232,
        "title":"Design and validation of zero-slack separable manipulator for Intracardiac Echocardiography",
        "summary":"Clinicians require substantial training and experience to become comfortable with steering Intracardiac echocardiography (ICE) catheter to localize and measure the area of treatment to watch for complications while device catheters are deployed in another access. Thus, it is reasonable that a robotic-assist system to hold and actively manipulate the ICE catheter could ease the workload of the physician. Existing commercially-available robotic systems and research prototypes all use existing commercially available ICE catheters based on multiple tendon-sheath mechanism (TSM). To motorize the existing TSM-based ICE catheter, the actuators interface with the outer handle knobs to manipulate four internal tendons. However, in practice, the actuators are located at a sterile, safe place far away from the ICE handle. Thus, to interface with knobs, there exist multiple coupled gear structures between two, leading to a highly nonlinear behavior (e.g. various slack, elasticity) alongside hysteresis phenomena in TSM. Since ICE catheters are designed for single use, the expensive actuators need to be located in a safe place so as to be reusable. Moreover, these actuators should interface as directly as possible with the tendons for accurate tip controls. In this paper, we introduce a separable ICE catheter robot with four tendon actuation: one part reusable and another disposable. Moreover, we propose a practical model and calibration method for our proposed mechanism so that four tendons are actuated simultaneously allowing for precise tip control and mitigating issues with conventional devices such as dead-zone and hysteresis with simple linear compensation. We consider an open-loop controller since many available ICE catheters are used without position-tracking sensors at the tip due to costs and single use",
        "completion1":"New zero-slack separable manipulator for Intracardiac Echocardiography offers precision control and ease of use.",
        "completion2":"Model and calibration method proposed for new ICE catheter robot promises accurate tendon actuation.",
        "completion3":"Open-loop controller considered for new device due to cost and single-use constraints.",
        "technologyreview":0.1663732304,
        "venturebeat":0.1007314926,
        "wired":0.0006959021,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.00645v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1648837041000,
        "code_mentioned":1,
        "readability":0.79
    },
    {
        "arxiv_id":"2204.04308v1",
        "predicted_newsworthiness":0.4872473236,
        "title":"Grounding Hindsight Instructions in Multi-Goal Reinforcement Learning for Robotics",
        "summary":"This paper focuses on robotic reinforcement learning with sparse rewards for natural language goal representations. An open problem is the sample-inefficiency that stems from the compositionality of natural language, and from the grounding of language in sensory data and actions. We address these issues with three contributions. We first present a mechanism for hindsight instruction replay utilizing expert feedback. Second, we propose a seq2seq model to generate linguistic hindsight instructions. Finally, we present a novel class of language-focused learning tasks. We show that hindsight instructions improve the learning performance, as expected. In addition, we also provide an unexpected result: We show that the learning performance of our agent can be improved by one third if, in a sense, the agent learns to talk to itself in a self-supervised manner. We achieve this by learning to generate linguistic instructions that would have been appropriate as a natural language goal for an originally unintended behavior. Our results indicate that the performance gain increases with the task-complexity.",
        "completion1":"Hindsight instructions shown to improve robot learning performance",
        "completion2":"Seq2seq model generates linguistic hindsight instructions",
        "completion3":"New class of language-focused learning tasks improves robot performance",
        "technologyreview":0.2880606836,
        "venturebeat":0.2221871385,
        "wired":0.0513492146,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.04308v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cl"
        ],
        "published":1649455296000,
        "code_mentioned":1,
        "readability":0.87
    },
    {
        "arxiv_id":"2207.07998v1",
        "predicted_newsworthiness":0.4687193537,
        "title":"No Grammar to Rule Them All: A Survey of JSON-style DSLs for Visualization",
        "summary":"There has been substantial growth in the use of JSON-based grammars, as well as other standard data serialization languages, to create visualizations. Each of these grammars serves a purpose: some focus on particular computational tasks (such as animation), some are concerned with certain chart types (such as maps), and some target specific data domains (such as ML). Despite the prominence of this interface form, there has been little detailed analysis of the characteristics of these languages. In this study, we survey and analyze the design and implementation of 57 JSON-style DSLs for visualization. We analyze these languages supported by a collected corpus of examples for each DSL (consisting of 4395 instances) across a variety of axes organized into concerns related to domain, conceptual model, language relationships, affordances, and general practicalities. We identify tensions throughout these areas, such as between formal and colloquial specifications, among types of users, and within the composition of languages. Through this work, we seek to support language implementers by elucidating the choices, opportunities, and tradeoffs in visualization DSL design.",
        "completion1":"57 JSON-style DSLs for visualization analyzed: tradeoffs, differences, and commonalities detailed.",
        "completion2":"No one grammar to rule them all: each JSON-style DSL has unique purpose and focus.",
        "completion3":"Analysis of JSON-style DSLs reveals design tensions and opportunities for improvement.",
        "technologyreview":0.18856468,
        "venturebeat":0.2079050576,
        "wired":0.0910120471,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.07998v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1657997250000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2205.15414v1",
        "predicted_newsworthiness":0.4431624283,
        "title":"A portfolio-based analysis method for competition results",
        "summary":"Competitions such as the MiniZinc Challenges or the SAT competitions have been very useful sources for comparing performance of different solving approaches and for advancing the state-of-the-arts of the fields. Traditional competition setting often focuses on producing a ranking between solvers based on their average performance across a wide range of benchmark problems and instances. While this is a sensible way to assess the relative performance of solvers, such ranking does not necessarily reflect the full potential of a solver, especially when we want to utilise a portfolio of solvers instead of a single one for solving a new problem. In this paper, I will describe a portfolio-based analysis method which can give complementary insights into the performance of participating solvers in a competition. The method is demonstrated on the results of the MiniZinc Challenges and new insights gained from the portfolio viewpoint are presented.",
        "completion1":"A portfolio-based analysis method for competition results offers new insights.",
        "completion2":"This method for analyzing competition results could change the way we view solvers.",
        "completion3":"A portfolio-based approach to analyzing competition results produces more reliable data.",
        "technologyreview":0.2599174224,
        "venturebeat":0.2424189045,
        "wired":0.0726629452,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.15414v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1653942045000,
        "code_mentioned":1,
        "readability":0.9
    },
    {
        "arxiv_id":"2201.06888v1",
        "predicted_newsworthiness":0.4025775322,
        "title":"Autoencoding Video Latents for Adversarial Video Generation",
        "summary":"Given the three dimensional complexity of a video signal, training a robust and diverse GAN based video generative model is onerous due to large stochasticity involved in data space. Learning disentangled representations of the data help to improve robustness and provide control in the sampling process. For video generation, there is a recent progress in this area by considering motion and appearance as orthogonal information and designing architectures that efficiently disentangle them. These approaches rely on handcrafting architectures that impose structural priors on the generator to decompose appearance and motion codes in the latent space. Inspired from the recent advancements in the autoencoder based image generation, we present AVLAE (Adversarial Video Latent AutoEncoder) which is a two stream latent autoencoder where the video distribution is learned by adversarial training. In particular, we propose to autoencode the motion and appearance latent vectors of the video generator in the adversarial setting. We demonstrate that our approach learns to disentangle motion and appearance codes even without the explicit structural composition in the generator. Several experiments with qualitative and quantitative results demonstrate the effectiveness of our method.",
        "completion1":"Autoencoding Video Latents for Adversarial Video Generation",
        "completion2":"Learning disentangled representations of the data help to improve robustness and provide control in the sampling process",
        "completion3":"Autoencoding Video Latents for Adversarial Video Generation helps to disentangle motion and appearance codes",
        "technologyreview":0.250180656,
        "venturebeat":0.1769252539,
        "wired":0.0686473255,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.06888v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1642506134000,
        "code_mentioned":1,
        "readability":0.78
    },
    {
        "arxiv_id":"2206.02664v1",
        "predicted_newsworthiness":0.3963262395,
        "title":"Learning with Capsules: A Survey",
        "summary":"Capsule networks were proposed as an alternative approach to Convolutional Neural Networks (CNNs) for learning object-centric representations, which can be leveraged for improved generalization and sample complexity. Unlike CNNs, capsule networks are designed to explicitly model part-whole hierarchical relationships by using groups of neurons to encode visual entities, and learn the relationships between those entities. Promising early results achieved by capsule networks have motivated the deep learning community to continue trying to improve their performance and scalability across several application areas. However, a major hurdle for capsule network research has been the lack of a reliable point of reference for understanding their foundational ideas and motivations. The aim of this survey is to provide a comprehensive overview of the capsule network research landscape, which will serve as a valuable resource for the community going forward. To that end, we start with an introduction to the fundamental concepts and motivations behind capsule networks, such as equivariant inference in computer vision. We then cover the technical advances in the capsule routing mechanisms and the various formulations of capsule networks, e.g. generative and geometric. Additionally, we provide a detailed explanation of how capsule networks relate to the popular attention mechanism in Transformers, and highlight non-trivial conceptual similarities between them in the context of representation learning. Afterwards, we explore the extensive applications of capsule networks in computer vision, video and motion, graph representation learning, natural language processing, medical imaging and many others. To conclude, we provide an in-depth discussion regarding the main hurdles in capsule network research, and highlight promising research directions for future work.",
        "completion1":"Capsule Networks: The New Wave of Deep Learning?",
        "completion2":"A Comprehensive Survey of Capsule Networks",
        "completion3":"Capsule Networks vs Convolutional Neural Networks: What's the Difference?",
        "technologyreview":0.2715412771,
        "venturebeat":0.2016410546,
        "wired":0.0642554921,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.02664v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1654527936000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2204.07203v1",
        "predicted_newsworthiness":0.348534076,
        "title":"EXPERT: Public Benchmarks for Dynamic Heterogeneous Academic Graphs",
        "summary":"Machine learning models that learn from dynamic graphs face nontrivial challenges in learning and inference as both nodes and edges change over time. The existing large-scale graph benchmark datasets that are widely used by the community primarily focus on homogeneous node and edge attributes and are static. In this work, we present a variety of large scale, dynamic heterogeneous academic graphs to test the effectiveness of models developed for multi-step graph forecasting tasks. Our novel datasets cover both context and content information extracted from scientific publications across two communities: Artificial Intelligence (AI) and Nuclear Nonproliferation (NN). In addition, we propose a systematic approach to improve the existing evaluation procedures used in the graph forecasting models.",
        "completion1":"EXPERT: Public Benchmarks for Dynamic Heterogeneous Academic Graphs",
        "completion2":"Machine learning models that learn from dynamic graphs face nontrivial challenges in learning and inference as both nodes and edges change over time",
        "completion3":"The existing large-scale graph benchmark datasets that are widely used by the community primarily focus on homogeneous node and edge attributes and are static",
        "technologyreview":0.3075540213,
        "venturebeat":0.2540673608,
        "wired":0.0905481145,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.07203v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1649965414000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2206.04670v1",
        "predicted_newsworthiness":0.3890436775,
        "title":"PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies",
        "summary":"PointNet++ is one of the most influential neural architectures for point cloud understanding. Although the accuracy of PointNet++ has been largely surpassed by recent networks such as PointMLP and Point Transformer, we find that a large portion of the performance gain is due to improved training strategies, i.e. data augmentation and optimization techniques, and increased model sizes rather than architectural innovations. Thus, the full potential of PointNet++ has yet to be explored. In this work, we revisit the classical PointNet++ through a systematic study of model training and scaling strategies, and offer two major contributions. First, we propose a set of improved training strategies that significantly improve PointNet++ performance. For example, we show that, without any change in architecture, the overall accuracy (OA) of PointNet++ on ScanObjectNN object classification can be raised from 77.9\\% to 86.1\\%, even outperforming state-of-the-art PointMLP. Second, we introduce an inverted residual bottleneck design and separable MLPs into PointNet++ to enable efficient and effective model scaling and propose PointNeXt, the next version of PointNets. PointNeXt can be flexibly scaled up and outperforms state-of-the-art methods on both 3D classification and segmentation tasks. For classification, PointNeXt reaches an overall accuracy of $87.7\\%$ on ScanObjectNN, surpassing PointMLP by $2.3\\%$, while being $10 \\times$ faster in inference. For semantic segmentation, PointNeXt establishes a new state-of-the-art performance with $74.9\\%$ mean IoU on S3DIS (6-fold cross-validation), being superior to the recent Point Transformer. The code and models are available at https:\/\/github.com\/guochengqian\/pointnext.",
        "completion1":"PointNeXt Outperforms PointNet++ and State-of-the-Art Methods on 3D Classification and Segmentation Tasks",
        "completion2":"Improved Training Strategies Raise Accuracy of PointNet++",
        "completion3":"Inverted Residual Bottleneck Design Enables Efficient and Effective Model Scaling for PointNeXt",
        "technologyreview":0.2313028597,
        "venturebeat":0.2022417732,
        "wired":0.0635663817,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.04670v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1654797594000,
        "code_mentioned":1,
        "readability":0.79
    },
    {
        "arxiv_id":"2207.00477v1",
        "predicted_newsworthiness":0.6954770583,
        "title":"Vision-based Conflict Detection within Crowds based on High-Resolution Human Pose Estimation for Smart and Safe Airport",
        "summary":"Future airports are becoming more complex and congested with the increasing number of travellers. While the airports are more likely to become hotspots for potential conflicts to break out which can cause serious delays to flights and several safety issues. An intelligent algorithm which renders security surveillance more effective in detecting conflicts would bring many benefits to the passengers in terms of their safety, finance, and travelling efficiency. This paper details the development of a machine learning model to classify conflicting behaviour in a crowd. HRNet is used to segment the images and then two approaches are taken to classify the poses of people in the frame via multiple classifiers. Among them, it was found that the support vector machine (SVM) achieved the most performant achieving precision of 94.37%. Where the model falls short is against ambiguous behaviour such as a hug or losing track of a subject in the frame. The resulting model has potential for deployment within an airport if improvements are made to cope with the vast number of potential passengers in view as well as training against further ambiguous behaviours which will arise in an airport setting. In turn, will provide the capability to enhance security surveillance and improve airport safety.",
        "completion1":"Machine learning algorithm can detect potential conflicts in airport crowds",
        "completion2":"Algorithm may help improve airport safety and security",
        "completion3":"Model still needs improvement to cope with vast number of potential passengers",
        "technologyreview":0.267163794,
        "venturebeat":0.218705978,
        "wired":0.0788280391,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.00477v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1656687252000,
        "code_mentioned":1,
        "readability":0.92
    },
    {
        "arxiv_id":"2201.11613v2",
        "predicted_newsworthiness":0.4269931149,
        "title":"Domain-Invariant Representation Learning from EEG with Private Encoders",
        "summary":"Deep learning based electroencephalography (EEG) signal processing methods are known to suffer from poor test-time generalization due to the changes in data distribution. This becomes a more challenging problem when privacy-preserving representation learning is of interest such as in clinical settings. To that end, we propose a multi-source learning architecture where we extract domain-invariant representations from dataset-specific private encoders. Our model utilizes a maximum-mean-discrepancy (MMD) based domain alignment approach to impose domain-invariance for encoded representations, which outperforms state-of-the-art approaches in EEG-based emotion classification. Furthermore, representations learned in our pipeline preserve domain privacy as dataset-specific private encoding alleviates the need for conventional, centralized EEG-based deep neural network training approaches with shared parameters.",
        "completion1":"Researchers Develop Method for Achieving Domain-Invariant Representation Learning from EEG Data",
        "completion2":"New Approach Outperforms State-of-the-Art Methods in EEG-based Emotion Classification",
        "completion3":"Domain Privacy Preserved in New EEG Deep Learning Pipeline",
        "technologyreview":0.2426014058,
        "venturebeat":0.202372146,
        "wired":0.0524734464,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.11613v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv",
            "cs.hc"
        ],
        "published":1643300066000,
        "code_mentioned":1,
        "readability":0.76
    },
    {
        "arxiv_id":"2201.13064v1",
        "predicted_newsworthiness":0.6148034075,
        "title":"Glowing Experience or Bad Trip? A Quantitative Analysis of User Reported Drug Experiences on Erowid.org",
        "summary":"Erowid.org is a website dedicated to documenting information about psychoactive substances, with over 36,000 user-submitted drug Experience Reports. We study the potential of these reports to provide information about characteristic experiences with drugs. First, we assess different kinds of drug experiences, such as 'addiction' or 'bad trips'. We quantitatively analyze how such experiences are related to substances and user variables. Furthermore, we classify positive and negative experiences as well as reported addiction using information about the consumer, substance, context and location of the drug experience. While variables based only on objective characteristics yield poor predictive performance for subjective experiences, we find subjective user reports can help to identify new patterns and impact factors on drug experiences. In particular, we found a positive association between addiction experiences and dextromethorphan, a substance with largely unknown withdrawal effects. Our research can help to gain a deeper sociological understanding of drug consumption and to identify relationships which may have clinical relevance. Moreover, it can show how non-mainstream social media platforms can be utilized to study characteristics of human behavior and how this can be done in an ethical way in collaboration with the platform providers.",
        "completion1":"Erowid.org: Website Documents Drug Experiences of Users",
        "completion2":"Study Shows How Reported Addiction Experience Varies by Substance",
        "completion3":"New Research Uses Erowid.org Data to Understand Subjective Drug Experiences",
        "technologyreview":0.3324000669,
        "venturebeat":0.2960732285,
        "wired":0.1480167161,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.13064v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1643618417000,
        "code_mentioned":0,
        "readability":0.89
    },
    {
        "arxiv_id":"2202.04134v1",
        "predicted_newsworthiness":0.4599066669,
        "title":"A Novel Ontology-guided Attribute Partitioning Ensemble Learning Model for Early Prediction of Cognitive Deficits using Quantitative Structural MRI in Very Preterm Infants",
        "summary":"Structural magnetic resonance imaging studies have shown that brain anatomical abnormalities are associated with cognitive deficits in preterm infants. Brain maturation and geometric features can be used with machine learning models for predicting later neurodevelopmental deficits. However, traditional machine learning models would suffer from a large feature-to-instance ratio (i.e., a large number of features but a small number of instances\/samples). Ensemble learning is a paradigm that strategically generates and integrates a library of machine learning classifiers and has been successfully used on a wide variety of predictive modeling problems to boost model performance. Attribute (i.e., feature) bagging method is the most commonly used feature partitioning scheme, which randomly and repeatedly draws feature subsets from the entire feature set. Although attribute bagging method can effectively reduce feature dimensionality to handle the large feature-to-instance ratio, it lacks consideration of domain knowledge and latent relationship among features. In this study, we proposed a novel Ontology-guided Attribute Partitioning (OAP) method to better draw feature subsets by considering domain-specific relationship among features. With the better partitioned feature subsets, we developed an ensemble learning framework, which is referred to as OAP Ensemble Learning (OAP-EL). We applied the OAP-EL to predict cognitive deficits at 2 year of age using quantitative brain maturation and geometric features obtained at term equivalent age in very preterm infants. We demonstrated that the proposed OAP-EL approach significantly outperformed the peer ensemble learning and traditional machine learning approaches.",
        "completion1":"OAP Ensemble Learning outperforms traditional machine learning for early prediction of cognitive deficits.",
        "completion2":"Ontology-guided Attribute Partitioning improves performance of ensemble learning models.",
        "completion3":"Early prediction of cognitive deficits possible with quantitative MRI in preterm infants.",
        "technologyreview":0.2435871367,
        "venturebeat":0.2090644771,
        "wired":0.0404048845,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.04134v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1644352002000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2203.05300v1",
        "predicted_newsworthiness":0.4579809632,
        "title":"Connecting Neural Response measurements & Computational Models of language: a non-comprehensive guide",
        "summary":"Understanding the neural basis of language comprehension in the brain has been a long-standing goal of various scientific research programs. Recent advances in language modelling and in neuroimaging methodology promise potential improvements in both the investigation of language's neurobiology and in the building of better and more human-like language models. This survey traces a line from early research linking Event Related Potentials and complexity measures derived from simple language models to contemporary studies employing Artificial Neural Network models trained on large corpora in combination with neural response recordings from multiple modalities using naturalistic stimuli.",
        "completion1":"New advances in neural response measurement & computational models of language could help improve understanding of language neurobiology.",
        "completion2":"Improved neuroimaging methodology &language modelling offer potential for better, more human-like language models.",
        "completion3":"Researchers tracing link from early Event Related Potentials research to contemporary Artificial Neural Network studies employing naturalistic stimuli.",
        "technologyreview":0.2195732186,
        "venturebeat":0.1550093045,
        "wired":0.0409439503,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.05300v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1646911494000,
        "code_mentioned":0,
        "readability":0.83
    },
    {
        "arxiv_id":"2201.04833v1",
        "predicted_newsworthiness":0.3824961552,
        "title":"SnapshotNet: Self-supervised Feature Learning for Point Cloud Data Segmentation Using Minimal Labeled Data",
        "summary":"Manually annotating complex scene point cloud datasets is both costly and error-prone. To reduce the reliance on labeled data, a new model called SnapshotNet is proposed as a self-supervised feature learning approach, which directly works on the unlabeled point cloud data of a complex 3D scene. The SnapshotNet pipeline includes three stages. In the snapshot capturing stage, snapshots, which are defined as local collections of points, are sampled from the point cloud scene. A snapshot could be a view of a local 3D scan directly captured from the real scene, or a virtual view of such from a large 3D point cloud dataset. Snapshots could also be sampled at different sampling rates or fields of view (FOVs), thus multi-FOV snapshots, to capture scale information from the scene. In the feature learning stage, a new pre-text task called multi-FOV contrasting is proposed to recognize whether two snapshots are from the same object or not, within the same FOV or across different FOVs. Snapshots go through two self-supervised learning steps: the contrastive learning step with both part and scale contrasting, followed by a snapshot clustering step to extract higher level semantic features. Then a weakly-supervised segmentation stage is implemented by first training a standard SVM classifier on the learned features with a small fraction of labeled snapshots. The trained SVM is used to predict labels for input snapshots and predicted labels are converted into point-wise label assignments for semantic segmentation of the entire scene using a voting procedure. The experiments are conducted on the Semantic3D dataset and the results have shown that the proposed method is capable of learning effective features from snapshots of complex scene data without any labels. Moreover, the proposed method has shown advantages when comparing to the SOA method on weakly-supervised point cloud semantic segmentation.",
        "completion1":"SnapshotNet: Self-supervised Feature Learning for Point Cloud Data Segmentation Using Minimal Labeled Data",
        "completion2":"New pre-text task called multi-FOV contrasting is proposed to recognize whether two snapshots are from the same object or not, within the same FOV or across different FOVs",
        "completion3":"The SnapshotNet pipeline includes three stages: snapshot capturing, feature learning, and weakly-supervised segmentation.",
        "technologyreview":0.1968270937,
        "venturebeat":0.1783179128,
        "wired":0.0503774381,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.04833v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1642062833000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2202.09484v1",
        "predicted_newsworthiness":0.3962080551,
        "title":"Missing Data Infill with Automunge",
        "summary":"Missing data is a fundamental obstacle in the practice of data science. This paper surveys a few conventions for imputation as available in the Automunge open source python library platform for tabular data preprocessing, including \"ML infill\" in which auto ML models are trained for target features from partitioned extracts of a training set. A series of validation experiments were performed to benchmark imputation scenarios towards downstream model performance, in which it was found for the given benchmark sets that in many cases ML infill outperformed for both numeric and categoric target features, and was otherwise at minimum within noise distributions of the other imputation scenarios. Evidence also suggested supplementing ML infill with the addition of support columns with boolean integer markers signaling presence of infill was usually beneficial to downstream model performance. We consider these results sufficient to recommend defaulting to ML infill for tabular learning, and further recommend supplementing imputations with support columns signaling presence of infill, each as can be prepared with push-button operation in the Automunge library. Our contributions include an auto ML derived missing data imputation library for tabular learning in the python ecosystem, fully integrated into a preprocessing platform with an extensive library of feature transformations, with a novel production friendly implementation that bases imputation models on a designated train set for consistent basis towards additional data.",
        "completion1":"Missing Data Infill with Automunge",
        "completion2":" ML infill outperforms for both numeric and categoric target features",
        "completion3":"Supplementing ML infill with the addition of support columns with boolean integer markers signaling presence of infill was usually beneficial to downstream model performance.",
        "technologyreview":0.2423749312,
        "venturebeat":0.2387114555,
        "wired":0.0575340995,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.09484v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1645231770000,
        "code_mentioned":1,
        "readability":0.79
    },
    {
        "arxiv_id":"2202.04772v1",
        "predicted_newsworthiness":0.4159109756,
        "title":"GrASP: Gradient-Based Affordance Selection for Planning",
        "summary":"Planning with a learned model is arguably a key component of intelligence. There are several challenges in realizing such a component in large-scale reinforcement learning (RL) problems. One such challenge is dealing effectively with continuous action spaces when using tree-search planning (e.g., it is not feasible to consider every action even at just the root node of the tree). In this paper we present a method for selecting affordances useful for planning -- for learning which small number of actions\/options from a continuous space of actions\/options to consider in the tree-expansion process during planning. We consider affordances that are goal-and-state-conditional mappings to actions\/options as well as unconditional affordances that simply select actions\/options available in all states. Our selection method is gradient based: we compute gradients through the planning procedure to update the parameters of the function that represents affordances. Our empirical work shows that it is feasible to learn to select both primitive-action and option affordances, and that simultaneously learning to select affordances and planning with a learned value-equivalent model can outperform model-free RL.",
        "completion1":"GrASP: Gradient-Based Affordance Selection for Planning",
        "completion2":"Planning with a learned model is a key component of intelligence",
        "completion3":"Empirical work shows that it is feasible to learn to select both primitive-action and option affordances",
        "technologyreview":0.229239482,
        "venturebeat":0.1807531589,
        "wired":0.0227711608,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.04772v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1644290676000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2206.07085v1",
        "predicted_newsworthiness":0.3968968043,
        "title":"Understanding the Generalization Benefit of Normalization Layers: Sharpness Reduction",
        "summary":"Normalization layers (e.g., Batch Normalization, Layer Normalization) were introduced to help with optimization difficulties in very deep nets, but they clearly also help generalization, even in not-so-deep nets. Motivated by the long-held belief that flatter minima lead to better generalization, this paper gives mathematical analysis and supporting experiments suggesting that normalization (together with accompanying weight-decay) encourages GD to reduce the sharpness of loss surface. Here \"sharpness\" is carefully defined given that the loss is scale-invariant, a known consequence of normalization. Specifically, for a fairly broad class of neural nets with normalization, our theory explains how GD with a finite learning rate enters the so-called Edge of Stability (EoS) regime, and characterizes the trajectory of GD in this regime via a continuous sharpness-reduction flow.",
        "completion1":"Normalization Layers Improve Generalization in Neural Networks",
        "completion2":"Batch Normalization and Layer Normalization Encourage GD to Reduce Sharpness of Loss Surface",
        "completion3":"Edge of Stability Regime Characterized by Continuous Sharpness-Reduction Flow",
        "technologyreview":0.2319200736,
        "venturebeat":0.1629632034,
        "wired":0.0445024146,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.07085v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1655230745000,
        "code_mentioned":1,
        "readability":0.78
    },
    {
        "arxiv_id":"2204.06028v2",
        "predicted_newsworthiness":0.4416775994,
        "title":"CUNI-KIT System for Simultaneous Speech Translation Task at IWSLT 2022",
        "summary":"In this paper, we describe our submission to the Simultaneous Speech Translation at IWSLT 2022. We explore strategies to utilize an offline model in a simultaneous setting without the need to modify the original model. In our experiments, we show that our onlinization algorithm is almost on par with the offline setting while being $3\\times$ faster than offline in terms of latency on the test set. We also show that the onlinized offline model outperforms the best IWSLT2021 simultaneous system in medium and high latency regimes and is almost on par in the low latency regime. We make our system publicly available.",
        "completion1":"Offline Translation Model Performs On-Par with Online Counterparts",
        "completion2":"CUNI-KIT System Outperforms IWSLT2021 Best in Medium and High Latency Regimes",
        "completion3":"System Made Publicly Available",
        "technologyreview":0.1853488995,
        "venturebeat":0.2036001512,
        "wired":0.0390061544,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.06028v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1649788220000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2201.06248v1",
        "predicted_newsworthiness":0.6320406478,
        "title":"Using Machine Learning Based Models for Personality Recognition",
        "summary":"Personality can be defined as the combination of behavior, emotion, motivation, and thoughts that aim at describing various aspects of human behavior based on a few stable and measurable characteristics. Considering the fact that our personality has a remarkable influence in our daily life, automatic recognition of a person's personality attributes can provide many essential practical applications in various aspects of cognitive science. deep learning based method for the task of personality recognition from text is proposed in this paper. Among various deep neural networks, Convolutional Neural Networks (CNN) have demonstrated profound efficiency in natural language processing and especially personality detection. Owing to the fact that various filter sizes in CNN may influence its performance, we decided to combine CNN with AdaBoost, a classical ensemble algorithm, to consider the possibility of using the contribution of various filter lengths and gasp their potential in the final classification via combining various classifiers with respective filter size using AdaBoost. Our proposed method was validated on the Essay dataset by conducting a series of experiments and the empirical results demonstrated the superiority of our proposed method compared to both machine learning and deep learning methods for the task of personality recognition.",
        "completion1":"Machine Learning models can now recognize personality types from text",
        "completion2":"Convolutional Neural Networks are the most efficient for this type of recognition",
        "completion3":"AdaBoost may improve the accuracy of predictions",
        "technologyreview":0.3221277086,
        "venturebeat":0.2833963163,
        "wired":0.0831813699,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.06248v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1642404051000,
        "code_mentioned":0,
        "readability":0.88
    },
    {
        "arxiv_id":"2207.02202v1",
        "predicted_newsworthiness":0.4539952198,
        "title":"CoBEVT: Cooperative Bird's Eye View Semantic Segmentation with Sparse Transformers",
        "summary":"Bird's eye view (BEV) semantic segmentation plays a crucial role in spatial sensing for autonomous driving. Although recent literature has made significant progress on BEV map understanding, they are all based on single-agent camera-based systems which are difficult to handle occlusions and detect distant objects in complex traffic scenes. Vehicle-to-Vehicle (V2V) communication technologies have enabled autonomous vehicles to share sensing information, which can dramatically improve the perception performance and range as compared to single-agent systems. In this paper, we propose CoBEVT, the first generic multi-agent multi-camera perception framework that can cooperatively generate BEV map predictions. To efficiently fuse camera features from multi-view and multi-agent data in an underlying Transformer architecture, we design a fused axial attention or FAX module, which can capture sparsely local and global spatial interactions across views and agents. The extensive experiments on the V2V perception dataset, OPV2V, demonstrate that CoBEVT achieves state-of-the-art performance for cooperative BEV semantic segmentation. Moreover, CoBEVT is shown to be generalizable to other tasks, including 1) BEV segmentation with single-agent multi-camera and 2) 3D object detection with multi-agent LiDAR systems, and achieves state-of-the-art performance with real-time inference speed.",
        "completion1":"CoBEVT: Cooperative Bird's Eye View Semantic Segmentation with Sparse Transformers.",
        "completion2":"Vehicle-to-Vehicle (V2V) communication technologies have enabled autonomous vehicles to share sensing information, which can dramatically improve the perception performance and range as compared to single-agent systems.",
        "completion3":"The extensive experiments on the V2V perception dataset, OPV2V, demonstrate that CoBEVT achieves state-of-the-art performance for cooperative BEV semantic segmentation. Moreover, CoBEVT is shown to be generalizable to other tasks, including 1) BEV segmentation with single-agent multi-camera and 2) 3D object detection with multi-agent LiDAR systems, and achieves state-of-the-art performance with real-time inference speed.",
        "technologyreview":0.2591352858,
        "venturebeat":0.2123836696,
        "wired":0.0480639814,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.02202v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1657043968000,
        "code_mentioned":0,
        "readability":0.81
    },
    {
        "arxiv_id":"2205.05857v1",
        "predicted_newsworthiness":0.5725541915,
        "title":"Comparing Open Arabic Named Entity Recognition Tools",
        "summary":"The main objective of this paper is to compare and evaluate the performances of three open Arabic NER tools: CAMeL, Hatmi, and Stanza. We collected a corpus consisting of 30 articles written in MSA and manually annotated all the entities of the person, organization, and location types at the article (document) level. Our results suggest a similarity between Stanza and Hatmi with the latter receiving the highest F1 score for the three entity types. However, CAMeL achieved the highest precision values for names of people and organizations. Following this, we implemented a \"merge\" method that combined the results from the three tools and a \"vote\" method that tagged named entities only when two of the three identified them as entities. Our results showed that merging achieved the highest overall F1 scores. Moreover, merging had the highest recall values while voting had the highest precision values for the three entity types. This indicates that merging is more suitable when recall is desired, while voting is optimal when precision is required. Finally, we collected a corpus of 21,635 articles related to COVID-19 and applied the merge and vote methods. Our analysis demonstrates the tradeoff between precision and recall for the two methods.",
        "completion1":"Three open Arabic NER tools compared and evaluated",
        "completion2":"Merge\" method found to be best overall",
        "completion3":"Precision vs. recall tradeoff demonstrated with COVID-19 corpus",
        "technologyreview":0.2255757885,
        "venturebeat":0.2142955304,
        "wired":0.0686350283,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.05857v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1652325366000,
        "code_mentioned":0,
        "readability":0.87
    },
    {
        "arxiv_id":"2202.04757v1",
        "predicted_newsworthiness":0.4583113707,
        "title":"A Novel Encoder-Decoder Network with Guided Transmission Map for Single Image Dehazing",
        "summary":"A novel Encoder-Decoder Network with Guided Transmission Map (EDN-GTM) for single image dehazing scheme is proposed in this paper. The proposed EDN-GTM takes conventional RGB hazy image in conjunction with its transmission map estimated by adopting dark channel prior as the inputs of the network. The proposed EDN-GTM utilizes U-Net for image segmentation as the core network and utilizes various modifications including spatial pyramid pooling module and Swish activation to achieve state-of-the-art dehazing performance. Experiments on benchmark datasets show that the proposed EDN-GTM outperforms most of traditional and deep learning-based image dehazing schemes in terms of PSNR and SSIM metrics. The proposed EDN-GTM furthermore proves its applicability to object detection problems. Specifically, when applied to an image preprocessing tool for driving object detection, the proposed EDN-GTM can efficiently remove haze and significantly improve detection accuracy by 4.73% in terms of mAP measure. The code is available at: https:\/\/github.com\/tranleanh\/edn-gtm.",
        "completion1":"EDN-GTM outperforms most traditional and deep learning-based image dehazing schemes.",
        "completion2":"The proposed EDN-GTM is shown to be an efficient image preprocessing tool for driving object detection.",
        "completion3":"The code for the proposed EDN-GTM is available online.",
        "technologyreview":0.2031968504,
        "venturebeat":0.1512599704,
        "wired":0.0274596578,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.04757v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1644292656000,
        "code_mentioned":1,
        "readability":0.73
    },
    {
        "arxiv_id":"2207.07486v1",
        "predicted_newsworthiness":0.6082518557,
        "title":"Securing name resolution in the IoT: DNS over CoAP",
        "summary":"In this paper, we present the design, implementation, and analysis of DNS over CoAP (DoC), a new proposal for secure and privacy-friendly name resolution of constrained IoT devices. We implement different design choices of DoC in RIOT, an open-source operating system for the IoT, evaluate performance measures in a testbed, compare with DNS over UDP and DNS over DTLS, and validate our protocol design based on empirical DNS IoT data. Our findings indicate that plain DoC is on par with common DNS solutions for the constrained IoT but significantly outperforms when additional, CoAP standard features are used such as block-wise transfer or caching. With OSCORE for end-to-end security, we can save more than 10 kBytes of code memory compared to DTLS while enabling group communication without compromising the trust chain when using intermediate proxies or caches. We also discuss a scheme for very restricted links that compresses redundant or excessive information by up to 70%.",
        "completion1":"New proposal for secure and privacy-friendly name resolution of constrained IoT devices",
        "completion2":"DoC outperforms DNS over UDP and DNS over DTLS",
        "completion3":"Scheme for very restricted links that compresses redundant or excessive information by up to 70%",
        "technologyreview":0.2230952341,
        "venturebeat":0.2242326181,
        "wired":0.0712863971,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.07486v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1657894253000,
        "code_mentioned":0,
        "readability":0.8
    },
    {
        "arxiv_id":"2204.06889v1",
        "predicted_newsworthiness":0.4126704798,
        "title":"Does BERT really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task",
        "summary":"Although transformer-based Neural Language Models demonstrate impressive performance on a variety of tasks, their generalization abilities are not well understood. They have been shown to perform strongly on subject-verb number agreement in a wide array of settings, suggesting that they learned to track syntactic dependencies during their training even without explicit supervision. In this paper, we examine the extent to which BERT is able to perform lexically-independent subject-verb number agreement (NA) on targeted syntactic templates. To do so, we disrupt the lexical patterns found in naturally occurring stimuli for each targeted structure in a novel fine-grained analysis of BERT's behavior. Our results on nonce sentences suggest that the model generalizes well for simple templates, but fails to perform lexically-independent syntactic generalization when as little as one attractor is present.",
        "completion1":"BERT's impressive performance on a variety of tasks belies its poor generalization abilities, new study finds.",
        "completion2":"Transformer-based neural language model struggles with lexical dependencies, researchers say.",
        "completion3":"BERT falls short in fine-grained analysis of syntactic task, research shows.",
        "technologyreview":0.2209202503,
        "venturebeat":0.1624257381,
        "wired":0.0460624113,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.06889v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1649935995000,
        "code_mentioned":1,
        "readability":0.79
    },
    {
        "arxiv_id":"2205.06564v1",
        "predicted_newsworthiness":0.7304451188,
        "title":"An Ethical Black Box for Social Robots: a draft Open Standard",
        "summary":"This paper introduces a draft open standard for the robot equivalent of an aircraft flight data recorder, which we call an ethical black box. This is a device, or software module, capable of securely recording operational data (sensor, actuator and control decisions) for a social robot, in order to support the investigation of accidents or near-miss incidents. The open standard, presented as an annex to this paper, is offered as a first draft for discussion within the robot ethics community. Our intention is to publish further drafts following feedback, in the hope that the standard will become a useful reference for social robot designers, operators and robot accident\/incident investigators.",
        "completion1":"New draft open standard for social robot ethical black boxes presented",
        "completion2":"Standard offers investigation support for robot accidents and near-miss incidents",
        "completion3":"Feedback from robot ethics community encouraged to perfect the standard",
        "technologyreview":0.3815401368,
        "venturebeat":0.3038953066,
        "wired":0.1251062415,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.06564v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1652441553000,
        "code_mentioned":0,
        "readability":0.91
    },
    {
        "arxiv_id":"2205.09753v1",
        "predicted_newsworthiness":0.4079318341,
        "title":"HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding",
        "summary":"One essential task for autonomous driving is to encode the information of a driving scene into vector representations so that the downstream task such as trajectory prediction could perform well. The driving scene is complicated, and there exists heterogeneity within elements, where they own diverse types of information i.e., agent dynamics, map routing, road lines, etc. Meanwhile, there also exist relativity across elements - meaning they have spatial relations with each other; such relations should be canonically represented regarding the relative measurements since the absolute value of the coordinate is meaningless. Taking these two observations into consideration, we propose a novel backbone, namely Heterogeneous Driving Graph Transformer (HDGT), which models the driving scene as a heterogeneous graph with different types of nodes and edges. For graph construction, each node represents either an agent or a road element and each edge represents their semantics relations such as Pedestrian-To-Crosswalk, Lane-To-Left-Lane. As for spatial relation encoding, instead of setting a fixed global reference, the coordinate information of the node as well as its in-edges is transformed to the local node-centric coordinate system. For the aggregation module in the graph neural network (GNN), we adopt the transformer structure in a hierarchical way to fit the heterogeneous nature of inputs. Experimental results show that the proposed method achieves new state-of-the-art on INTERACTION Prediction Challenge and Waymo Open Motion Challenge, in which we rank 1st and 2nd respectively regarding the minADE\/minFDE metric.",
        "completion1":"Autonomous driving: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding",
        "completion2":"Machine learning: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding",
        "completion3":"Artificial intelligence: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding",
        "technologyreview":0.2506377127,
        "venturebeat":0.1932408557,
        "wired":0.0477539596,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.09753v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.cv",
            "cs.lg",
            "cs.ro"
        ],
        "published":1651302510000,
        "code_mentioned":0,
        "readability":0.82
    },
    {
        "arxiv_id":"2207.05256v1",
        "predicted_newsworthiness":0.4523216681,
        "title":"Normalized Feature Distillation for Semantic Segmentation",
        "summary":"As a promising approach in model compression, knowledge distillation improves the performance of a compact model by transferring the knowledge from a cumbersome one. The kind of knowledge used to guide the training of the student is important. Previous distillation methods in semantic segmentation strive to extract various forms of knowledge from the features, which involve elaborate manual design relying on prior information and have limited performance gains. In this paper, we propose a simple yet effective feature distillation method called normalized feature distillation (NFD), aiming to enable effective distillation with the original features without the need to manually design new forms of knowledge. The key idea is to prevent the student from focusing on imitating the magnitude of the teacher's feature response by normalization. Our method achieves state-of-the-art distillation results for semantic segmentation on Cityscapes, VOC 2012, and ADE20K datasets. Code will be available.",
        "completion1":"Normalized Feature Distillation for Semantic Segmentation achieves state-of-the-art results.",
        "completion2":"Normalized Feature Distillation is a simple yet effective method that doesn't require manual design.",
        "completion3":"Code for Normalized Feature Distillation will be available soon.",
        "technologyreview":0.216189677,
        "venturebeat":0.1669150236,
        "wired":0.0445154575,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.05256v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1657590865000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2202.13715v2",
        "predicted_newsworthiness":0.4174211386,
        "title":"Fast and Compute-efficient Sampling-based Local Exploration Planning via Distribution Learning",
        "summary":"Exploration is a fundamental problem in robotics. While sampling-based planners have shown high performance, they are oftentimes compute intensive and can exhibit high variance. To this end, we propose to directly learn the underlying distribution of informative views based on the spatial context in the robot's map. We further explore a variety of methods to also learn the information gain. We show in thorough experimental evaluation that our proposed system improves exploration performance by up to 28% over classical methods, and find that learning the gains in addition to the sampling distribution can provide favorable performance vs. compute trade-offs for compute-constrained systems. We demonstrate in simulation and on a low-cost mobile robot that our system generalizes well to varying environments.",
        "completion1":"New system improves exploration performance by up to 28%.",
        "completion2":"Learns underlying distribution of informative views.",
        "completion3":"Shows in simulation and on low-cost mobile robot that system generalizes well to varying environments.",
        "technologyreview":0.2176903375,
        "venturebeat":0.1672602443,
        "wired":0.0330709371,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.13715v2",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1646050609000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2201.03425v1",
        "predicted_newsworthiness":0.5351174403,
        "title":"Towards Trustworthy AutoGrading of Short, Multi-lingual, Multi-type Answers",
        "summary":"Autograding short textual answers has become much more feasible due to the rise of NLP and the increased availability of question-answer pairs brought about by a shift to online education. Autograding performance is still inferior to human grading. The statistical and black-box nature of state-of-the-art machine learning models makes them untrustworthy, raising ethical concerns and limiting their practical utility. Furthermore, the evaluation of autograding is typically confined to small, monolingual datasets for a specific question type. This study uses a large dataset consisting of about 10 million question-answer pairs from multiple languages covering diverse fields such as math and language, and strong variation in question and answer syntax. We demonstrate the effectiveness of fine-tuning transformer models for autograding for such complex datasets. Our best hyperparameter-tuned model yields an accuracy of about 86.5\\%, comparable to the state-of-the-art models that are less general and more tuned to a specific type of question, subject, and language. More importantly, we address trust and ethical concerns. By involving humans in the autograding process, we show how to improve the accuracy of automatically graded answers, achieving accuracy equivalent to that of teaching assistants. We also show how teachers can effectively control the type of errors made by the system and how they can validate efficiently that the autograder's performance on individual exams is close to the expected performance.",
        "completion1":"Towards Trustworthy AutoGrading of Short, Multi-lingual, Multi-type Answers",
        "completion2":"Autograding short textual answers has become much more feasible due to the rise of NLP and the increased availability of question-answer pairs brought about by a shift to online education.",
        "completion3":"The statistical and black-box nature of state-of-the-art machine learning models makes them untrustworthy, raising ethical concerns and limiting their practical utility.",
        "technologyreview":0.3132610711,
        "venturebeat":0.2568417692,
        "wired":0.0708762726,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.03425v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1641125844000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2201.10582v1",
        "predicted_newsworthiness":0.3765309613,
        "title":"Out-of-Domain Semantics to the Rescue! Zero-Shot Hybrid Retrieval Models",
        "summary":"The pre-trained language model (eg, BERT) based deep retrieval models achieved superior performance over lexical retrieval models (eg, BM25) in many passage retrieval tasks. However, limited work has been done to generalize a deep retrieval model to other tasks and domains. In this work, we carefully select five datasets, including two in-domain datasets and three out-of-domain datasets with different levels of domain shift, and study the generalization of a deep model in a zero-shot setting. Our findings show that the performance of a deep retrieval model is significantly deteriorated when the target domain is very different from the source domain that the model was trained on. On the contrary, lexical models are more robust across domains. We thus propose a simple yet effective framework to integrate lexical and deep retrieval models. Our experiments demonstrate that these two models are complementary, even when the deep model is weaker in the out-of-domain setting. The hybrid model obtains an average of 20.4% relative gain over the deep retrieval model, and an average of 9.54% over the lexical model in three out-of-domain datasets.",
        "completion1":"Out-of-Domain Semantics Helps Zero-Shot Hybrid Retrieval Models",
        "completion2":"Deep Retrieval Models Struggle in Out-of-Domain Setting",
        "completion3":"proposed framework to integrate lexical and deep retrieval models",
        "technologyreview":0.1658834222,
        "venturebeat":0.1404522046,
        "wired":0.0497077545,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.10582v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1643137525000,
        "code_mentioned":0,
        "readability":0.83
    },
    {
        "arxiv_id":"2207.01910v2",
        "predicted_newsworthiness":0.5045188841,
        "title":"Multi-Scored Sleep Databases: How to Exploit the Multiple-Labels in Automated Sleep Scoring",
        "summary":"Study Objectives: Inter-scorer variability in scoring polysomnograms is a well-known problem. Most of the existing automated sleep scoring systems are trained using labels annotated by a single scorer, whose subjective evaluation is transferred to the model. When annotations from two or more scorers are available, the scoring models are usually trained on the scorer consensus. The averaged scorer's subjectivity is transferred into the model, losing information about the internal variability among different scorers. In this study, we aim to insert the multiple-knowledge of the different physicians into the training procedure.The goal is to optimize a model training, exploiting the full information that can be extracted from the consensus of a group of scorers. Methods: We train two lightweight deep learning based models on three different multi-scored databases. We exploit the label smoothing technique together with a soft-consensus (LSSC) distribution to insert the multiple-knowledge in the training procedure of the model. We introduce the averaged cosine similarity metric (ACS) to quantify the similarity between the hypnodensity-graph generated by the models with-LSSC and the hypnodensity-graph generated by the scorer consensus. Results: The performance of the models improves on all the databases when we train the models with our LSSC. We found an increase in ACS (up to 6.4%) between the hypnodensity-graph generated by the models trained with-LSSC and the hypnodensity-graph generated by the consensus. Conclusions: Our approach definitely enables a model to better adapt to the consensus of the group of scorers. Future work will focus on further investigations on different scoring architectures.",
        "completion1":"Multi-Scored Sleep Databases: How to Exploit the Multiple-Labels in Automated Sleep Scoring",
        "completion2":"Study Objectives: Inter-scorer variability in scoring polysomnograms is a well-known problem",
        "completion3":"Most of the existing automated sleep scoring systems are trained using labels annotated by a single scorer, whose subjective evaluation is transferred to the model",
        "technologyreview":0.2541164832,
        "venturebeat":0.2126941149,
        "wired":0.0409199079,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.01910v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1657014081000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2207.04735v1",
        "predicted_newsworthiness":0.4725894191,
        "title":"A Miniature 3-DoF Flexible Parallel Robotic Wrist Using NiTi Wires for Gastrointestinal Endoscopic Surgery",
        "summary":"Gastrointestinal endoscopic surgery (GES) has high requirements for instruments' size and distal dexterity, because of the narrow endoscopic channel and long, tortuous human gastrointestinal tract. This paper utilized Nickel-Titanium (NiTi) wires to develop a miniature 3-DoF (pitch-yaw-translation) flexible parallel robotic wrist (FPRW). Additionally, we assembled an electric knife on the wrist's connection interface and then teleoperated it to perform an endoscopic submucosal dissection (ESD) on porcine stomachs. The effective performance in each ESD workflow proves that the designed FPRW has sufficient workspace, high distal dexterity, and high positioning accuracy.",
        "completion1":"A new miniature 3-DoF flexible parallel robotic wrist using NiTi wires has been developed for gastrointestinal endoscopic surgery.",
        "completion2":"The new wrist is smaller than previous models and offers increased dexterity and accuracy.",
        "completion3":"The electric knife attached to the wrist proved effective in performing an endoscopic submucosal dissection on porcine stomachs.",
        "technologyreview":0.176247949,
        "venturebeat":0.1129342867,
        "wired":0.0171368505,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.04735v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1657532138000,
        "code_mentioned":0,
        "readability":0.63
    },
    {
        "arxiv_id":"2205.15254v2",
        "predicted_newsworthiness":0.4528162989,
        "title":"Pooling Revisited: Your Receptive Field is Suboptimal",
        "summary":"The size and shape of the receptive field determine how the network aggregates local information and affect the overall performance of a model considerably. Many components in a neural network, such as kernel sizes and strides for convolution and pooling operations, influence the configuration of a receptive field. However, they still rely on hyperparameters, and the receptive fields of existing models result in suboptimal shapes and sizes. Hence, we propose a simple yet effective Dynamically Optimized Pooling operation, referred to as DynOPool, which optimizes the scale factors of feature maps end-to-end by learning the desirable size and shape of its receptive field in each layer. Any kind of resizing modules in a deep neural network can be replaced by the operations with DynOPool at a minimal cost. Also, DynOPool controls the complexity of a model by introducing an additional loss term that constrains computational cost. Our experiments show that the models equipped with the proposed learnable resizing module outperform the baseline networks on multiple datasets in image classification and semantic segmentation.",
        "completion1":"pooling revisited: your receptive field is suboptimal",
        "completion2":"a simple yet effective Dynamically Optimized Pooling operation, referred to as DynOPool",
        "completion3":"the models equipped with the proposed learnable resizing module outperform the baseline networks",
        "technologyreview":0.247107816,
        "venturebeat":0.1975272566,
        "wired":0.0567273952,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.15254v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1653930220000,
        "code_mentioned":0,
        "readability":0.79
    },
    {
        "arxiv_id":"2207.02549v1",
        "predicted_newsworthiness":0.4092553803,
        "title":"Light-weight spatio-temporal graphs for segmentation and ejection fraction prediction in cardiac ultrasound",
        "summary":"Accurate and consistent predictions of echocardiography parameters are important for cardiovascular diagnosis and treatment. In particular, segmentations of the left ventricle can be used to derive ventricular volume, ejection fraction (EF) and other relevant measurements. In this paper we propose a new automated method called EchoGraphs for predicting ejection fraction and segmenting the left ventricle by detecting anatomical keypoints. Models for direct coordinate regression based on Graph Convolutional Networks (GCNs) are used to detect the keypoints. GCNs can learn to represent the cardiac shape based on local appearance of each keypoint, as well as global spatial and temporal structures of all keypoints combined. We evaluate our EchoGraphs model on the EchoNet benchmark dataset. Compared to semantic segmentation, GCNs show accurate segmentation and improvements in robustness and inference runtime. EF is computed simultaneously to segmentations and our method also obtains state-of-the-art ejection fraction estimation. Source code is available online: https:\/\/github.com\/guybenyosef\/EchoGraphs.",
        "completion1":"New method for predicting ejection fraction and segmenting left ventricle offers state-of-the-art results.",
        "completion2":"Graph Convolutional Networks improve accuracy and runtime for cardiac ultrasound segmentation and EF prediction.",
        "completion3":"EchoGraphs model outperforms semantic segmentation for EF estimation and left ventricle segmentation in EchoNet benchmark dataset.",
        "technologyreview":0.2009700296,
        "venturebeat":0.1600591225,
        "wired":0.0323110659,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.02549v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1657101824000,
        "code_mentioned":1,
        "readability":0.69
    },
    {
        "arxiv_id":"2201.09280v4",
        "predicted_newsworthiness":0.6371877756,
        "title":"SpiroMask: Measuring Lung Function Using Consumer-Grade Masks",
        "summary":"According to the World Health Organisation (WHO), 235 million people suffer from respiratory illnesses and four million people die annually due to air pollution. Regular lung health monitoring can lead to prognoses about deteriorating lung health conditions. This paper presents our system SpiroMask that retrofits a microphone in consumer-grade masks (N95 and cloth masks) for continuous lung health monitoring. We evaluate our approach on 48 participants (including 14 with lung health issues) and find that we can estimate parameters such as lung volume and respiration rate within the approved error range by the American Thoracic Society (ATS). Further, we show that our approach is robust to sensor placement inside the mask.",
        "completion1":"SpiroMask: Measuring Lung Function Using Consumer-Grade Masks",
        "completion2":"Regular lung health monitoring can lead to prognoses about deteriorating lung health conditions",
        "completion3":"SpiroMask is robust to sensor placement inside the mask",
        "technologyreview":0.2573643246,
        "venturebeat":0.2287313409,
        "wired":0.065759336,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.09280v4",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.lg"
        ],
        "published":1642948358000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2207.00067v1",
        "predicted_newsworthiness":0.467641265,
        "title":"Rethinking Unsupervised Domain Adaptation for Semantic Segmentation",
        "summary":"Unsupervised domain adaptation (UDA) adapts a model trained on one domain to a novel domain using only unlabeled data. So many studies have been conducted, especially for semantic segmentation due to its high annotation cost. The existing studies stick to the basic assumption that no labeled sample is available for the new domain. However, this assumption has several issues. First, it is pretty unrealistic, considering the standard practice of ML to confirm the model's performance before its deployment; the confirmation needs labeled data. Second, any UDA method will have a few hyper-parameters, needing a certain amount of labeled data. To rectify this misalignment with reality, we rethink UDA from a data-centric point of view. Specifically, we start with the assumption that we do have access to a minimum level of labeled data. Then, we ask how many labeled samples are necessary for finding satisfactory hyper-parameters of existing UDA methods. How well does it work if we use the same data to train the model, e.g., finetuning? We conduct experiments to answer these questions with popular scenarios, {GTA5, SYNTHIA}$\\rightarrow$Cityscapes. Our findings are as follows: i) for some UDA methods, good hyper-parameters can be found with only a few labeled samples (i.e., images), e.g., five, but this does not apply to others, and ii) finetuning outperforms most existing UDA methods with only ten labeled images.",
        "completion1":"Rethinking Unsupervised Domain Adaptation for Semantic Segmentation",
        "completion2":"Unsupervised domain adaptation may not be as necessary as previously thought",
        "completion3":"Finetuning outperforms most existing UDA methods",
        "technologyreview":0.2241830769,
        "venturebeat":0.1701366745,
        "wired":0.0450538756,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.00067v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1656616403000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2205.09592v1",
        "predicted_newsworthiness":0.485364472,
        "title":"Transferable Physical Attack against Object Detection with Separable Attention",
        "summary":"Transferable adversarial attack is always in the spotlight since deep learning models have been demonstrated to be vulnerable to adversarial samples. However, existing physical attack methods do not pay enough attention on transferability to unseen models, thus leading to the poor performance of black-box attack.In this paper, we put forward a novel method of generating physically realizable adversarial camouflage to achieve transferable attack against detection models. More specifically, we first introduce multi-scale attention maps based on detection models to capture features of objects with various resolutions. Meanwhile, we adopt a sequence of composite transformations to obtain the averaged attention maps, which could curb model-specific noise in the attention and thus further boost transferability. Unlike the general visualization interpretation methods where model attention should be put on the foreground object as much as possible, we carry out attack on separable attention from the opposite perspective, i.e. suppressing attention of the foreground and enhancing that of the background. Consequently, transferable adversarial camouflage could be yielded efficiently with our novel attention-based loss function. Extensive comparison experiments verify the superiority of our method to state-of-the-art methods.",
        "completion1":"Transferable Physical Attack against Object Detection with Separable Attention",
        "completion2":"Transferable Adversarial Attack Achieves Transferability to Unseen Models",
        "completion3":"3.\u9ed2\u5b50\u306e\u30d0\u30b9\u30b1 \u30e2\u30c7\u30eb\u30ba \u65b0\u30e2\u30c7\u30eb\u300c\u30ab\u30e2\u30d5\u30e9\u30fc\u30b8\u30e5\u300d\u767b\u5834",
        "technologyreview":0.2652328744,
        "venturebeat":0.1865941708,
        "wired":0.077418666,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.09592v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1652970895000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2203.02453v1",
        "predicted_newsworthiness":0.4344704594,
        "title":"Real-Time Hybrid Mapping of Populated Indoor Scenes using a Low-Cost Monocular UAV",
        "summary":"Unmanned aerial vehicles (UAVs) have been used for many applications in recent years, from urban search and rescue, to agricultural surveying, to autonomous underground mine exploration. However, deploying UAVs in tight, indoor spaces, especially close to humans, remains a challenge. One solution, when limited payload is required, is to use micro-UAVs, which pose less risk to humans and typically cost less to replace after a crash. However, micro-UAVs can only carry a limited sensor suite, e.g. a monocular camera instead of a stereo pair or LiDAR, complicating tasks like dense mapping and markerless multi-person 3D human pose estimation, which are needed to operate in tight environments around people. Monocular approaches to such tasks exist, and dense monocular mapping approaches have been successfully deployed for UAV applications. However, despite many recent works on both marker-based and markerless multi-UAV single-person motion capture, markerless single-camera multi-person 3D human pose estimation remains a much earlier-stage technology, and we are not aware of existing attempts to deploy it in an aerial context. In this paper, we present what is thus, to our knowledge, the first system to perform simultaneous mapping and multi-person 3D human pose estimation from a monocular camera mounted on a single UAV. In particular, we show how to loosely couple state-of-the-art monocular depth estimation and monocular 3D human pose estimation approaches to reconstruct a hybrid map of a populated indoor scene in real time. We validate our component-level design choices via extensive experiments on the large-scale ScanNet and GTA-IM datasets. To evaluate our system-level performance, we also construct a new Oxford Hybrid Mapping dataset of populated indoor scenes.",
        "completion1":"First system to perform simultaneous mapping and multi-person 3D human pose estimation from a monocular camera mounted on a single UAV",
        "completion2":"Monocular approaches to such tasks exist, and dense monocular mapping approaches have been successfully deployed for UAV applications",
        "completion3":"Despite many recent works on both marker-based and markerless multi-UAV single-person motion capture, markerless single-camera multi-person 3D human pose estimation remains a much earlier-stage technology",
        "technologyreview":0.2140385486,
        "venturebeat":0.1948420362,
        "wired":0.0550044894,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.02453v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ro"
        ],
        "published":1646415086000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2206.05807v3",
        "predicted_newsworthiness":0.4248038317,
        "title":"Over-Generation Cannot Be Rewarded: Length-Adaptive Average Lagging for Simultaneous Speech Translation",
        "summary":"Simultaneous speech translation (SimulST) systems aim at generating their output with the lowest possible latency, which is normally computed in terms of Average Lagging (AL). In this paper we highlight that, despite its widespread adoption, AL provides underestimated scores for systems that generate longer predictions compared to the corresponding references. We also show that this problem has practical relevance, as recent SimulST systems have indeed a tendency to over-generate. As a solution, we propose LAAL (Length-Adaptive Average Lagging), a modified version of the metric that takes into account the over-generation phenomenon and allows for unbiased evaluation of both under-\/over-generating systems.",
        "completion1":"Simultaneous speech translation systems inaccurate, over-generate",
        "completion2":"Length-Adaptive Average Lagging proposed to fix SimulST systems",
        "completion3":"SimulST systems need update according to new LAAL metric",
        "technologyreview":0.19870861,
        "venturebeat":0.1879690788,
        "wired":0.0587436572,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.05807v3",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1655056808000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2201.06384v1",
        "predicted_newsworthiness":0.6443834704,
        "title":"Cyberbullying Classifiers are Sensitive to Model-Agnostic Perturbations",
        "summary":"A limited amount of studies investigates the role of model-agnostic adversarial behavior in toxic content classification. As toxicity classifiers predominantly rely on lexical cues, (deliberately) creative and evolving language-use can be detrimental to the utility of current corpora and state-of-the-art models when they are deployed for content moderation. The less training data is available, the more vulnerable models might become. This study is, to our knowledge, the first to investigate the effect of adversarial behavior and augmentation for cyberbullying detection. We demonstrate that model-agnostic lexical substitutions significantly hurt classifier performance. Moreover, when these perturbed samples are used for augmentation, we show models become robust against word-level perturbations at a slight trade-off in overall task performance. Augmentations proposed in prior work on toxicity prove to be less effective. Our results underline the need for such evaluations in online harm areas with small corpora. The perturbed data, models, and code are available for reproduction at https:\/\/github.com\/cmry\/augtox",
        "completion1":"Cyberbullying classifiers are sensitive to model-agnostic perturbations, study finds",
        "completion2":"Study: Model-agnostic adversarial behavior can hurt cyberbullying classification performance",
        "completion3":"Cyberbullying classifiers may be vulnerable to creative and evolving language use",
        "technologyreview":0.3514072939,
        "venturebeat":0.2591934296,
        "wired":0.1034306997,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.06384v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.cy",
            "cs.si"
        ],
        "published":1642423707000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2202.09256v2",
        "predicted_newsworthiness":0.5549652294,
        "title":"Traffic-Aware Dynamic Functional Split for 5G Cloud Radio Access Networks",
        "summary":"The recent adaption of virtualization technologies in the next generation mobile network enables 5G base station to be segregated into a Radio Unit (RU), a Distributed Unit (DU), and a Central Unit (CU) to support Cloud based Radio Access Networks (C-RAN). RU and DU are connected through a fronthaul link. In contrast, CU and DU are connected through a midhaul link. Although virtualization of CU gives benefits of centralization to the operators, there are other issues to be solved such as optimization of midhaul bandwidth and computing resources at edge cloud and central cloud where the DUs and CUs are deployed, respectively. In this paper, we propose a dynamic functional split selection for the DUs in 5G C-RAN by adopting to traffic heterogeneity where the midhaul bandwidth is limited. We propose an optimization problem that maximizes the centralization of the C-RAN system by operating more number of DUs on split Option-7 by changing the channel bandwidth of the DUs. The dynamic selection of split options among each CU-DU pair gives 90% centralization over the static functional split for a given midhaul bandwidth.",
        "completion1":"5G C-RAN: Traffic-Aware Dynamic Functional Split Achieves Optimal Centralization",
        "completion2":"New 5G Cloud Radio Access Network Solves Midhaul Bandwidth Issues",
        "completion3":"5G C-RAN: Dynamic Functional Split improves Centralization by up to 90%",
        "technologyreview":0.1642342734,
        "venturebeat":0.2050335372,
        "wired":0.0338186388,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.09256v2",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1645198666000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2203.16063v2",
        "predicted_newsworthiness":0.4776984477,
        "title":"Pay Attention to Hidden States for Video Deblurring: Ping-Pong Recurrent Neural Networks and Selective Non-Local Attention",
        "summary":"Video deblurring models exploit information in the neighboring frames to remove blur caused by the motion of the camera and the objects. Recurrent Neural Networks~(RNNs) are often adopted to model the temporal dependency between frames via hidden states. When motion blur is strong, however, hidden states are hard to deliver proper information due to the displacement between different frames. While there have been attempts to update the hidden states, it is difficult to handle misaligned features beyond the receptive field of simple modules. Thus, we propose 2 modules to supplement the RNN architecture for video deblurring. First, we design Ping-Pong RNN~(PPRNN) that acts on updating the hidden states by referring to the features from the current and the previous time steps alternately. PPRNN gathers relevant information from the both features in an iterative and balanced manner by utilizing its recurrent architecture. Second, we use a Selective Non-Local Attention~(SNLA) module to additionally refine the hidden state by aligning it with the positional information from the input frame feature. The attention score is scaled by the relevance to the input feature to focus on the necessary information. By paying attention to hidden states with both modules, which have strong synergy, our PAHS framework improves the representation powers of RNN structures and achieves state-of-the-art deblurring performance on standard benchmarks and real-world videos.",
        "completion1":"New RNN architecture promises better video deblurring performance",
        "completion2":"Ping-Pong RNN and Selective Non-local Attention improve hidden state representation for video deblurring",
        "completion3":"PAHS framework achieves state-of-the-art deblurring performance",
        "technologyreview":0.1957112947,
        "venturebeat":0.1527021573,
        "wired":0.0538765721,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.16063v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1648617665000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2202.12259v1",
        "predicted_newsworthiness":0.5756599815,
        "title":"Learning from the Pros: Extracting Professional Goalkeeper Technique from Broadcast Footage",
        "summary":"As an amateur goalkeeper playing grassroots soccer, who better to learn from than top professional goalkeepers? In this paper, we harness computer vision and machine learning models to appraise the save technique of professionals in a way those at lower levels can learn from. We train an unsupervised machine learning model using 3D body pose data extracted from broadcast footage to learn professional goalkeeper technique. Then, an \"expected saves\" model is developed, from which we can identify the optimal goalkeeper technique in different match contexts.",
        "completion1":"Amateur Goalkeepers Can Now Learn from the Pros Thanks to New Machine Learning Model",
        "completion2":"Computer Vision and Machine Learning Used to Analyse Professional Goalkeeper Technique",
        "completion3":"New \"Expected Saves\" Model Helps Amateur Goalkeepers Improve Their Technique",
        "technologyreview":0.2546703981,
        "venturebeat":0.2042212272,
        "wired":0.0385496097,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.12259v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1645553850000,
        "code_mentioned":1,
        "readability":0.93
    },
    {
        "arxiv_id":"2205.03907v1",
        "predicted_newsworthiness":0.4504906124,
        "title":"Network Traffic Anomaly Detection Method Based on Multi scale Residual Feature",
        "summary":"To address the problem that traditional network traffic anomaly detection algorithms do not suffi-ciently mine potential features in long time domain, an anomaly detection method based on mul-ti-scale residual features of network traffic is proposed. The original traffic is divided into subse-quences of different time spans using sliding windows, and each subsequence is decomposed and reconstructed into data sequences of different levels using wavelet transform technique; the stacked autoencoder (SAE) constructs similar feature space using normal network traffic, and gen-erates reconstructed error vector using the difference between reconstructed samples and input samples in the similar feature space; the multi-path residual group is used to learn reconstructed error The traffic classification is completed by a lightweight classifier. The experimental results show that the detection performance of the proposed method for anomalous network traffic is sig-nificantly improved compared with traditional methods; it confirms that the longer time span and more S transformation scales have positive effects on discovering potential diversity information in the original network traffic.",
        "completion1":"Anomaly detection method based on multi-scale residual features of network traffic is proposed.",
        "completion2":"The original traffic is divided into subsequences of different time spans using sliding windows, and each subsequence is decomposed and reconstructed into data sequences of different levels using wavelet transform technique.",
        "completion3":"The stacked autoencoder constructs similar feature space using normal network traffic, and generates reconstructed error vector using the difference between reconstructed samples and input samples in the similar feature space.",
        "technologyreview":0.1832265458,
        "venturebeat":0.1686932485,
        "wired":0.0333037657,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.03907v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni",
            "cs.ai"
        ],
        "published":1652026704000,
        "code_mentioned":0,
        "readability":0.82
    },
    {
        "arxiv_id":"2201.07619v1",
        "predicted_newsworthiness":0.448663963,
        "title":"CAST: Character labeling in Animation using Self-supervision by Tracking",
        "summary":"Cartoons and animation domain videos have very different characteristics compared to real-life images and videos. In addition, this domain carries a large variability in styles. Current computer vision and deep-learning solutions often fail on animated content because they were trained on natural images. In this paper we present a method to refine a semantic representation suitable for specific animated content. We first train a neural network on a large-scale set of animation videos and use the mapping to deep features as an embedding space. Next, we use self-supervision to refine the representation for any specific animation style by gathering many examples of animated characters in this style, using a multi-object tracking. These examples are used to define triplets for contrastive loss training. The refined semantic space allows better clustering of animated characters even when they have diverse manifestations. Using this space we can build dictionaries of characters in an animation videos, and define specialized classifiers for specific stylistic content (e.g., characters in a specific animation series) with very little user effort. These classifiers are the basis for automatically labeling characters in animation videos. We present results on a collection of characters in a variety of animation styles.",
        "completion1":"CAST: Character Labeling in Animation using Self-supervision by Tracking",
        "completion2":"A new method to refine a semantic representation suitable for specific animated content",
        "completion3":"Using self-supervision to refine the representation for any specific animation style",
        "technologyreview":0.2278604749,
        "venturebeat":0.1908304428,
        "wired":0.0558434748,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.07619v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1642602103000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2207.08021v1",
        "predicted_newsworthiness":0.4430903041,
        "title":"Role of reward shaping in object-goal navigation",
        "summary":"Deep reinforcement learning approaches have been a popular method for visual navigation tasks in the computer vision and robotics community of late. In most cases, the reward function has a binary structure, i.e., a large positive reward is provided when the agent reaches goal state, and a negative step penalty is assigned for every other state in the environment. A sparse signal like this makes the learning process challenging, specially in big environments, where a large number of sequential actions need to be taken to reach the target. We introduce a reward shaping mechanism which gradually adjusts the reward signal based on distance to the goal. Detailed experiments conducted using the AI2-THOR simulation environment demonstrate the efficacy of the proposed approach for object-goal navigation tasks.",
        "completion1":"Sparse reward signal makes learning deep reinforcement navigation difficult.",
        "completion2":"AI2-THOR simulation environment demonstrates efficacy of new reward shaping approach for object-goal navigation tasks.",
        "completion3":"New mechanism adjusts reward signal based on distance to goal, making learning process easier for deep reinforcementnavigation training.",
        "technologyreview":0.282008794,
        "venturebeat":0.2215965133,
        "wired":0.0494716539,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.08021v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658007118000,
        "code_mentioned":0,
        "readability":0.87
    },
    {
        "arxiv_id":"2202.03120v1",
        "predicted_newsworthiness":0.5157509608,
        "title":"To Tune or Not To Tune? Zero-shot Models for Legal Case Entailment",
        "summary":"There has been mounting evidence that pretrained language models fine-tuned on large and diverse supervised datasets can transfer well to a variety of out-of-domain tasks. In this work, we investigate this transfer ability to the legal domain. For that, we participated in the legal case entailment task of COLIEE 2021, in which we use such models with no adaptations to the target domain. Our submissions achieved the highest scores, surpassing the second-best team by more than six percentage points. Our experiments confirm a counter-intuitive result in the new paradigm of pretrained language models: given limited labeled data, models with little or no adaptation to the target task can be more robust to changes in the data distribution than models fine-tuned on it. Code is available at https:\/\/github.com\/neuralmind-ai\/coliee.",
        "completion1":"Pretrained language models fine-tuned on large and diverse supervised datasets can transfer well to a variety of out-of-domain tasks.",
        "completion2":"In this work, we investigate this transfer ability to the legal domain.",
        "completion3":"Our submissions achieved the highest scores, surpassing the second-best team by more than six percentage points.",
        "technologyreview":0.2356547959,
        "venturebeat":0.1933109879,
        "wired":0.0523661982,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.03120v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1644238968000,
        "code_mentioned":1,
        "readability":0.89
    },
    {
        "arxiv_id":"2207.08557v1",
        "predicted_newsworthiness":0.6231789595,
        "title":"AlexU-AIC at Arabic Hate Speech 2022: Contrast to Classify",
        "summary":"Online presence on social media platforms such as Facebook and Twitter has become a daily habit for internet users. Despite the vast amount of services the platforms offer for their users, users suffer from cyber-bullying, which further leads to mental abuse and may escalate to cause physical harm to individuals or targeted groups. In this paper, we present our submission to the Arabic Hate Speech 2022 Shared Task Workshop (OSACT5 2022) using the associated Arabic Twitter dataset. The shared task consists of 3 sub-tasks, sub-task A focuses on detecting whether the tweet is offensive or not. Then, For offensive Tweets, sub-task B focuses on detecting whether the tweet is hate speech or not. Finally, For hate speech Tweets, sub-task C focuses on detecting the fine-grained type of hate speech among six different classes. Transformer models proved their efficiency in classification tasks, but with the problem of over-fitting when fine-tuned on a small or an imbalanced dataset. We overcome this limitation by investigating multiple training paradigms such as Contrastive learning and Multi-task learning along with Classification fine-tuning and an ensemble of our top 5 performers. Our proposed solution achieved 0.841, 0.817, and 0.476 macro F1-average in sub-tasks A, B, and C respectively.",
        "completion1":"AlexU-AIC at Arabic Hate Speech 2022: Contrast to Classify",
        "completion2":"Using the associated Arabic Twitter dataset, the shared task consists of 3 sub-tasks, sub-task A focuses on detecting whether the tweet is offensive or not.",
        "completion3":"For hate speech Tweets, sub-task C focuses on detecting the fine-grained type of hate speech among six different classes.",
        "technologyreview":0.3254471777,
        "venturebeat":0.2478218018,
        "wired":0.0852344884,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.08557v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1658147631000,
        "code_mentioned":0,
        "readability":0.89
    },
    {
        "arxiv_id":"2205.04977v1",
        "predicted_newsworthiness":0.5660015929,
        "title":"The Future of Hybrid Meetings",
        "summary":"Meetings are typically considered to be the fuel of an organization's productivity -- a place where employees discuss ideas and make collective decisions. However, it is no secret that meetings are also often perceived as wasteful vacuums, depleting employee morale and productivity, likely due to the fact that current technologies fall short in fully supporting physical or virtual meeting experience. In this position paper, we discuss the three key elements that make a meeting successful (i.e., execution, psychological safety, and physical comfort), and present new tools for hybrid meetings that incorporate those elements. As past research has focused on supporting meeting execution (the first element), we set the roadmap for future research on the two other elements: on psychological safety by articulating how new technologies could make meeting useful for all participants, ensure all participants give and receive appropriate levels of attention, and enable all participants to feel and make others feel comfortable; and on physical comfort by dwelling on how new technologies could make the meeting experience comfortable by integrating all human senses. We also discuss the potential danger of these technologies inadvertently becoming surveillance tools.",
        "completion1":"The Future of Meetings: New Tools for Hybrid Meetings",
        "completion2":"The Three Key Elements of a Successful Meeting",
        "completion3":"The Potential Dangers of New Surveillance Tools",
        "technologyreview":0.3085002032,
        "venturebeat":0.314212853,
        "wired":0.1293095236,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.04977v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1652196829000,
        "code_mentioned":0,
        "readability":0.92
    },
    {
        "arxiv_id":"2203.16885v1",
        "predicted_newsworthiness":0.4049611817,
        "title":"A bilingual approach to specialised adjectives through word embeddings in the karstology domain",
        "summary":"We present an experiment in extracting adjectives which express a specific semantic relation using word embeddings. The results of the experiment are then thoroughly analysed and categorised into groups of adjectives exhibiting formal or semantic similarity. The experiment and analysis are performed for English and Croatian in the domain of karstology using data sets and methods developed in the TermFrame project. The main original contributions of the article are twofold: firstly, proposing a new and promising method of extracting semantically related words relevant for terminology, and secondly, providing a detailed evaluation of the output so that we gain a better understanding of the domain-specific semantic structures on the one hand and the types of similarities extracted by word embeddings on the other.",
        "completion1":"A new method for extracting semantically related words.",
        "completion2":"A detailed evaluation of the output of word embeddings.",
        "completion3":"The types of similarities extracted by word embeddings.",
        "technologyreview":0.1173945598,
        "venturebeat":0.0891564958,
        "wired":0.0096320591,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.16885v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1648715235000,
        "code_mentioned":0,
        "readability":0.82
    },
    {
        "arxiv_id":"2201.07099v1",
        "predicted_newsworthiness":0.4587316928,
        "title":"Inferring Commonsense Explanations as Prompts for Future Event Generation",
        "summary":"Future Event Generation aims to generate fluent and reasonable future event descriptions given preceding events. It requires not only fluent text generation but also commonsense reasoning to maintain the coherence of the entire event story. However, existing FEG methods are easily trapped into repeated or general events without imposing any logical constraint to the generation process. In this paper, we propose a novel explainable FEG framework that consists of a commonsense inference model (IM) and an event generation model (GM). The IM, which is pre-trained on a commonsense knowledge graph ATOMIC, learns to interpret the preceding events and conducts commonsense reasoning to reveal the characters psychology such as intent, reaction, and needs as latent variables. GM further takes the commonsense knowledge as prompts to guide and enforce the generation of logistically coherent future events. As unique merit, the commonsense prompts can be further decoded into textual descriptions, yielding explanations for the future event. Automatic and human evaluation demonstrate that our approach can generate more coherent, specific, and logical future events than the strong baselines.",
        "completion1":"AI system uses commonsense reasoning to generate explanation for future events.",
        "completion2":"Commonsense reasoning helps AI system generate more logical future events.",
        "completion3":"Explainable FEG framework helpsAI system generate more specific and coherent future events.",
        "technologyreview":0.2547459641,
        "venturebeat":0.2306302165,
        "wired":0.0948801612,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.07099v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1642522883000,
        "code_mentioned":0,
        "readability":0.81
    },
    {
        "arxiv_id":"2204.02311v3",
        "predicted_newsworthiness":0.4852919291,
        "title":"PaLM: Scaling Language Modeling with Pathways",
        "summary":"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
        "completion1":"Google's PaLM: A 540-Billion Parameter, Densely Activated Transformer Language Model",
        "completion2":"Google Trains Largest-Ever Language Model: PaLM",
        "completion3":"Google's PaLM outperforms state-of-the-art on hundreds of language understanding and generation benchmarks",
        "technologyreview":0.3431091051,
        "venturebeat":0.2849152307,
        "wired":0.0990465572,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.02311v3",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1649175105000,
        "code_mentioned":1,
        "readability":0.88
    },
    {
        "arxiv_id":"2205.01156v1",
        "predicted_newsworthiness":0.4533615914,
        "title":"SELC: Self-Ensemble Label Correction Improves Learning with Noisy Labels",
        "summary":"Deep neural networks are prone to overfitting noisy labels, resulting in poor generalization performance. To overcome this problem, we present a simple and effective method self-ensemble label correction (SELC) to progressively correct noisy labels and refine the model. We look deeper into the memorization behavior in training with noisy labels and observe that the network outputs are reliable in the early stage. To retain this reliable knowledge, SELC uses ensemble predictions formed by an exponential moving average of network outputs to update the original noisy labels. We show that training with SELC refines the model by gradually reducing supervision from noisy labels and increasing supervision from ensemble predictions. Despite its simplicity, compared with many state-of-the-art methods, SELC obtains more promising and stable results in the presence of class-conditional, instance-dependent, and real-world label noise. The code is available at https:\/\/github.com\/MacLLL\/SELC.",
        "completion1":"Self-Ensemble Label Correction Improves Learning with Noisy Labels",
        "completion2":"Deep neural networks are prone to overfitting noisy labels, resulting in poor generalization performance",
        "completion3":"To overcome this problem, we present a simple and effective method self-ensemble label correction to progressively correct noisy labels and refine the model",
        "technologyreview":0.2487808142,
        "venturebeat":0.2069312627,
        "wired":0.0550752328,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.01156v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1651516967000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2206.06476v1",
        "predicted_newsworthiness":0.4519629974,
        "title":"Explainable Mixed Data Representation and Lossless Visualization Toolkit for Knowledge Discovery",
        "summary":"Developing Machine Learning (ML) algorithms for heterogeneous\/mixed data is a longstanding problem. Many ML algorithms are not applicable to mixed data, which include numeric and non-numeric data, text, graphs and so on to generate interpretable models. Another longstanding problem is developing algorithms for lossless visualization of multidimensional mixed data. The further progress in ML heavily depends on success interpretable ML algorithms for mixed data and lossless interpretable visualization of multidimensional data. The later allows developing interpretable ML models using visual knowledge discovery by end-users, who can bring valuable domain knowledge which is absent in the training data. The challenges for mixed data include: (1) generating numeric coding schemes for non-numeric attributes for numeric ML algorithms to provide accurate and interpretable ML models, (2) generating methods for lossless visualization of n-D non-numeric data and visual rule discovery in these visualizations. This paper presents a classification of mixed data types, analyzes their importance for ML and present the developed experimental toolkit to deal with mixed data. It combines the Data Types Editor, VisCanvas data visualization and rule discovery system which is available on GitHub.",
        "completion1":"Explaining the Importance of Mixed Data Representation in Machine Learning",
        "completion2":"Developing a Toolkit for Lossless Visualization of Multidimensional Mixed Data",
        "completion3":"Using Visual Knowledge Discovery to Interpret Machine Learning Models",
        "technologyreview":0.2727819701,
        "venturebeat":0.2576578672,
        "wired":0.0709193111,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.06476v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1655154898000,
        "code_mentioned":1,
        "readability":0.77
    },
    {
        "arxiv_id":"2201.02127v1",
        "predicted_newsworthiness":0.6980009837,
        "title":"Sentiment Analysis and Sarcasm Detection of Indian General Election Tweets",
        "summary":"Social Media usage has increased to an all-time high level in today's digital world. The majority of the population uses social media tools (like Twitter, Facebook, YouTube, etc.) to share their thoughts and experiences with the community. Analysing the sentiments and opinions of the common public is very important for both the government and the business people. This is the reason behind the activeness of many media agencies during the election time for performing various kinds of opinion polls. In this paper, we have worked towards analysing the sentiments of the people of India during the Lok Sabha election of 2019 using the Twitter data of that duration. We have built an automatic tweet analyser using the Transfer Learning technique to handle the unsupervised nature of this problem. We have used the Linear Support Vector Classifiers method in our Machine Learning model, also, the Term Frequency Inverse Document Frequency (TF-IDF) methodology for handling the textual data of tweets. Further, we have increased the capability of the model to address the sarcastic tweets posted by some of the users, which has not been yet considered by the researchers in this domain.",
        "completion1":"India's Sentiment Analysis and Sarcasm Detection of General Election Tweets",
        "completion2":"ocial Media Usage at All-Time High During India's General Election",
        "completion3":"witter Data reveals Sentiments of People during India's Lok Sabha Elections",
        "technologyreview":0.2875095625,
        "venturebeat":0.2416853769,
        "wired":0.0902324836,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.02127v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.cl",
            "cs.lg"
        ],
        "published":1641231000000,
        "code_mentioned":0,
        "readability":0.89
    },
    {
        "arxiv_id":"2204.04711v1",
        "predicted_newsworthiness":0.4364600018,
        "title":"Data Augmentation for Biomedical Factoid Question Answering",
        "summary":"We study the effect of seven data augmentation (da) methods in factoid question answering, focusing on the biomedical domain, where obtaining training instances is particularly difficult. We experiment with data from the BioASQ challenge, which we augment with training instances obtained from an artificial biomedical machine reading comprehension dataset, or via back-translation, information retrieval, word substitution based on word2vec embeddings, or masked language modeling, question generation, or extending the given passage with additional context. We show that da can lead to very significant performance gains, even when using large pre-trained Transformers, contributing to a broader discussion of if\/when da benefits large pre-trained models. One of the simplest da methods, word2vec-based word substitution, performed best and is recommended. We release our artificial training instances and code.",
        "completion1":"Data Augmentation Helps Biomedical Factoid Question Answering",
        "completion2":"Seven Data Augmentation Methods Studied for Factoid Question Answering",
        "completion3":"Word2Vec-Based Word Substitution Performs Best for Data Augmentation",
        "technologyreview":0.2641539113,
        "venturebeat":0.2158486037,
        "wired":0.0722286398,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.04711v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1649606273000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2202.02212v3",
        "predicted_newsworthiness":0.5982654459,
        "title":"SSHA: Video Violence Recognition and Localization Using a Semi-Supervised Hard Attention Model",
        "summary":"Current human-based surveillance systems are prone to inadequate availability and reliability. Artificial intelligence-based solutions are compelling, considering their reliability and precision in the face of an increasing adaption of surveillance systems. Exceedingly efficient and precise machine learning models are required to effectively utilize the extensive volume of high-definition surveillance imagery. This study focuses on improving the accuracy of the methods and models used in automated surveillance systems to recognize and localize human violence in video footage. The proposed model uses an I3D backbone pretrained on the Kinetics dataset and has achieved state-of-the-art accuracy of 90.4% and 98.7% on RWF and Hockey datasets, respectively. The semi-supervised hard attention mechanism has enabled the proposed method to fully capture the available information in a high-resolution video by processing the necessary video regions in great detail.",
        "completion1":"New AI model can recognize and localize human violence in video footage with state-of-the-art accuracy.",
        "completion2":"Semi-supervised hard attention mechanism enables proposed method to capture necessary video regions in great detail.",
        "completion3":"Proposed model uses I3D backbone pretrained on Kinetics dataset to achieve high accuracy rates.",
        "technologyreview":0.2498016123,
        "venturebeat":0.1963822932,
        "wired":0.0673899459,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.02212v3",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1643991326000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2206.15444v1",
        "predicted_newsworthiness":0.3332522204,
        "title":"Learning Functions on Multiple Sets using Multi-Set Transformers",
        "summary":"We propose a general deep architecture for learning functions on multiple permutation-invariant sets. We also show how to generalize this architecture to sets of elements of any dimension by dimension equivariance. We demonstrate that our architecture is a universal approximator of these functions, and show superior results to existing methods on a variety of tasks including counting tasks, alignment tasks, distinguishability tasks and statistical distance measurements. This last task is quite important in Machine Learning. Although our approach is quite general, we demonstrate that it can generate approximate estimates of KL divergence and mutual information that are more accurate than previous techniques that are specifically designed to approximate those statistical distances.",
        "completion1":"Deep Learning Architecture Can Learn Functions on Multiple Permutation-Invariant Sets",
        "completion2":"New Approach to Approximating KL Divergence and Mutual Information is More Accurate Than Previous Methods",
        "completion3":"Deep Learning Architecture Shows Superior Results to Existing Methods on Variety of Tasks",
        "technologyreview":0.2263388189,
        "venturebeat":0.1713408752,
        "wired":0.037326959,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.15444v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1656610755000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2202.09721v1",
        "predicted_newsworthiness":0.4854419994,
        "title":"3DRM:Pair-wise relation module for 3D object detection",
        "summary":"Context has proven to be one of the most important factors in object layout reasoning for 3D scene understanding. Existing deep contextual models either learn holistic features for context encoding or rely on pre-defined scene templates for context modeling. We argue that scene understanding benefits from object relation reasoning, which is capable of mitigating the ambiguity of 3D object detections and thus helps locate and classify the 3D objects more accurately and robustly. To achieve this, we propose a novel 3D relation module (3DRM) which reasons about object relations at pair-wise levels. The 3DRM predicts the semantic and spatial relationships between objects and extracts the object-wise relation features. We demonstrate the effects of 3DRM by plugging it into proposal-based and voting-based 3D object detection pipelines, respectively. Extensive evaluations show the effectiveness and generalization of 3DRM on 3D object detection. Our source code is available at https:\/\/github.com\/lanlan96\/3DRM.",
        "completion1":"3D object detection gets a boost from 3DRM",
        "completion2":"3DRM helps to accurately classify and locate 3D objects",
        "completion3":"3DRM is available for use",
        "technologyreview":0.2228247103,
        "venturebeat":0.1947263552,
        "wired":0.0615173033,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.09721v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1645326395000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2201.07365v2",
        "predicted_newsworthiness":0.4089091738,
        "title":"Improving Neural Machine Translation by Denoising Training",
        "summary":"We present a simple and effective pretraining strategy {D}en{o}ising {T}raining DoT for neural machine translation. Specifically, we update the model parameters with source- and target-side denoising tasks at the early stage and then tune the model normally. Notably, our approach does not increase any parameters or training steps, requiring the parallel data merely. Experiments show that DoT consistently improves the neural machine translation performance across 12 bilingual and 16 multilingual directions (data size ranges from 80K to 20M). In addition, we show that DoT can complement existing data manipulation strategies, i.e. curriculum learning, knowledge distillation, data diversification, bidirectional training, and back-translation. Encouragingly, we found that DoT outperforms costly pretrained model mBART in high-resource settings. Analyses show DoT is a novel in-domain cross-lingual pretraining strategy and could offer further improvements with task-relevant self-supervisions.",
        "completion1":"DoT\" Denoising Training: A Simple and Effective Pretraining Strategy for Neural Machine Translation",
        "completion2":"DoT Outperforms Costly Pretrained Model mBART in High-Resource Settings",
        "completion3":"Analyses Show DoT is a Novel In-Domain Cross-Lingual Pretraining Strategy",
        "technologyreview":0.2044564641,
        "venturebeat":0.1698770293,
        "wired":0.0448433971,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.07365v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1642551098000,
        "code_mentioned":1,
        "readability":0.78
    },
    {
        "arxiv_id":"2204.04240v1",
        "predicted_newsworthiness":0.681063365,
        "title":"Controlling Traffic with Humanoid Social Robot",
        "summary":"The advancement of technology such as artificial intelligence, machine learning and internet of things it became easy to develop more humanoid robots and automate different processes. An interactive robot must have high social behavior so that it can be easily accepted by the people using it. In this study we designed a traffic police robot (TRAPROB) to automate the traffic control at intersection. The human police officer experiences high stress because of long duty hours as well as pose the risk of accidents. The digital electronic signals are automatic but we want to create a system which is more human like and looks like an officer controlling the traffic at intersection. We used Thiago++ robot in this study and modified its look to like a police officer, and then programmed it to imitate and make gestures just like traffic police officer makes gestures for controlling traffic. We evaluated the looks, gestures, functionality, and social behavior of the robot. We asked a limited sample of two participants to identify the TRAPBOT, rate its look, the social behaviors and gestures in comparison to a real life police officer. we found that people can identify the robot as traffic police robot. Our analysis also shows that TRAPBOT has appearance like a traffic robot and can make similar signal gestures as a traffic police officer.",
        "completion1":"Controlling Traffic with Humanoid Social Robot",
        "completion2":"The advancement of technology such as artificial intelligence, machine learning and internet of things it became easy to develop more humanoid robots and automate different processes.",
        "completion3":"We used Thiago++ robot in this study and modified its look to like a police officer, and then programmed it to imitate and make gestures just like traffic police officer makes gestures for controlling traffic.",
        "technologyreview":0.348937366,
        "venturebeat":0.2801541294,
        "wired":0.0963322931,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.04240v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.hc"
        ],
        "published":1649442281000,
        "code_mentioned":0,
        "readability":0.9
    },
    {
        "arxiv_id":"2202.10032v1",
        "predicted_newsworthiness":0.5053451259,
        "title":"Domain-level Pairwise Semantic Interaction for Aspect-Based Sentiment Classification",
        "summary":"Aspect-based sentiment classification (ABSC) is a very challenging subtask of sentiment analysis (SA) and suffers badly from the class-imbalance. Existing methods only process sentences independently, without considering the domain-level relationship between sentences, and fail to provide effective solutions to the problem of class-imbalance. From an intuitive point of view, sentences in the same domain often have high-level semantic connections. The interaction of their high-level semantic features can force the model to produce better semantic representations, and find the similarities and nuances between sentences better. Driven by this idea, we propose a plug-and-play Pairwise Semantic Interaction (PSI) module, which takes pairwise sentences as input, and obtains interactive information by learning the semantic vectors of the two sentences. Subsequently, different gates are generated to effectively highlight the key semantic features of each sentence. Finally, the adversarial interaction between the vectors is used to make the semantic representation of two sentences more distinguishable. Experimental results on four ABSC datasets show that, in most cases, PSI is superior to many competitive state-of-the-art baselines and can significantly alleviate the problem of class-imbalance.",
        "completion1":"Domain-level Pairwise Semantic Interaction for Aspect-Based Sentiment Classification",
        "completion2":"Aspect-based sentiment classification is a very challenging subtask of sentiment analysis",
        "completion3":"Existing methods only process sentences independently, without considering the domain-level relationship between sentences, and fail to provide effective solutions to the problem of class-imbalance.",
        "technologyreview":0.2318131152,
        "venturebeat":0.2017635198,
        "wired":0.0747783374,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.10032v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1645430357000,
        "code_mentioned":0,
        "readability":0.82
    },
    {
        "arxiv_id":"2203.02589v1",
        "predicted_newsworthiness":0.5091867198,
        "title":"Network Services Anomalies in NFV: Survey, Taxonomy, and Verification Methods",
        "summary":"Network Function Virtualization (NFV) has emerged as a disruptive networking architecture whose galloping evolution is prompting enterprises to outsource network functions to the cloud and ultimately harvest the fruits of cloud computing, including elasticity, pay-as-you-go billing model, and on-demand services provisioning. However, many reluctant enterprises oppose the benefits of this outsourcing to their critical and pressing concerns about security, trust, and compliance. The latter anticipate possible security and QoS policy violations stemming from dishonest behaviors by cloud providers, attacks by co-resident competitors, misconfiguration by cloud administrators, or implementations flaws by NFV developers. As a result, migrating sensitive workloads to the cloud requires enterprises to first assess risks by gaining knowledge of possible network services' anomalies and second, to build trust in the cloud by designing effective mechanisms to detect such anomalies. This survey provides scrutiny of network services anomalies that may occur in the NFV environments. We first present a taxonomy of network service anomalies and analyze their negative impacts on critical service attributes, including security and performance. Second, we compare and classify the existing anomalies' verification mechanisms from the literature. Finally, we point out the literature gap and identify future research directions for anomalies verification in NFV.",
        "completion1":"NFV: Survey of possible network service anomalies and verification methods",
        "completion2":"Negative impacts of network service anomalies on security and performance",
        "completion3":"Existing anomalies' verification mechanisms from the literature",
        "technologyreview":0.2765213804,
        "venturebeat":0.3135952219,
        "wired":0.0885329888,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.02589v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1646432218000,
        "code_mentioned":0,
        "readability":0.83
    },
    {
        "arxiv_id":"2202.06565v1",
        "predicted_newsworthiness":0.5058131197,
        "title":"Single-stage Rotate Object Detector via Two Points with Solar Corona Heatmap",
        "summary":"Oriented object detection is a crucial task in computer vision. Current top-down oriented detection methods usually directly detect entire objects, and not only neglecting the authentic direction of targets, but also do not fully utilise the key semantic information, which causes a decrease in detection accuracy. In this study, we developed a single-stage rotating object detector via two points with a solar corona heatmap (ROTP) to detect oriented objects. The ROTP predicts parts of the object and then aggregates them to form a whole image. Herein, we meticulously represent an object in a random direction using the vertex, centre point with width, and height. Specifically, we regress two heatmaps that characterise the relative location of each object, which enhances the accuracy of locating objects and avoids deviations caused by angle predictions. To rectify the central misjudgement of the Gaussian heatmap on high-aspect ratio targets, we designed a solar corona heatmap generation method to improve the perception difference between the central and non-central samples. Additionally, we predicted the vertex relative to the direction of the centre point to connect two key points that belong to the same goal. Experiments on the HRSC 2016, UCASAOD, and DOTA datasets show that our ROTP achieves the most advanced performance with a simpler modelling and less manual intervention.",
        "completion1":"Single-stage Rotate Object Detector via Two Points with Solar Corona Heatmap developed to detect oriented objects",
        "completion2":"ROTP predicts parts of the object and then aggregates them to form a whole image",
        "completion3":"To rectify the central misjudgement of the Gaussian heatmap on high-aspect ratio targets, we designed a solar corona heatmap generation method to improve the perception difference between the central and non-central samples",
        "technologyreview":0.2249325065,
        "venturebeat":0.1844266204,
        "wired":0.0569767682,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.06565v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1644829641000,
        "code_mentioned":0,
        "readability":0.81
    },
    {
        "arxiv_id":"2201.11898v2",
        "predicted_newsworthiness":0.4422602521,
        "title":"Indicative Image Retrieval: Turning Blackbox Learning into Grey",
        "summary":"Deep learning became the game changer for image retrieval soon after it was introduced. It promotes the feature extraction (by representation learning) as the core of image retrieval, with the relevance\/matching evaluation being degenerated into simple similarity metrics. In many applications, we need the matching evidence to be indicated rather than just have the ranked list (e.g., the locations of the target proteins\/cells\/lesions in medical images). It is like the matched words need to be highlighted in search engines. However, this is not easy to implement without explicit relevance\/matching modeling. The deep representation learning models are not feasible because of their blackbox nature. In this paper, we revisit the importance of relevance\/matching modeling in deep learning era with an indicative retrieval setting. The study shows that it is possible to skip the representation learning and model the matching evidence directly. By removing the dependency on the pre-trained models, it has avoided a lot of related issues (e.g., the domain gap between classification and retrieval, the detail-diffusion caused by convolution, and so on). More importantly, the study demonstrates that the matching can be explicitly modeled and backtracked later for generating the matching evidence indications. It can improve the explainability of deep inference. Our method obtains a best performance in literature on both Oxford-5k and Paris-6k, and sets a new record of 97.77% on Oxford-5k (97.81% on Paris-6k) without extracting any deep features.",
        "completion1":"Indicative Image Retrieval: Turning Blackbox Learning into Grey",
        "completion2":"Study Shows That It Is Possible to Skip Representation Learning and Model Matching Evidence Directly",
        "completion3":"New Record of 97.77% Set on Oxford-5k Without Extracting Any Deep Features",
        "technologyreview":0.2726516802,
        "venturebeat":0.207612585,
        "wired":0.0493680074,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.11898v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.ir"
        ],
        "published":1643336469000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2207.06345v1",
        "predicted_newsworthiness":0.354469512,
        "title":"You Only Align Once: Bidirectional Interaction for Spatial-Temporal Video Super-Resolution",
        "summary":"Spatial-Temporal Video Super-Resolution (ST-VSR) technology generates high-quality videos with higher resolution and higher frame rates. Existing advanced methods accomplish ST-VSR tasks through the association of Spatial and Temporal video super-resolution (S-VSR and T-VSR). These methods require two alignments and fusions in S-VSR and T-VSR, which is obviously redundant and fails to sufficiently explore the information flow of consecutive spatial LR frames. Although bidirectional learning (future-to-past and past-to-future) was introduced to cover all input frames, the direct fusion of final predictions fails to sufficiently exploit intrinsic correlations of bidirectional motion learning and spatial information from all frames. We propose an effective yet efficient recurrent network with bidirectional interaction for ST-VSR, where only one alignment and fusion is needed. Specifically, it first performs backward inference from future to past, and then follows forward inference to super-resolve intermediate frames. The backward and forward inferences are assigned to learn structures and details to simplify the learning task with joint optimizations. Furthermore, a Hybrid Fusion Module (HFM) is designed to aggregate and distill information to refine spatial information and reconstruct high-quality video frames. Extensive experiments on two public datasets demonstrate that our method outperforms state-of-the-art methods in efficiency, and reduces calculation cost by about 22%.",
        "completion1":"New ST-VSR technology generates high-quality videos with higher resolution and higher frame rates.",
        "completion2":"Existing advanced methods accomplish ST-VSR tasks through the association of Spatial and Temporal video super-resolution (S-VSR and T-VSR).",
        "completion3":"We propose an effective yet efficient recurrent network with bidirectional interaction for ST-VSR, where only one alignment and fusion is needed.",
        "technologyreview":0.179410939,
        "venturebeat":0.1615012541,
        "wired":0.0546419314,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.06345v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1657731676000,
        "code_mentioned":0,
        "readability":0.79
    },
    {
        "arxiv_id":"2204.07146v1",
        "predicted_newsworthiness":0.4241511699,
        "title":"GelSight Fin Ray: Incorporating Tactile Sensing into a Soft Compliant Robotic Gripper",
        "summary":"To adapt to constantly changing environments and be safe for human interaction, robots should have compliant and soft characteristics as well as the ability to sense the world around them. Even so, the incorporation of tactile sensing into a soft compliant robot, like the Fin Ray finger, is difficult due to its deformable structure. Not only does the frame need to be modified to allow room for a vision sensor, which enables intricate tactile sensing, the robot must also retain its original mechanically compliant properties. However, adding high-resolution tactile sensors to soft fingers is difficult since many sensorized fingers, such as GelSight-based ones, are rigid and function under the assumption that changes in the sensing region are only from tactile contact and not from finger compliance. A sensorized soft robotic finger needs to be able to separate its overall proprioceptive changes from its tactile information. To this end, this paper introduces the novel design of a GelSight Fin Ray, which embodies both the ability to passively adapt to any object it grasps and the ability to perform high-resolution tactile reconstruction, object orientation estimation, and marker tracking for shear and torsional forces. Having these capabilities allow soft and compliant robots to perform more manipulation tasks that require sensing. One such task the finger is able to perform successfully is a kitchen task: wine glass reorientation and placement, which is difficult to do with external vision sensors but is easy with tactile sensing. The development of this sensing technology could also potentially be applied to other soft compliant grippers, increasing their viability in many different fields.",
        "completion1":"Robots Get a Grip: New Fin Ray Embodies Soft Sensing Characteristics",
        "completion2":"GelSight Fin Ray Makes Robots More Human-Friendly",
        "completion3":"Incorporating Tactile Sensing Into a Soft Compliant Robotic Gripper",
        "technologyreview":0.1776275792,
        "venturebeat":0.1226239913,
        "wired":0.0119020903,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.07146v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1649958961000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2204.06433v1",
        "predicted_newsworthiness":0.5284676559,
        "title":"A Systematic Comparison of Simulation Software for Robotic Arm Manipulation using ROS2",
        "summary":"Simulation software is a powerful tool for robotics research, allowing the virtual representation of the real world. However with the rise of the Robot Operating System (ROS), there are new simulation software packages that have not been compared within the literature. This paper proposes a systematic review of simulation software that are compatible with ROS version 2. The focus is research in robotics arm manipulation as it represents the most often used robotic application in industry and their future applicability to digital twins. For this, we thus benchmark simulation software under similar parameters, tasks and scenarios, and evaluate them in terms of their capability for long-term operations, success at completing a task, repeatability and resource usage. We find that there is no best simulation software overall, but two simulation packages (Ignition and Webots) have higher stability than other while, in terms of resources usage, PyBullet and Coppeliasim consume less than their competitors.",
        "completion1":"New study compares simulation software for robotic arm manipulation",
        "completion2":"which simulation software is best for ROS2?",
        "completion3":"Research finds no 'best' simulation software, but some are more stable than others",
        "technologyreview":0.2244141663,
        "venturebeat":0.1890875848,
        "wired":0.0303738303,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.06433v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1649861498000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2203.10652v2",
        "predicted_newsworthiness":0.4292650078,
        "title":"Continual Sequence Generation with Adaptive Compositional Modules",
        "summary":"Continual learning is essential for real-world deployment when there is a need to quickly adapt the model to new tasks without forgetting knowledge of old tasks. Existing work on continual sequence generation either always reuses existing parameters to learn new tasks, which is vulnerable to catastrophic forgetting on dissimilar tasks, or blindly adds new parameters for every new task, which could prevent knowledge sharing between similar tasks. To get the best of both worlds, in this work, we propose continual sequence generation with adaptive compositional modules to adaptively add modules in transformer architectures and compose both old and new modules for new tasks. We also incorporate pseudo experience replay to facilitate knowledge transfer in those shared modules. Experiment results on various sequences of generation tasks show that our framework can adaptively add modules or reuse modules based on task similarity, outperforming state-of-the-art baselines in terms of both performance and parameter efficiency. We make our code public at https:\/\/github.com\/GT-SALT\/Adaptive-Compositional-Modules.",
        "completion1":"Continual Sequence Generation with Adaptive Compositional Modules",
        "completion2":"Existing work on continual sequence generation either always reuses existing parameters to learn new tasks, which is vulnerable to catastrophic forgetting on dissimilar tasks, or blindly adds new parameters for every new task, which could prevent knowledge sharing between similar tasks.",
        "completion3":"In this work, we propose continual sequence generation with adaptive compositional modules to adaptively add modules in transformer architectures and compose both old and new modules for new tasks.",
        "technologyreview":0.2606709336,
        "venturebeat":0.2264314168,
        "wired":0.0605773698,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.10652v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1647811368000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2202.11351v1",
        "predicted_newsworthiness":0.4313572225,
        "title":"A Semi-Synthetic Dataset Generation Framework for Causal Inference in Recommender Systems",
        "summary":"Accurate recommendation and reliable explanation are two key issues for modern recommender systems. However, most recommendation benchmarks only concern the prediction of user-item ratings while omitting the underlying causes behind the ratings. For example, the widely-used Yahoo!R3 dataset contains little information on the causes of the user-movie ratings. A solution could be to conduct surveys and require the users to provide such information. In practice, the user surveys can hardly avoid compliance issues and sparse user responses, which greatly hinders the exploration of causality-based recommendation. To better support the studies of causal inference and further explanations in recommender systems, we propose a novel semi-synthetic data generation framework for recommender systems where causal graphical models with missingness are employed to describe the causal mechanism of practical recommendation scenarios. To illustrate the use of our framework, we construct a semi-synthetic dataset with Causal Tags And Ratings (CTAR), based on the movies as well as their descriptive tags and rating information collected from a famous movie rating website. Using the collected data and the causal graph, the user-item-ratings and their corresponding user-item-tags are automatically generated, which provides the reasons (selected tags) why the user rates the items. Descriptive statistics and baseline results regarding the CTAR dataset are also reported. The proposed data generation framework is not limited to recommendation, and the released APIs can be used to generate customized datasets for other research tasks.",
        "completion1":"New Framework for Generating Semi-Synthetic Datasets Offers Causal Insight into Recommender Systems",
        "completion2":"CTAR: A Semi-Synthetic Dataset for Causal Inference in Recommender Systems",
        "completion3":"New Data Generation Framework Could Help Explain Recommendations",
        "technologyreview":0.2576751388,
        "venturebeat":0.2536522476,
        "wired":0.0949481677,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.11351v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1645605536000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2205.08479v3",
        "predicted_newsworthiness":0.441155335,
        "title":"Opportunistic Routing in Quantum Networks",
        "summary":"Unlike classical routing algorithms, quantum routing algorithms make use of entangled states - a type of resources that have a limited lifetime and need to be regenerated after consumption. In a nutshell, quantum routing algorithms have to use these resources efficiently, while optimizing some objectives such as the total waiting time. Current routing algorithms tend to keep a routing request waiting until all of the resources on its path are available. In this paper, we introduce a new way of managing entanglement resources in an opportunistic fashion: a request can move forward along its path as soon as possible (even if some resources on its path are not ready). We show that this opportunistic approach is fundamentally better than conventional approaches. In particular, our results indicate that this new approach achieves a 30-50% improvement in the average total waiting time and average link waiting time compared with several state-of-the-art routing algorithms. As a by-product of this work, we develop a new simulator for quantum routing, which can be used to evaluate various design choices under different scenarios.",
        "completion1":"Researchers Introduce New, More Efficient Quantum Routing Algorithm",
        "completion2":"Study Shows Opportunistic Routing Reduces Waiting Time in Quantum Networks",
        "completion3":"Simulator for Quantum Routing Developed as By-Product of New Approach",
        "technologyreview":0.1525416144,
        "venturebeat":0.1173467945,
        "wired":0.0330471813,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.08479v3",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1652805850000,
        "code_mentioned":1,
        "readability":0.89
    },
    {
        "arxiv_id":"2202.01291v1",
        "predicted_newsworthiness":0.6988782557,
        "title":"Computer sciences and synthesis: retrospective and perspective",
        "summary":"The problem of synthesis in computer sciences, including cybernetics, artificial intelligence and system analysis, is analyzed. Main methods of realization this problem are discussed. Ways of search universal method of creation universal synthetic science are represented. As example of such universal method polymetric analysis is given. Perspective of further development of this research, including application polymetric method for the resolution main problems of computer sciences, is analyzed too.",
        "completion1":"Computer Sciences and Synthesis: Retrospective and Perspective",
        "completion2":"The Problem of Synthesis in Computer Sciences, Including Cybernetics, Artificial Intelligence and System Analysis",
        "completion3":"Ways of Search Universal Method of Creation Universal Synthetic Science Are Represented",
        "technologyreview":0.2385136409,
        "venturebeat":0.1646256965,
        "wired":0.0238488909,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.01291v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1643172165000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2203.05898v1",
        "predicted_newsworthiness":0.3439224681,
        "title":"Hyperbolic Image Segmentation",
        "summary":"For image segmentation, the current standard is to perform pixel-level optimization and inference in Euclidean output embedding spaces through linear hyperplanes. In this work, we show that hyperbolic manifolds provide a valuable alternative for image segmentation and propose a tractable formulation of hierarchical pixel-level classification in hyperbolic space. Hyperbolic Image Segmentation opens up new possibilities and practical benefits for segmentation, such as uncertainty estimation and boundary information for free, zero-label generalization, and increased performance in low-dimensional output embeddings.",
        "completion1":"Hyperbolic Image Segmentation Proven Better Than Euclidean for Pixel-Level Classification",
        "completion2":"Hyperbolic Space Offers Tangible Benefits for Image Segmentation",
        "completion3":"Hyperbolic Manifolds Improve Image Segmentation",
        "technologyreview":0.1629901327,
        "venturebeat":0.1252025571,
        "wired":0.0326744461,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.05898v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1647004071000,
        "code_mentioned":1,
        "readability":0.7
    },
    {
        "arxiv_id":"2207.02619v1",
        "predicted_newsworthiness":0.5257960561,
        "title":"Multimodal Hydrostatic Actuators for Wearable Robots: A Preliminary Assessment of Mass-Saving and Energy-Efficiency Opportunities",
        "summary":"Wearable robots are limited by their actuators performances because they must bear the weight of their own power system and energy source. This paper explores the idea of leveraging hybrid modes to meet multiple operating points with a lightweight and efficient system by using hydraulic valves to dynamically reconfigure the connections of a hydrostatic actuator. The analyzed opportunities consist in 1) switching between a highly geared power source or a fast power source, 2) dynamically connecting an energy accumulator and 3) using a locking mechanism for holding. Based on a knee exoskeleton case study analysis, results show that switching between gearing ratio can lead to a lighter and more efficient actuator. Also, results show that using an accumulator to provide a preload continuous force has great mass-saving potential, but does not reduce mass significantly if used as a power booster for short transients. Finally, using a locking valve can slightly reduce battery mass if the work cycle includes frequent stops. The operating principles of the proposed multimodal schemes are demonstrated with a one-DOF prototype.",
        "completion1":"Wearable Robots Could Soon Be More Efficient Thanks to Multimodal Hydrostatic Actuators",
        "completion2":"New Study Shows Ways to Improve Efficiency for Wearable Robots",
        "completion3":"How Multimodal Hydrostatic Actuators Can Help Wearable Robots Save Mass and Energy",
        "technologyreview":0.1696713727,
        "venturebeat":0.1205769332,
        "wired":0.018401748,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.02619v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1657110256000,
        "code_mentioned":0,
        "readability":0.83
    },
    {
        "arxiv_id":"2202.08946v1",
        "predicted_newsworthiness":0.6241070172,
        "title":"Symphony: Composing Interactive Interfaces for Machine Learning",
        "summary":"Interfaces for machine learning (ML), information and visualizations about models or data, can help practitioners build robust and responsible ML systems. Despite their benefits, recent studies of ML teams and our interviews with practitioners (n=9) showed that ML interfaces have limited adoption in practice. While existing ML interfaces are effective for specific tasks, they are not designed to be reused, explored, and shared by multiple stakeholders in cross-functional teams. To enable analysis and communication between different ML practitioners, we designed and implemented Symphony, a framework for composing interactive ML interfaces with task-specific, data-driven components that can be used across platforms such as computational notebooks and web dashboards. We developed Symphony through participatory design sessions with 10 teams (n=31), and discuss our findings from deploying Symphony to 3 production ML projects at Apple. Symphony helped ML practitioners discover previously unknown issues like data duplicates and blind spots in models while enabling them to share insights with other stakeholders.",
        "completion1":"Symphony: Composing Interactive Interfaces for Machine Learning",
        "completion2":"Interactive ML interfaces help build robust systems, but have limited adoption",
        "completion3":"Symphony helps practitioners discover unknown issues and share insights",
        "technologyreview":0.3734470331,
        "venturebeat":0.3645122952,
        "wired":0.1260344497,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.08946v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ai",
            "cs.lg"
        ],
        "published":1645144050000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2201.03028v1",
        "predicted_newsworthiness":0.5163565482,
        "title":"Development of a hybrid machine-learning and optimization tool for performance-based solar shading design",
        "summary":"Solar shading design should be done for the desired Indoor Environmental Quality (IEQ) in the early design stages. This field can be very challenging and time-consuming also requires experts, sophisticated software, and a large amount of money. The primary purpose of this research is to design a simple tool to study various models of solar shadings and make decisions easier and faster in the early stages. Database generation methods, artificial intelligence, and optimization have been used to achieve this goal. This tool includes two main parts of 1. predicting the performance of the user-selected model along with proposing effective parameters and 2. proposing optimal pre-prepared models to the user. In this regard, initially, a side-lit shoebox model with variable parameters was modeled parametrically, and five common solar shading models with their variables were applied to the space. For each solar shadings and the state without shading, metrics related to daylight and glare, view, and initial costs were simulated. The database generated in this research includes 87912 alternatives and six calculated metrics introduced to optimized machine learning models, including neural network, random Forrest, support vector regression, and k nearest neighbor. According to the results, the most accurate and fastest estimation model was Random Forrest, with an r2_score of 0.967 to 1. Then, sensitivity analysis was performed to identify the most influential parameters for each shading model and the state without it. This analysis distinguished the most effective parameters, including window orientation, WWR, room width, length, and shading depth. Finally, by optimizing the estimation function of machine learning models with the NSGA II algorithm, about 7300 optimal models were identified. The developed tool can evaluate various design alternatives in less than a few seconds for each.",
        "completion1":"Development of hybrid machine-learning and optimization tool for performance-based solar shading design",
        "completion2":"Solar shading design should be done for the desired Indoor Environmental Quality in the early design stages",
        "completion3":"Primary purpose of this research is to design a simple tool to study various models of solar shadings and make decisions easier and faster in the early stages",
        "technologyreview":0.2092472463,
        "venturebeat":0.1830553565,
        "wired":0.0425871765,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.03028v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1641740073000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2207.06531v1",
        "predicted_newsworthiness":0.4409339569,
        "title":"Reachability Analysis of a General Class of Neural Ordinary Differential Equations",
        "summary":"Continuous deep learning models, referred to as Neural Ordinary Differential Equations (Neural ODEs), have received considerable attention over the last several years. Despite their burgeoning impact, there is a lack of formal analysis techniques for these systems. In this paper, we consider a general class of neural ODEs with varying architectures and layers, and introduce a novel reachability framework that allows for the formal analysis of their behavior. The methods developed for the reachability analysis of neural ODEs are implemented in a new tool called NNVODE. Specifically, our work extends an existing neural network verification tool to support neural ODEs. We demonstrate the capabilities and efficacy of our methods through the analysis of a set of benchmarks that include neural ODEs used for classification, and in control and dynamical systems, including an evaluation of the efficacy and capabilities of our approach with respect to existing software tools within the continuous-time systems reachability literature, when it is possible to do so.",
        "completion1":"Analysis of neural ODEs shows promise for continuous deep learning models",
        "completion2":"Formal reachability framework introduced for general class of neural ODEs",
        "completion3":"NNVODE tool extends existing neural network verification tool to support neural ODEs",
        "technologyreview":0.2167858993,
        "venturebeat":0.1710723567,
        "wired":0.030994408,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.06531v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1657749952000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2207.08087v2",
        "predicted_newsworthiness":0.4051962066,
        "title":"Automatic Context Pattern Generation for Entity Set Expansion",
        "summary":"Entity Set Expansion (ESE) is a valuable task that aims to find entities of the target semantic class described by given seed entities. Various NLP and IR downstream applications have benefited from ESE due to its ability to discover knowledge. Although existing bootstrapping methods have achieved great progress, most of them still rely on manually pre-defined context patterns. A non-negligible shortcoming of the pre-defined context patterns is that they cannot be flexibly generalized to all kinds of semantic classes, and we call this phenomenon as \"semantic sensitivity\". To address this problem, we devise a context pattern generation module that utilizes autoregressive language models (e.g., GPT-2) to automatically generate high-quality context patterns for entities. In addition, we propose the GAPA, a novel ESE framework that leverages the aforementioned GenerAted PAtterns to expand target entities. Extensive experiments and detailed analyses on three widely used datasets demonstrate the effectiveness of our method. All the codes of our experiments will be available for reproducibility.",
        "completion1":"Automatic Context Pattern Generation for Entity Set Expansion",
        "completion2":"GAPA: A Novel ESE Framework Leveraging GenerAted PAtterns",
        "completion3":"Extensive Experiments and Detailed Analyses on Three Widely Used Datasets Demonstrate the Effectiveness of Our Method",
        "technologyreview":0.201066128,
        "venturebeat":0.2017630989,
        "wired":0.0556143373,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.08087v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ir"
        ],
        "published":1658040635000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2207.08939v1",
        "predicted_newsworthiness":0.3816964431,
        "title":"Learning Sparsity-Promoting Regularizers using Bilevel Optimization",
        "summary":"We present a method for supervised learning of sparsity-promoting regularizers for denoising signals and images. Sparsity-promoting regularization is a key ingredient in solving modern signal reconstruction problems; however, the operators underlying these regularizers are usually either designed by hand or learned from data in an unsupervised way. The recent success of supervised learning (mainly convolutional neural networks) in solving image reconstruction problems suggests that it could be a fruitful approach to designing regularizers. Towards this end, we propose to denoise signals using a variational formulation with a parametric, sparsity-promoting regularizer, where the parameters of the regularizer are learned to minimize the mean squared error of reconstructions on a training set of ground truth image and measurement pairs. Training involves solving a challenging bilievel optimization problem; we derive an expression for the gradient of the training loss using the closed-form solution of the denoising problem and provide an accompanying gradient descent algorithm to minimize it. Our experiments with structured 1D signals and natural images show that the proposed method can learn an operator that outperforms well-known regularizers (total variation, DCT-sparsity, and unsupervised dictionary learning) and collaborative filtering for denoising. While the approach we present is specific to denoising, we believe that it could be adapted to the larger class of inverse problems with linear measurement models, giving it applicability in a wide range of signal reconstruction settings.",
        "completion1":"A New Method for Supervised Learning of Sparsity-Promoting Regularizers",
        "completion2":"Bilievel Optimization Used to Improve Signal Reconstruction",
        "completion3":"How gradient descent can help bilievel optimization problems",
        "technologyreview":0.1595740899,
        "venturebeat":0.1105756883,
        "wired":0.0208360183,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.08939v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658177402000,
        "code_mentioned":0,
        "readability":0.78
    },
    {
        "arxiv_id":"2202.08494v1",
        "predicted_newsworthiness":0.3546456844,
        "title":"Learning continuous models for continuous physics",
        "summary":"Dynamical systems that evolve continuously over time are ubiquitous throughout science and engineering. Machine learning (ML) provides data-driven approaches to model and predict the dynamics of such systems. A core issue with this approach is that ML models are typically trained on discrete data, using ML methodologies that are not aware of underlying continuity properties, which results in models that often do not capture the underlying continuous dynamics of a system of interest. As a result, these ML models are of limited use for for many scientific and engineering applications. To address this challenge, we develop a convergence test based on numerical analysis theory. Our test verifies whether a model has learned a function that accurately approximates a system's underlying continuous dynamics. Models that fail this test fail to capture relevant dynamics, rendering them of limited utility for many scientific prediction tasks; while models that pass this test enable both better interpolation and better extrapolation in multiple ways. Our results illustrate how principled numerical analysis methods can be coupled with existing ML training\/testing methodologies to validate models for science and engineering applications.",
        "completion1":"Scientists Develop Convergence Test to Validate Machine Learning Models",
        "completion2":"Numerical Analysis Theory Used to Improve Machine Learning Methods",
        "completion3":"New Method Ensures Machine Learning Models Accurately Capture Continuous Dynamics",
        "technologyreview":0.242208985,
        "venturebeat":0.1824968974,
        "wired":0.0443583961,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.08494v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1645084606000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2201.01592v1",
        "predicted_newsworthiness":0.3906924619,
        "title":"Biphasic Face Photo-Sketch Synthesis via Semantic-Driven Generative Adversarial Network with Graph Representation Learning",
        "summary":"In recent years, significant progress has been achieved in biphasic face photo-sketch synthesis with the development of Generative Adversarial Network (GAN). Biphasic face photo-sketch synthesis could be applied in wide-ranging fields such as digital entertainment and law enforcement. However, generating realistic photos and distinct sketches suffers from great challenges due to the low quality of sketches and complex photo variations in the real scenes. To this end, we propose a novel Semantic-Driven Generative Adversarial Network to address the above issues, cooperating with the Graph Representation Learning. Specifically, we inject class-wise semantic layouts into the generator to provide style-based spatial supervision for synthesized face photos and sketches. In addition, to improve the fidelity of the generated results, we leverage the semantic layouts to construct two types of Representational Graphs which indicate the intra-class semantic features and inter-class structural features of the synthesized images. Furthermore, we design two types of constraints based on the proposed Representational Graphs which facilitate the preservation of the details in generated face photos and sketches. Moreover, to further enhance the perceptual quality of synthesized images, we propose a novel biphasic training strategy which is dedicated to refine the generated results through Iterative Cycle Training. Extensive experiments are conducted on CUFS and CUFSF datasets to demonstrate the prominent ability of our proposed method which achieves the state-of-the-art performance.",
        "completion1":"Biphasic Face Photo-Sketch Synthesis via Semantic-Driven Generative Adversarial Network with Graph Representation Learning.",
        "completion2":"In recent years, significant progress has been achieved in biphasic face photo-sketch synthesis with the development of Generative Adversarial Network .",
        "completion3":"Biphasic face photo-sketch synthesis could be applied in wide-ranging fields such as digital entertainment and law enforcement.",
        "technologyreview":0.2274685099,
        "venturebeat":0.1633614084,
        "wired":0.0625885825,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.01592v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1641388454000,
        "code_mentioned":0,
        "readability":0.79
    },
    {
        "arxiv_id":"2206.04184v1",
        "predicted_newsworthiness":0.4571281824,
        "title":"Abstraction not Memory: BERT and the English Article System",
        "summary":"Article prediction is a task that has long defied accurate linguistic description. As such, this task is ideally suited to evaluate models on their ability to emulate native-speaker intuition. To this end, we compare the performance of native English speakers and pre-trained models on the task of article prediction set up as a three way choice (a\/an, the, zero). Our experiments with BERT show that BERT outperforms humans on this task across all articles. In particular, BERT is far superior to humans at detecting the zero article, possibly because we insert them using rules that the deep neural model can easily pick up. More interestingly, we find that BERT tends to agree more with annotators than with the corpus when inter-annotator agreement is high but switches to agreeing more with the corpus as inter-annotator agreement drops. We contend that this alignment with annotators, despite being trained on the corpus, suggests that BERT is not memorising article use, but captures a high level generalisation of article use akin to human intuition.",
        "completion1":"BERT outperforms humans on the task of article prediction",
        "completion2":"BERT is far superior to humans at detecting the zero article",
        "completion3":"BERT tends to agree more with annotators than with the corpus when inter-annotator agreement is high",
        "technologyreview":0.2877452885,
        "venturebeat":0.2244077773,
        "wired":0.0792626318,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.04184v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1654727814000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2203.11442v4",
        "predicted_newsworthiness":0.3595479383,
        "title":"Associating Objects with Scalable Transformers for Video Object Segmentation",
        "summary":"This paper investigates how to realize better and more efficient embedding learning to tackle the semi-supervised video object segmentation under challenging multi-object scenarios. The state-of-the-art methods learn to decode features with a single positive object and thus have to match and segment each target separately under multi-object scenarios, consuming multiple times computation resources. To solve the problem, we propose an Associating Objects with Transformers (AOT) approach to match and decode multiple objects jointly and collaboratively. In detail, AOT employs an identification mechanism to associate multiple targets into the same high-dimensional embedding space. Thus, we can simultaneously process multiple objects' matching and segmentation decoding as efficiently as processing a single object. To sufficiently model multi-object association, a Long Short-Term Transformer (LSTT) is devised to construct hierarchical matching and propagation. Based on AOT, we further propose a more flexible and robust framework, Associating Objects with Scalable Transformers (AOST), in which a scalable version of LSTT is designed to enable run-time adaptation of accuracy-efficiency trade-offs. Besides, AOST introduces a better layer-wise manner to couple identification and vision embeddings. We conduct extensive experiments on multi-object and single-object benchmarks to examine AOT series frameworks. Compared to the state-of-the-art competitors, our methods can maintain times of run-time efficiency with superior performance. Notably, we achieve new state-of-the-art performance on three popular benchmarks, i.e., YouTube-VOS (86.5%), DAVIS 2017 Val\/Test (87.0%\/84.7%), and DAVIS 2016 (93.0%). Project page: https:\/\/github.com\/z-x-yang\/AOT.",
        "completion1":"AOT employs an identification mechanism to associate multiple targets into the same high-dimensional embedding space.",
        "completion2":"AOT series frameworks can maintain times of run-time efficiency with superior performance.",
        "completion3":"AOST introduces a better layer-wise manner to couple identification and vision embeddings.",
        "technologyreview":0.1932617899,
        "venturebeat":0.1632821067,
        "wired":0.049496516,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.11442v4",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1647920007000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2204.07305v1",
        "predicted_newsworthiness":0.4737291833,
        "title":"Pushing the Limits of Simple Pipelines for Few-Shot Learning: External Data and Fine-Tuning Make a Difference",
        "summary":"Few-shot learning (FSL) is an important and topical problem in computer vision that has motivated extensive research into numerous methods spanning from sophisticated meta-learning methods to simple transfer learning baselines. We seek to push the limits of a simple-but-effective pipeline for more realistic and practical settings of few-shot image classification. To this end, we explore few-shot learning from the perspective of neural network architecture, as well as a three stage pipeline of network updates under different data supplies, where unsupervised external data is considered for pre-training, base categories are used to simulate few-shot tasks for meta-training, and the scarcely labelled data of an novel task is taken for fine-tuning. We investigate questions such as: (1) How pre-training on external data benefits FSL? (2) How state-of-the-art transformer architectures can be exploited? and (3) How fine-tuning mitigates domain shift? Ultimately, we show that a simple transformer-based pipeline yields surprisingly good performance on standard benchmarks such as Mini-ImageNet, CIFAR-FS, CDFSL and Meta-Dataset. Our code and demo are available at https:\/\/hushell.github.io\/pmf.",
        "completion1":"Pushing the Limits of Simple Pipelines for Few-Shot Learning: External Data and Fine-Tuning Make a Difference",
        "completion2":"A Simple But Effective Pipeline for Few-Shot Learning",
        "completion3":"Exploiting State-of-the-Art Transformer Architectures for Few-Shot Learning",
        "technologyreview":0.2354207829,
        "venturebeat":0.1824558217,
        "wired":0.0623587341,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.07305v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1649991358000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2202.05423v1",
        "predicted_newsworthiness":0.3616537521,
        "title":"Understanding Curriculum Learning in Policy Optimization for Solving Combinatorial Optimization Problems",
        "summary":"Over the recent years, reinforcement learning (RL) has shown impressive performance in finding strategic solutions for game environments, and recently starts to show promising results in solving combinatorial optimization (CO) problems, inparticular when coupled with curriculum learning to facilitate training. Despite emerging empirical evidence, theoretical study on why RL helps is still at its early stage. This paper presents the first systematic study on policy optimization methods for solving CO problems. We show that CO problems can be naturally formulated as latent Markov Decision Processes (LMDPs), and prove convergence bounds on natural policy gradient (NPG) for solving LMDPs. Furthermore, our theory explains the benefit of curriculum learning: it can find a strong sampling policy and reduce the distribution shift, a critical quantity that governs the convergence rate in our theorem. For a canonical combinatorial problem, Secretary Problem, we formally prove that distribution shift is reduced exponentially with curriculum learning. Our theory also shows we can simplify the curriculum learning scheme used in prior work from multi-step to single-step. Lastly, we provide extensive experiments on Secretary Problem and Online Knapsack to empirically verify our findings.",
        "completion1":"Study Shows Curriculum Learning in Policy Optimization Reduces Distribution Shift for Combinatorial Optimization Problems",
        "completion2":"Policy Optimization Methods for Solving Combinatorial Optimization Problems Converge with Curriculum Learning",
        "completion3":"Theory Explains why Reinforcement Learning Helps in Combinatorial Optimization Problems",
        "technologyreview":0.2350840359,
        "venturebeat":0.209957699,
        "wired":0.0378766698,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.05423v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1644549435000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2204.10238v1",
        "predicted_newsworthiness":0.4368693164,
        "title":"HEATGait: Hop-Extracted Adjacency Technique in Graph Convolution based Gait Recognition",
        "summary":"Biometric authentication using gait has become a promising field due to its unobtrusive nature. Recent approaches in model-based gait recognition techniques utilize spatio-temporal graphs for the elegant extraction of gait features. However, existing methods often rely on multi-scale operators for extracting long-range relationships among joints resulting in biased weighting. In this paper, we present HEATGait, a gait recognition system that improves the existing multi-scale graph convolution by efficient hop-extraction technique to alleviate the issue. Combined with preprocessing and augmentation techniques, we propose a powerful feature extractor that utilizes ResGCN to achieve state-of-the-art performance in model-based gait recognition on the CASIA-B gait dataset.",
        "completion1":"New gait recognition system improves accuracy by efficiently extracting relationships among joints.",
        "completion2":"HEATGait offers state-of-the-art performance in model-based gait recognition.",
        "completion3":"Biometric authentication using gait becomes more reliable with HEATGait's improved accuracy.",
        "technologyreview":0.1997808424,
        "venturebeat":0.1644660314,
        "wired":0.0418177479,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.10238v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1650557638000,
        "code_mentioned":0,
        "readability":0.72
    },
    {
        "arxiv_id":"2204.04344v1",
        "predicted_newsworthiness":0.4449690999,
        "title":"Towards Better Chinese-centric Neural Machine Translation for Low-resource Languages",
        "summary":"The last decade has witnessed enormous improvements in science and technology, stimulating the growing demand for economic and cultural exchanges in various countries. Building a neural machine translation (NMT) system has become an urgent trend, especially in the low-resource setting. However, recent work tends to study NMT systems for low-resource languages centered on English, while few works focus on low-resource NMT systems centered on other languages such as Chinese. To achieve this, the low-resource multilingual translation challenge of the 2021 iFLYTEK AI Developer Competition provides the Chinese-centric multilingual low-resource NMT tasks, where participants are required to build NMT systems based on the provided low-resource samples. In this paper, we present the winner competition system that leverages monolingual word embeddings data enhancement, bilingual curriculum learning, and contrastive re-ranking. In addition, a new Incomplete-Trust (In-trust) loss function is proposed to replace the traditional cross-entropy loss when training. The experimental results demonstrate that the implementation of these ideas leads better performance than other state-of-the-art methods. All the experimental codes are released at: https:\/\/github.com\/WENGSYX\/Low-resource-text-translation.",
        "completion1":"Chinese-centric Neural Machine Translation Improves for Low-resource Languages",
        "completion2":"Monolingual Word Embeddings Data Enhancement, Bilingual Curriculum Learning, and Contrastive Re-ranking Lead to Better Performance",
        "completion3":"New Incomplete-Trust Loss Function Proven Effective",
        "technologyreview":0.232011779,
        "venturebeat":0.1855532458,
        "wired":0.0589332069,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.04344v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1649466337000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2205.15858v1",
        "predicted_newsworthiness":0.4578826142,
        "title":"Automatic Diagnosis of Schizophrenia and Attention Deficit Hyperactivity Disorder in rs-fMRI Modality using Convolutional Autoencoder Model and Interval Type-2 Fuzzy Regression",
        "summary":"Nowadays, many people worldwide suffer from brain disorders, and their health is in danger. So far, numerous methods have been proposed for the diagnosis of Schizophrenia (SZ) and attention deficit hyperactivity disorder (ADHD), among which functional magnetic resonance imaging (fMRI) modalities are known as a popular method among physicians. This paper presents an SZ and ADHD intelligent detection method of resting-state fMRI (rs-fMRI) modality using a new deep learning (DL) method. The University of California Los Angeles (UCLA) dataset, which contains the rs-fMRI modalities of SZ and ADHD patients, has been used for experiments. The FMRIB software library (FSL) toolbox first performed preprocessing on rs-fMRI data. Then, a convolutional Autoencoder (CNN-AE) model with the proposed number of layers is used to extract features from rs-fMRI data. In the classification step, a new fuzzy method called interval type-2 fuzzy regression (IT2FR) is introduced and then optimized by genetic algorithm (GA), particle swarm optimization (PSO), and gray wolf optimization (GWO) techniques. Also, the results of IT2FR methods are compared with multilayer perceptron (MLP), k-nearest neighbors (KNN), support vector machine (SVM), random forest (RF), decision tree (DT), and adaptive neuro-fuzzy inference system (ANFIS) methods. The experiment results show that the IT2FR method with the GWO optimization algorithm has achieved satisfactory results compared to other classifier methods. Finally, the proposed classification technique was able to provide 72.71% accuracy.",
        "completion1":"New AI method can automatically detect schizophrenia and ADHD with 72.71% accuracy",
        "completion2":"Using fMRI data, convolutional autoencoder model can detect brain disorders",
        "completion3":"University of California Los Angeles dataset used to train new deep learning model",
        "technologyreview":0.2240835541,
        "venturebeat":0.1815087976,
        "wired":0.0306115789,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.15858v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1654009649000,
        "code_mentioned":0,
        "readability":0.74
    },
    {
        "arxiv_id":"2201.02352v1",
        "predicted_newsworthiness":0.4251869658,
        "title":"Degrees of Freedom Analysis of Mechanisms using the New Zebra Crossing Method",
        "summary":"Mobility, which is a basic property for a mechanism has to be analyzed to find the degrees of freedom. A quick method for calculation of degrees of freedom in a mechanism is proposed in this work. The mechanism is represented in a way that resembles a zebra crossing. An algorithm is proposed which is used to determine the mobility from the zebra crossing diagram. This algorithm takes into account the number of patches between the black patches, the number of joints attached to the fixed link and the number of loops in the mechanism. A number of cases have been discussed which fail to give the desired results using the widely used classical Kutzbach-Grubler formula.",
        "completion1":"Quick and easy method for finding the degrees of freedom of a mechanism",
        "completion2":"New Zebra Crossing Method offers accurate results",
        "completion3":"The Kutzbach-Grubler formula fails to give the desired results in some cases",
        "technologyreview":0.1337787487,
        "venturebeat":0.0752333545,
        "wired":-0.0160571981,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.02352v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1641540906000,
        "code_mentioned":0,
        "readability":0.88
    },
    {
        "arxiv_id":"2207.09814v1",
        "predicted_newsworthiness":0.3776273393,
        "title":"NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis",
        "summary":"In this paper, we present NUWA-Infinity, a generative model for infinite visual synthesis, which is defined as the task of generating arbitrarily-sized high-resolution images or long-duration videos. An autoregressive over autoregressive generation mechanism is proposed to deal with this variable-size generation task, where a global patch-level autoregressive model considers the dependencies between patches, and a local token-level autoregressive model considers dependencies between visual tokens within each patch. A Nearby Context Pool (NCP) is introduced to cache-related patches already generated as the context for the current patch being generated, which can significantly save computation costs without sacrificing patch-level dependency modeling. An Arbitrary Direction Controller (ADC) is used to decide suitable generation orders for different visual synthesis tasks and learn order-aware positional embeddings. Compared to DALL-E, Imagen and Parti, NUWA-Infinity can generate high-resolution images with arbitrary sizes and support long-duration video generation additionally. Compared to NUWA, which also covers images and videos, NUWA-Infinity has superior visual synthesis capabilities in terms of resolution and variable-size generation. The GitHub link is https:\/\/github.com\/microsoft\/NUWA. The homepage link is https:\/\/nuwa-infinity.microsoft.com.",
        "completion1":"NUWA-Infinity: A Generative Model for Infinite Visual Synthesis",
        "completion2":"NUWA-Infinity Can Generate High-Resolution Images of Arbitrary Size",
        "completion3":"NUWA-Infinity Supports Long-Duration Video Generation",
        "technologyreview":0.2080555054,
        "venturebeat":0.1763822289,
        "wired":0.0535864141,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.09814v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658314555000,
        "code_mentioned":1,
        "readability":0.76
    },
    {
        "arxiv_id":"2201.01240v1",
        "predicted_newsworthiness":0.7284833255,
        "title":"Feedback and Engagement on an Introductory Programming Module",
        "summary":"We ran a study on engagement and achievement for a first year undergraduate programming module which used an online learning environment containing tasks which generate automated feedback. Students could also access human feedback from traditional labs. We gathered quantitative data on engagement and achievement which allowed us to split the cohort into 6 groups. We then ran interviews with students after the end of the module to produce qualitative data on perceptions of what feedback is, how useful it is, the uses made of it, and how it bears on engagement. A general finding was that human and automated feedback are different but complementary. However there are different feedback needs by group. Our findings imply: (1) that a blended human-automated feedback approach improves engagement; and (2) that this approach needs to be differentiated according to type of student. We give implications for the design of feedback for programming modules.",
        "completion1":"Introductory programming module uses online learning environment with automated feedback.",
        "completion2":"Study shows that human and automated feedback are different but complementary.",
        "completion3":"Differentiation of feedback needs according to type of student improves engagement.",
        "technologyreview":0.2540012556,
        "venturebeat":0.2250831867,
        "wired":0.0954889947,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.01240v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1641315189000,
        "code_mentioned":0,
        "readability":0.89
    },
    {
        "arxiv_id":"2207.03758v1",
        "predicted_newsworthiness":0.519334208,
        "title":"Virtual Axle Detector based on Analysis of Bridge Acceleration Measurements by Fully Convolutional Network",
        "summary":"In the practical application of the Bridge Weigh-In-Motion (BWIM) methods, the position of the wheels or axles during the passage of a vehicle is in most cases a prerequisite. To avoid the use of conventional axle detectors and bridge type specific methods, we propose a novel method for axle detection through the placement of accelerometers at any point of a bridge. In order to develop a model that is as simple and comprehensible as possible, the axle detection task is implemented as a binary classification problem instead of a regression problem. The model is implemented as a Fully Convolutional Network to process signals in the form of Continuous Wavelet Transforms. This allows passages of any length to be processed in a single step with maximum efficiency while utilising multiple scales in a single evaluation. This enables our method to use acceleration signals at any location of the bridge structure serving as Virtual Axle Detectors (VADs) without being limited to specific structural types of bridges. To test the proposed method, we analysed 3787 train passages recorded on a steel trough railway bridge of a long-distance traffic line. Our results on the measurement data show that our model detects 95% of the axes, thus, 128,599 of 134,800 previously unseen axles were correctly detected. In total, 90% of the axles can be detected with a maximum spatial error of 20cm, with a maximum velocity of $v_{\\mathrm{max}}=56,3~\\mathrm{m\/s}$. The analysis shows that our developed model can use accelerometers as VADs even under real operating conditions.",
        "completion1":"A new method for axle detection through the placement of accelerometers at any point of a bridge.",
        "completion2":"The axle detection task is implemented as a binary classification problem instead of a regression problem.",
        "completion3":"The model is implemented as a Fully Convolutional Network to process signals in the form of Continuous Wavelet Transforms.",
        "technologyreview":0.2070564881,
        "venturebeat":0.1640150447,
        "wired":0.0281535531,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.03758v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1657270864000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2205.07882v1",
        "predicted_newsworthiness":0.609970554,
        "title":"Aligning Robot Representations with Humans",
        "summary":"As robots are increasingly deployed in real-world scenarios, a key question is how to best transfer knowledge learned in one environment to another, where shifting constraints and human preferences render adaptation challenging. A central challenge remains that often, it is difficult (perhaps even impossible) to capture the full complexity of the deployment environment, and therefore the desired tasks, at training time. Consequently, the representation, or abstraction, of the tasks the human hopes for the robot to perform in one environment may be misaligned with the representation of the tasks that the robot has learned in another. We postulate that because humans will be the ultimate evaluator of system success in the world, they are best suited to communicating the aspects of the tasks that matter to the robot. Our key insight is that effective learning from human input requires first explicitly learning good intermediate representations and then using those representations for solving downstream tasks. We highlight three areas where we can use this approach to build interactive systems and offer future directions of work to better create advanced collaborative robots.",
        "completion1":"Robot Representations Must Be Aligned With Humans to Be Effective",
        "completion2":"Good Intermediate Representations are Key to Learning From Human Input",
        "completion3":"Creating Advanced Collaborative Robots Depends on Understanding Human Preferences",
        "technologyreview":0.3759095263,
        "venturebeat":0.2883611396,
        "wired":0.0844478467,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.07882v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ai",
            "cs.ro"
        ],
        "published":1652629865000,
        "code_mentioned":0,
        "readability":0.88
    },
    {
        "arxiv_id":"2201.08702v1",
        "predicted_newsworthiness":0.3539412562,
        "title":"Dual Contrastive Learning: Text Classification via Label-Aware Data Augmentation",
        "summary":"Contrastive learning has achieved remarkable success in representation learning via self-supervision in unsupervised settings. However, effectively adapting contrastive learning to supervised learning tasks remains as a challenge in practice. In this work, we introduce a dual contrastive learning (DualCL) framework that simultaneously learns the features of input samples and the parameters of classifiers in the same space. Specifically, DualCL regards the parameters of the classifiers as augmented samples associating to different labels and then exploits the contrastive learning between the input samples and the augmented samples. Empirical studies on five benchmark text classification datasets and their low-resource version demonstrate the improvement in classification accuracy and confirm the capability of learning discriminative representations of DualCL.",
        "completion1":"Dual Contrastive Learning: Text Classification via Label-Aware Data Augmentation",
        "completion2":"Contrastive learning has achieved remarkable success in representation learning via self-supervision in unsupervised settings",
        "completion3":"Effectively adapting contrastive learning to supervised learning tasks remains as a challenge in practice.",
        "technologyreview":0.228417073,
        "venturebeat":0.1986760813,
        "wired":0.053248437,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.08702v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai",
            "cs.lg"
        ],
        "published":1642773585000,
        "code_mentioned":1,
        "readability":0.75
    },
    {
        "arxiv_id":"2202.00846v1",
        "predicted_newsworthiness":0.4668496737,
        "title":"Adaptive Experimentation with Delayed Binary Feedback",
        "summary":"Conducting experiments with objectives that take significant delays to materialize (e.g. conversions, add-to-cart events, etc.) is challenging. Although the classical \"split sample testing\" is still valid for the delayed feedback, the experiment will take longer to complete, which also means spending more resources on worse-performing strategies due to their fixed allocation schedules. Alternatively, adaptive approaches such as \"multi-armed bandits\" are able to effectively reduce the cost of experimentation. But these methods generally cannot handle delayed objectives directly out of the box. This paper presents an adaptive experimentation solution tailored for delayed binary feedback objectives by estimating the real underlying objectives before they materialize and dynamically allocating variants based on the estimates. Experiments show that the proposed method is more efficient for delayed feedback compared to various other approaches and is robust in different settings. In addition, we describe an experimentation product powered by this algorithm. This product is currently deployed in the online experimentation platform of JD.com, a large e-commerce company and a publisher of digital ads.",
        "completion1":"Adaptive Experimentation with Delayed Binary Feedback",
        "completion2":"Split Sample Testing",
        "completion3":"Multi-Armed Bandits",
        "technologyreview":0.215270708,
        "venturebeat":0.2432709607,
        "wired":0.064439888,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.00846v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.lg"
        ],
        "published":1643766430000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2205.07816v1",
        "predicted_newsworthiness":0.5602036272,
        "title":"RAN Slicing: Towards Multi-Tenancy IN 5G Radio Access Networks",
        "summary":"A significant purpose of 5G networks is allowing sharing resources among different network tenants such as service providers and Mobile Virtual network Operators. Numerous domains are taken in account regarding resource sharing containing different infrastructure (storage, compute and networking), Radio Access Network (RAN) and Radio Frequency (RF) spectrum. RAN and spectrum, transport. Spectrum sharing and RAN are anticipated as the fundamental part in multi-tenant 5G network. Nevertheless, there is a shortage of evaluation platforms to determine the number of benefits that can be acquired from multilevel spectrum sharing rather than single-level spectrum sharing. The work presented in this paper intend to address this issue by presenting a modified SimuLTE model is used for evaluating active RAN based on multi-tenant 5G networks. The result shows an understanding into the actual advantages of RAN slicing for multi-tenants in 5G networks.",
        "completion1":"RAN Slicing: Towards Multi-Tenancy IN 5G Radio Access Networks",
        "completion2":"RAN Slicing: Achieving Multi-Tenancy IN 5G Radio Access Networks",
        "completion3":"RAN Slicing: Enhancing Multi-Tenancy IN 5G Radio Access Networks",
        "technologyreview":0.1758001959,
        "venturebeat":0.2061332648,
        "wired":0.0425128891,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.07816v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1652721276000,
        "code_mentioned":0,
        "readability":0.9
    },
    {
        "arxiv_id":"2201.10848v1",
        "predicted_newsworthiness":0.4474893399,
        "title":"Comparison of Depth Estimation Setups from Stereo Endoscopy and Optical Tracking for Point Measurements",
        "summary":"To support minimally-invasive intraoperative mitral valve repair, quantitative measurements from the valve can be obtained using an infra-red tracked stylus. It is desirable to view such manually measured points together with the endoscopic image for further assistance. Therefore, hand-eye calibration is required that links both coordinate systems and is a prerequisite to project the points onto the image plane. A complementary approach to this is to use a vision-based endoscopic stereo-setup to detect and triangulate points of interest, to obtain the 3D coordinates. In this paper, we aim to compare both approaches on a rigid phantom and two patient-individual silicone replica which resemble the intraoperative scenario. The preliminary results indicate that 3D landmark estimation, either labeled manually or through partly automated detection with a deep learning approach, provides more accurate triangulated depth measurements when performed with a tailored image-based method than with stylus measurements.",
        "completion1":"New study finds image-based depth estimation more accurate than stylus measurements",
        "completion2":"3D landmark estimation provides more accurate triangulated depth measurements",
        "completion3":"Comparison of Depth Estimation Setups from Stereo Endoscopy and Optical Tracking for Point Measurements",
        "technologyreview":0.2118144899,
        "venturebeat":0.1841137979,
        "wired":0.0207980718,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.10848v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1643192146000,
        "code_mentioned":0,
        "readability":0.78
    },
    {
        "arxiv_id":"2202.09868v1",
        "predicted_newsworthiness":0.6377494259,
        "title":"ExAIS: Executable AI Semantics",
        "summary":"Neural networks can be regarded as a new programming paradigm, i.e., instead of building ever-more complex programs through (often informal) logical reasoning in the programmers' mind, complex 'AI' systems are built by optimising generic neural network models with big data. In this new paradigm, AI frameworks such as TensorFlow and PyTorch play a key role, which is as essential as the compiler for traditional programs. It is known that the lack of a proper semantics for programming languages (such as C), i.e., a correctness specification for compilers, has contributed to many problematic program behaviours and security issues. While it is in general hard to have a correctness specification for compilers due to the high complexity of programming languages and their rapid evolution, we have a unique opportunity to do it right this time for neural networks (which have a limited set of functions, and most of them have stable semantics). In this work, we report our effort on providing a correctness specification of neural network frameworks such as TensorFlow. We specify the semantics of almost all TensorFlow layers in the logical programming language Prolog. We demonstrate the usefulness of the semantics through two applications. One is a fuzzing engine for TensorFlow, which features a strong oracle and a systematic way of generating valid neural networks. The other is a model validation approach which enables consistent bug reporting for TensorFlow models.",
        "completion1":"Executable AI Semantics\" Could Reduce Security Issues",
        "completion2":"Fuzzing engine for TensorFlow\" Could Find Unknown Bugs",
        "completion3":"Model validation approach\" could enable consistent bug reporting",
        "technologyreview":0.3113367568,
        "venturebeat":0.2647674428,
        "wired":0.0585653797,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.09868v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.lg"
        ],
        "published":1645378414000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2206.02056v1",
        "predicted_newsworthiness":0.563567739,
        "title":"Interpretable Models Capable of Handling Systematic Missingness in Imbalanced Classes and Heterogeneous Datasets",
        "summary":"Application of interpretable machine learning techniques on medical datasets facilitate early and fast diagnoses, along with getting deeper insight into the data. Furthermore, the transparency of these models increase trust among application domain experts. Medical datasets face common issues such as heterogeneous measurements, imbalanced classes with limited sample size, and missing data, which hinder the straightforward application of machine learning techniques. In this paper we present a family of prototype-based (PB) interpretable models which are capable of handling these issues. The models introduced in this contribution show comparable or superior performance to alternative techniques applicable in such situations. However, unlike ensemble based models, which have to compromise on easy interpretation, the PB models here do not. Moreover we propose a strategy of harnessing the power of ensembles while maintaining the intrinsic interpretability of the PB models, by averaging the model parameter manifolds. All the models were evaluated on a synthetic (publicly available dataset) in addition to detailed analyses of two real-world medical datasets (one publicly available). Results indicated that the models and strategies we introduced addressed the challenges of real-world medical data, while remaining computationally inexpensive and transparent, as well as similar or superior in performance compared to their alternatives.",
        "completion1":"New Interpretable Machine Learning Models Handle Missing Data in Imbalanced Classes",
        "completion2":"Prototype-Based Models offer Transparency and Accuracy for Medical Datasets",
        "completion3":"Averaging Model Parameter Manifolds Increases Efficiency of Ensemble Based Models",
        "technologyreview":0.2723450354,
        "venturebeat":0.2313176666,
        "wired":0.050898996,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.02056v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1654374039000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2203.05842v1",
        "predicted_newsworthiness":0.561671716,
        "title":"Multiple Inputs Neural Networks for Medicare fraud Detection",
        "summary":"Medicare fraud results in considerable losses for governments and insurance companies and results in higher premiums from clients. Medicare fraud costs around 13 billion euros in Europe and between 21 billion and 71 billion US dollars per year in the United States. This study aims to use artificial neural network based classifiers to predict medicare fraud. The main difficulty using machine learning techniques in fraud detection or more generally anomaly detection is that the data sets are highly imbalanced. To detect medicare frauds, we propose a multiple inputs deep neural network based classifier with a Long-short Term Memory (LSTM) autoencoder component. This architecture makes it possible to take into account many sources of data without mixing them and makes the classification task easier for the final model. The latent features extracted from the LSTM autoencoder have a strong discriminating power and separate the providers into homogeneous clusters. We use the data sets from the Centers for Medicaid and Medicare Services (CMS) of the US federal government. The CMS provides publicly available data that brings together all of the cost price requests sent by American hospitals to medicare companies. Our results show that although baseline artificial neural network give good performances, they are outperformed by our multiple inputs neural networks. We have shown that using a LSTM autoencoder to embed the provider behavior gives better results and makes the classifiers more robust to class imbalance.",
        "completion1":"Multiple Inputs Neural Networks for Medicare fraud Detection",
        "completion2":"Medicare fraud costs around 13 billion euros in Europe and between 21 billion and 71 billion US dollars per year in the United States",
        "completion3":"This study aims to use artificial neural network based classifiers to predict medicare fraud",
        "technologyreview":0.2892171818,
        "venturebeat":0.2698327421,
        "wired":0.0752673683,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.05842v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1646995013000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2201.07865v1",
        "predicted_newsworthiness":0.5075791271,
        "title":"Pipe Climbing Robot",
        "summary":"This paper presents the plan of an in-pipe climbing robot that works utilizing a novel Three-Output Open Differential(3-OOD) component to navigate complex organizations of lines. Customary wheeled\/followed in-pipe climbing robots are inclined to slip and haul while navigating in pipe twists. The 3-OOD component helps in accomplishing the original aftereffect of wiping out slip and drag in the robot tracks during movement. The proposed differential understands the practical capacities of the customary two-yield differential, which is accomplished the initial time for a differential with three results. The 3-OOD component precisely tweaks the track rates of the robot in light of the powers applied on each track inside the line organization, by wiping out the requirement for any dynamic control. The recreation of the robot crossing in the line network in various directions and in pipe-twists without slip shows the proposed plan's adequacy",
        "completion1":"Pipe-Climbing Robot Uses Innovative Three-Output Differential To Negate Slip And Drag.",
        "completion2":"New Pipe-Climbing Robot Eliminates Need For Active Control With Three-Output Differential.",
        "completion3":"3-Output Differential On Pipe-Climbing Robot prevents slip, drag in line networks.",
        "technologyreview":0.1430909452,
        "venturebeat":0.1068357851,
        "wired":0.0044314446,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.07865v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1642627850000,
        "code_mentioned":0,
        "readability":0.81
    },
    {
        "arxiv_id":"2207.08640v2",
        "predicted_newsworthiness":0.5694557021,
        "title":"Lightweight Automated Feature Monitoring for Data Streams",
        "summary":"Monitoring the behavior of automated real-time stream processing systems has become one of the most relevant problems in real world applications. Such systems have grown in complexity relying heavily on high dimensional input data, and data hungry Machine Learning (ML) algorithms. We propose a flexible system, Feature Monitoring (FM), that detects data drifts in such data sets, with a small and constant memory footprint and a small computational cost in streaming applications. The method is based on a multi-variate statistical test and is data driven by design (full reference distributions are estimated from the data). It monitors all features that are used by the system, while providing an interpretable features ranking whenever an alarm occurs (to aid in root cause analysis). The computational and memory lightness of the system results from the use of Exponential Moving Histograms. In our experimental study, we analyze the system's behavior with its parameters and, more importantly, show examples where it detects problems that are not directly related to a single feature. This illustrates how FM eliminates the need to add custom signals to detect specific types of problems and that monitoring the available space of features is often enough.",
        "completion1":"We propose a flexible system, Feature Monitoring , that detects data drifts in such data sets, with a small and constant memory footprint and a small computational cost in streaming applications.",
        "completion2":"The method is based on a multi-variate statistical test and is data driven by design (full reference distributions are estimated from the data).",
        "completion3":"It monitors all features that are used by the system, while providing an interpretable features ranking whenever an alarm occurs (to aid in root cause analysis).",
        "technologyreview":0.241324448,
        "venturebeat":0.2549536866,
        "wired":0.0851040539,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.08640v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658155091000,
        "code_mentioned":1,
        "readability":0.88
    },
    {
        "arxiv_id":"2207.07689v1",
        "predicted_newsworthiness":0.7046657206,
        "title":"Strict baselines for Covid-19 forecasting and ML perspective for USA and Russia",
        "summary":"Currently, the evolution of Covid-19 allows researchers to gather the datasets accumulated over 2 years and to use them in predictive analysis. In turn, this makes it possible to assess the efficiency potential of more complex predictive models, including neural networks with different forecast horizons. In this paper, we present the results of a consistent comparative study of different types of methods for predicting the dynamics of the spread of Covid-19 based on regional data for two countries: the United States and Russia. We used well-known statistical methods (e.g., Exponential Smoothing), a \"tomorrow-as-today\" approach, as well as a set of classic machine learning models trained on data from individual regions. Along with them, a neural network model based on Long short-term memory (LSTM) layers was considered, the training samples of which aggregate data from all regions of two countries: the United States and Russia. Efficiency evaluation was carried out using cross-validation according to the MAPE metric. It is shown that for complicated periods characterized by a large increase in the number of confirmed daily cases, the best results are shown by the LSTM model trained on all regions of both countries, showing an average Mean Absolute Percentage Error (MAPE) of 18%, 30%, 37% for Russia and 31%, 41%, 50% for US for predictions at forecast horizons of 14, 28, and 42 days, respectively.",
        "completion1":"ML perspective for USA and Russia's strict baselines for Covid-19 forecasting",
        "completion2":" tomorrow-as-today approach, as well as a set of classic machine learning models trained on data from individual regions",
        "completion3":"LSTM model trained on all regions of both countries, showing an average Mean Absolute Percentage Error  of 18%, 30%, 37% for Russia and 31%, 41%, 50% for US for predictions at forecast horizons of 14, 28, and 42 days, respectively.",
        "technologyreview":0.3248670759,
        "venturebeat":0.2361598819,
        "wired":0.0735619999,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.07689v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1657909296000,
        "code_mentioned":1,
        "readability":0.9
    },
    {
        "arxiv_id":"2204.13853v1",
        "predicted_newsworthiness":0.4445895813,
        "title":"Detecting Textual Adversarial Examples Based on Distributional Characteristics of Data Representations",
        "summary":"Although deep neural networks have achieved state-of-the-art performance in various machine learning tasks, adversarial examples, constructed by adding small non-random perturbations to correctly classified inputs, successfully fool highly expressive deep classifiers into incorrect predictions. Approaches to adversarial attacks in natural language tasks have boomed in the last five years using character-level, word-level, phrase-level, or sentence-level textual perturbations. While there is some work in NLP on defending against such attacks through proactive methods, like adversarial training, there is to our knowledge no effective general reactive approaches to defence via detection of textual adversarial examples such as is found in the image processing literature. In this paper, we propose two new reactive methods for NLP to fill this gap, which unlike the few limited application baselines from NLP are based entirely on distribution characteristics of learned representations: we adapt one from the image processing literature (Local Intrinsic Dimensionality (LID)), and propose a novel one (MultiDistance Representation Ensemble Method (MDRE)). Adapted LID and MDRE obtain state-of-the-art results on character-level, word-level, and phrase-level attacks on the IMDB dataset as well as on the later two with respect to the MultiNLI dataset. For future research, we publish our code.",
        "completion1":"Detecting Textual Adversarial Examples Based on Distributional Characteristics of Data Representations",
        "completion2":"Approaches to Adversarial Attacks in Natural Language Tasks",
        "completion3":" State-of-the-art performance in various machine learning tasks, adversarial examples",
        "technologyreview":0.313891318,
        "venturebeat":0.2454925754,
        "wired":0.086987404,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.13853v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1651199522000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2202.10826v1",
        "predicted_newsworthiness":0.3952156454,
        "title":"Relation Regularized Scene Graph Generation",
        "summary":"Scene graph generation (SGG) is built on top of detected objects to predict object pairwise visual relations for describing the image content abstraction. Existing works have revealed that if the links between objects are given as prior knowledge, the performance of SGG is significantly improved. Inspired by this observation, in this article, we propose a relation regularized network (R2-Net), which can predict whether there is a relationship between two objects and encode this relation into object feature refinement and better SGG. Specifically, we first construct an affinity matrix among detected objects to represent the probability of a relationship between two objects. Graph convolution networks (GCNs) over this relation affinity matrix are then used as object encoders, producing relation-regularized representations of objects. With these relation-regularized features, our R2-Net can effectively refine object labels and generate scene graphs. Extensive experiments are conducted on the visual genome dataset for three SGG tasks (i.e., predicate classification, scene graph classification, and scene graph detection), demonstrating the effectiveness of our proposed method. Ablation studies also verify the key roles of our proposed components in performance improvement.",
        "completion1":"R2-Net Proposes Relation Regularization for Scene Graph Generation",
        "completion2":"GCNs used as Object Encoders in R2-Net",
        "completion3":"Extensive Experiments Conducted on Visual Genome Dataset for SGG Tasks",
        "technologyreview":0.2329973117,
        "venturebeat":0.1707470371,
        "wired":0.0443167171,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.10826v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1645529809000,
        "code_mentioned":0,
        "readability":0.81
    },
    {
        "arxiv_id":"2206.06619v1",
        "predicted_newsworthiness":0.4878344306,
        "title":"TransVG++: End-to-End Visual Grounding with Language Conditioned Vision Transformer",
        "summary":"In this work, we explore neat yet effective Transformer-based frameworks for visual grounding. The previous methods generally address the core problem of visual grounding, i.e., multi-modal fusion and reasoning, with manually-designed mechanisms. Such heuristic designs are not only complicated but also make models easily overfit specific data distributions. To avoid this, we first propose TransVG, which establishes multi-modal correspondences by Transformers and localizes referred regions by directly regressing box coordinates. We empirically show that complicated fusion modules can be replaced by a simple stack of Transformer encoder layers with higher performance. However, the core fusion Transformer in TransVG is stand-alone against uni-modal encoders, and thus should be trained from scratch on limited visual grounding data, which makes it hard to be optimized and leads to sub-optimal performance. To this end, we further introduce TransVG++ to make two-fold improvements. For one thing, we upgrade our framework to a purely Transformer-based one by leveraging Vision Transformer (ViT) for vision feature encoding. For another, we devise Language Conditioned Vision Transformer that removes external fusion modules and reuses the uni-modal ViT for vision-language fusion at the intermediate layers. We conduct extensive experiments on five prevalent datasets, and report a series of state-of-the-art records.",
        "completion1":"TransVG++ outperforms previous transformer-based frameworks for visual grounding.",
        "completion2":"TransVG++ is a purely transformer-based framework that removes external fusion modules.",
        "completion3":"TransVG++ is language conditioned, making it easier to optimize and leading to better performance.",
        "technologyreview":0.2379036072,
        "venturebeat":0.1980792663,
        "wired":0.0600847922,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.06619v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1655188058000,
        "code_mentioned":1,
        "readability":0.76
    },
    {
        "arxiv_id":"2206.01627v1",
        "predicted_newsworthiness":0.4560944584,
        "title":"Pruning for Interpretable, Feature-Preserving Circuits in CNNs",
        "summary":"Deep convolutional neural networks are a powerful model class for a range of computer vision problems, but it is difficult to interpret the image filtering process they implement, given their sheer size. In this work, we introduce a method for extracting 'feature-preserving circuits' from deep CNNs, leveraging methods from saliency-based neural network pruning. These circuits are modular sub-functions, embedded within the network, containing only a subset of convolutional kernels relevant to a target feature. We compare the efficacy of 3 saliency-criteria for extracting these sparse circuits. Further, we show how 'sub-feature' circuits can be extracted, that preserve a feature's responses to particular images, dividing the feature into even sparser filtering processes. We also develop a tool for visualizing 'circuit diagrams', which render the entire image filtering process implemented by circuits in a parsable format.",
        "completion1":"Pruning for Interpretable, Feature-Preserving Circuits in CNNs",
        "completion2":"Saliency-based Neural Network Pruning",
        "completion3":"Sub-feature Circuits",
        "technologyreview":0.238312745,
        "venturebeat":0.1713159084,
        "wired":0.0594659246,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.01627v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1654269160000,
        "code_mentioned":1,
        "readability":0.79
    },
    {
        "arxiv_id":"2202.05148v1",
        "predicted_newsworthiness":0.4395502033,
        "title":"Identifying Weaknesses in Machine Translation Metrics Through Minimum Bayes Risk Decoding: A Case Study for COMET",
        "summary":"Neural metrics have achieved impressive correlation with human judgements in the evaluation of machine translation systems, but before we can safely optimise towards such metrics, we should be aware of (and ideally eliminate) biases towards bad translations that receive high scores. Our experiments show that sample-based Minimum Bayes Risk decoding can be used to explore and quantify such weaknesses. When applying this strategy to COMET for en-de and de-en, we find that COMET models are not sensitive enough to discrepancies in numbers and named entities. We further show that these biases cannot be fully removed by simply training on additional synthetic data.",
        "completion1":"Neural metrics not sufficient for machine translation evaluation, study finds",
        "completion2":"Study identifies weaknesses in current machine translation metrics",
        "completion3":"Sample-based Minimum Bayes Risk decoding can help explore and quantify biases in machine translation models",
        "technologyreview":0.2415221809,
        "venturebeat":0.2048982302,
        "wired":0.0678095827,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.05148v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1644512852000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2205.07403v4",
        "predicted_newsworthiness":0.4767314593,
        "title":"PillarNet: Real-Time and High-Performance Pillar-based 3D Object Detection",
        "summary":"Real-time and high-performance 3D object detection is of critical importance for autonomous driving. Recent top-performing 3D object detectors mainly rely on point-based or 3D voxel-based convolutions, which are both computationally inefficient for onboard deployment. In contrast, pillar-based methods use solely 2D convolutions, which consume less computation resources, but they lag far behind their voxel-based counterparts in detection accuracy. In this paper, by examining the primary performance gap between pillar- and voxel-based detectors, we develop a real-time and high-performance pillar-based detector, dubbed PillarNet. The proposed PillarNet consists of a powerful encoder network for effective pillar feature learning, a neck network for spatial-semantic feature fusion and the commonly used detect head. Using only 2D convolutions, PillarNet is flexible to an optional pillar size and compatible with classical 2D CNN backbones, such as VGGNet and ResNet.Additionally, PillarNet benefits from our designed orientation-decoupled IoU regression loss along with the IoU-aware prediction branch. Extensive experimental results on large-scale nuScenes Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet performs well over the state-of-the-art 3D detectors in terms of effectiveness and efficiency. The source code is available at https:\/\/github.com\/agent-sgs\/PillarNet.git.",
        "completion1":"Researchers Develop Real-Time and High-Performance Pillar-Based 3D Object Detection",
        "completion2":"New Detector Uses 2D Convolution to Improve Efficiency",
        "completion3":"PillarNet Outperforms State-of-the-Art 3D Detectors",
        "technologyreview":0.2394717143,
        "venturebeat":0.2156071431,
        "wired":0.0576025602,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.07403v4",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1652660090000,
        "code_mentioned":1,
        "readability":0.78
    },
    {
        "arxiv_id":"2202.05607v2",
        "predicted_newsworthiness":0.4445297574,
        "title":"Online Decision Transformer",
        "summary":"Recent work has shown that offline reinforcement learning (RL) can be formulated as a sequence modeling problem (Chen et al., 2021; Janner et al., 2021) and solved via approaches similar to large-scale language modeling. However, any practical instantiation of RL also involves an online component, where policies pretrained on passive offline datasets are finetuned via taskspecific interactions with the environment. We propose Online Decision Transformers (ODT), an RL algorithm based on sequence modeling that blends offline pretraining with online finetuning in a unified framework. Our framework uses sequence-level entropy regularizers in conjunction with autoregressive modeling objectives for sample-efficient exploration and finetuning. Empirically, we show that ODT is competitive with the state-of-the-art in absolute performance on the D4RL benchmark but shows much more significant gains during the finetuning procedure.",
        "completion1":"Online Decision Transformer: A new RL algorithm based on sequence modeling that blends offline pretraining with online finetuning in a unified framework.",
        "completion2":"Online Decision Transformer: A new RL algorithm that is competitive with the state-of-the-art in absolute performance on the D4RL benchmark.",
        "completion3":"Online Decision Transformer: A new RL algorithm that shows much more significant gains during the finetuning procedure than the state-of-the-art.",
        "technologyreview":0.2351106553,
        "venturebeat":0.2015594422,
        "wired":0.049109629,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.05607v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1644587004000,
        "code_mentioned":1,
        "readability":0.74
    },
    {
        "arxiv_id":"2201.03369v1",
        "predicted_newsworthiness":0.5081533923,
        "title":"QoS and Resource aware Security Orchestration System",
        "summary":"Network Function Virtualization (NFV) and Software Distributed Networking (SDN) technologies play a crucial role in enabling 5G system and beyond. A synergy between these both technologies has been identified for enabling a new concept dubbed service function chains (SFC) that aims to reduce both the capital expenditures (CAPEX) and operating expenses (OPEX). The SFC paradigm considers different constraints and key performance indicators (KPIs), that includes QoS and different resources, for enabling network slice services. However, the large-scale, complexity and security issues brought by these technologies create an extra overhead for ensuring secure network slicing. To cope with these challenges, this paper proposes a cost-efficient optimized SFC management system that enables the creation of SFCs for enabling efficient and secure network slices. The proposed system considers the network and computational resources and current network security levels to ensure trusted deployments. The simulation results demonstrated the efficiency of the proposed solution for achieving its designed objectives. The proposed solution efficiently manages the SFCs by optimizing deployment costs and reducing overall end-to-end delay",
        "completion1":"QoS and Resource aware Security Orchestration System to enable efficient and secure network slices.",
        "completion2":"The proposed system considers the network and computational resources and current network security levels to ensure trusted deployments.",
        "completion3":"The simulation results demonstrated the efficiency of the proposed solution for achieving its designed objectives.",
        "technologyreview":0.2189601858,
        "venturebeat":0.2756816067,
        "wired":0.0517368428,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.03369v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1641366706000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2201.04275v2",
        "predicted_newsworthiness":0.3917278101,
        "title":"PhysNLU: A Language Resource for Evaluating Natural Language Understanding and Explanation Coherence in Physics",
        "summary":"In order for language models to aid physics research, they must first encode representations of mathematical and natural language discourse which lead to coherent explanations, with correct ordering and relevance of statements. We present a collection of datasets developed to evaluate the performance of language models in this regard, which measure capabilities with respect to sentence ordering, position, section prediction, and discourse coherence. Analysis of the data reveals equations and sub-disciplines which are most common in physics discourse, as well as the sentence-level frequency of equations and expressions. We present baselines that demonstrate how contemporary language models are challenged by coherence related tasks in physics, even when trained on mathematical natural language objectives.",
        "completion1":"A language resource for evaluating natural language understanding and explanation coherence in physics has been developed.",
        "completion2":"The resource includes datasets that measure capabilities with respect to sentence ordering, position, section prediction, and discourse coherence.",
        "completion3":"Analysis of the data reveals equations and sub-disciplines which are most common in physics discourse, as well as the sentence-level frequency of equations and expressions.",
        "technologyreview":0.1909135865,
        "venturebeat":0.1388768243,
        "wired":0.0627295609,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.04275v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1641954760000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2204.09656v1",
        "predicted_newsworthiness":0.3995966637,
        "title":"A Fast Post-Training Pruning Framework for Transformers",
        "summary":"Pruning is an effective way to reduce the huge inference cost of large Transformer models. However, prior work on model pruning requires retraining the model. This can add high cost and complexity to model deployment, making it difficult to use in many practical situations. To address this, we propose a fast post-training pruning framework for Transformers that does not require any retraining. Given a resource constraint and a sample dataset, our framework automatically prunes the Transformer model using structured sparsity methods. To retain high accuracy without retraining, we introduce three novel techniques: (i) a lightweight mask search algorithm that finds which heads and filters to prune based on the Fisher information; (ii) mask rearrangement that complements the search algorithm; and (iii) mask tuning that reconstructs the output activations for each layer. We apply our method to BERT-BASE and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD benchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x speedup in inference latency, while maintaining < 1% loss in accuracy. Importantly, our framework prunes Transformers in less than 3 minutes on a single GPU, which is over two orders of magnitude faster than existing pruning approaches that retrain. Our code is publicly available.",
        "completion1":"A Fast Post-Training Pruning Framework for Transformers",
        "completion2":"Pruning is an effective way to reduce the huge inference cost of large Transformer models",
        "completion3":"To address this, we propose a fast post-training pruning framework for Transformers that does not require any retraining.",
        "technologyreview":0.2439292072,
        "venturebeat":0.1953884534,
        "wired":0.0530330062,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.09656v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1648539671000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2207.00182v1",
        "predicted_newsworthiness":0.336792188,
        "title":"Leveraging Monocular Disparity Estimation for Single-View Reconstruction",
        "summary":"We present a fine-tuning method to improve the appearance of 3D geometries reconstructed from single images. We leverage advances in monocular depth estimation to obtain disparity maps and present a novel approach to transforming 2D normalized disparity maps into 3D point clouds by solving an optimization on the relevant camera parameters, After creating a 3D point cloud from disparity, we introduce a method to combine the new point cloud with existing information to form a more faithful and detailed final geometry. We demonstrate the efficacy of our approach with multiple experiments on both synthetic and real images.",
        "completion1":"Fine-tuning method improves appearance of 3D geometries reconstructed from single images",
        "completion2":"Monocular depth estimation used to obtain disparity maps",
        "completion3":"New point cloud created from disparity",
        "technologyreview":0.1559516929,
        "venturebeat":0.1439063092,
        "wired":0.0418991846,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.00182v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1656644740000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2207.01674v1",
        "predicted_newsworthiness":0.5163439697,
        "title":"GazBy: Gaze-Based BERT Model to Incorporate Human Attention in Neural Information Retrieval",
        "summary":"This paper is interested in investigating whether human gaze signals can be leveraged to improve state-of-the-art search engine performance and how to incorporate this new input signal marked by human attention into existing neural retrieval models. In this paper, we propose GazBy ({\\bf Gaz}e-based {\\bf B}ert model for document relevanc{\\bf y}), a light-weight joint model that integrates human gaze fixation estimation into transformer models to predict document relevance, incorporating more nuanced information about cognitive processing into information retrieval (IR). We evaluate our model on the Text Retrieval Conference (TREC) Deep Learning (DL) 2019 and 2020 Tracks. Our experiments show encouraging results and illustrate the effective and ineffective entry points for using human gaze to help with transformer-based neural retrievers. With the rise of virtual reality (VR) and augmented reality (AR), human gaze data will become more available. We hope this work serves as a first step exploring using gaze signals in modern neural search engines.",
        "completion1":"GazBy: Gaze-Based BERT Model to Incorporate Human Attention in Neural Information Retrieval",
        "completion2":"Incorporating human attention into existing neural retrieval models",
        "completion3":"Using human gaze signals to help with transformer-based neural retrievers",
        "technologyreview":0.2654656506,
        "venturebeat":0.2408071984,
        "wired":0.0783603973,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.01674v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1656960648000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2207.01755v1",
        "predicted_newsworthiness":0.4256790301,
        "title":"Attention Guided Network for Salient Object Detection in Optical Remote Sensing Images",
        "summary":"Due to the extreme complexity of scale and shape as well as the uncertainty of the predicted location, salient object detection in optical remote sensing images (RSI-SOD) is a very difficult task. The existing SOD methods can satisfy the detection performance for natural scene images, but they are not well adapted to RSI-SOD due to the above-mentioned image characteristics in remote sensing images. In this paper, we propose a novel Attention Guided Network (AGNet) for SOD in optical RSIs, including position enhancement stage and detail refinement stage. Specifically, the position enhancement stage consists of a semantic attention module and a contextual attention module to accurately describe the approximate location of salient objects. The detail refinement stage uses the proposed self-refinement module to progressively refine the predicted results under the guidance of attention and reverse attention. In addition, the hybrid loss is applied to supervise the training of the network, which can improve the performance of the model from three perspectives of pixel, region and statistics. Extensive experiments on two popular benchmarks demonstrate that AGNet achieves competitive performance compared to other state-of-the-art methods. The code will be available at https:\/\/github.com\/NuaaYH\/AGNet.",
        "completion1":"A new method for detecting salient objects in optical remote sensing images is proposed.",
        "completion2":"The Attention Guided Network outperforms existing methods in detection performance.",
        "completion3":"The code for the Attention Guided Network will be available to the public.",
        "technologyreview":0.1863866617,
        "venturebeat":0.151882357,
        "wired":0.034848698,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.01755v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1656982863000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2203.05922v1",
        "predicted_newsworthiness":0.4515580057,
        "title":"Visualizing and Understanding Patch Interactions in Vision Transformer",
        "summary":"Vision Transformer (ViT) has become a leading tool in various computer vision tasks, owing to its unique self-attention mechanism that learns visual representations explicitly through cross-patch information interactions. Despite having good success, the literature seldom explores the explainability of vision transformer, and there is no clear picture of how the attention mechanism with respect to the correlation across comprehensive patches will impact the performance and what is the further potential. In this work, we propose a novel explainable visualization approach to analyze and interpret the crucial attention interactions among patches for vision transformer. Specifically, we first introduce a quantification indicator to measure the impact of patch interaction and verify such quantification on attention window design and indiscriminative patches removal. Then, we exploit the effective responsive field of each patch in ViT and devise a window-free transformer architecture accordingly. Extensive experiments on ImageNet demonstrate that the exquisitely designed quantitative method is shown able to facilitate ViT model learning, leading the top-1 accuracy by 4.28% at most. Moreover, the results on downstream fine-grained recognition tasks further validate the generalization of our proposal.",
        "completion1":"A new method for visualizing and understanding patch interactions in Vision Transformer.",
        "completion2":"This method could help improve the accuracy of ViT models by up to 4.28%.",
        "completion3":"The method is also effective for downstream fine-grained recognition tasks.",
        "technologyreview":0.2660473326,
        "venturebeat":0.1970690333,
        "wired":0.0682144718,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.05922v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1647006491000,
        "code_mentioned":1,
        "readability":0.79
    },
    {
        "arxiv_id":"2206.06427v2",
        "predicted_newsworthiness":0.4662438132,
        "title":"A Multi-purpose Real Haze Benchmark with Quantifiable Haze Levels and Ground Truth",
        "summary":"Imagery collected from outdoor visual environments is often degraded due to the presence of dense smoke or haze. A key challenge for research in scene understanding in these degraded visual environments (DVE) is the lack of representative benchmark datasets. These datasets are required to evaluate state-of-the-art object recognition and other computer vision algorithms in degraded settings. In this paper, we address some of these limitations by introducing the first paired real image benchmark dataset with hazy and haze-free images, and in-situ haze density measurements. This dataset was produced in a controlled environment with professional smoke generating machines that covered the entire scene, and consists of images captured from the perspective of both an unmanned aerial vehicle (UAV) and an unmanned ground vehicle (UGV). We also evaluate a set of representative state-of-the-art dehazing approaches as well as object detectors on the dataset. The full dataset presented in this paper, including the ground truth object classification bounding boxes and haze density measurements, is provided for the community to evaluate their algorithms at: https:\/\/a2i2-archangel.vision. A subset of this dataset has been used for the Object Detection in Haze Track of CVPR UG2 2022 challenge.",
        "completion1":"First-ever paired real image benchmark dataset with hazy and haze-free images released.",
        "completion2":"In-situ haze density measurements provide ground truth for object recognition in degraded settings.",
        "completion3":"State-of-the-art dehazing approaches evaluated on new dataset.",
        "technologyreview":0.2541811512,
        "venturebeat":0.2097965149,
        "wired":0.0681372869,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.06427v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1655147646000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2201.07422v1",
        "predicted_newsworthiness":0.3392923651,
        "title":"Self-Supervised Deep Blind Video Super-Resolution",
        "summary":"Existing deep learning-based video super-resolution (SR) methods usually depend on the supervised learning approach, where the training data is usually generated by the blurring operation with known or predefined kernels (e.g., Bicubic kernel) followed by a decimation operation. However, this does not hold for real applications as the degradation process is complex and cannot be approximated by these idea cases well. Moreover, obtaining high-resolution (HR) videos and the corresponding low-resolution (LR) ones in real-world scenarios is difficult. To overcome these problems, we propose a self-supervised learning method to solve the blind video SR problem, which simultaneously estimates blur kernels and HR videos from the LR videos. As directly using LR videos as supervision usually leads to trivial solutions, we develop a simple and effective method to generate auxiliary paired data from original LR videos according to the image formation of video SR, so that the networks can be better constrained by the generated paired data for both blur kernel estimation and latent HR video restoration. In addition, we introduce an optical flow estimation module to exploit the information from adjacent frames for HR video restoration. Experiments show that our method performs favorably against state-of-the-art ones on benchmarks and real-world videos.",
        "completion1":"Self-Supervised Deep Learning Method Developed for Blind Video Super-Resolution",
        "completion2":"New Method Overcomes hurdles for Real-World Video Super-Resolution",
        "completion3":"Optical Flow Estimation Module Aids in High-Resolution Video Restoration",
        "technologyreview":0.1510711534,
        "venturebeat":0.115867914,
        "wired":0.0178714749,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.07422v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1642569524000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2205.00476v2",
        "predicted_newsworthiness":0.3787491825,
        "title":"None Class Ranking Loss for Document-Level Relation Extraction",
        "summary":"Document-level relation extraction (RE) aims at extracting relations among entities expressed across multiple sentences, which can be viewed as a multi-label classification problem. In a typical document, most entity pairs do not express any pre-defined relation and are labeled as \"none\" or \"no relation\". For good document-level RE performance, it is crucial to distinguish such none class instances (entity pairs) from those of pre-defined classes (relations). However, most existing methods only estimate the probability of pre-defined relations independently without considering the probability of \"no relation\". This ignores the context of entity pairs and the label correlations between the none class and pre-defined classes, leading to sub-optimal predictions. To address this problem, we propose a new multi-label loss that encourages large margins of label confidence scores between each pre-defined class and the none class, which enables captured label correlations and context-dependent thresholding for label prediction. To gain further robustness against positive-negative imbalance and mislabeled data that could appear in real-world RE datasets, we propose a margin regularization and a margin shifting technique. Experimental results demonstrate that our method significantly outperforms existing multi-label losses for document-level RE and works well in other multi-label tasks such as emotion classification when none class instances are available for training.",
        "completion1":"New Multi-label Loss for Document-level Relation Extraction",
        "completion2":"None Class Ranking Loss for Document-Level Relation Extraction",
        "completion3":"Context-Dependent Thresholding for Label Prediction",
        "technologyreview":0.190909617,
        "venturebeat":0.1704229322,
        "wired":0.051089593,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.00476v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1651415077000,
        "code_mentioned":1,
        "readability":0.88
    },
    {
        "arxiv_id":"2203.14098v2",
        "predicted_newsworthiness":0.4158704033,
        "title":"Uncertainty-aware Contrastive Distillation for Incremental Semantic Segmentation",
        "summary":"A fundamental and challenging problem in deep learning is catastrophic forgetting, i.e. the tendency of neural networks to fail to preserve the knowledge acquired from old tasks when learning new tasks. This problem has been widely investigated in the research community and several Incremental Learning (IL) approaches have been proposed in the past years. While earlier works in computer vision have mostly focused on image classification and object detection, more recently some IL approaches for semantic segmentation have been introduced. These previous works showed that, despite its simplicity, knowledge distillation can be effectively employed to alleviate catastrophic forgetting. In this paper, we follow this research direction and, inspired by recent literature on contrastive learning, we propose a novel distillation framework, Uncertainty-aware Contrastive Distillation (\\method). In a nutshell, \\method~is operated by introducing a novel distillation loss that takes into account all the images in a mini-batch, enforcing similarity between features associated to all the pixels from the same classes, and pulling apart those corresponding to pixels from different classes. In order to mitigate catastrophic forgetting, we contrast features of the new model with features extracted by a frozen model learned at the previous incremental step. Our experimental results demonstrate the advantage of the proposed distillation technique, which can be used in synergy with previous IL approaches, and leads to state-of-art performance on three commonly adopted benchmarks for incremental semantic segmentation. The code is available at \\url{https:\/\/github.com\/ygjwd12345\/UCD}.",
        "completion1":"Uncertainty-aware Contrastive Distillation for Incremental Semantic Segmentation",
        "completion2":"A fundamental and challenging problem in deep learning is catastrophic forgetting, i.e. the tendency of neural networks to fail to preserve the knowledge acquired from old tasks when learning new tasks",
        "completion3":"More recently some IL approaches for semantic segmentation have been introduced, which showed that, despite its simplicity, knowledge distillation can be effectively employed to alleviate catastrophic forgetting",
        "technologyreview":0.1955504515,
        "venturebeat":0.1383443259,
        "wired":0.0247570083,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.14098v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1648308732000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2207.01514v1",
        "predicted_newsworthiness":0.7523215497,
        "title":"Improving Medical Systems in the United States using Knowledge-Based Systems",
        "summary":"America has one of the best medical systems in the world. The medical treatment care options offered by the medical system make it sophisticated. However, many American patients are not receiving health care on a regular basis, and at the same time, they cannot afford it. Also, the current medical system has many flaws such as high medical treatment costs and lack of doctors to accommodate many patients. This paper presents the principles of medical artificial intelligence called the knowledge based system. Doctors can remotely check and monitor their patients health data, medical history, how and what medical tests were done, and the lab results. The patients have access to detailed health information online and do not need to make an appointment with doctors to check their health on a daily basis. One doctor can check many patients simultaneously online (when medical centers are understaffed) and do not need to spend a lot of time with patients. Thus, doctors save more money for patients, because patients will no longer be transporting to medical centers to receive routine health check-ups. Patients do not need to overpay for their insurance because they will have access to the knowledge-based system, and the system will save the patients money to have their health checked and reduce the number of unnecessary medical exams. This paper undertakes a brief overview of research work done in a knowledge based system rule based expert systems in the field of medical practices.",
        "completion1":"Medical AI could save the United States healthcare system millions of dollars.",
        "completion2":"octors can now remotely monitor their patients' health data using knowledge-based systems.",
        "completion3":"his new type of medical system has many benefits for both doctors and patients.",
        "technologyreview":0.3111326369,
        "venturebeat":0.2796880979,
        "wired":0.0584245824,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.01514v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1654637478000,
        "code_mentioned":0,
        "readability":0.97
    },
    {
        "arxiv_id":"2206.04436v1",
        "predicted_newsworthiness":0.4877076011,
        "title":"Towards Safe Reinforcement Learning via Constraining Conditional Value-at-Risk",
        "summary":"Though deep reinforcement learning (DRL) has obtained substantial success, it may encounter catastrophic failures due to the intrinsic uncertainty of both transition and observation. Most of the existing methods for safe reinforcement learning can only handle transition disturbance or observation disturbance since these two kinds of disturbance affect different parts of the agent; besides, the popular worst-case return may lead to overly pessimistic policies. To address these issues, we first theoretically prove that the performance degradation under transition disturbance and observation disturbance depends on a novel metric of Value Function Range (VFR), which corresponds to the gap in the value function between the best state and the worst state. Based on the analysis, we adopt conditional value-at-risk (CVaR) as an assessment of risk and propose a novel reinforcement learning algorithm of CVaR-Proximal-Policy-Optimization (CPPO) which formalizes the risk-sensitive constrained optimization problem by keeping its CVaR under a given threshold. Experimental results show that CPPO achieves a higher cumulative reward and is more robust against both observation and transition disturbances on a series of continuous control tasks in MuJoCo.",
        "completion1":"Towards Safe Reinforcement Learning via Constraining Conditional Value-at-Risk",
        "completion2":"Novel reinforcement learning algorithm of CVaR-Proximal-Policy-Optimization",
        "completion3":"Experimental results show that CPPO achieves a higher cumulative reward and is more robust against both observation and transition disturbances on a series of continuous control tasks in MuJoCo.",
        "technologyreview":0.2337972601,
        "venturebeat":0.1767584571,
        "wired":0.0376320188,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.04436v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1654775874000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2202.08010v1",
        "predicted_newsworthiness":0.3708690988,
        "title":"360 Depth Estimation in the Wild -- The Depth360 Dataset and the SegFuse Network",
        "summary":"Single-view depth estimation from omnidirectional images has gained popularity with its wide range of applications such as autonomous driving and scene reconstruction. Although data-driven learning-based methods demonstrate significant potential in this field, scarce training data and ineffective 360 estimation algorithms are still two key limitations hindering accurate estimation across diverse domains. In this work, we first establish a large-scale dataset with varied settings called Depth360 to tackle the training data problem. This is achieved by exploring the use of a plenteous source of data, 360 videos from the internet, using a test-time training method that leverages unique information in each omnidirectional sequence. With novel geometric and temporal constraints, our method generates consistent and convincing depth samples to facilitate single-view estimation. We then propose an end-to-end two-branch multi-task learning network, SegFuse, that mimics the human eye to effectively learn from the dataset and estimate high-quality depth maps from diverse monocular RGB images. With a peripheral branch that uses equirectangular projection for depth estimation and a foveal branch that uses cubemap projection for semantic segmentation, our method predicts consistent global depth while maintaining sharp details at local regions. Experimental results show favorable performance against the state-of-the-art methods.",
        "completion1":"A new large-scale dataset called Depth360 aims to improve depth estimation from omnidirectional images.",
        "completion2":"The SegFuse network is an end-to-end two-branch multi-task learning network that can estimate high-quality depth maps from diverse monocular RGB images.",
        "completion3":"360 video data from the internet can be used to generate consistent and convincing depth samples for single-view estimation.",
        "technologyreview":0.2205719674,
        "venturebeat":0.1950799019,
        "wired":0.0556782927,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.08010v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1645012591000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2201.05918v2",
        "predicted_newsworthiness":0.4457669646,
        "title":"Recursive Least Squares Advantage Actor-Critic Algorithms",
        "summary":"As an important algorithm in deep reinforcement learning, advantage actor critic (A2C) has been widely succeeded in both discrete and continuous control tasks with raw pixel inputs, but its sample efficiency still needs to improve more. In traditional reinforcement learning, actor-critic algorithms generally use the recursive least squares (RLS) technology to update the parameter of linear function approximators for accelerating their convergence speed. However, A2C algorithms seldom use this technology to train deep neural networks (DNNs) for improving their sample efficiency. In this paper, we propose two novel RLS-based A2C algorithms and investigate their performance. Both proposed algorithms, called RLSSA2C and RLSNA2C, use the RLS method to train the critic network and the hidden layers of the actor network. The main difference between them is at the policy learning step. RLSSA2C uses an ordinary first-order gradient descent algorithm and the standard policy gradient to learn the policy parameter. RLSNA2C uses the Kronecker-factored approximation, the RLS method and the natural policy gradient to learn the compatible parameter and the policy parameter. In addition, we analyze the complexity and convergence of both algorithms, and present three tricks for further improving their convergence speed. Finally, we demonstrate the effectiveness of both algorithms on 40 games in the Atari 2600 environment and 11 tasks in the MuJoCo environment. From the experimental results, it is shown that our both algorithms have better sample efficiency than the vanilla A2C on most games or tasks, and have higher computational efficiency than other two state-of-the-art algorithms.",
        "completion1":"Two RLS-based A2C Algorithms Proven to be More Effective Than Vanilla A2C",
        "completion2":"RLSSA2C and RLSNA2C: New Algorithms for Accelerated Learning in RL",
        "completion3":"Sample Efficiency in Deep Reinforcement Learning Improved by New RLS-based A2C Algorithms",
        "technologyreview":0.2976975542,
        "venturebeat":0.2402831269,
        "wired":0.0582032373,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.05918v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1642276826000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2203.03608v2",
        "predicted_newsworthiness":0.699661607,
        "title":"Emotion Regulation and Dynamics of Moral Concerns During the Early COVID-19 Pandemic",
        "summary":"The COVID-19 pandemic has upended daily life around the globe, posing a threat to public health. Intuitively, we expect that surging cases and deaths would lead to fear, distress and other negative emotions. However, using state-of-the-art methods to measure sentiment, emotions, and moral concerns in social media messages posted in the early stage of the pandemic, we see a counter-intuitive rise in positive affect. We hypothesize that the increase of positivity is associated with a decrease of uncertainty and emotion regulation. Finally, we identify a partisan divide in moral and emotional reactions that emerged after the first US death. Overall, these results show how collective emotional states have changed since the pandemic began, and how social media can provide a useful tool to understand, and even regulate, diverse patterns underlying human affect.",
        "completion1":"Emotion Regulation During the Early COVID-19 Pandemic",
        "completion2":"The COVID-19 Pandemic: A Rise in Positive Affect",
        "completion3":"Partisan Divide in Moral and Emotional Reactions to COVID-19",
        "technologyreview":0.390013939,
        "venturebeat":0.2566372551,
        "wired":0.1374397561,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.03608v2",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.cy"
        ],
        "published":1646679510000,
        "code_mentioned":0,
        "readability":0.9
    },
    {
        "arxiv_id":"2203.05355v1",
        "predicted_newsworthiness":0.6827435905,
        "title":"SATLab at SemEval-2022 Task 4: Trying to Detect Patronizing and Condescending Language with only Character and Word N-grams",
        "summary":"A logistic regression model only fed with character and word n-grams is proposed for the SemEval-2022 Task 4 on Patronizing and Condescending Language Detection (PCL). It obtained an average level of performance, well above the performance of a system that tries to guess without using any knowledge about the task, but much lower than the best teams. As the proposed model is very similar to the one that performed well on a task requiring to automatically identify hate speech and offensive content, this paper confirms the difficulty of PCL detection.",
        "completion1":"SATLab's logistic regression model for detecting patronizing and condescending language falls short of best teams",
        "completion2":"Study: Most automated systems for detecting patronizing and condescending language are inadequate",
        "completion3":"SATLab's character and word n-gram model shows potential for identifying PCL",
        "technologyreview":0.2858139386,
        "venturebeat":0.2427309499,
        "wired":0.0738581425,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.05355v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1646917788000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2203.01225v1",
        "predicted_newsworthiness":0.4166166389,
        "title":"Video Question Answering: Datasets, Algorithms and Challenges",
        "summary":"Video Question Answering (VideoQA) aims to answer natural language questions according to the given videos. It has earned increasing attention with recent research trends in joint vision and language understanding. Yet, compared with ImageQA, VideoQA is largely underexplored and progresses slowly. Although different algorithms have continually been proposed and shown success on different VideoQA datasets, we find that there lacks a meaningful survey to categorize them, which seriously impedes its advancements. This paper thus provides a clear taxonomy and comprehensive analyses to VideoQA, focusing on the datasets, algorithms, and unique challenges. We then point out the research trend of studying beyond factoid QA to inference QA towards the cognition of video contents, Finally, we conclude some promising directions for future exploration.",
        "completion1":"Researchers studyingVideoQA find that it lags behind ImageQA in development and this paper provides a survey to help categorize algorithms and challenges in order to move forward.",
        "completion2":"The paper focuses on the datasets, algorithms, and unique challenges of VideoQA in order to provide a clear taxonomy and analysis.",
        "completion3":"Future research directions for VideoQA are concluded with an emphasis on cognition of video contents beyond factoid QA to inference QA.",
        "technologyreview":0.2258498766,
        "venturebeat":0.1881164862,
        "wired":0.0567546387,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.01225v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1646238849000,
        "code_mentioned":0,
        "readability":0.8
    },
    {
        "arxiv_id":"2207.01166v1",
        "predicted_newsworthiness":0.5158644077,
        "title":"Target-absent Human Attention",
        "summary":"The prediction of human gaze behavior is important for building human-computer interactive systems that can anticipate a user's attention. Computer vision models have been developed to predict the fixations made by people as they search for target objects. But what about when the image has no target? Equally important is to know how people search when they cannot find a target, and when they would stop searching. In this paper, we propose the first data-driven computational model that addresses the search-termination problem and predicts the scanpath of search fixations made by people searching for targets that do not appear in images. We model visual search as an imitation learning problem and represent the internal knowledge that the viewer acquires through fixations using a novel state representation that we call Foveated Feature Maps (FFMs). FFMs integrate a simulated foveated retina into a pretrained ConvNet that produces an in-network feature pyramid, all with minimal computational overhead. Our method integrates FFMs as the state representation in inverse reinforcement learning. Experimentally, we improve the state of the art in predicting human target-absent search behavior on the COCO-Search18 dataset",
        "completion1":"Target-Absent Human Attention: A New Computational Model",
        "completion2":"How to Search When You Can't Find a Target: New Model Predicts Human Behavior",
        "completion3":"Anticipating User Attention in Target-Absent Visual Searches: A Novel Approach",
        "technologyreview":0.2651369565,
        "venturebeat":0.2242897723,
        "wired":0.0554721368,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.01166v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1656901924000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2205.03168v2",
        "predicted_newsworthiness":0.4955510926,
        "title":"Defending against Reconstruction Attacks through Differentially Private Federated Learning for Classification of Heterogeneous Chest X-Ray Data",
        "summary":"Privacy regulations and the physical distribution of heterogeneous data are often primary concerns for the development of deep learning models in a medical context. This paper evaluates the feasibility of differentially private federated learning for chest X-ray classification as a defense against data privacy attacks. To the best of our knowledge, we are the first to directly compare the impact of differentially private training on two different neural network architectures, DenseNet121 and ResNet50. Extending the federated learning environments previously analyzed in terms of privacy, we simulated a heterogeneous and imbalanced federated setting by distributing images from the public CheXpert and Mendeley chest X-ray datasets unevenly among 36 clients. Both non-private baseline models achieved an area under the receiver operating characteristic curve (AUC) of $0.94$ on the binary classification task of detecting the presence of a medical finding. We demonstrate that both model architectures are vulnerable to privacy violation by applying image reconstruction attacks to local model updates from individual clients. The attack was particularly successful during later training stages. To mitigate the risk of privacy breach, we integrated R\\'enyi differential privacy with a Gaussian noise mechanism into local model training. We evaluate model performance and attack vulnerability for privacy budgets $\\epsilon \\in$ {1, 3, 6, 10}. The DenseNet121 achieved the best utility-privacy trade-off with an AUC of $0.94$ for $\\epsilon$ = 6. Model performance deteriorated slightly for individual clients compared to the non-private baseline. The ResNet50 only reached an AUC of $0.76$ in the same privacy setting. Its performance was inferior to that of the DenseNet121 for all considered privacy constraints, suggesting that the DenseNet121 architecture is more robust to differentially private training.",
        "completion1":"New study suggests DenseNet121 is more robust to differentially private training.",
        "completion2":"Implementation of differentially private federated learning for classification of heterogeneous chest X-ray data.",
        "completion3":"Evaluating the feasibility of differentially private federated learning for chest X-ray classification.",
        "technologyreview":0.2705779724,
        "venturebeat":0.2145638583,
        "wired":0.055126233,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.03168v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1651840324000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2203.05061v1",
        "predicted_newsworthiness":0.543261207,
        "title":"HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural Language Processing",
        "summary":"Deep learning algorithms are dependent on the availability of large-scale annotated clinical text datasets. The lack of such publicly available datasets is the biggest bottleneck for the development of clinical Natural Language Processing(NLP) systems. Zero-Shot Learning(ZSL) refers to the use of deep learning models to classify instances from new classes of which no training data have been seen before. Prompt-based learning is an emerging ZSL technique where we define task-based templates for NLP tasks. We developed a novel prompt-based clinical NLP framework called HealthPrompt and applied the paradigm of prompt-based learning on clinical texts. In this technique, rather than fine-tuning a Pre-trained Language Model(PLM), the task definitions are tuned by defining a prompt template. We performed an in-depth analysis of HealthPrompt on six different PLMs in a no-data setting. Our experiments prove that prompts effectively capture the context of clinical texts and perform remarkably well without any training data.",
        "completion1":"Zero-Shot Learning Paradigm for Clinical Natural Language Processing",
        "completion2":"Deep learning algorithms depend on availability of large-scale annotated clinical text datasets",
        "completion3":"Lack of such publicly available datasets is biggest bottleneck for development of clinical Natural Language Processing systems",
        "technologyreview":0.2633391083,
        "venturebeat":0.2303448616,
        "wired":0.0626905509,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.05061v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai",
            "cs.ir"
        ],
        "published":1646862268000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2206.05554v1",
        "predicted_newsworthiness":0.5181152419,
        "title":"Incremental Information Gain Mining Of Temporal Relational Streams",
        "summary":"This paper studies the problem of mining for data values with high information gain in relational tables. High information gain can help data analysts and secondary data mining algorithms gain insights into strong statistical dependencies and causality relationship between key metrics. In this paper, we will study the problem of high information gain identification for scenarios involving temporal relations where new records are added continuously to the relations. We show that information gain can be efficiently maintained in an incremental fashion, making it possible to monitor continuously high information gain values.",
        "completion1":"New Algorithm Achieves Nearly Real-Time Insights Into Key Metrics",
        "completion2":"Statistical Dependencies and Causality Can Be Monitored Continuously",
        "completion3":"Incremental Information Gain Mining Of Temporal Relational Streams: A Breakthrough In Data Analysis",
        "technologyreview":0.1755595643,
        "venturebeat":0.1858508028,
        "wired":0.0436437013,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.05554v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1654962888000,
        "code_mentioned":0,
        "readability":0.88
    },
    {
        "arxiv_id":"2201.10109v1",
        "predicted_newsworthiness":0.4256443256,
        "title":"Flexible Design on Deterministic IP Networking for Mixed Traffic Transmission",
        "summary":"Deterministic IP (DIP) networking is a promising technique that can provide delay-bounded transmission in large-scale networks. Nevertheless, DIP faces several challenges in the mixed traffic scenarios, including (i) the capability of ultra-low latency communications, (ii) the simultaneous satisfaction of diverse QoS requirements, and (iii) the network efficiency. The problems are more formidable in the dynamic surroundings without prior knowledge of traffic demands. To address the above-mentioned issues, this paper designs a flexible DIP (FDIP) network. In the proposed network, we classify the queues at the output port into multiple groups. Each group operates with different cycle lengths. FDIP can assign the time-sensitive flows with different groups, hence delivering diverse QoS requirements, simultaneously. The ultra-low latency communication can be achieved by specific groups with short cycle lengths. Moreover, the flexible scheduling with diverse cycle lengths improves resource utilization, hence increasing the throughput (i.e., the number of acceptable time-sensitive flows). We formulate a throughput maximization problem that jointly considers the admission control, transmission path selection, and cycle length assignment. A branch and bound (BnB)-based heuristic is developed. Simulation results show that the proposed FDIP significantly outperforms the standard DIP in terms of both the throughput and the latency guarantees.",
        "completion1":"New DIP network design promises better throughput and lower latency",
        "completion2":"Mixed traffic transmission gets boost from FDIP network design",
        "completion3":"promising new technique offers solutions to long-standing networking issues",
        "technologyreview":0.092435773,
        "venturebeat":0.1005391975,
        "wired":0.0025310379,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.10109v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1643089989000,
        "code_mentioned":0,
        "readability":0.81
    },
    {
        "arxiv_id":"2202.05385v2",
        "predicted_newsworthiness":0.5291290002,
        "title":"Cyclops: Open Platform for Scale Truck Platooning",
        "summary":"Cyclops, introduced in this paper, is an open research platform for everyone that wants to validate novel ideas and approaches in the area of self-driving heavy-duty vehicle platooning. The platform consists of multiple 1\/14 scale semi-trailer trucks, a scale proving ground, and associated computing, communication and control modules that enable self-driving on the proving ground. A perception system for each vehicle is composed of a lidar-based object tracking system and a lane detection\/control system. The former is to maintain the gap to the leading vehicle and the latter is to maintain the vehicle within the lane by steering control. The lane detection system is optimized for truck platooning where the field of view of the front-facing camera is severely limited due to a small gap to the leading vehicle. This platform is particularly amenable to validate mitigation strategies for safety-critical situations. Indeed, a simplex structure is adopted in the embedded module for testing various fail safe operations. We illustrate a scenario where camera sensor fails in the perception system but the vehicle operates at a reduced capacity to a graceful stop. Details of the Cyclops including 3D CAD designs and algorithm source codes are released for those who want to build similar testbeds.",
        "completion1":"Cyclops: Open Platform for Scale Truck Platooning",
        "completion2":"Cyclops Introduces Module for Testing Various Fail Safe Operations",
        "completion3":"Cyclops Releases Details of 3D CAD Designs and Algorithm Source Codes",
        "technologyreview":0.2786473345,
        "venturebeat":0.2216306704,
        "wired":0.0350077163,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.05385v2",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1644541291000,
        "code_mentioned":1,
        "readability":0.88
    },
    {
        "arxiv_id":"2202.05929v1",
        "predicted_newsworthiness":0.62420265,
        "title":"Improving Image-recognition Edge Caches with a Generative Adversarial Network",
        "summary":"Image recognition is an essential task in several mobile applications. For instance, a smartphone can process a landmark photo to gather more information about its location. If the device does not have enough computational resources available, it offloads the processing task to a cloud infrastructure. Although this approach solves resource shortages, it introduces a communication delay. Image-recognition caches on the Internet's edge can mitigate this problem. These caches run on servers close to mobile devices and stores information about previously recognized images. If the server receives a request with a photo stored in its cache, it replies to the device, avoiding cloud offloading. The main challenge for this cache is to verify if the received image matches a stored one. Furthermore, for outdoor photos, it is difficult to compare them if one was taken in the daytime and the other at nighttime. In that case, the cache might wrongly infer that they refer to different places, offloading the processing to the cloud. This work shows that a well-known generative adversarial network, called ToDayGAN, can solve this problem by generating daytime images using nighttime ones. We can thus use this translation to populate a cache with synthetic photos that can help image matching. We show that our solution reduces cloud offloading and, therefore, the application's latency.",
        "completion1":"Image-recognition caches on the Internet's edge can mitigate this problem",
        "completion2":"his work shows that a well-known generative adversarial network, called ToDayGAN, can solve this problem by generating daytime images using nighttime ones",
        "completion3":"e show that our solution reduces cloud offloading and, therefore, the application's latency",
        "technologyreview":0.3022013327,
        "venturebeat":0.2592572334,
        "wired":0.0896600282,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.05929v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni",
            "cs.lg"
        ],
        "published":1644621125000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2207.08046v3",
        "predicted_newsworthiness":0.4953539162,
        "title":"MDM:Visual Explanations for Neural Networks via Multiple Dynamic Mask",
        "summary":"The Class Activation Maps(CAM) lookup of a neural network can tell us what regions the neural network is focusing on when making a decision.We propose an algorithm Multiple Dynamic Mask (MDM), which is a general saliency graph query method with interpretability of inference process. The algorithm is based on an assumption: when a picture is input into a trained neural network, only the activation features related to classification will affect the classification results of the neural network, and the features unrelated to classification will hardly affect the classification results of the network. MDM: A learning-based end-to-end algorithm for finding regions of interest for neural network classification.It has the following advantages: 1. It has the interpretability of the reasoning process, and the reasoning process conforms to human cognition. 2. It is universal, it can be used for any neural network and does not depend on the internal structure of the neural network. 3. The search performance is better. The algorithm is based on learning and has the ability to adapt to different data and networks. The performance is better than the method proposed in the previous paper. For the MDM saliency map search algorithm, we experimentally compared ResNet and DenseNet as the trained neural network. The recent advanced saliency map search method and the results of MDM on the performance indicators of each search effect item, the performance of MDM has reached the state of the art. We applied the MDM method to the interpretable neural network ProtoPNet and XProtoNet, which improved the model's interpretability prototype search performance. And we visualize the effect of convolutional neural architecture and Transformer architecture in saliency map search, illustrating the interpretability and generality of MDM.",
        "completion1":"Explanations for Neural Networks: MDM's Multiple Dynamic Mask",
        "completion2":"A learning-based end-to-end algorithm for finding regions of interest: MDM",
        "completion3":"The performance of MDM compared to ResNet and DenseNet",
        "technologyreview":0.2994700883,
        "venturebeat":0.2343731119,
        "wired":0.063493422,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.08046v3",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658017516000,
        "code_mentioned":0,
        "readability":0.8
    },
    {
        "arxiv_id":"2204.07299v1",
        "predicted_newsworthiness":0.5376105022,
        "title":"Where to Go for the Holidays: Towards Mixed-Type Dialogs for Clarification of User Goals",
        "summary":"Most dialog systems posit that users have figured out clear and specific goals before starting an interaction. For example, users have determined the departure, the destination, and the travel time for booking a flight. However, in many scenarios, limited by experience and knowledge, users may know what they need, but still struggle to figure out clear and specific goals by determining all the necessary slots. In this paper, we identify this challenge and make a step forward by collecting a new human-to-human mixed-type dialog corpus. It contains 5k dialog sessions and 168k utterances for 4 dialog types and 5 domains. Within each session, an agent first provides user-goal-related knowledge to help figure out clear and specific goals, and then help achieve them. Furthermore, we propose a mixed-type dialog model with a novel Prompt-based continual learning mechanism. Specifically, the mechanism enables the model to continually strengthen its ability on any specific type by utilizing existing dialog corpora effectively.",
        "completion1":"New mixed-type dialog corpus helps users figure out clear and specific goals.",
        "completion2":"Prompt-based continual learning mechanism strengthens ability to utilize existing dialog corpora effectively.",
        "completion3":"Mixed-type dialog model proposed for goal clarification in dialogue systems.",
        "technologyreview":0.270662366,
        "venturebeat":0.2647985899,
        "wired":0.0792199053,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.07299v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1649989845000,
        "code_mentioned":1,
        "readability":0.89
    },
    {
        "arxiv_id":"2203.07171v2",
        "predicted_newsworthiness":0.5227803468,
        "title":"Orchestrated Value Mapping for Reinforcement Learning",
        "summary":"We present a general convergent class of reinforcement learning algorithms that is founded on two distinct principles: (1) mapping value estimates to a different space using arbitrary functions from a broad class, and (2) linearly decomposing the reward signal into multiple channels. The first principle enables incorporating specific properties into the value estimator that can enhance learning. The second principle, on the other hand, allows for the value function to be represented as a composition of multiple utility functions. This can be leveraged for various purposes, e.g. dealing with highly varying reward scales, incorporating a priori knowledge about the sources of reward, and ensemble learning. Combining the two principles yields a general blueprint for instantiating convergent algorithms by orchestrating diverse mapping functions over multiple reward channels. This blueprint generalizes and subsumes algorithms such as Q-Learning, Log Q-Learning, and Q-Decomposition. In addition, our convergence proof for this general class relaxes certain required assumptions in some of these algorithms. Based on our theory, we discuss several interesting configurations as special cases. Finally, to illustrate the potential of the design space that our theory opens up, we instantiate a particular algorithm and evaluate its performance on the Atari suite.",
        "completion1":"Orchestrated Value Mapping for Reinforcement Learning Could Enhance Learning",
        "completion2":"New Algorithm Could Help Machines Learn Better and Faster",
        "completion3":" researchers Present a General Class of Reinforcement Learning Algorithms",
        "technologyreview":0.2754495838,
        "venturebeat":0.234246522,
        "wired":0.0639703936,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.07171v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1647270824000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2205.07545v1",
        "predicted_newsworthiness":0.543146721,
        "title":"Heri-Graphs: A Workflow of Creating Datasets for Multi-modal Machine Learning on Graphs of Heritage Values and Attributes with Social Media",
        "summary":"Values (why to conserve) and Attributes (what to conserve) are essential concepts of cultural heritage. Recent studies have been using social media to map values and attributes conveyed by public to cultural heritage. However, it is rare to connect heterogeneous modalities of images, texts, geo-locations, timestamps, and social network structures to mine the semantic and structural characteristics therein. This study presents a methodological workflow for constructing such multi-modal datasets using posts and images on Flickr for graph-based machine learning (ML) tasks concerning heritage values and attributes. After data pre-processing using state-of-the-art ML models, the multi-modal information of visual contents and textual semantics are modelled as node features and labels, while their social relationships and spatiotemporal contexts are modelled as links in Multi-Graphs. The workflow is tested in three cities containing UNESCO World Heritage properties - Amsterdam, Suzhou, and Venice, which yielded datasets with high consistency for semi-supervised learning tasks. The entire process is formally described with mathematical notations, ready to be applied in provisional tasks both as ML problems with technical relevance and as urban\/heritage study questions with societal interests. This study could also benefit the understanding and mapping of heritage values and attributes for future research in global cases, aiming at inclusive heritage management practices.",
        "completion1":"Heri-Graphs: A Workflow of Creating Datasets for Multi-modal Machine Learning on Graphs of Heritage Values and Attributes with Social Media",
        "completion2":"Study Presents Methodological Workflow for Constructing Multi-Modal Datasets Using Posts and Images on Flickr",
        "completion3":"Workflow Yields High Consistency Datasets for Semi-Supervised Learning Tasks in Three Test Cities",
        "technologyreview":0.290284233,
        "venturebeat":0.2457514198,
        "wired":0.0996140083,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.07545v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.cy"
        ],
        "published":1652694345000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2203.03226v2",
        "predicted_newsworthiness":0.4138925805,
        "title":"Signature and Log-signature for the Study of Empirical Distributions Generated with GANs",
        "summary":"In this paper, we develop a new and systematic method to explore and analyze samples taken by NASA Perseverance on the surface of the planet Mars. A novel in this context PCA adaptive t-SNE is proposed, as well as the introduction of statistical measures to study the goodness of fit of the sample distribution. We go beyond visualization by generating synthetic imagery using Stylegan2-ADA that resemble the original terrain distribution. We also conduct synthetic image generation using the recently introduced Scored-based Generative Modeling. We bring forward the use of the recently developed Signature Transform as a way to measure the similarity between image distributions and provide detailed acquaintance and extensive evaluations. We are the first to pioneer RMSE and MAE Signature and log-signature as an alternative to measure GAN convergence. Insights on state-of-the-art instance segmentation of the samples by the use of a model DeepLabv3 are also given.",
        "completion1":"Pioneering the use of RMSE and MAE Signature and log-signature to measure GAN convergence",
        "completion2":"Insights on state-of-the-art instance segmentation of the samples by the use of a model DeepLabv3",
        "completion3":"A novel in this context PCA adaptive t-SNE is proposed, as well as the introduction of statistical measures to study the goodness of fit of the sample distribution",
        "technologyreview":0.2276158489,
        "venturebeat":0.1676402685,
        "wired":0.0483087784,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.03226v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1646645167000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2203.04886v1",
        "predicted_newsworthiness":0.4765194931,
        "title":"Reverse Engineering $\\ell_p$ attacks: A block-sparse optimization approach with recovery guarantees",
        "summary":"Deep neural network-based classifiers have been shown to be vulnerable to imperceptible perturbations to their input, such as $\\ell_p$-bounded norm adversarial attacks. This has motivated the development of many defense methods, which are then broken by new attacks, and so on. This paper focuses on a different but related problem of reverse engineering adversarial attacks. Specifically, given an attacked signal, we study conditions under which one can determine the type of attack ($\\ell_1$, $\\ell_2$ or $\\ell_\\infty$) and recover the clean signal. We pose this problem as a block-sparse recovery problem, where both the signal and the attack are assumed to lie in a union of subspaces that includes one subspace per class and one subspace per attack type. We derive geometric conditions on the subspaces under which any attacked signal can be decomposed as the sum of a clean signal plus an attack. In addition, by determining the subspaces that contain the signal and the attack, we can also classify the signal and determine the attack type. Experiments on digit and face classification demonstrate the effectiveness of the proposed approach.",
        "completion1":"Researchers Develop AI that Can Reverse Engineer Adversarial Attacks",
        "completion2":"AI Can Now Classify and Determine the Type of Adversarial Attack",
        "completion3":"New Approach to Adversarial Attacks Shows Promise",
        "technologyreview":0.164970602,
        "venturebeat":0.1060859737,
        "wired":0.0220934716,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.04886v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1646845039000,
        "code_mentioned":0,
        "readability":0.9
    },
    {
        "arxiv_id":"2207.01614v1",
        "predicted_newsworthiness":0.4128885439,
        "title":"Beyond mAP: Re-evaluating and Improving Performance in Instance Segmentation with Semantic Sorting and Contrastive Flow",
        "summary":"Top-down instance segmentation methods improve mAP by hedging bets on low-confidence predictions to match a ground truth. Moreover, the query-key paradigm of top-down methods leads to the instance merging problem. An excessive number of duplicate predictions leads to the (over)counting error, and the independence of category and localization branches leads to the naming error. The de-facto mAP metric doesn't capture these errors, as we show that a trivial dithering scheme can simultaneously increase mAP with hedging errors. To this end, we propose two graph-based metrics that quantifies the amount of hedging both inter-and intra-class. We conjecture the source of the hedging problem is due to feature merging and propose a) Contrastive Flow Field to encode contextual differences between instances as a supervisory signal, and b) Semantic Sorting and NMS step to suppress duplicates and incorrectly categorized prediction. Ablations show that our method encodes contextual information better than baselines, and experiments on COCO our method simultaneously reduces merging and hedging errors compared to state-of-the-art instance segmentation methods.",
        "completion1":" Beyond mAP: Re-evaluating and Improving Performance in Instance Segmentation with Semantic Sorting and Contrastive Flow",
        "completion2":" Top-down instance segmentation methods improve mAP by hedging bets on low-confidence predictions to match a ground truth",
        "completion3":" The de-facto mAP metric doesn't capture these errors, as we show that a trivial dithering scheme can simultaneously increase mAP with hedging errors.",
        "technologyreview":0.208336963,
        "venturebeat":0.173095029,
        "wired":0.0444267551,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.01614v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1656957374000,
        "code_mentioned":0,
        "readability":0.8
    },
    {
        "arxiv_id":"2204.13663v3",
        "predicted_newsworthiness":0.6291060017,
        "title":"ADVISER: AI-Driven Vaccination Intervention Optimiser for Increasing Vaccine Uptake in Nigeria",
        "summary":"More than 5 million children under five years die from largely preventable or treatable medical conditions every year, with an overwhelmingly large proportion of deaths occurring in under-developed countries with low vaccination uptake. One of the United Nations' sustainable development goals (SDG 3) aims to end preventable deaths of newborns and children under five years of age. We focus on Nigeria, where the rate of infant mortality is appalling. We collaborate with HelpMum, a large non-profit organization in Nigeria to design and optimize the allocation of heterogeneous health interventions under uncertainty to increase vaccination uptake, the first such collaboration in Nigeria. Our framework, ADVISER: AI-Driven Vaccination Intervention Optimiser, is based on an integer linear program that seeks to maximize the cumulative probability of successful vaccination. Our optimization formulation is intractable in practice. We present a heuristic approach that enables us to solve the problem for real-world use-cases. We also present theoretical bounds for the heuristic method. Finally, we show that the proposed approach outperforms baseline methods in terms of vaccination uptake through experimental evaluation. HelpMum is currently planning a pilot program based on our approach to be deployed in the largest city of Nigeria, which would be the first deployment of an AI-driven vaccination uptake program in the country and hopefully, pave the way for other data-driven programs to improve health outcomes in Nigeria.",
        "completion1":"ADVISER: AI-Driven Vaccination Intervention Optimiser for Increasing Vaccine Uptake in Nigeria",
        "completion2":"More than 5 million children under five years die from largely preventable or treatable medical conditions every year",
        "completion3":"UN sustainable development goal (SDG 3) aims to end preventable deaths of newborns and children under five years of age",
        "technologyreview":0.3489676332,
        "venturebeat":0.2778581839,
        "wired":0.0814342939,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.13663v3",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.cy"
        ],
        "published":1651166757000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2205.01480v1",
        "predicted_newsworthiness":0.4710540335,
        "title":"Residual Graph Convolutional Recurrent Networks For Multi-step Traffic Flow Forecasting",
        "summary":"Traffic flow forecasting is essential for traffic planning, control and management. The main challenge of traffic forecasting tasks is accurately capturing traffic networks' spatial and temporal correlation. Although there are many traffic forecasting methods, most of them still have limitations in capturing spatial and temporal correlations. To improve traffic forecasting accuracy, we propose a new Spatial-temporal forecasting model, namely the Residual Graph Convolutional Recurrent Network (RGCRN). The model uses our proposed Residual Graph Convolutional Network (ResGCN) to capture the fine-grained spatial correlation of the traffic road network and then uses a Bi-directional Gated Recurrent Unit (BiGRU) to model time series with spatial information and obtains the temporal correlation by analysing the change in information transfer between the forward and reverse neurons of the time series data. Our comparative experimental results on two real datasets show that RGCRN improves on average by 20.66% compared to the best baseline model. You can get our source code and data through https:\/\/github.com\/zhangshqii\/RGCRN.",
        "completion1":"Residual Graph Convolutional Recurrent Networks (ResGCN) improve traffic forecasting accuracy by 20.66%.",
        "completion2":"ResGCN captures fine-grained spatial correlation of traffic road network.",
        "completion3":"Bi-directional Gated Recurrent Unit (BiGRU) models time series with spatial information.",
        "technologyreview":0.2268218664,
        "venturebeat":0.1951499268,
        "wired":0.0597048785,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.01480v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1651584218000,
        "code_mentioned":1,
        "readability":0.78
    },
    {
        "arxiv_id":"2201.06386v1",
        "predicted_newsworthiness":0.6230984824,
        "title":"Visual Identification of Problematic Bias in Large Label Spaces",
        "summary":"While the need for well-trained, fair ML systems is increasing ever more, measuring fairness for modern models and datasets is becoming increasingly difficult as they grow at an unprecedented pace. One key challenge in scaling common fairness metrics to such models and datasets is the requirement of exhaustive ground truth labeling, which cannot always be done. Indeed, this often rules out the application of traditional analysis metrics and systems. At the same time, ML-fairness assessments cannot be made algorithmically, as fairness is a highly subjective matter. Thus, domain experts need to be able to extract and reason about bias throughout models and datasets to make informed decisions. While visual analysis tools are of great help when investigating potential bias in DL models, none of the existing approaches have been designed for the specific tasks and challenges that arise in large label spaces. Addressing the lack of visualization work in this area, we propose guidelines for designing visualizations for such large label spaces, considering both technical and ethical issues. Our proposed visualization approach can be integrated into classical model and data pipelines, and we provide an implementation of our techniques open-sourced as a TensorBoard plug-in. With our approach, different models and datasets for large label spaces can be systematically and visually analyzed and compared to make informed fairness assessments tackling problematic bias.",
        "completion1":"Researchers Develop Visual Identification Tool to Measure Bias in AI Systems",
        "completion2":"Tool Helps Domain Experts Detect and Reason About Bias in Machine Learning Models",
        "completion3":"TensorBoard Plugin Aids in Fairness Assessment of Large-Scale AI Systems",
        "technologyreview":0.3358961961,
        "venturebeat":0.2726055759,
        "wired":0.1083073507,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.06386v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.hc"
        ],
        "published":1642423868000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2204.07335v1",
        "predicted_newsworthiness":0.4582744195,
        "title":"A Keypoint-based Global Association Network for Lane Detection",
        "summary":"Lane detection is a challenging task that requires predicting complex topology shapes of lane lines and distinguishing different types of lanes simultaneously. Earlier works follow a top-down roadmap to regress predefined anchors into various shapes of lane lines, which lacks enough flexibility to fit complex shapes of lanes due to the fixed anchor shapes. Lately, some works propose to formulate lane detection as a keypoint estimation problem to describe the shapes of lane lines more flexibly and gradually group adjacent keypoints belonging to the same lane line in a point-by-point manner, which is inefficient and time-consuming during postprocessing. In this paper, we propose a Global Association Network (GANet) to formulate the lane detection problem from a new perspective, where each keypoint is directly regressed to the starting point of the lane line instead of point-by-point extension. Concretely, the association of keypoints to their belonged lane line is conducted by predicting their offsets to the corresponding starting points of lanes globally without dependence on each other, which could be done in parallel to greatly improve efficiency. In addition, we further propose a Lane-aware Feature Aggregator (LFA), which adaptively captures the local correlations between adjacent keypoints to supplement local information to the global association. Extensive experiments on two popular lane detection benchmarks show that our method outperforms previous methods with F1 score of 79.63% on CULane and 97.71% on Tusimple dataset with high FPS. The code will be released at https:\/\/github.com\/Wolfwjs\/GANet.",
        "completion1":"A new Global Association Network for Lane Detection is proposed which is more efficient and accurate than previous methods.",
        "completion2":"The new Global Association Network for Lane Detection outperforms previous methods with F1 score of 79.63% on CULane and 97.71% on Tusimple dataset with high FPS.",
        "completion3":"Code for the new Global Association Network for Lane Detection will be released to the public.",
        "technologyreview":0.2187175513,
        "venturebeat":0.1716548593,
        "wired":0.0351686687,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.07335v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1650000244000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2204.10297v1",
        "predicted_newsworthiness":0.5243340257,
        "title":"Learning to Fold Real Garments with One Arm: A Case Study in Cloud-Based Robotics Research",
        "summary":"Autonomous fabric manipulation is a longstanding challenge in robotics, but evaluating progress is difficult due to the cost and diversity of robot hardware. Using Reach, a cloud robotics platform that enables low-latency remote execution of control policies on physical robots, we present the first systematic benchmarking of fabric manipulation algorithms on physical hardware. We develop 4 novel learning-based algorithms that model expert actions, keypoints, reward functions, and dynamic motions, and we compare these against 4 learning-free and inverse dynamics algorithms on the task of folding a crumpled T-shirt with a single robot arm. The entire lifecycle of data collection, model training, and policy evaluation is performed remotely without physical access to the robot workcell. Results suggest a new algorithm combining imitation learning with analytic methods achieves 84% of human-level performance on the folding task. See https:\/\/sites.google.com\/berkeley.edu\/cloudfolding for all data, code, models, and supplemental material.",
        "completion1":"Robots learn to fold clothes, just like humans",
        "completion2":"New algorithm enables robots to fold clothes as well as humans",
        "completion3":"Imitation learning combined with analytic methods achieves 84% of human-level performance on the folding task",
        "technologyreview":0.3189809689,
        "venturebeat":0.2615245157,
        "wired":0.0686842286,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.10297v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai"
        ],
        "published":1650562280000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2205.02803v2",
        "predicted_newsworthiness":0.5134707325,
        "title":"Development of Interpretable Machine Learning Models to Detect Arrhythmia based on ECG Data",
        "summary":"The analysis of electrocardiogram (ECG) signals can be time consuming as it is performed manually by cardiologists. Therefore, automation through machine learning (ML) classification is being increasingly proposed which would allow ML models to learn the features of a heartbeat and detect abnormalities. The lack of interpretability hinders the application of Deep Learning in healthcare. Through interpretability of these models, we would understand how a machine learning algorithm makes its decisions and what patterns are being followed for classification. This thesis builds Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) classifiers based on state-of-the-art models and compares their performance and interpretability to shallow classifiers. Here, both global and local interpretability methods are exploited to understand the interaction between dependent and independent variables across the entire dataset and to examine model decisions in each sample, respectively. Partial Dependence Plots, Shapley Additive Explanations, Permutation Feature Importance, and Gradient Weighted Class Activation Maps (Grad-Cam) are the four interpretability techniques implemented on time-series ML models classifying ECG rhythms. In particular, we exploit Grad-Cam, which is a local interpretability technique and examine whether its interpretability varies between correctly and incorrectly classified ECG beats within each class. Furthermore, the classifiers are evaluated using K-Fold cross-validation and Leave Groups Out techniques, and we use non-parametric statistical testing to examine whether differences are significant. It was found that Grad-CAM was the most effective interpretability technique at explaining predictions of proposed CNN and LSTM models. We concluded that all high performing classifiers looked at the QRS complex of the ECG rhythm when making predictions.",
        "completion1":"Development of Interpretable Machine Learning Models to Detect Arrhythmia based on ECG Data",
        "completion2":"The analysis of electrocardiogram signals can be time consuming as it is performed manually by cardiologists. Therefore, automation through machine learning classification is being increasingly proposed which would allow ML models to learn the features of a heartbeat and detect abnormalities.",
        "completion3":"The lack of interpretability hinders the application of Deep Learning in healthcare. Through interpretability of these models, we would understand how a machine learning algorithm makes its decisions and what patterns are being followed for classification.",
        "technologyreview":0.2773862879,
        "venturebeat":0.227634694,
        "wired":0.055699904,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.02803v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1651771773000,
        "code_mentioned":1,
        "readability":0.79
    },
    {
        "arxiv_id":"2202.12403v3",
        "predicted_newsworthiness":0.4795047616,
        "title":"Learning Transferable Reward for Query Object Localization with Policy Adaptation",
        "summary":"We propose a reinforcement learning based approach to query object localization, for which an agent is trained to localize objects of interest specified by a small exemplary set. We learn a transferable reward signal formulated using the exemplary set by ordinal metric learning. Our proposed method enables test-time policy adaptation to new environments where the reward signals are not readily available, and outperforms fine-tuning approaches that are limited to annotated images. In addition, the transferable reward allows repurposing the trained agent from one specific class to another class. Experiments on corrupted MNIST, CU-Birds, and COCO datasets demonstrate the effectiveness of our approach.",
        "completion1":"Researchers develop AI that can learn to locate objects across different environments",
        "completion2":"AI system outperforms existing methods for object localization",
        "completion3":"Transferable reward signal enables AI to adapt to new environments",
        "technologyreview":0.2440345778,
        "venturebeat":0.1927815558,
        "wired":0.0443491105,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.12403v3",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1645743134000,
        "code_mentioned":1,
        "readability":0.79
    },
    {
        "arxiv_id":"2203.02650v1",
        "predicted_newsworthiness":0.4204288041,
        "title":"Vision-based Distributed Multi-UAV Collision Avoidance via Deep Reinforcement Learning for Navigation",
        "summary":"Online path planning for multiple unmanned aerial vehicle (multi-UAV) systems is considered a challenging task. It needs to ensure collision-free path planning in real-time, especially when the multi-UAV systems can become very crowded on certain occasions. In this paper, we presented a vision-based decentralized collision-avoidance policy for multi-UAV systems, which takes depth images and inertial measurements as sensory inputs and outputs UAV's steering commands. The policy is trained together with the latent representation of depth images using a policy gradient-based reinforcement learning algorithm and autoencoder in the multi-UAV threedimensional workspaces. Each UAV follows the same trained policy and acts independently to reach the goal without colliding or communicating with other UAVs. We validate our policy in various simulated scenarios. The experimental results show that our learned policy can guarantee fully autonomous collision-free navigation for multi-UAV in the three-dimensional workspaces with good robustness and scalability.",
        "completion1":"Vision-based deep reinforcement learning algorithm allows for safe, autonomous navigation of multiple unmanned aerial vehicles in three-dimensional space.",
        "completion2":"New algorithm overcomes challenges of online path planning for multi-UAV systems, allowing for collision-free paths in real time.",
        "completion3":"Trained policy can be applied to various simulated scenarios, guaranteeing fully autonomous collision-free navigation for multiple UAVs.",
        "technologyreview":0.2554480697,
        "venturebeat":0.2127228489,
        "wired":0.0510181562,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.02650v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1646449261000,
        "code_mentioned":0,
        "readability":0.81
    },
    {
        "arxiv_id":"2205.02371v1",
        "predicted_newsworthiness":0.4512409728,
        "title":"A Bayesian Detect to Track System for Robust Visual Object Tracking and Semi-Supervised Model Learning",
        "summary":"Object tracking is one of the fundamental problems in visual recognition tasks and has achieved significant improvements in recent years. The achievements often come with the price of enormous hardware consumption and expensive labor effort for consecutive labeling. A missing ingredient for robust tracking is achieving performance with minimal modification on network structure and semi-supervised learning intermittent labeled frames. In this paper, we ad-dress these problems in a Bayesian tracking and detection framework parameterized by neural network outputs. In our framework, the tracking and detection process is formulated in a probabilistic way as multi-objects dynamics and network detection uncertainties. With our formulation, we propose a particle filter-based approximate sampling algorithm for tracking object state estimation. Based on our particle filter inference algorithm, a semi-supervised learn-ing algorithm is utilized for learning tracking network on intermittent labeled frames by variational inference. In our experiments, we provide both mAP and probability-based detection measurements for comparison between our algorithm with non-Bayesian solutions. We also train a semi-supervised tracking network on M2Cai16-Tool-Locations Dataset and compare our results with supervised learning on fully labeled frames.",
        "completion1":"New Bayesian object tracking system is more robust and minimally invasive.",
        "completion2":"Object tracking system uses particle filter for state estimation.",
        "completion3":"Semi-supervised learning algorithm used to train network on intermittent labeled frames.",
        "technologyreview":0.1926582162,
        "venturebeat":0.1520471625,
        "wired":0.0322018821,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.02371v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1651709937000,
        "code_mentioned":0,
        "readability":0.79
    },
    {
        "arxiv_id":"2204.04364v1",
        "predicted_newsworthiness":0.7165435529,
        "title":"Application of machine learning for predicting the spread of COVID-19",
        "summary":"The spread of diseases has been studied for many years, but it receives a particular focus recently due to the outbreak and spread of COVID-19. Studies show that the spread of COVID-19 can be characterized by the Susceptible-Infectious-Recovered-Deceased (SIRD) model with containment coefficients (due to quarantine and keeping social distance). This project aims to apply the machine learning technique to predict the severity of COVID-19 and the effect of quarantine, keeping social distance, working from home, and wearing masks on the transmission of the disease. This work deepens our understanding of disease transmission and reveals the importance of following policies.",
        "completion1":"Machine learning predicts spread of COVID-19",
        "completion2":"Study shows quarantine, social distancing effective in slowing COVID-19",
        "completion3":"Wearing masks found to reduce transmission of COVID-19",
        "technologyreview":0.3909723384,
        "venturebeat":0.2923652072,
        "wired":0.0871010189,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.04364v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1649471898000,
        "code_mentioned":0,
        "readability":0.89
    },
    {
        "arxiv_id":"2204.02310v1",
        "predicted_newsworthiness":0.7697497344,
        "title":"Improving Human-AI Partnerships in Child Welfare: Understanding Worker Practices, Challenges, and Desires for Algorithmic Decision Support",
        "summary":"AI-based decision support tools (ADS) are increasingly used to augment human decision-making in high-stakes, social contexts. As public sector agencies begin to adopt ADS, it is critical that we understand workers' experiences with these systems in practice. In this paper, we present findings from a series of interviews and contextual inquiries at a child welfare agency, to understand how they currently make AI-assisted child maltreatment screening decisions. Overall, we observe how workers' reliance upon the ADS is guided by (1) their knowledge of rich, contextual information beyond what the AI model captures, (2) their beliefs about the ADS's capabilities and limitations relative to their own, (3) organizational pressures and incentives around the use of the ADS, and (4) awareness of misalignments between algorithmic predictions and their own decision-making objectives. Drawing upon these findings, we discuss design implications towards supporting more effective human-AI decision-making.",
        "completion1":"Child welfare workers are increasingly relying on AI-based decision support tools to help make screening decisions for child maltreatment.",
        "completion2":"However, these workers still rely heavily on their own knowledge and experience when making decisions, due to limitations with the ADS tool.",
        "completion3":"With more public sector agencies beginning to adopt ADS, it is critical that we understand how best to utilize these tools in order to make effective decisions.",
        "technologyreview":0.4638902917,
        "venturebeat":0.3976618161,
        "wired":0.1356658428,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.02310v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ai",
            "cs.cy"
        ],
        "published":1649175049000,
        "code_mentioned":0,
        "readability":0.88
    },
    {
        "arxiv_id":"2201.12055v2",
        "predicted_newsworthiness":0.5006296113,
        "title":"Automated Feature Extraction on AsMap for Emotion Classification using EEG",
        "summary":"Emotion recognition using EEG has been widely studied to address the challenges associated with affective computing. Using manual feature extraction methods on EEG signals results in sub-optimal performance by the learning models. With the advancements in deep learning as a tool for automated feature engineering, in this work, a hybrid of manual and automatic feature extraction methods has been proposed. The asymmetry in different brain regions is captured in a 2D vector, termed the AsMap, from the differential entropy features of EEG signals. These AsMaps are then used to extract features automatically using a convolutional neural network model. The proposed feature extraction method has been compared with differential entropy and other feature extraction methods such as relative asymmetry, differential asymmetry and differential caudality. Experiments are conducted using the SJTU emotion EEG dataset and the DEAP dataset on different classification problems based on the number of classes. Results obtained indicate that the proposed method of feature extraction results in higher classification accuracy, outperforming the other feature extraction methods. The highest classification accuracy of 97.10% is achieved on a three-class classification problem using the SJTU emotion EEG dataset. Further, this work has also assessed the impact of window size on classification accuracy.",
        "completion1":"EEG Used to Assess Emotions",
        "completion2":"Study: EEG Can Automatically Extract Features for Emotion Classification",
        "completion3":"New Method of Feature Extraction Outperforms Other Methods",
        "technologyreview":0.2581348686,
        "venturebeat":0.2181480844,
        "wired":0.0669610741,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.12055v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1643369909000,
        "code_mentioned":0,
        "readability":0.79
    },
    {
        "arxiv_id":"2203.03959v1",
        "predicted_newsworthiness":0.5456820952,
        "title":"Enhancing Door Detection for Autonomous Mobile Robots with Environment-Specific Data Collection",
        "summary":"Door detection represents a fundamental capability for autonomous mobile robots employed in tasks involving indoor navigation. Recognizing the presence of a door and its status (open or closed) can induce a remarkable impact on the navigation performance, especially for dynamic settings where doors can enable or disable passages, hence changing the actual topology of the map. In this work, we address the problem of building a door detector module for an autonomous mobile robot deployed in a long-term scenario, namely operating in the same environment for a long time, thus observing the same set of doors from different points of view. First, we show how the mainstream approach for door detection, based on object recognition, falls short in considering the constrained perception setup typical of a mobile robot. Hence, we devise a method to build a dataset of images taken from a robot's perspective and we exploit it to obtain a door detector based on an established deep-learning object-recognition method. We then exploit the long-term assumption of our scenario to qualify the model on the robot working environment via fine-tuning with additional images acquired in the deployment environment. Our experimental analysis shows how this method can achieve good performance and highlights a trade-off between costs and benefits of the fine-tuning approach.",
        "completion1":"Enhancing Door Detection for Autonomous Mobile Robots with Environment-Specific Data Collection",
        "completion2":"Door detection important for autonomous mobile robots",
        "completion3":"Deep learning object recognition method used to detect doors for autonomous mobile robot",
        "technologyreview":0.2886311496,
        "venturebeat":0.2267692704,
        "wired":0.0566452645,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.03959v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1646731958000,
        "code_mentioned":1,
        "readability":0.89
    },
    {
        "arxiv_id":"2204.09601v1",
        "predicted_newsworthiness":0.6005768543,
        "title":"Extraction of Sleep Information from Clinical Notes of Alzheimer's Disease Patients Using Natural Language Processing",
        "summary":"Alzheimer's Disease (AD) is the most common form of dementia in the United States. Sleep is one of the lifestyle-related factors that has been shown critical for optimal cognitive function in old age. . However, there is a lack of research studying the association between sleep and AD incidence. A major bottleneck for conducting such research is that the traditional way to acquire sleep information is time-consuming, inefficient, non-scalable, and limited to patients' subjective experience. In this study, we developed a rule-based NLP algorithm and machine learning models to automate the extraction of sleep-related concepts, including snoring, napping, sleep problem, bad sleep quality, daytime sleepiness, night wakings, and sleep duration, from the clinical notes of patients diagnosed with AD. We trained and validated the proposed models on the clinical notes retrieved from the University of Pittsburgh of Medical Center (UPMC). The results show that the rule-based NLP algorithm consistently achieved the best performance for all sleep concepts.",
        "completion1":"A new study shows that sleep is critical for optimal cognitive function in old age.",
        "completion2":"The study found that a major bottleneck for conducting research on the association between sleep and AD incidence is that the traditional way to acquire sleep information is time-consuming, inefficient, non-scalable, and limited to patients' subjective experience.",
        "completion3":"The study developed a rule-based NLP algorithm and machine learning models to automate the extraction of sleep-related concepts from the clinical notes of patients diagnosed with AD.",
        "technologyreview":0.2485608594,
        "venturebeat":0.2133909453,
        "wired":0.0419457464,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.09601v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1646774419000,
        "code_mentioned":1,
        "readability":0.87
    },
    {
        "arxiv_id":"2204.03357v1",
        "predicted_newsworthiness":0.3995012364,
        "title":"Parameter-Efficient Abstractive Question Answering over Tables or Text",
        "summary":"A long-term ambition of information seeking QA systems is to reason over multi-modal contexts and generate natural answers to user queries. Today, memory intensive pre-trained language models are adapted to downstream tasks such as QA by fine-tuning the model on QA data in a specific modality like unstructured text or structured tables. To avoid training such memory-hungry models while utilizing a uniform architecture for each modality, parameter-efficient adapters add and train small task-specific bottle-neck layers between transformer layers. In this work, we study parameter-efficient abstractive QA in encoder-decoder models over structured tabular data and unstructured textual data using only 1.5% additional parameters for each modality. We also ablate over adapter layers in both encoder and decoder modules to study the efficiency-performance trade-off and demonstrate that reducing additional trainable parameters down to 0.7%-1.0% leads to comparable results. Our models out-perform current state-of-the-art models on tabular QA datasets such as Tablesum and FeTaQA, and achieve comparable performance on a textual QA dataset such as NarrativeQA using significantly less trainable parameters than fine-tuning.",
        "completion1":"A new approach to question answering is parameter-efficient and outperforms current methods.",
        "completion2":"Abstractive question answering can be done with less than 2% additional parameters.",
        "completion3":"Encoder-decoder models are more efficient for QA than fine-tuning memory-hungry models.",
        "technologyreview":0.2097484406,
        "venturebeat":0.1881925904,
        "wired":0.0525278808,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.03357v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1649328989000,
        "code_mentioned":1,
        "readability":0.75
    },
    {
        "arxiv_id":"2203.02797v2",
        "predicted_newsworthiness":0.3596195702,
        "title":"ClueGraphSum: Let Key Clues Guide the Cross-Lingual Abstractive Summarization",
        "summary":"Cross-Lingual Summarization (CLS) is the task to generate a summary in one language for an article in a different language. Previous studies on CLS mainly take pipeline methods or train the end-to-end model using the translated parallel data. However, the quality of generated cross-lingual summaries needs more further efforts to improve, and the model performance has never been evaluated on the hand-written CLS dataset. Therefore, we first propose a clue-guided cross-lingual abstractive summarization method to improve the quality of cross-lingual summaries, and then construct a novel hand-written CLS dataset for evaluation. Specifically, we extract keywords, named entities, etc. of the input article as key clues for summarization and then design a clue-guided algorithm to transform an article into a graph with less noisy sentences. One Graph encoder is built to learn sentence semantics and article structures and one Clue encoder is built to encode and translate key clues, ensuring the information of important parts are reserved in the generated summary. These two encoders are connected by one decoder to directly learn cross-lingual semantics. Experimental results show that our method has stronger robustness for longer inputs and substantially improves the performance over the strong baseline, achieving an improvement of 8.55 ROUGE-1 (English-to-Chinese summarization) and 2.13 MoverScore (Chinese-to-English summarization) scores over the existing SOTA.",
        "completion1":"ClueGraphSum: Let Key Clues Guide the Cross-Lingual Abstractive Summarization",
        "completion2":"Cross-Lingual Summarization is the task to generate a summary in one language for an article in a different language",
        "completion3":"Previous studies on CLS mainly take pipeline methods or train the end-to-end model using the translated parallel data.",
        "technologyreview":0.155645277,
        "venturebeat":0.1302800892,
        "wired":0.0425439345,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.02797v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1646503271000,
        "code_mentioned":1,
        "readability":0.79
    },
    {
        "arxiv_id":"2203.03886v1",
        "predicted_newsworthiness":0.4492062353,
        "title":"Boosting Mask R-CNN Performance for Long, Thin Forensic Traces with Pre-Segmentation and IoU Region Merging",
        "summary":"Mask R-CNN has recently achieved great success in the field of instance segmentation. However, weaknesses of the algorithm have been repeatedly pointed out as well, especially in the segmentation of long, sparse objects whose orientation is not exclusively horizontal or vertical. We present here an approach that significantly improves the performance of the algorithm by first pre-segmenting the images with a PSPNet algorithm. To further improve its prediction, we have developed our own cost functions and heuristics in the form of training strategies, which can prevent so-called (early) overfitting and achieve a more targeted convergence. Furthermore, due to the high variance of the images, especially for PSPNet, we aimed to develop strategies for a high robustness and generalization, which are also presented here.",
        "completion1":"New approach significantly improves accuracy of Mask R-CNN algorithm",
        "completion2":"PSPNet pre-segmentation boosts Mask R-CNN performance for long, thin forensic traces",
        "completion3":"IoU region merging improves accuracy of Mask R-CNN algorithm for instance segmentation",
        "technologyreview":0.1937595657,
        "venturebeat":0.1507562668,
        "wired":0.0260919339,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.03886v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.ro"
        ],
        "published":1646723896000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2205.03375v1",
        "predicted_newsworthiness":0.3713033504,
        "title":"Summary Markov Models for Event Sequences",
        "summary":"Datasets involving sequences of different types of events without meaningful time stamps are prevalent in many applications, for instance when extracted from textual corpora. We propose a family of models for such event sequences -- summary Markov models -- where the probability of observing an event type depends only on a summary of historical occurrences of its influencing set of event types. This Markov model family is motivated by Granger causal models for time series, with the important distinction that only one event can occur in a position in an event sequence. We show that a unique minimal influencing set exists for any set of event types of interest and choice of summary function, formulate two novel models from the general family that represent specific sequence dynamics, and propose a greedy search algorithm for learning them from event sequence data. We conduct an experimental investigation comparing the proposed models with relevant baselines, and illustrate their knowledge acquisition and discovery capabilities through case studies involving sequences from text.",
        "completion1":"Summary Markov Models for Event Sequences: A New Way to Model Time Series Data",
        "completion2":"Summary Markov Models Help Discover Patterns in Textual Data",
        "completion3":"Summary Markov Models Prove Effective in Time Series Data Analysis",
        "technologyreview":0.1840027192,
        "venturebeat":0.1728272733,
        "wired":0.0500791314,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.03375v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1651857384000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2205.01903v1",
        "predicted_newsworthiness":0.416717455,
        "title":"Self-Taught Metric Learning without Labels",
        "summary":"We present a novel self-taught framework for unsupervised metric learning, which alternates between predicting class-equivalence relations between data through a moving average of an embedding model and learning the model with the predicted relations as pseudo labels. At the heart of our framework lies an algorithm that investigates contexts of data on the embedding space to predict their class-equivalence relations as pseudo labels. The algorithm enables efficient end-to-end training since it demands no off-the-shelf module for pseudo labeling. Also, the class-equivalence relations provide rich supervisory signals for learning an embedding space. On standard benchmarks for metric learning, it clearly outperforms existing unsupervised learning methods and sometimes even beats supervised learning models using the same backbone network. It is also applied to semi-supervised metric learning as a way of exploiting additional unlabeled data, and achieves the state of the art by boosting performance of supervised learning substantially.",
        "completion1":"Self-Taught Metric Learning Outperforms Supervised Learning",
        "completion2":"New Algorithm for Unsupervised Metric Learning",
        "completion3":"State of the Art Semi-Supervised Metric Learning",
        "technologyreview":0.1960229191,
        "venturebeat":0.1588999075,
        "wired":0.0395946699,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.01903v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1651643320000,
        "code_mentioned":0,
        "readability":0.83
    },
    {
        "arxiv_id":"2206.07959v1",
        "predicted_newsworthiness":0.4560427135,
        "title":"A Simple Baseline for BEV Perception Without LiDAR",
        "summary":"Building 3D perception systems for autonomous vehicles that do not rely on LiDAR is a critical research problem because of the high expense of LiDAR systems compared to cameras and other sensors. Current methods use multi-view RGB data collected from cameras around the vehicle and neurally \"lift\" features from the perspective images to the 2D ground plane, yielding a \"bird's eye view\" (BEV) feature representation of the 3D space around the vehicle. Recent research focuses on the way the features are lifted from images to the BEV plane. We instead propose a simple baseline model, where the \"lifting\" step simply averages features from all projected image locations, and find that it outperforms the current state-of-the-art in BEV vehicle segmentation. Our ablations show that batch size, data augmentation, and input resolution play a large part in performance. Additionally, we reconsider the utility of radar input, which has previously been either ignored or found non-helpful by recent works. With a simple RGB-radar fusion module, we obtain a sizable boost in performance, approaching the accuracy of a LiDAR-enabled system.",
        "completion1":"A Simple Baseline for BEV Perception Without LiDAR",
        "completion2":"Simple Baseline Model outperforms current state-of-the-art in BEV vehicle segmentation",
        "completion3":"RGB-radar fusion module approaches accuracy of a LiDAR-enabled system",
        "technologyreview":0.24261223,
        "venturebeat":0.1927976166,
        "wired":0.0382585205,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.07959v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1655362652000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2204.09874v1",
        "predicted_newsworthiness":0.3626709443,
        "title":"Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics",
        "summary":"Recent work incorporates pre-trained word embeddings such as BERT embeddings into Neural Topic Models (NTMs), generating highly coherent topics. However, with high-quality contextualized document representations, do we really need sophisticated neural models to obtain coherent and interpretable topics? In this paper, we conduct thorough experiments showing that directly clustering high-quality sentence embeddings with an appropriate word selecting method can generate more coherent and diverse topics than NTMs, achieving also higher efficiency and simplicity.",
        "completion1":"Is Neural Topic Modelling Better than Clustering? An Empirical Study Shows That Clustering With Contextual Embeddings for Topics May Be More Effective",
        "completion2":"New Research Suggests That Clustering High-Quality Sentence Embeddings May Generate More Coherent And Diverse Topics Than Neural Topic Models",
        "completion3":"Can Clustering Contextual Embeddings For Topics Be More Effective Than Neural Topic Modeling?",
        "technologyreview":0.1641695746,
        "venturebeat":0.1404151187,
        "wired":0.0443006898,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.09874v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1650515211000,
        "code_mentioned":1,
        "readability":0.72
    },
    {
        "arxiv_id":"2206.03697v1",
        "predicted_newsworthiness":0.4599514891,
        "title":"Blind Face Restoration: Benchmark Datasets and a Baseline Model",
        "summary":"Blind Face Restoration (BFR) aims to construct a high-quality (HQ) face image from its corresponding low-quality (LQ) input. Recently, many BFR methods have been proposed and they have achieved remarkable success. However, these methods are trained or evaluated on privately synthesized datasets, which makes it infeasible for the subsequent approaches to fairly compare with them. To address this problem, we first synthesize two blind face restoration benchmark datasets called EDFace-Celeb-1M (BFR128) and EDFace-Celeb-150K (BFR512). State-of-the-art methods are benchmarked on them under five settings including blur, noise, low resolution, JPEG compression artifacts, and the combination of them (full degradation). To make the comparison more comprehensive, five widely-used quantitative metrics and two task-driven metrics including Average Face Landmark Distance (AFLD) and Average Face ID Cosine Similarity (AFICS) are applied. Furthermore, we develop an effective baseline model called Swin Transformer U-Net (STUNet). The STUNet with U-net architecture applies an attention mechanism and a shifted windowing scheme to capture long-range pixel interactions and focus more on significant features while still being trained efficiently. Experimental results show that the proposed baseline method performs favourably against the SOTA methods on various BFR tasks.",
        "completion1":"STUNet outperforms state-of-the-art methods for blind face restoration",
        "completion2":"EDFace-Celeb-1M and EDFace-Celeb-150K benchmark datasets released for blind face restoration",
        "completion3":"Five widely used quantitative metrics and two task-driven metrics applied to state-of-the art methods for blind face restoration",
        "technologyreview":0.2320020992,
        "venturebeat":0.1676789112,
        "wired":0.0502478126,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.03697v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1654670064000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2201.11443v2",
        "predicted_newsworthiness":0.5420608483,
        "title":"Yes-Yes-Yes: Proactive Data Collection for ACL Rolling Review and Beyond",
        "summary":"The shift towards publicly available text sources has enabled language processing at unprecedented scale, yet leaves under-serviced the domains where public and openly licensed data is scarce. Proactively collecting text data for research is a viable strategy to address this scarcity, but lacks systematic methodology taking into account the many ethical, legal and confidentiality-related aspects of data collection. Our work presents a case study on proactive data collection in peer review -- a challenging and under-resourced NLP domain. We outline ethical and legal desiderata for proactive data collection and introduce \"Yes-Yes-Yes\", the first donation-based peer reviewing data collection workflow that meets these requirements. We report on the implementation of Yes-Yes-Yes at ACL Rolling Review and empirically study the implications of proactive data collection for dataset size and the biases induced by the donation behavior on the peer reviewing platform.",
        "completion1":"Yes-Yes-Yes\": Proactive Data Collection for ACL Rolling Review and Beyond",
        "completion2":"The shift towards publicly available text sources has enabled language processing at unprecedented scale, yet leaves under-serviced the domains where public and openly licensed data is scarce.",
        "completion3":"Proactively collecting text data for research is a viable strategy to address this scarcity, but lacks systematic methodology taking into account the many ethical, legal and confidentiality-related aspects of data collection.",
        "technologyreview":0.3323811314,
        "venturebeat":0.2895400112,
        "wired":0.1045200693,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.11443v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1643281363000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2203.09333v1",
        "predicted_newsworthiness":0.3423207386,
        "title":"Modulated Contrast for Versatile Image Synthesis",
        "summary":"Perceiving the similarity between images has been a long-standing and fundamental problem underlying various visual generation tasks. Predominant approaches measure the inter-image distance by computing pointwise absolute deviations, which tends to estimate the median of instance distributions and leads to blurs and artifacts in the generated images. This paper presents MoNCE, a versatile metric that introduces image contrast to learn a calibrated metric for the perception of multifaceted inter-image distances. Unlike vanilla contrast which indiscriminately pushes negative samples from the anchor regardless of their similarity, we propose to re-weight the pushing force of negative samples adaptively according to their similarity to the anchor, which facilitates the contrastive learning from informative negative samples. Since multiple patch-level contrastive objectives are involved in image distance measurement, we introduce optimal transport in MoNCE to modulate the pushing force of negative samples collaboratively across multiple contrastive objectives. Extensive experiments over multiple image translation tasks show that the proposed MoNCE outperforms various prevailing metrics substantially. The code is available at https:\/\/github.com\/fnzhan\/MoNCE.",
        "completion1":"Modulated Contrast for Versatile Image Synthesis.",
        "completion2":"MoNCE Outperforms Prevailing Metrics Substantially.",
        "completion3":"Code for MoNCE Available.",
        "technologyreview":0.2018437111,
        "venturebeat":0.1560397137,
        "wired":0.0466056125,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.09333v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1647525826000,
        "code_mentioned":1,
        "readability":0.76
    },
    {
        "arxiv_id":"2207.02454v1",
        "predicted_newsworthiness":0.5452105995,
        "title":"Ordinal Regression via Binary Preference vs Simple Regression: Statistical and Experimental Perspectives",
        "summary":"Ordinal regression with anchored reference samples (ORARS) has been proposed for predicting the subjective Mean Opinion Score (MOS) of input stimuli automatically. The ORARS addresses the MOS prediction problem by pairing a test sample with each of the pre-scored anchored reference samples. A trained binary classifier is then used to predict which sample, test or anchor, is better statistically. Posteriors of the binary preference decision are then used to predict the MOS of the test sample. In this paper, rigorous framework, analysis, and experiments to demonstrate that ORARS are advantageous over simple regressions are presented. The contributions of this work are: 1) Show that traditional regression can be reformulated into multiple preference tests to yield a better performance, which is confirmed with simulations experimentally; 2) Generalize ORARS to other regression problems and verify its effectiveness; 3) Provide some prerequisite conditions which can insure proper application of ORARS.",
        "completion1":"ORARS offers advantages over simple regressions, according to new study",
        "completion2":"Study provides insights into when ORARS should be used",
        "completion3":"ORARS can be applied to various regression problems",
        "technologyreview":0.1963060785,
        "venturebeat":0.1743003232,
        "wired":0.0511157523,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.02454v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1657086739000,
        "code_mentioned":0,
        "readability":0.75
    },
    {
        "arxiv_id":"2202.05129v1",
        "predicted_newsworthiness":0.4869083467,
        "title":"Help Me Explore: Minimal Social Interventions for Graph-Based Autotelic Agents",
        "summary":"In the quest for autonomous agents learning open-ended repertoires of skills, most works take a Piagetian perspective: learning trajectories are the results of interactions between developmental agents and their physical environment. The Vygotskian perspective, on the other hand, emphasizes the centrality of the socio-cultural environment: higher cognitive functions emerge from transmissions of socio-cultural processes internalized by the agent. This paper argues that both perspectives could be coupled within the learning of autotelic agents to foster their skill acquisition. To this end, we make two contributions: 1) a novel social interaction protocol called Help Me Explore (HME), where autotelic agents can benefit from both individual and socially guided exploration. In social episodes, a social partner suggests goals at the frontier of the learning agent knowledge. In autotelic episodes, agents can either learn to master their own discovered goals or autonomously rehearse failed social goals; 2) GANGSTR, a graph-based autotelic agent for manipulation domains capable of decomposing goals into sequences of intermediate sub-goals. We show that when learning within HME, GANGSTR overcomes its individual learning limits by mastering the most complex configurations (e.g. stacks of 5 blocks) with only few social interventions.",
        "completion1":"Social Interventions Foster Skill Acquisition in Autonomous Agents",
        "completion2":"Graph-Based Autotelic Agents Benefit From Social Interactions",
        "completion3":"Learning Protocol Helps Autonomous Agents Explore Their Environment",
        "technologyreview":0.3174971028,
        "venturebeat":0.2312094637,
        "wired":0.0775572854,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.05129v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.hc"
        ],
        "published":1644510868000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2203.01635v1",
        "predicted_newsworthiness":0.4518981125,
        "title":"Parallel feature selection based on the trace ratio criterion",
        "summary":"The growth of data today poses a challenge in management and inference. While feature extraction methods are capable of reducing the size of the data for inference, they do not help in minimizing the cost of data storage. On the other hand, feature selection helps to remove the redundant features and therefore is helpful not only in inference but also in reducing management costs. This work presents a novel parallel feature selection approach for classification, namely Parallel Feature Selection using Trace criterion (PFST), which scales up to very large datasets. Our method uses trace criterion, a measure of class separability used in Fisher's Discriminant Analysis, to evaluate feature usefulness. We analyzed the criterion's desirable properties theoretically. Based on the criterion, PFST rapidly finds important features out of a set of features for big datasets by first making a forward selection with early removal of seemingly redundant features parallelly. After the most important features are included in the model, we check back their contribution for possible interaction that may improve the fit. Lastly, we make a backward selection to check back possible redundant added by the forward steps. We evaluate our methods via various experiments using Linear Discriminant Analysis as the classifier on selected features. The experiments show that our method can produce a small set of features in a fraction of the amount of time by the other methods under comparison. In addition, the classifier trained on the features selected by PFST not only achieves better accuracy than the ones chosen by other approaches but can also achieve better accuracy than the classification on all available features.",
        "completion1":"Parallel Feature Selection using Trace criterion' rapidly finds important features out of a set of features for big datasets.",
        "completion2":"The work uses trace criterion, a measure of class separability used in Fisher's Discriminant Analysis, to evaluate feature usefulness.",
        "completion3":"PFST can produce a small set of features in a fraction of the amount of time by the other methods under comparison.",
        "technologyreview":0.2193401351,
        "venturebeat":0.2000364647,
        "wired":0.0464302186,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.01635v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1646304633000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2204.07151v1",
        "predicted_newsworthiness":0.3777270889,
        "title":"Deformable Sprites for Unsupervised Video Decomposition",
        "summary":"We describe a method to extract persistent elements of a dynamic scene from an input video. We represent each scene element as a \\emph{Deformable Sprite} consisting of three components: 1) a 2D texture image for the entire video, 2) per-frame masks for the element, and 3) non-rigid deformations that map the texture image into each video frame. The resulting decomposition allows for applications such as consistent video editing. Deformable Sprites are a type of video auto-encoder model that is optimized on individual videos, and does not require training on a large dataset, nor does it rely on pre-trained models. Moreover, our method does not require object masks or other user input, and discovers moving objects of a wider variety than previous work. We evaluate our approach on standard video datasets and show qualitative results on a diverse array of Internet videos. Code and video results can be found at https:\/\/deformable-sprites.github.io",
        "completion1":"New method for video decomposition doesn't require object masks or user input",
        "completion2":"Deformable Sprites could enable more consistent video editing",
        "completion3":"Code and video results for Deformable Sprites available online",
        "technologyreview":0.193856597,
        "venturebeat":0.1725272002,
        "wired":0.0526141931,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.07151v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1649959082000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2201.10983v1",
        "predicted_newsworthiness":0.4777365909,
        "title":"Online POI Recommendation: Learning Dynamic Geo-Human Interactions in Streams",
        "summary":"In this paper, we focus on the problem of modeling dynamic geo-human interactions in streams for online POI recommendations. Specifically, we formulate the in-stream geo-human interaction modeling problem into a novel deep interactive reinforcement learning framework, where an agent is a recommender and an action is a next POI to visit. We uniquely model the reinforcement learning environment as a joint and connected composition of users and geospatial contexts (POIs, POI categories, functional zones). An event that a user visits a POI in stream updates the states of both users and geospatial contexts; the agent perceives the updated environment state to make online recommendations. Specifically, we model a mixed-user event stream by unifying all users, visits, and geospatial contexts as a dynamic knowledge graph stream, in order to model human-human, geo-human, geo-geo interactions. We design an exit mechanism to address the expired information challenge, devise a meta-path method to address the recommendation candidate generation challenge, and develop a new deep policy network structure to address the varying action space challenge, and, finally, propose an effective adversarial training method for optimization. Finally, we present extensive experiments to demonstrate the enhanced performance of our method.",
        "completion1":"Online POI Recommendation: Learning Dynamic Geo-Human Interactions in Streams",
        "completion2":"In this paper, we focus on the problem of modeling dynamic geo-human interactions in streams for online POI recommendations",
        "completion3":"We uniquely model the reinforcement learning environment as a joint and connected composition of users and geospatial contexts (POIs, POI categories, functional zones)",
        "technologyreview":0.2560397223,
        "venturebeat":0.2388067264,
        "wired":0.0964014047,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.10983v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.ai",
            "cs.lg"
        ],
        "published":1642610989000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2207.06530v1",
        "predicted_newsworthiness":0.4510119796,
        "title":"Estimation of Soft Robotic Bladder Compression for Smart Helmets using IR Range Finding and Hall Effect Magnetic Sensing",
        "summary":"This research focuses on soft robotic bladders that are used to monitor and control the interaction between a user's head and the shell of a Smart Helmet. Compression of these bladders determines impact dissipation; hence the focus of this paper is sensing and estimation of bladder compression. An IR rangefinder-based solution is evaluated using regression techniques as well as a Neural Network to estimate bladder compression. A Hall-Effect (HE) magnetic sensing system is also examined where HE sensors embedded in the base of the bladder sense the position of a magnet in the top of the bladder. The paper presents the HE sensor array, signal processing of HE voltage data, and then a Neural Network (NN) for predicting bladder compression. Efficacy of different training data sets on NN performance is studied. Different NN configurations are examined to determine a configuration that provides accurate estimates with as few nodes as possible. Different bladder compression profiles are evaluated to characterize IR range finding and HE based techniques in application scenarios.",
        "completion1":"Estimation of Soft Robotic Bladder Compression for Smart Helmets",
        "completion2":"IR Range Finding and Hall Effect Magnetic Sensing to Determine Impact Dissipation",
        "completion3":"Neural Network Trained to Estimate Bladder Compression",
        "technologyreview":0.2277772655,
        "venturebeat":0.1814973499,
        "wired":0.0390552077,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.06530v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1657749789000,
        "code_mentioned":0,
        "readability":0.79
    },
    {
        "arxiv_id":"2201.02848v1",
        "predicted_newsworthiness":0.3544285763,
        "title":"Learning Sample Importance for Cross-Scenario Video Temporal Grounding",
        "summary":"The task of temporal grounding aims to locate video moment in an untrimmed video, with a given sentence query. This paper for the first time investigates some superficial biases that are specific to the temporal grounding task, and proposes a novel targeted solution. Most alarmingly, we observe that existing temporal ground models heavily rely on some biases (e.g., high preference on frequent concepts or certain temporal intervals) in the visual modal. This leads to inferior performance when generalizing the model in cross-scenario test setting. To this end, we propose a novel method called Debiased Temporal Language Localizer (DebiasTLL) to prevent the model from naively memorizing the biases and enforce it to ground the query sentence based on true inter-modal relationship. Debias-TLL simultaneously trains two models. By our design, a large discrepancy of these two models' predictions when judging a sample reveals higher probability of being a biased sample. Harnessing the informative discrepancy, we devise a data re-weighing scheme for mitigating the data biases. We evaluate the proposed model in cross-scenario temporal grounding, where the train \/ test data are heterogeneously sourced. Experiments show large-margin superiority of the proposed method in comparison with state-of-the-art competitors.",
        "completion1":"Study Shows That Existing Temporal Grounding Models Rely Heavily on Biases",
        "completion2":"Researchers Propose New Method to Prevent Models from Naively Memorizing Biases",
        "completion3":"Experiments Show Large-Margin Superiority of Proposed Method",
        "technologyreview":0.212401555,
        "venturebeat":0.1807313826,
        "wired":0.0695611593,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.02848v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ir"
        ],
        "published":1641656498000,
        "code_mentioned":0,
        "readability":0.78
    },
    {
        "arxiv_id":"2205.10911v1",
        "predicted_newsworthiness":0.7589394066,
        "title":"Power and accountability in reinforcement learning applications to environmental policy",
        "summary":"Machine learning (ML) methods already permeate environmental decision-making, from processing high-dimensional data on earth systems to monitoring compliance with environmental regulations. Of the ML techniques available to address pressing environmental problems (e.g., climate change, biodiversity loss), Reinforcement Learning (RL) may both hold the greatest promise and present the most pressing perils. This paper explores how RL-driven policy refracts existing power relations in the environmental domain while also creating unique challenges to ensuring equitable and accountable environmental decision processes. We leverage examples from RL applications to climate change mitigation and fisheries management to explore how RL technologies shift the distribution of power between resource users, governing bodies, and private industry.",
        "completion1":"Reinforcement Learning May Be Both Promise and Peril for Environmental Policy",
        "completion2":"How Reinforcement Learning Shifts Power Relations in Environmental Decision-Making",
        "completion3":"Ensuring Equity and Accountability in Reinforcement Learning Applications to Environmental Policy",
        "technologyreview":0.3556563456,
        "venturebeat":0.2694443232,
        "wired":0.1148808966,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.10911v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy",
            "cs.lg"
        ],
        "published":1653247897000,
        "code_mentioned":1,
        "readability":0.87
    },
    {
        "arxiv_id":"2207.02845v1",
        "predicted_newsworthiness":0.668650959,
        "title":"Automating the Design and Development of Gradient Descent Trained Expert System Networks",
        "summary":"Prior work introduced a gradient descent trained expert system that conceptually combines the learning capabilities of neural networks with the understandability and defensible logic of an expert system. This system was shown to be able to learn patterns from data and to perform decision-making at levels rivaling those reported by neural network systems. The principal limitation of the approach, though, was the necessity for the manual development of a rule-fact network (which is then trained using backpropagation). This paper proposes a technique for overcoming this significant limitation, as compared to neural networks. Specifically, this paper proposes the use of larger and denser-than-application need rule-fact networks which are trained, pruned, manually reviewed and then re-trained for use. Multiple types of networks are evaluated under multiple operating conditions and these results are presented and assessed. Based on these individual experimental condition assessments, the proposed technique is evaluated. The data presented shows that error rates as low as 3.9% (mean, 1.2% median) can be obtained, demonstrating the efficacy of this technique for many applications.",
        "completion1":"Automating the Design and Development of Gradient Descent Trained Expert System Networks.",
        "completion2":"Automating Neural Network Systems with Backpropagation.",
        "completion3":"Achieving Low Error Rates with Automated Techniques for Gradient Descent Trained Expert System Networks.",
        "technologyreview":0.3116740449,
        "venturebeat":0.2645576599,
        "wired":0.0383456591,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.02845v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1656960910000,
        "code_mentioned":1,
        "readability":0.88
    },
    {
        "arxiv_id":"2205.04260v1",
        "predicted_newsworthiness":0.3313003717,
        "title":"EASE: Entity-Aware Contrastive Learning of Sentence Embedding",
        "summary":"We present EASE, a novel method for learning sentence embeddings via contrastive learning between sentences and their related entities. The advantage of using entity supervision is twofold: (1) entities have been shown to be a strong indicator of text semantics and thus should provide rich training signals for sentence embeddings; (2) entities are defined independently of languages and thus offer useful cross-lingual alignment supervision. We evaluate EASE against other unsupervised models both in monolingual and multilingual settings. We show that EASE exhibits competitive or better performance in English semantic textual similarity (STS) and short text clustering (STC) tasks and it significantly outperforms baseline methods in multilingual settings on a variety of tasks. Our source code, pre-trained models, and newly constructed multilingual STC dataset are available at https:\/\/github.com\/studio-ousia\/ease.",
        "completion1":"EASE: Entity-Aware Contrastive Learning of Sentence Embedding",
        "completion2":"The advantage of using entity supervision is twofold",
        "completion3":"We show that EASE exhibits competitive or better performance in English semantic textual similarity and short text clustering tasks and it significantly outperforms baseline methods in multilingual settings on a variety of tasks.",
        "technologyreview":0.1342054182,
        "venturebeat":0.1158835617,
        "wired":0.0251209179,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.04260v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1652102564000,
        "code_mentioned":1,
        "readability":0.78
    },
    {
        "arxiv_id":"2207.00287v1",
        "predicted_newsworthiness":0.334335447,
        "title":"DALG: Deep Attentive Local and Global Modeling for Image Retrieval",
        "summary":"Deeply learned representations have achieved superior image retrieval performance in a retrieve-then-rerank manner. Recent state-of-the-art single stage model, which heuristically fuses local and global features, achieves promising trade-off between efficiency and effectiveness. However, we notice that efficiency of existing solutions is still restricted because of their multi-scale inference paradigm. In this paper, we follow the single stage art and obtain further complexity-effectiveness balance by successfully getting rid of multi-scale testing. To achieve this goal, we abandon the widely-used convolution network giving its limitation in exploring diverse visual patterns, and resort to fully attention based framework for robust representation learning motivated by the success of Transformer. Besides applying Transformer for global feature extraction, we devise a local branch composed of window-based multi-head attention and spatial attention to fully exploit local image patterns. Furthermore, we propose to combine the hierarchical local and global features via a cross-attention module, instead of using heuristically fusion as previous art does. With our Deep Attentive Local and Global modeling framework (DALG), extensive experimental results show that efficiency can be significantly improved while maintaining competitive results with the state of the arts.",
        "completion1":"DALG: A deep learning framework for image retrieval that is both efficient and effective.",
        "completion2":"DALG: A new approach to image retrieval that combines local and global features.",
        "completion3":"DALG: A deep learning framework that uses attention-based Transformer networks for robust representation learning.",
        "technologyreview":0.1988656539,
        "venturebeat":0.1496287625,
        "wired":0.043888363,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.00287v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1656667935000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2204.10181v1",
        "predicted_newsworthiness":0.5300831909,
        "title":"WordAlchemy: A transformer-based Reverse Dictionary",
        "summary":"A reverse dictionary takes a target word's description as input and returns the words that fit the description. Reverse Dictionaries are useful for new language learners, anomia patients, and for solving common tip-of-the-tongue problems (lethologica). Currently, there does not exist any Reverse Dictionary provider with support for any Indian Language. We present a novel open-source cross-lingual reverse dictionary system with support for Indian languages. In this paper, we propose a transformer-based deep learning approach to tackle the limitations faced by the existing systems using the mT5 model. This architecture uses the Translation Language Modeling (TLM) technique, rather than the conventional BERT's Masked Language Modeling (MLM) technique.",
        "completion1":"Introducing WordAlchemy: A transformer-based Reverse Dictionary.",
        "completion2":"WordAlchemy helps new language learners, anomia patients, and those with tip-of-the-tongue problems.",
        "completion3":"A deep learning approach to cross-lingual reverse dictionaries using the mT5 model.",
        "technologyreview":0.2486973006,
        "venturebeat":0.2122336565,
        "wired":0.0630380386,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.10181v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1650109308000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2202.10934v2",
        "predicted_newsworthiness":0.5613287121,
        "title":"Letters of the Alphabet: Discovering Natural Feature Sets",
        "summary":"Deep learning networks find intricate features in large datasets using the backpropagation algorithm. This algorithm repeatedly adjusts the network connections.' weights and examining the \"hidden\" nodes behavior between the input and output layer provides better insight into how neural networks create feature representations. Experiments built on each other show that activity differences computed within a layer can guide learning. A simple neural network is used, which includes a data set comprised of the alphabet letters, where each letter forms 81 input nodes comprised of 0 and 1s and a single hidden layer and an output layer. The first experiment explains how the hidden layers in this simple neural network represent the input data's features. The second experiment attempts to reverse-engineer the neural network to find the alphabet's natural feature sets. As the network interprets features, we can understand how it derives the natural feature sets for a given data. This understanding is essential to delve deeper into deep generative models, such as Boltzmann machines. Deep generative models are a class of unsupervised deep learning algorithms. The primary function of deep generative models is to find the natural feature sets for a given data set.",
        "completion1":"Deep learning networks find intricate features in large datasets.",
        "completion2":"The backpropagation algorithm is used to repeatedly adjust the network connections' weights.",
        "completion3":"Activity differences computed within a layer can guide learning.",
        "technologyreview":0.3772331328,
        "venturebeat":0.2925351807,
        "wired":0.0920686252,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.10934v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1645214157000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2203.09622v1",
        "predicted_newsworthiness":0.3996673186,
        "title":"Modeling of a Quadruped Robot with Spine Joints and Full-Dynamics Simulation Environment Construction",
        "summary":"This paper presents modeling and simulation of a spined quadruped robot. Extended literature survey is employed and spine joints researches of the quadruped robots are classified. Most of the researchers execute simplified quadruped robot models in their simulations. This survey reveals the need for the full-body spined quadruped simulation environment. First, the kinematics and dynamics modeling of the active spined quadruped robot is obtained. Since quadruped robots are floating-base robots, all derivations are performed with respect to an inertial frame. The motion equations are acquired by the Lagrangian approach. The simulation environment is constructed in the MATLAB\/Simulink platform, considering its rich library, powerful solvers, and suitable and resilient environment in integrating controllers. The computation speed of the simulation environment is increased by using optimized MATLAB functions. Precise and accurate contact model is utilized in the simulation environment. We foreseen that the provided full-dynamics simulation environment will be helpful for further spine joint studies on the quadruped robot field.",
        "completion1":"Construction of a full-dynamics simulation environment for quadruped robot modeling",
        "completion2":"Use of an optimized MATLAB function for increased computation speed in the simulation environment",
        "completion3":"Implementation of a precise and accurate contact model in the simulation environment",
        "technologyreview":0.1188509902,
        "venturebeat":0.0894920146,
        "wired":-0.0188870422,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.09622v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1647552362000,
        "code_mentioned":0,
        "readability":0.74
    },
    {
        "arxiv_id":"2204.10429v1",
        "predicted_newsworthiness":0.4993827376,
        "title":"Curriculum Learning for Goal-Oriented Semantic Communications with a Common Language",
        "summary":"Goal-oriented semantic communication will be a pillar of next-generation wireless networks. Despite significant recent efforts in this area, most prior works are focused on specific data types (e.g., image or audio), and they ignore the goal and effectiveness aspects of semantic transmissions. In contrast, in this paper, a holistic goal-oriented semantic communication framework is proposed to enable a speaker and a listener to cooperatively execute a set of sequential tasks in a dynamic environment. A common language based on a hierarchical belief set is proposed to enable semantic communications between speaker and listener. The speaker, acting as an observer of the environment, utilizes the beliefs to transmit an initial description of its observation (called event) to the listener. The listener is then able to infer on the transmitted description and complete it by adding related beliefs to the transmitted beliefs of the speaker. As such, the listener reconstructs the observed event based on the completed description, and it then takes appropriate action in the environment based on the reconstructed event. An optimization problem is defined to determine the perfect and abstract description of the events while minimizing the transmission and inference costs with constraints on the task execution time and belief efficiency. Then, a novel bottom-up curriculum learning (CL) framework based on reinforcement learning is proposed to solve the optimization problem and enable the speaker and listener to gradually identify the structure of the belief set and the perfect and abstract description of the events. Simulation results show that the proposed CL method outperforms traditional RL in terms of convergence time, task execution cost and time, reliability, and belief efficiency.",
        "completion1":"A novel bottom-up curriculum learning framework is proposed to enable a speaker and listener to gradually identify the structure of a belief set and the perfect and abstract description of events.",
        "completion2":"The proposed CL method outperforms traditional RL in terms of convergence time, task execution cost and time, reliability, and belief efficiency.",
        "completion3":"The goal-oriented semantic communication framework will be a pillar of next-generation wireless networks.",
        "technologyreview":0.2304613196,
        "venturebeat":0.2035500372,
        "wired":0.0486646478,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.10429v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni",
            "cs.lg"
        ],
        "published":1650580566000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2201.05613v2",
        "predicted_newsworthiness":0.5355731236,
        "title":"The Dark Side of the Language: Pre-trained Transformers in the DarkNet",
        "summary":"Pre-trained Transformers are challenging human performances in many natural language processing tasks. The gigantic datasets used for pre-training seem to be the key for their success on existing tasks. In this paper, we explore how a range of pre-trained natural language understanding models perform on truly novel and unexplored data, provided by classification tasks over a DarkNet corpus. Surprisingly, results show that syntactic and lexical neural networks largely outperform pre-trained Transformers. This seems to suggest that pre-trained Transformers have serious difficulties in adapting to radically novel texts.",
        "completion1":"Pre-trained Transformers Struggle in the DarkNet",
        "completion2":"Syntactic and Lexical Neural Networks Outperform Pre-trained Transformers",
        "completion3":"Pre-trained Transformers have Difficulties Adapting toRadically Novel Texts",
        "technologyreview":0.3341030565,
        "venturebeat":0.2813704202,
        "wired":0.1127637808,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.05613v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1642176249000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2201.03719v1",
        "predicted_newsworthiness":0.3988221458,
        "title":"A Low-Cost, Highly Customizable Solution for Position Estimation in Modular Robots",
        "summary":"Accurate position sensing is important for state estimation and control in robotics. Reliable and accurate position sensors are usually expensive and difficult to customize. Incorporating them into systems that have very tight volume constraints such as modular robots are particularly difficult. PaintPots are low-cost, reliable, and highly customizable position sensors, but their performance is highly dependent on the manufacturing and calibration process. This paper presents a Kalman filter with a simplified observation model developed to deal with the non-linearity issues that result in the use of low-cost microcontrollers. In addition, a complete solution for the use of PaintPots in a variety of sensing modalities including manufacturing, characterization, and estimation is presented for an example modular robot, SMORES-EP. This solution can be easily adapted to a wide range of applications.",
        "completion1":"A Low-Cost, Highly Customizable Solution for Position Estimation in Modular Robots",
        "completion2":"Reliable and accurate position sensors are usually expensive and difficult to customize",
        "completion3":"A complete solution for the use of PaintPots in a variety of sensing modalities including manufacturing, characterization, and estimation is presented for an example modular robot",
        "technologyreview":0.1895277315,
        "venturebeat":0.1297635762,
        "wired":-0.0139153899,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.03719v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1641862033000,
        "code_mentioned":0,
        "readability":0.83
    },
    {
        "arxiv_id":"2205.03484v1",
        "predicted_newsworthiness":0.4342290757,
        "title":"Diverse Imitation Learning via Self-Organizing Generative Models",
        "summary":"Imitation learning is the task of replicating expert policy from demonstrations, without access to a reward function. This task becomes particularly challenging when the expert exhibits a mixture of behaviors. Prior work has introduced latent variables to model variations of the expert policy. However, our experiments show that the existing works do not exhibit appropriate imitation of individual modes. To tackle this problem, we adopt an encoder-free generative model for behavior cloning (BC) to accurately distinguish and imitate different modes. Then, we integrate it with GAIL to make the learning robust towards compounding errors at unseen states. We show that our method significantly outperforms the state of the art across multiple experiments.",
        "completion1":"Diverse Imitation Learning via Self-Organizing Generative Models",
        "completion2":"Imitation learning is the task of replicating expert policy from demonstrations, without access to a reward function",
        "completion3":"This task becomes particularly challenging when the expert exhibits a mixture of behaviors",
        "technologyreview":0.2185397852,
        "venturebeat":0.1494298852,
        "wired":0.0241287256,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.03484v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1651874131000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2202.08516v2",
        "predicted_newsworthiness":0.4423789176,
        "title":"SAITS: Self-Attention-based Imputation for Time Series",
        "summary":"Missing data in time series is a pervasive problem that puts obstacles in the way of advanced analysis. A popular solution is imputation, where the fundamental challenge is to determine what values should be filled in. This paper proposes SAITS, a novel method based on the self-attention mechanism for missing value imputation in multivariate time series. Trained by a joint-optimization approach, SAITS learns missing values from a weighted combination of two diagonally-masked self-attention (DMSA) blocks. DMSA explicitly captures both the temporal dependencies and feature correlations between time steps, which improves imputation accuracy and training speed. Meanwhile, the weighted-combination design enables SAITS to dynamically assign weights to the learned representations from two DMSA blocks according to the attention map and the missingness information. Extensive experiments quantitatively and qualitatively demonstrate that SAITS outperforms the state-of-the-art methods on the time-series imputation task efficiently and reveal SAITS' potential to improve the learning performance of pattern recognition models on incomplete time-series data from the real world.",
        "completion1":"A new method for imputing missing values in time series data is proposed - SAITS.",
        "completion2":"SAITS outperforms state-of-the-art methods for time-series imputation.",
        "completion3":"SAITS has the potential to improve the learning performance of pattern recognition models on incomplete time-series data.",
        "technologyreview":0.2092481442,
        "venturebeat":0.1906303359,
        "wired":0.0523806115,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.08516v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1645087242000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2207.01220v1",
        "predicted_newsworthiness":0.566442635,
        "title":"BusiNet -- a Light and Fast Text Detection Network for Business Documents",
        "summary":"For digitizing or indexing physical documents, Optical Character Recognition (OCR), the process of extracting textual information from scanned documents, is a vital technology. When a document is visually damaged or contains non-textual elements, existing technologies can yield poor results, as erroneous detection results can greatly affect the quality of OCR. In this paper we present a detection network dubbed BusiNet aimed at OCR of business documents. Business documents often include sensitive information and as such they cannot be uploaded to a cloud service for OCR. BusiNet was designed to be fast and light so it could run locally preventing privacy issues. Furthermore, BusiNet is built to handle scanned document corruption and noise using a specialized synthetic dataset. The model is made robust to unseen noise by employing adversarial training strategies. We perform an evaluation on publicly available datasets demonstrating the usefulness and broad applicability of our model.",
        "completion1":"New Text Detection Network Aims to Improve OCR for Business Documents",
        "completion2":"BusiNet Designed to Run Locally, Preventing Privacy Issues",
        "completion3":"Adversarial Training Strategies Used to Make Model Robust to Unseen Noise",
        "technologyreview":0.3012834509,
        "venturebeat":0.2674984236,
        "wired":0.0796314633,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.01220v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1656914929000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2201.05848v2",
        "predicted_newsworthiness":0.5777993416,
        "title":"TWikiL -- The Twitter Wikipedia Link Dataset",
        "summary":"Recent research has shown how strongly Wikipedia and other web services or platforms are connected. For example, search engines rely heavily on surfacing Wikipedia links to satisfy their users' information needs and volunteer-created Wikipedia content frequently gets re-used on other social media platforms like Reddit. However, publicly accessible datasets that enable researchers to study the interrelationship between Wikipedia and other platforms are sparse. In addition to that, most studies only focus on certain points in time and don't consider the historical perspective. To begin solving these problems we developed TWikiL, the Twitter Wikipedia Link Dataset, which contains all Wikipedia links posted on Twitter in the period 2006 to January 2021. We extract Wikipedia links from Tweets and enrich the referenced articles with their respective Wikidata identifiers and Wikipedia topic categories, which will make this dataset immediately useful for a large range of scholarly use cases. In this paper, we describe the data collection process, perform an initial exploratory analysis and present a comprehensive overview of how this dataset can be useful for the research community.",
        "completion1":"researchers develop TWikiL to study interrelationship between Wikipedia and other platforms",
        "completion2":"TWikiL contains all Wikipedia links posted on Twitter from 2006-2021",
        "completion3":"initial exploratory analysis shows a range of scholarly use cases for the dataset",
        "technologyreview":0.3204712573,
        "venturebeat":0.267210753,
        "wired":0.1123967945,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.05848v2",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1642253525000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2206.13202v2",
        "predicted_newsworthiness":0.7569596424,
        "title":"Human-AI Collaboration in Decision-Making: Beyond Learning to Defer",
        "summary":"Human-AI collaboration (HAIC) in decision-making aims to create synergistic teaming between human decision-makers and AI systems. Learning to defer (L2D) has been presented as a promising framework to determine who among humans and AI should make which decisions in order to optimize the performance and fairness of the combined system. Nevertheless, L2D entails several often unfeasible requirements, such as the availability of predictions from humans for every instance or ground-truth labels that are independent from said humans. Furthermore, neither L2D nor alternative approaches tackle fundamental issues of deploying HAIC systems in real-world settings, such as capacity management or dealing with dynamic environments. In this paper, we aim to identify and review these and other limitations, pointing to where opportunities for future research in HAIC may lie.",
        "completion1":"Human-AI Collaboration in Decision-Making: Beyond Learning to Defer",
        "completion2":"Human-AI collaboration  in decision-making: Review of limitations and opportunities",
        "completion3":"Learning to defer (L2D): unfeasible requirements and fundamental issues",
        "technologyreview":0.4159244073,
        "venturebeat":0.3619190909,
        "wired":0.0985380307,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.13202v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.hc"
        ],
        "published":1656330055000,
        "code_mentioned":0,
        "readability":0.89
    },
    {
        "arxiv_id":"2202.04768v1",
        "predicted_newsworthiness":0.3337869888,
        "title":"Boosting Graph Neural Networks by Injecting Pooling in Message Passing",
        "summary":"There has been tremendous success in the field of graph neural networks (GNNs) as a result of the development of the message-passing (MP) layer, which updates the representation of a node by combining it with its neighbors to address variable-size and unordered graphs. Despite the fruitful progress of MP GNNs, their performance can suffer from over-smoothing, when node representations become too similar and even indistinguishable from one another. Furthermore, it has been reported that intrinsic graph structures are smoothed out as the GNN layer increases. Inspired by the edge-preserving bilateral filters used in image processing, we propose a new, adaptable, and powerful MP framework to prevent over-smoothing. Our bilateral-MP estimates a pairwise modular gradient by utilizing the class information of nodes, and further preserves the global graph structure by using the gradient when the aggregating function is applied. Our proposed scheme can be generalized to all ordinary MP GNNs. Experiments on five medium-size benchmark datasets using four state-of-the-art MP GNNs indicate that the bilateral-MP improves performance by alleviating over-smoothing. By inspecting quantitative measurements, we additionally validate the effectiveness of the proposed mechanism in preventing the over-smoothing issue.",
        "completion1":"Boosting Graph Neural Networks by Injecting Pooling in Message Passing",
        "completion2":"New MP framework prevents over-smoothing in GNNs",
        "completion3":"Bilateral-MP improves performance of state-of-the-art MP GNNs",
        "technologyreview":0.2115524652,
        "venturebeat":0.1560650256,
        "wired":0.0411381154,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.04768v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1644308480000,
        "code_mentioned":1,
        "readability":0.78
    },
    {
        "arxiv_id":"2202.11499v1",
        "predicted_newsworthiness":0.6941758088,
        "title":"Fairness-Aware Naive Bayes Classifier for Data with Multiple Sensitive Features",
        "summary":"Fairness-aware machine learning seeks to maximise utility in generating predictions while avoiding unfair discrimination based on sensitive attributes such as race, sex, religion, etc. An important line of work in this field is enforcing fairness during the training step of a classifier. A simple yet effective binary classification algorithm that follows this strategy is two-naive-Bayes (2NB), which enforces statistical parity - requiring that the groups comprising the dataset receive positive labels with the same likelihood. In this paper, we generalise this algorithm into N-naive-Bayes (NNB) to eliminate the simplification of assuming only two sensitive groups in the data and instead apply it to an arbitrary number of groups. We propose an extension of the original algorithm's statistical parity constraint and the post-processing routine that enforces statistical independence of the label and the single sensitive attribute. Then, we investigate its application on data with multiple sensitive features and propose a new constraint and post-processing routine to enforce differential fairness, an extension of established group-fairness constraints focused on intersectionalities. We empirically demonstrate the effectiveness of the NNB algorithm on US Census datasets and compare its accuracy and debiasing performance, as measured by disparate impact and DF-$\\epsilon$ score, with similar group-fairness algorithms. Finally, we lay out important considerations users should be aware of before incorporating this algorithm into their application, and direct them to further reading on the pros, cons, and ethical implications of using statistical parity as a fairness criterion.",
        "completion1":"New algorithm NNB eliminates unfair discrimination based on sensitive attributes.",
        "completion2":"NNB offers accuracy and debiasing performance improvements for group-fairness algorithms.",
        "completion3":"Important considerations users should be aware of before incorporating NNB into their application.",
        "technologyreview":0.2975886802,
        "venturebeat":0.2382482657,
        "wired":0.0763329218,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.11499v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1645623141000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2203.05765v1",
        "predicted_newsworthiness":0.3575057072,
        "title":"Tevatron: An Efficient and Flexible Toolkit for Dense Retrieval",
        "summary":"Recent rapid advancements in deep pre-trained language models and the introductions of large datasets have powered research in embedding-based dense retrieval. While several good research papers have emerged, many of them come with their own software stacks. These stacks are typically optimized for some particular research goals instead of efficiency or code structure. In this paper, we present Tevatron, a dense retrieval toolkit optimized for efficiency, flexibility, and code simplicity. Tevatron provides a standardized pipeline for dense retrieval including text processing, model training, corpus\/query encoding, and search. This paper presents an overview of Tevatron and demonstrates its effectiveness and efficiency across several IR and QA data sets. We also show how Tevatron's flexible design enables easy generalization across datasets, model architectures, and accelerator platforms(GPU\/TPU). We believe Tevatron can serve as an effective software foundation for dense retrieval system research including design, modeling, and optimization.",
        "completion1":"Tevatron: A New Toolkit for Dense Retrieval",
        "completion2":"Tevatron Optimized for Efficiency, Flexibility, and Code Simplicity",
        "completion3":"Tevatron Enables Easy Generalization Across Datasets, Model Architectures, and Accelerator Platforms",
        "technologyreview":0.2450136822,
        "venturebeat":0.2412275875,
        "wired":0.0729513295,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.05765v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.cl"
        ],
        "published":1646977665000,
        "code_mentioned":1,
        "readability":0.75
    },
    {
        "arxiv_id":"2204.14243v2",
        "predicted_newsworthiness":0.4282887472,
        "title":"Training Naturalized Semantic Parsers with Very Little Data",
        "summary":"Semantic parsing is an important NLP problem, particularly for voice assistants such as Alexa and Google Assistant. State-of-the-art (SOTA) semantic parsers are seq2seq architectures based on large language models that have been pretrained on vast amounts of text. To better leverage that pretraining, recent work has explored a reformulation of semantic parsing whereby the output sequences are themselves natural language sentences, but in a controlled fragment of natural language. This approach delivers strong results, particularly for few-shot semantic parsing, which is of key importance in practice and the focus of our paper. We push this line of work forward by introducing an automated methodology that delivers very significant additional improvements by utilizing modest amounts of unannotated data, which is typically easy to obtain. Our method is based on a novel synthesis of four techniques: joint training with auxiliary unsupervised tasks; constrained decoding; self-training; and paraphrasing. We show that this method delivers new SOTA few-shot performance on the Overnight dataset, particularly in very low-resource settings, and very compelling few-shot results on a new semantic parsing dataset.",
        "completion1":"State-of-the-art semantic parsers get significant boost from very little extra data, new study finds",
        "completion2":"New technique allows semantic parsers to better leverage large language models",
        "completion3":"Few-shot semantic parsing performance improved with new methodology",
        "technologyreview":0.2444667241,
        "venturebeat":0.2201724033,
        "wired":0.0684555803,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.14243v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1651252494000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2205.06714v1",
        "predicted_newsworthiness":0.4647950001,
        "title":"Learning Keypoints from Synthetic Data for Robotic Cloth Folding",
        "summary":"Robotic cloth manipulation is challenging due to its deformability, which makes determining its full state infeasible. However, for cloth folding, it suffices to know the position of a few semantic keypoints. Convolutional neural networks (CNN) can be used to detect these keypoints, but require large amounts of annotated data, which is expensive to collect. To overcome this, we propose to learn these keypoint detectors purely from synthetic data, enabling low-cost data collection. In this paper, we procedurally generate images of towels and use them to train a CNN. We evaluate the performance of this detector for folding towels on a unimanual robot setup and find that the grasp and fold success rates are 77% and 53%, respectively. We conclude that learning keypoint detectors from synthetic data for cloth folding and related tasks is a promising research direction, discuss some failures and relate them to future work. A video of the system, as well as the codebase, more details on the CNN architecture and the training setup can be found at https:\/\/github.com\/tlpss\/workshop-icra-2022-cloth-keypoints.git.",
        "completion1":"Robots Could Soon Be Folding Your Towels, Thanks To This New AI",
        "completion2":"Folding Towels with Robots just got a Whole Lot Easier",
        "completion3":"This New AI Will Help Robots Fold Your Towels Like a Pro",
        "technologyreview":0.2641056149,
        "venturebeat":0.1928805113,
        "wired":0.0460360071,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.06714v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai",
            "cs.lg"
        ],
        "published":1652456329000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2201.05820v1",
        "predicted_newsworthiness":0.4602613198,
        "title":"Offline-Online Associated Camera-Aware Proxies for Unsupervised Person Re-identification",
        "summary":"Recently, unsupervised person re-identification (Re-ID) has received increasing research attention due to its potential for label-free applications. A promising way to address unsupervised Re-ID is clustering-based, which generates pseudo labels by clustering and uses the pseudo labels to train a Re-ID model iteratively. However, most clustering-based methods take each cluster as a pseudo identity class, neglecting the intra-cluster variance mainly caused by the change of cameras. To address this issue, we propose to split each single cluster into multiple proxies according to camera views. The camera-aware proxies explicitly capture local structures within clusters, by which the intra-ID variance and inter-ID similarity can be better tackled. Assisted with the camera-aware proxies, we design two proxy-level contrastive learning losses that are, respectively, based on offline and online association results. The offline association directly associates proxies according to the clustering and splitting results, while the online strategy dynamically associates proxies in terms of up-to-date features to reduce the noise caused by the delayed update of pseudo labels. The combination of two losses enable us to train a desirable Re-ID model. Extensive experiments on three person Re-ID datasets and one vehicle Re-ID dataset show that our proposed approach demonstrates competitive performance with state-of-the-art methods. Code will be available at: https:\/\/github.com\/Terminator8758\/O2CAP.",
        "completion1":"New camera-aware proxies address intra-cluster variance for unsupervised person re-identification.",
        "completion2":"Proxy-level contrastive learning losses enable desirable Re-ID model training.",
        "completion3":"New method achieves competitive performance with state-of-the-art methods.",
        "technologyreview":0.1756238896,
        "venturebeat":0.1367200214,
        "wired":0.0353802631,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.05820v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1642241523000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2203.08656v1",
        "predicted_newsworthiness":0.3687683559,
        "title":"Learning Representation for Bayesian Optimization with Collision-free Regularization",
        "summary":"Bayesian optimization has been challenged by datasets with large-scale, high-dimensional, and non-stationary characteristics, which are common in real-world scenarios. Recent works attempt to handle such input by applying neural networks ahead of the classical Gaussian process to learn a latent representation. We show that even with proper network design, such learned representation often leads to collision in the latent space: two points with significantly different observations collide in the learned latent space, leading to degraded optimization performance. To address this issue, we propose LOCo, an efficient deep Bayesian optimization framework which employs a novel regularizer to reduce the collision in the learned latent space and encourage the mapping from the latent space to the objective value to be Lipschitz continuous. LOCo takes in pairs of data points and penalizes those too close in the latent space compared to their target space distance. We provide a rigorous theoretical justification for LOCo by inspecting the regret of this dynamic-embedding-based Bayesian optimization algorithm, where the neural network is iteratively retrained with the regularizer. Our empirical results demonstrate the effectiveness of LOCo on several synthetic and real-world benchmark Bayesian optimization tasks.",
        "completion1":"New deep Bayesian optimization framework LOCo aims to address the issue of learned representation leading to collision in latent space.",
        "completion2":"LOCo employs a novel regularizer to reduce collisions in the learned latent space and encourage Lipschitz continuity between the latent space and objective value.",
        "completion3":"Rigorous theoretical justification is provided for LOCo, with empirical results demonstrating effectiveness on several synthetic and real-world benchmark tasks.",
        "technologyreview":0.2569548378,
        "venturebeat":0.210416193,
        "wired":0.0583380169,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.08656v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1647441856000,
        "code_mentioned":1,
        "readability":0.76
    },
    {
        "arxiv_id":"2205.15501v1",
        "predicted_newsworthiness":0.4150261428,
        "title":"Multi-Entanglement Routing Design over Quantum Networks",
        "summary":"Quantum networks are considered as a promising future platform for quantum information exchange and quantum applications, which have capabilities far beyond the traditional communication networks. Remote quantum entanglement is an essential component of a quantum network. How to efficiently design a multi-routing entanglement protocol is a fundamental yet challenging problem. In this paper, we study a quantum entanglement routing problem to simultaneously maximize the number of quantum-user pairs and their expected throughput. Our approach is to formulate the problem as two sequential integer programming steps. We propose efficient entanglement routing algorithms for the two integer programming steps and analyze their time complexity and performance bounds. Results of evaluation highlight that our approach outperforms existing solutions in both served quantum-user pairs numbers and the network expected throughput.",
        "completion1":"A New Approach to Quantum Entanglement Routing Outperforms Existing Solutions",
        "completion2":"Maximizing the Number of Quantum-User Pairs and Their Expected Throughput",
        "completion3":"Efficient Entanglement Routing Algorithms for a Fundamental yet Challenging Problem",
        "technologyreview":0.1210448787,
        "venturebeat":0.0901623747,
        "wired":0.0119023493,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.15501v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1653961964000,
        "code_mentioned":0,
        "readability":0.8
    },
    {
        "arxiv_id":"2207.01494v1",
        "predicted_newsworthiness":0.5285064485,
        "title":"Auxiliary Task Guided Interactive Attention Model for Question Difficulty Prediction",
        "summary":"Online learning platforms conduct exams to evaluate the learners in a monotonous way, where the questions in the database may be classified into Bloom's Taxonomy as varying levels in complexity from basic knowledge to advanced evaluation. The questions asked in these exams to all learners are very much static. It becomes important to ask new questions with different difficulty levels to each learner to provide a personalized learning experience. In this paper, we propose a multi-task method with an interactive attention mechanism, Qdiff, for jointly predicting Bloom's Taxonomy and difficulty levels of academic questions. We model the interaction between the predicted bloom taxonomy representations and the input representations using an attention mechanism to aid in difficulty prediction. The proposed learning method would help learn representations that capture the relationship between Bloom's taxonomy and difficulty labels. The proposed multi-task method learns a good input representation by leveraging the relationship between the related tasks and can be used in similar settings where the tasks are related. The results demonstrate that the proposed method performs better than training only on difficulty prediction. However, Bloom's labels may not always be given for some datasets. Hence we soft label another dataset with a model fine-tuned to predict Bloom's labels to demonstrate the applicability of our method to datasets with only difficulty labels.",
        "completion1":"New multi-task method Qdiff can predict difficulty levels of academic questions.",
        "completion2":"Qdiff uses interactive attention mechanism to model relationship between Bloom's Taxonomy and difficulty labels.",
        "completion3":"Qdiff outperforms other methods for difficulty prediction, including those that only use a single task.",
        "technologyreview":0.2233321363,
        "venturebeat":0.1902166664,
        "wired":0.0380886937,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.01494v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy",
            "cs.cl"
        ],
        "published":1653422130000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2203.07544v2",
        "predicted_newsworthiness":0.3319594447,
        "title":"A Unified Framework for Rank-based Evaluation Metrics for Link Prediction in Knowledge Graphs",
        "summary":"The link prediction task on knowledge graphs without explicit negative triples in the training data motivates the usage of rank-based metrics. Here, we review existing rank-based metrics and propose desiderata for improved metrics to address lack of interpretability and comparability of existing metrics to datasets of different sizes and properties. We introduce a simple theoretical framework for rank-based metrics upon which we investigate two avenues for improvements to existing metrics via alternative aggregation functions and concepts from probability theory. We finally propose several new rank-based metrics that are more easily interpreted and compared accompanied by a demonstration of their usage in a benchmarking of knowledge graph embedding models.",
        "completion1":"A Unified Framework for Rank-based Evaluation Metrics for Link Prediction in Knowledge Graphs",
        "completion2":"Improved Metrics to Address Lack of Interpretability and Comparability of Existing Metrics",
        "completion3":"New Rank-based Metrics That Are More Easily Interpreted and Compared",
        "technologyreview":0.1746613208,
        "venturebeat":0.1533694858,
        "wired":0.0361213104,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.07544v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1647299386000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2206.02115v1",
        "predicted_newsworthiness":0.3650923961,
        "title":"Learning Binarized Graph Representations with Multi-faceted Quantization Reinforcement for Top-K Recommendation",
        "summary":"Learning vectorized embeddings is at the core of various recommender systems for user-item matching. To perform efficient online inference, representation quantization, aiming to embed the latent features by a compact sequence of discrete numbers, recently shows the promising potentiality in optimizing both memory and computation overheads. However, existing work merely focuses on numerical quantization whilst ignoring the concomitant information loss issue, which, consequently, leads to conspicuous performance degradation. In this paper, we propose a novel quantization framework to learn Binarized Graph Representations for Top-K Recommendation (BiGeaR). BiGeaR introduces multi-faceted quantization reinforcement at the pre-, mid-, and post-stage of binarized representation learning, which substantially retains the representation informativeness against embedding binarization. In addition to saving the memory footprint, BiGeaR further develops solid online inference acceleration with bitwise operations, providing alternative flexibility for the realistic deployment. The empirical results over five large real-world benchmarks show that BiGeaR achieves about 22%~40% performance improvement over the state-of-the-art quantization-based recommender system, and recovers about 95%~102% of the performance capability of the best full-precision counterpart with over 8x time and space reduction.",
        "completion1":"A New Way to Learn Binarized Graph Representations for Top-K Recommendation",
        "completion2":"BiGeaR: A Novel Quantization Framework for Top-K Recommendation",
        "completion3":"22%~40% Performance Improvement with BiGeaR Over State-of-the-Art",
        "technologyreview":0.1826375484,
        "venturebeat":0.1800027204,
        "wired":0.0621504443,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.02115v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1654415201000,
        "code_mentioned":1,
        "readability":0.74
    },
    {
        "arxiv_id":"2203.04831v1",
        "predicted_newsworthiness":0.4118210901,
        "title":"Automatic Language Identification for Celtic Texts",
        "summary":"Language identification is an important Natural Language Processing task. It has been thoroughly researched in the literature. However, some issues are still open. This work addresses the identification of the related low-resource languages on the example of the Celtic language family. This work's main goals were: (1) to collect the dataset of three Celtic languages; (2) to prepare a method to identify the languages from the Celtic family, i.e. to train a successful classification model; (3) to evaluate the influence of different feature extraction methods, and explore the applicability of the unsupervised models as a feature extraction technique; (4) to experiment with the unsupervised feature extraction on a reduced annotated set. We collected a new dataset including Irish, Scottish, Welsh and English records. We tested supervised models such as SVM and neural networks with traditional statistical features alongside the output of clustering, autoencoder, and topic modelling methods. The analysis showed that the unsupervised features could serve as a valuable extension to the n-gram feature vectors. It led to an improvement in performance for more entangled classes. The best model achieved a 98\\% F1 score and 97\\% MCC. The dense neural network consistently outperformed the SVM model. The low-resource languages are also challenging due to the scarcity of available annotated training data. This work evaluated the performance of the classifiers using the unsupervised feature extraction on the reduced labelled dataset to handle this issue. The results uncovered that the unsupervised feature vectors are more robust to the labelled set reduction. Therefore, they proved to help achieve comparable classification performance with much less labelled data.",
        "completion1":"Automatic Language Identification for Celtic Texts",
        "completion2":"Low-Resource Languages Are Challenging For Automatic Language Identification",
        "completion3":"Unsupervised Feature Extraction Proves To Help Achieve Comparable Classification Performance",
        "technologyreview":0.1815704255,
        "venturebeat":0.1548523,
        "wired":0.0173097672,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.04831v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1646841853000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2201.09828v1",
        "predicted_newsworthiness":0.4791200794,
        "title":"MMLatch: Bottom-up Top-down Fusion for Multimodal Sentiment Analysis",
        "summary":"Current deep learning approaches for multimodal fusion rely on bottom-up fusion of high and mid-level latent modality representations (late\/mid fusion) or low level sensory inputs (early fusion). Models of human perception highlight the importance of top-down fusion, where high-level representations affect the way sensory inputs are perceived, i.e. cognition affects perception. These top-down interactions are not captured in current deep learning models. In this work we propose a neural architecture that captures top-down cross-modal interactions, using a feedback mechanism in the forward pass during network training. The proposed mechanism extracts high-level representations for each modality and uses these representations to mask the sensory inputs, allowing the model to perform top-down feature masking. We apply the proposed model for multimodal sentiment recognition on CMU-MOSEI. Our method shows consistent improvements over the well established MulT and over our strong late fusion baseline, achieving state-of-the-art results.",
        "completion1":"MMLatch: Bottom-up Top-down Fusion for Multimodal Sentiment Analysis",
        "completion2":"Current deep learning approaches for multimodal fusion rely on bottom-up fusion of high and mid-level latent modality representations (late\/mid fusion) or low level sensory inputs (early fusion).",
        "completion3":"Models of human perception highlight the importance of top-down fusion, where high-level representations affect the way sensory inputs are perceived, i.e. cognition affects perception.",
        "technologyreview":0.2372180881,
        "venturebeat":0.1945506513,
        "wired":0.0770895331,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.09828v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1643046484000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2203.02475v1",
        "predicted_newsworthiness":0.4416480658,
        "title":"Cooperative Task and Motion Planning for Multi-Arm Assembly Systems",
        "summary":"Multi-robot assembly systems are becoming increasingly appealing in manufacturing due to their ability to automatically, flexibly, and quickly construct desired structural designs. However, effectively planning for these systems in a manner that ensures each robot is simultaneously productive, and not idle, is challenging due to (1) the close proximity that the robots must operate in to manipulate the structure and (2) the inherent structural partial orderings on when each part can be installed. In this paper, we present a task and motion planning framework that jointly plans safe, low-makespan plans for a team of robots to assemble complex spatial structures. Our framework takes a hierarchical approach that, at the high level, uses Mixed-integer Linear Programs to compute an abstract plan comprised of an allocation of robots to tasks subject to precedence constraints and, at the low level, builds on a state-of-the-art algorithm for Multi-Agent Path Finding to plan collision-free robot motions that realize this abstract plan. Critical to our approach is the inclusion of certain collision constraints and movement durations during high-level planning, which better informs the search for abstract plans that are likely to be both feasible and low-makespan while keeping the search tractable. We demonstrate our planning system on several challenging assembly domains with several (sometimes heterogeneous) robots with grippers or suction plates for assembling structures with up to 23 objects involving Lego bricks, bars, plates, or irregularly shaped blocks.",
        "completion1":"Cooperative Task and Motion Planning for Multi-Arm Assembly Systems.",
        "completion2":"Multi-robot assembly systems are becoming increasingly appealing in manufacturing due to their ability to automatically, flexibly, and quickly construct desired structural designs.",
        "completion3":"However, effectively planning for these systems in a manner that ensures each robot is simultaneously productive, and not idle, is challenging due to the close proximity that the robots must operate in to manipulate the structure and the inherent structural partial orderings on when each part can be installed.",
        "technologyreview":0.2000400841,
        "venturebeat":0.1583853848,
        "wired":0.0235509168,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.02475v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai"
        ],
        "published":1646417569000,
        "code_mentioned":0,
        "readability":0.85
    },
    {
        "arxiv_id":"2205.04507v1",
        "predicted_newsworthiness":0.4933201843,
        "title":"PinnerFormer: Sequence Modeling for User Representation at Pinterest",
        "summary":"Sequential models have become increasingly popular in powering personalized recommendation systems over the past several years. These approaches traditionally model a user's actions on a website as a sequence to predict the user's next action. While theoretically simplistic, these models are quite challenging to deploy in production, commonly requiring streaming infrastructure to reflect the latest user activity and potentially managing mutable data for encoding a user's hidden state. Here we introduce PinnerFormer, a user representation trained to predict a user's future long-term engagement using a sequential model of a user's recent actions. Unlike prior approaches, we adapt our modeling to a batch infrastructure via our new dense all-action loss, modeling long-term future actions instead of next action prediction. We show that by doing so, we significantly close the gap between batch user embeddings that are generated once a day and realtime user embeddings generated whenever a user takes an action. We describe our design decisions via extensive offline experimentation and ablations and validate the efficacy of our approach in A\/B experiments showing substantial improvements in Pinterest's user retention and engagement when comparing PinnerFormer against our previous user representation. PinnerFormer is deployed in production as of Fall 2021.",
        "completion1":"Pinterest Introduces PinnerFormer to Improve User Recommendations",
        "completion2":"PinnerFormer: a new way to recommend pins to users",
        "completion3":"Pinterest's PinnerFormer uses sequential models to predict user engagement",
        "technologyreview":0.2698534313,
        "venturebeat":0.2676525521,
        "wired":0.1173849262,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.04507v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1652120811000,
        "code_mentioned":0,
        "readability":0.85
    },
    {
        "arxiv_id":"2201.01537v2",
        "predicted_newsworthiness":0.4738739441,
        "title":"Few-shot Domain Adaptation for IMU Denoising",
        "summary":"Different application scenarios will cause IMU to exhibit different error characteristics which will cause trouble to robot application. However, most data processing methods need to be designed for specific scenario. To solve this problem, we propose a few-shot domain adaptation method. In this work, a domain adaptation framework is considered for denoising the IMU, a reconstitution loss is designed to improve domain adaptability. In addition, in order to further improve the adaptability in the case of limited data, a few-shot training strategy is adopted. In the experiment, we quantify our method on two datasets (EuRoC and TUM-VI) and two real robots (car and quadruped robot) with three different precision IMUs. According to the experimental results, the adaptability of our framework is verified by t-SNE. In orientation results, our proposed method shows the great denoising performance.",
        "completion1":"Few-shot Domain Adaptation for IMU Denoising offers great denoising performance",
        "completion2":"Few-shot Domain Adaptation for IMU Denoising improves domain adaptability",
        "completion3":"Few-shot Domain Adaptation for IMU Denoising is adopted to improve the adaptability in the case of limited data",
        "technologyreview":0.2023679678,
        "venturebeat":0.1527108467,
        "wired":0.015794285,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.01537v2",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1641379999000,
        "code_mentioned":0,
        "readability":0.83
    },
    {
        "arxiv_id":"2203.02586v1",
        "predicted_newsworthiness":0.5363369905,
        "title":"Concept-based Explanations for Out-Of-Distribution Detectors",
        "summary":"Out-of-distribution (OOD) detection plays a crucial role in ensuring the safe deployment of deep neural network (DNN) classifiers. While a myriad of methods have focused on improving the performance of OOD detectors, a critical gap remains in interpreting their decisions. We help bridge this gap by providing explanations for OOD detectors based on learned high-level concepts. We first propose two new metrics for assessing the effectiveness of a particular set of concepts for explaining OOD detectors: 1) detection completeness, which quantifies the sufficiency of concepts for explaining an OOD-detector's decisions, and 2) concept separability, which captures the distributional separation between in-distribution and OOD data in the concept space. Based on these metrics, we propose a framework for learning a set of concepts that satisfy the desired properties of detection completeness and concept separability and demonstrate the framework's effectiveness in providing concept-based explanations for diverse OOD techniques. We also show how to identify prominent concepts that contribute to the detection results via a modified Shapley value-based importance score.",
        "completion1":"New framework provides concept-based explanations for out-of-distribution detectors.",
        "completion2":"Framework helps bridge the gap in interpreting decisions made by out-of-distribution detectors.",
        "completion3":"Modification to Shapley value importance score helps identify prominent concepts that contribute to detection results.",
        "technologyreview":0.2998614244,
        "venturebeat":0.2576415687,
        "wired":0.0731338498,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.02586v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1646431900000,
        "code_mentioned":0,
        "readability":0.81
    },
    {
        "arxiv_id":"2203.15880v2",
        "predicted_newsworthiness":0.4882403886,
        "title":"Proactive Image Manipulation Detection",
        "summary":"Image manipulation detection algorithms are often trained to discriminate between images manipulated with particular Generative Models (GMs) and genuine\/real images, yet generalize poorly to images manipulated with GMs unseen in the training. Conventional detection algorithms receive an input image passively. By contrast, we propose a proactive scheme to image manipulation detection. Our key enabling technique is to estimate a set of templates which when added onto the real image would lead to more accurate manipulation detection. That is, a template protected real image, and its manipulated version, is better discriminated compared to the original real image vs. its manipulated one. These templates are estimated using certain constraints based on the desired properties of templates. For image manipulation detection, our proposed approach outperforms the prior work by an average precision of 16% for CycleGAN and 32% for GauGAN. Our approach is generalizable to a variety of GMs showing an improvement over prior work by an average precision of 10% averaged across 12 GMs. Our code is available at https:\/\/www.github.com\/vishal3477\/proactive_IMD.",
        "completion1":"New image manipulation detection technique could help curb fake news",
        "completion2":"Engineers develop proactive scheme to detect image tampering",
        "completion3":"Protected images' more accurately discriminate between real and manipulated versions, study finds",
        "technologyreview":0.2156784342,
        "venturebeat":0.1643142259,
        "wired":0.0309109327,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.15880v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1648584124000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2207.03815v1",
        "predicted_newsworthiness":0.5118098496,
        "title":"ErgoTac-Belt: Anticipatory Vibrotactile Feedback to Lead \\\\Centre of Pressure during Walking",
        "summary":"Balance and gait disorders are the second leading cause of falls, which, along with consequent injuries, are reported as major public health problems all over the world. For patients who do not require mechanical support, vibrotactile feedback interfaces have proven to be a successful approach in restoring balance. Most of the existing strategies assess trunk or head tilt and velocity or plantar forces, and are limited to the analysis of stance. On the other hand, central to balance control is the need to maintain the body's centre of pressure (CoP) within feasible limits of the support polygon (SP), as in standing, or on track to a new SP, as in walking. Hence, this paper proposes an exploratory study to investigate whether vibrotactile feedback can be employed to lead human CoP during walking. The ErgoTac-Belt vibrotactile device is introduced to instruct the users about the direction to take, both in the antero-posterior and medio-lateral axes. An anticipatory strategy is adopted here, to give the users enough time to react to the stimuli. Experiments on ten healthy subjects demonstrated the promising capability of the proposed device to guide the users' CoP along a predefined reference path, with similar performance as the one achieved with visual feedback. Future developments will investigate our strategy and device in guiding the CoP of elderly or individuals with vestibular impairments, who may not be aware of or, able to figure out, a safe and ergonomic CoP path.",
        "completion1":"ErgoTac-Belt: Anticipatory Vibrotactile Feedback to Lead Centre of Pressure during Walking",
        "completion2":"Vibrotactile feedback interfaces have proven to be a successful approach in restoring balance",
        "completion3":"The ErgoTac-Belt vibrotactile device is introduced to instruct the users about the direction to take, both in the antero-posterior and medio-lateral axes.",
        "technologyreview":0.1900775891,
        "venturebeat":0.1829383345,
        "wired":0.0385131368,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.03815v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ro"
        ],
        "published":1657277417000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2205.10667v4",
        "predicted_newsworthiness":0.4405091814,
        "title":"Individual Topology Structure of Eye Movement Trajectories",
        "summary":"Traditionally, extracting patterns from eye movement data relies on statistics of different macro-events such as fixations and saccades. This requires an additional preprocessing step to separate the eye movement subtypes, often with a number of parameters on which the classification results depend. Besides that, definitions of such macro events are formulated in different ways by different researchers. We propose an application of a new class of features to the quantitative analysis of personal eye movement trajectories structure. This new class of features based on algebraic topology allows extracting patterns from different modalities of gaze such as time series of coordinates and amplitudes, heatmaps, and point clouds in a unified way at all scales from micro to macro. We experimentally demonstrate the competitiveness of the new class of features with the traditional ones and their significant synergy while being used together for the person authentication task on the recently published eye movement trajectories dataset.",
        "completion1":"Individual Topology Structure of Eye Movement Trajectories Could Revolutionize Pattern Analysis",
        "completion2":"New Class of Features for Eye Movement Trajectories Allow for Unified Pattern Extraction Across Modalities",
        "completion3":"Algebraic Topology-Based Features Yield Competitive Results for Person Authentication",
        "technologyreview":0.2208565253,
        "venturebeat":0.1912824298,
        "wired":0.0583485263,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.10667v4",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.hc",
            "cs.lg"
        ],
        "published":1653165045000,
        "code_mentioned":0,
        "readability":0.82
    },
    {
        "arxiv_id":"2206.01524v2",
        "predicted_newsworthiness":0.4522947868,
        "title":"Anomaly detection in surveillance videos using transformer based attention model",
        "summary":"Surveillance footage can catch a wide range of realistic anomalies. This research suggests using a weakly supervised strategy to avoid annotating anomalous segments in training videos, which is time consuming. In this approach only video level labels are used to obtain frame level anomaly scores. Weakly supervised video anomaly detection (WSVAD) suffers from the wrong identification of abnormal and normal instances during the training process. Therefore it is important to extract better quality features from the available videos. WIth this motivation, the present paper uses better quality transformer-based features named Videoswin Features followed by the attention layer based on dilated convolution and self attention to capture long and short range dependencies in temporal domain. This gives us a better understanding of available videos. The proposed framework is validated on real-world dataset i.e. ShanghaiTech Campus dataset which results in competitive performance than current state-of-the-art methods. The model and the code are available at https:\/\/github.com\/kapildeshpande\/Anomaly-Detection-in-Surveillance-Videos",
        "completion1":"Surveillance footage can catch a wide range of realistic anomalies, research suggests.",
        "completion2":"Weakly supervised video anomaly detection suffers from the wrong identification of abnormal and normal instances during the training process, study finds.",
        "completion3":"Using transformer-based features and attention layer may improve video anomaly detection, new research suggests.",
        "technologyreview":0.2222000968,
        "venturebeat":0.1750158382,
        "wired":0.0514279846,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.01524v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1654258779000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2206.01634v1",
        "predicted_newsworthiness":0.4018073881,
        "title":"Reinforcement Learning with Neural Radiance Fields",
        "summary":"It is a long-standing problem to find effective representations for training reinforcement learning (RL) agents. This paper demonstrates that learning state representations with supervision from Neural Radiance Fields (NeRFs) can improve the performance of RL compared to other learned representations or even low-dimensional, hand-engineered state information. Specifically, we propose to train an encoder that maps multiple image observations to a latent space describing the objects in the scene. The decoder built from a latent-conditioned NeRF serves as the supervision signal to learn the latent space. An RL algorithm then operates on the learned latent space as its state representation. We call this NeRF-RL. Our experiments indicate that NeRF as supervision leads to a latent space better suited for the downstream RL tasks involving robotic object manipulations like hanging mugs on hooks, pushing objects, or opening doors. Video: https:\/\/dannydriess.github.io\/nerf-rl",
        "completion1":"Neural Radiance Fields (NeRFs) Can Improve Reinforcement Learning Performance",
        "completion2":"RL Algorithm that Operates on Learned Latent Space Shows Promise for Robotics Tasks",
        "completion3":"Encoder Mapping Images to Latent Space Could Help RL Agents",
        "technologyreview":0.2656883546,
        "venturebeat":0.1894902394,
        "wired":0.053415483,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.01634v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv",
            "cs.ro"
        ],
        "published":1654269728000,
        "code_mentioned":1,
        "readability":0.77
    },
    {
        "arxiv_id":"2207.01054v1",
        "predicted_newsworthiness":0.6579470119,
        "title":"Multi-aspect Multilingual and Cross-lingual Parliamentary Speech Analysis",
        "summary":"Parliamentary and legislative debate transcripts provide an exciting insight into elected politicians' opinions, positions, and policy preferences. They are interesting for political and social sciences as well as linguistics and natural language processing (NLP). Exiting research covers discussions within individual parliaments. In contrast, we apply advanced NLP methods to a joint and comparative analysis of six national parliaments (Bulgarian, Czech, French, Slovene, Spanish, and United Kingdom) between 2017 and 2020, whose transcripts are a part of the ParlaMint dataset collection. Using a uniform methodology, we analyze topics discussed, emotions, and sentiment. We assess if the age, gender, and political orientation of speakers can be detected from speeches. The results show some commonalities and many surprising differences among the analyzed countries.",
        "completion1":"Multi-aspect Multilingual and Cross-lingual Parliamentary Speech Analysis",
        "completion2":"A joint and comparative analysis of six national parliaments",
        "completion3":"The age, gender, and political orientation of speakers",
        "technologyreview":0.2523474062,
        "venturebeat":0.1875293006,
        "wired":0.0715736963,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.01054v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1656858692000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2201.06578v2",
        "predicted_newsworthiness":0.4085457716,
        "title":"Collapse by Conditioning: Training Class-conditional GANs with Limited Data",
        "summary":"Class-conditioning offers a direct means to control a Generative Adversarial Network (GAN) based on a discrete input variable. While necessary in many applications, the additional information provided by the class labels could even be expected to benefit the training of the GAN itself. On the contrary, we observe that class-conditioning causes mode collapse in limited data settings, where unconditional learning leads to satisfactory generative ability. Motivated by this observation, we propose a training strategy for class-conditional GANs (cGANs) that effectively prevents the observed mode-collapse by leveraging unconditional learning. Our training strategy starts with an unconditional GAN and gradually injects the class conditioning into the generator and the objective function. The proposed method for training cGANs with limited data results not only in stable training but also in generating high-quality images, thanks to the early-stage exploitation of the shared information across classes. We analyze the observed mode collapse problem in comprehensive experiments on four datasets. Our approach demonstrates outstanding results compared with state-of-the-art methods and established baselines. The code is available at https:\/\/github.com\/mshahbazi72\/transitional-cGAN",
        "completion1":"Collapse by Conditioning: Training Class-conditional GANs with Limited Data",
        "completion2":"Class-conditioning offers a direct means to control a Generative Adversarial Network",
        "completion3":"On the contrary, we observe that class-conditioning causes mode collapse in limited data settings",
        "technologyreview":0.2053470544,
        "venturebeat":0.1359939422,
        "wired":0.0425588216,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.06578v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1642445963000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2201.12664v1",
        "predicted_newsworthiness":0.5908522676,
        "title":"A Deep CNN Architecture with Novel Pooling Layer Applied to Two Sudanese Arabic Sentiment Datasets",
        "summary":"Arabic sentiment analysis has become an important research field in recent years. Initially, work focused on Modern Standard Arabic (MSA), which is the most widely-used form. Since then, work has been carried out on several different dialects, including Egyptian, Levantine and Moroccan. Moreover, a number of datasets have been created to support such work. However, up until now, less work has been carried out on Sudanese Arabic, a dialect which has 32 million speakers. In this paper, two new publicly available datasets are introduced, the 2-Class Sudanese Sentiment Dataset (SudSenti2) and the 3-Class Sudanese Sentiment Dataset (SudSenti3). Furthermore, a CNN architecture, SCM, is proposed, comprising five CNN layers together with a novel pooling layer, MMA, to extract the best features. This SCM+MMA model is applied to SudSenti2 and SudSenti3 with accuracies of 92.75% and 84.39%. Next, the model is compared to other deep learning classifiers and shown to be superior on these new datasets. Finally, the proposed model is applied to the existing Saudi Sentiment Dataset and to the MSA Hotel Arabic Review Dataset with accuracies 85.55% and 90.01%.",
        "completion1":"CNN model proposed for Sudanese Arabic sentiment analysis outperforms other classifiers",
        "completion2":"New datasets introduced for Sudanese Arabic sentiment analysis",
        "completion3":"SCM+MMA model proposed for Sudanese Arabic sentiment analysis",
        "technologyreview":0.2423252629,
        "venturebeat":0.1978953406,
        "wired":0.0636197517,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.12664v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1643492008000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2205.16001v2",
        "predicted_newsworthiness":0.4485970106,
        "title":"Cluster-based Evaluation of Automatically Generated Text",
        "summary":"While probabilistic language generators have improved dramatically over the last few years, the automatic evaluation metrics used to assess them have not kept pace with this progress. In the domain of language generation, a good metric must correlate highly with human judgements. Yet, with few exceptions, there is a lack of such metrics in the literature. In this work, we analyse the general paradigm of language generator evaluation. We first discuss the computational and qualitative issues with using automatic evaluation metrics that operate on probability distributions over strings, the backbone of most language generators. We then propose the use of distributions over clusters instead, where we cluster strings based on their text embeddings (obtained from a pretrained language model). While we find the biases introduced by this substitution to be quite strong, we observe that, empirically, this methodology leads to metric estimators with higher correlation with human judgements, while simultaneously reducing estimator variance. We finish the paper with a probing analysis, which leads us to conclude that -- by encoding syntactic- and coherence-level features of text, while ignoring surface-level features -- these clusters may simply be better equipped to evaluate state-of-the-art language models.",
        "completion1":"Cluster-based Evaluation of Automatically Generated Text",
        "completion2":"A Better Way to Evaluate Language Models",
        "completion3":"Evaluating Language Models with Syntactic and Coherence-level Features",
        "technologyreview":0.2248642699,
        "venturebeat":0.1939266332,
        "wired":0.0787880872,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.16001v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1654019929000,
        "code_mentioned":0,
        "readability":0.83
    },
    {
        "arxiv_id":"2205.01929v4",
        "predicted_newsworthiness":0.5450394062,
        "title":"Explain to Not Forget: Defending Against Catastrophic Forgetting with XAI",
        "summary":"The ability to continuously process and retain new information like we do naturally as humans is a feat that is highly sought after when training neural networks. Unfortunately, the traditional optimization algorithms often require large amounts of data available during training time and updates wrt. new data are difficult after the training process has been completed. In fact, when new data or tasks arise, previous progress may be lost as neural networks are prone to catastrophic forgetting. Catastrophic forgetting describes the phenomenon when a neural network completely forgets previous knowledge when given new information. We propose a novel training algorithm called training by explaining in which we leverage Layer-wise Relevance Propagation in order to retain the information a neural network has already learned in previous tasks when training on new data. The method is evaluated on a range of benchmark datasets as well as more complex data. Our method not only successfully retains the knowledge of old tasks within the neural networks but does so more resource-efficiently than other state-of-the-art solutions.",
        "completion1":"New training algorithm allows neural networks to retain information more effectively, preventing catastrophic forgetting.",
        "completion2":"This new algorithm could help neural networks keep pace with human learning.",
        "completion3":"Training by explaining: a novel algorithm for mitigating catastrophic forgetting in neural networks.",
        "technologyreview":0.3041007819,
        "venturebeat":0.2451507223,
        "wired":0.062682121,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.01929v4",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1651651249000,
        "code_mentioned":1,
        "readability":0.87
    },
    {
        "arxiv_id":"2206.15351v1",
        "predicted_newsworthiness":0.4631436201,
        "title":"Deep Learning to See: Towards New Foundations of Computer Vision",
        "summary":"The remarkable progress in computer vision over the last few years is, by and large, attributed to deep learning, fueled by the availability of huge sets of labeled data, and paired with the explosive growth of the GPU paradigm. While subscribing to this view, this book criticizes the supposed scientific progress in the field and proposes the investigation of vision within the framework of information-based laws of nature. Specifically, the present work poses fundamental questions about vision that remain far from understood, leading the reader on a journey populated by novel challenges resonating with the foundations of machine learning. The central thesis is that for a deeper understanding of visual computational processes, it is necessary to look beyond the applications of general purpose machine learning algorithms and focus instead on appropriate learning theories that take into account the spatiotemporal nature of the visual signal.",
        "completion1":"Deep Learning to See: Towards New Foundations of Computer Vision",
        "completion2":"The remarkable progress in computer vision over the last few years is, by and large, attributed to deep learning",
        "completion3":"This book criticizes the supposed scientific progress in the field and proposes the investigation of vision within the framework of information-based laws of nature.",
        "technologyreview":0.3393970199,
        "venturebeat":0.2444619138,
        "wired":0.0799138511,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.15351v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1656602436000,
        "code_mentioned":0,
        "readability":0.9
    },
    {
        "arxiv_id":"2202.06205v2",
        "predicted_newsworthiness":0.5228000843,
        "title":"StoryBuddy: A Human-AI Collaborative Chatbot for Parent-Child Interactive Storytelling with Flexible Parental Involvement",
        "summary":"Despite its benefits for children's skill development and parent-child bonding, many parents do not often engage in interactive storytelling by having story-related dialogues with their child due to limited availability or challenges in coming up with appropriate questions. While recent advances made AI generation of questions from stories possible, the fully-automated approach excludes parent involvement, disregards educational goals, and underoptimizes for child engagement. Informed by need-finding interviews and participatory design (PD) results, we developed StoryBuddy, an AI-enabled system for parents to create interactive storytelling experiences. StoryBuddy's design highlighted the need for accommodating dynamic user needs between the desire for parent involvement and parent-child bonding and the goal of minimizing parent intervention when busy. The PD revealed varied assessment and educational goals of parents, which StoryBuddy addressed by supporting configuring question types and tracking child progress. A user study validated StoryBuddy's usability and suggested design insights for future parent-AI collaboration systems.",
        "completion1":"StoryBuddy: A Human-AI Collaborative Chatbot for Parent-Child Interactive Storytelling",
        "completion2":"StoryBuddy: Aims to Minimize Parent Intervention When Busy",
        "completion3":"StoryBuddy: User Study Validates Usability and Suggests Design Insights",
        "technologyreview":0.3072607589,
        "venturebeat":0.2914004611,
        "wired":0.1118569833,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.06205v2",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ai",
            "cs.cl",
            "cs.lg"
        ],
        "published":1644728008000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2206.01589v1",
        "predicted_newsworthiness":0.4306498152,
        "title":"OdomBeyondVision: An Indoor Multi-modal Multi-platform Odometry Dataset Beyond the Visible Spectrum",
        "summary":"This paper presents a multimodal indoor odometry dataset, OdomBeyondVision, featuring multiple sensors across the different spectrum and collected with different mobile platforms. Not only does OdomBeyondVision contain the traditional navigation sensors, sensors such as IMUs, mechanical LiDAR, RGBD camera, it also includes several emerging sensors such as the single-chip mmWave radar, LWIR thermal camera and solid-state LiDAR. With the above sensors on UAV, UGV and handheld platforms, we respectively recorded the multimodal odometry data and their movement trajectories in various indoor scenes and different illumination conditions. We release the exemplar radar, radar-inertial and thermal-inertial odometry implementations to demonstrate their results for future works to compare against and improve upon. The full dataset including toolkit and documentation is publicly available at: https:\/\/github.com\/MAPS-Lab\/OdomBeyondVision.",
        "completion1":"OdomBeyondVision: An Indoor Multi-modal Multi-platform Odometry Dataset Beyond the Visible Spectrum",
        "completion2":"This paper presents a multimodal indoor odometry dataset, OdomBeyondVision, featuring multiple sensors across the different spectrum and collected with different mobile platforms.",
        "completion3":"With the above sensors on UAV, UGV and handheld platforms, we respectively recorded the multimodal odometry data and their movement trajectories in various indoor scenes and different illumination conditions.",
        "technologyreview":0.1872485818,
        "venturebeat":0.1965939973,
        "wired":0.0316639928,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.01589v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1654265980000,
        "code_mentioned":1,
        "readability":0.74
    },
    {
        "arxiv_id":"2203.15935v2",
        "predicted_newsworthiness":0.5407921815,
        "title":"Graph Neural Networks in IoT: A Survey",
        "summary":"The Internet of Things (IoT) boom has revolutionized almost every corner of people's daily lives: healthcare, home, transportation, manufacturing, supply chain, and so on. With the recent development of sensor and communication technologies, IoT devices including smart wearables, cameras, smartwatches, and autonomous vehicles can accurately measure and perceive their surrounding environment. Continuous sensing generates massive amounts of data and presents challenges for machine learning. Deep learning models (e.g., convolution neural networks and recurrent neural networks) have been extensively employed in solving IoT tasks by learning patterns from multi-modal sensory data. Graph Neural Networks (GNNs), an emerging and fast-growing family of neural network models, can capture complex interactions within sensor topology and have been demonstrated to achieve state-of-the-art results in numerous IoT learning tasks. In this survey, we present a comprehensive review of recent advances in the application of GNNs to the IoT field, including a deep dive analysis of GNN design in various IoT sensing environments, an overarching list of public data and source code from the collected publications, and future research directions. To keep track of newly published works, we collect representative papers and their open-source implementations and create a Github repository at https:\/\/github.com\/GuiminDong\/GNN4IoT.",
        "completion1":"Graph Neural Networks in IoT: A Survey",
        "completion2":"Graph Neural Networks in IoT: A Comprehensive Review",
        "completion3":"Graph Neural Networks in IoT: An Overview",
        "technologyreview":0.2635024752,
        "venturebeat":0.2411579687,
        "wired":0.0586052647,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.15935v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1648592879000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2203.02458v1",
        "predicted_newsworthiness":0.4754105909,
        "title":"Comprehension of Subtitles from Re-Translating Simultaneous Speech Translation",
        "summary":"In simultaneous speech translation, one can vary the size of the output window, system latency and sometimes the allowed level of rewriting. The effect of these properties on readability and comprehensibility has not been tested with modern neural translation systems. In this work, we propose an evaluation method and investigate the effects on comprehension and user preferences. It is a pilot study with 14 users on 2 hours of German documentaries or speeches with online translations into Czech. We collect continuous feedback and answers on factual questions. Our results show that the subtitling layout or flicker have a little effect on comprehension, in contrast to machine translation itself and individual competence. Other results show that users with a limited knowledge of the source language have different preferences to stability and latency than the users with zero knowledge. The results are statistically insignificant, however, we show that our method works and can be reproduced in larger volume.",
        "completion1":"Comprehension of subtitles from retranslation of simultaneous speech translation studied",
        "completion2":"User preferences for stability and latency in machine translation investigated",
        "completion3":"Little effect of subtitling layout or flicker on comprehension found",
        "technologyreview":0.226293175,
        "venturebeat":0.2024539172,
        "wired":0.0828456184,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.02458v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1646415699000,
        "code_mentioned":1,
        "readability":0.87
    },
    {
        "arxiv_id":"2202.05940v1",
        "predicted_newsworthiness":0.4970573482,
        "title":"Automatic Curriculum Generation for Learning Adaptation in Networking",
        "summary":"As deep reinforcement learning (RL) showcases its strengths in networking and systems, its pitfalls also come to the public's attention--when trained to handle a wide range of network workloads and previously unseen deployment environments, RL policies often manifest suboptimal performance and poor generalizability. To tackle these problems, we present Genet, a new training framework for learning better RL-based network adaptation algorithms. Genet is built on the concept of curriculum learning, which has proved effective against similar issues in other domains where RL is extensively employed. At a high level, curriculum learning gradually presents more difficult environments to the training, rather than choosing them randomly, so that the current RL model can make meaningful progress in training. However, applying curriculum learning in networking is challenging because it remains unknown how to measure the \"difficulty\" of a network environment. Instead of relying on handcrafted heuristics to determine the environment's difficulty level, our insight is to utilize traditional rule-based (non-RL) baselines: If the current RL model performs significantly worse in a network environment than the baselines, then the model's potential to improve when further trained in this environment is substantial. Therefore, Genet automatically searches for the environments where the current model falls significantly behind a traditional baseline scheme and iteratively promotes these environments as the training progresses. Through evaluating Genet on three use cases--adaptive video streaming, congestion control, and load balancing, we show that Genet produces RL policies which outperform both regularly trained RL policies and traditional baselines in each context, not only under synthetic workloads but also in real environments.",
        "completion1":"New training framework for deep reinforcement learning produces better policies for network adaptation.",
        "completion2":"Genet outperforms regularly trained RL policies and traditional baselines in three use cases.",
        "completion3":"New framework offers potential solution to deep reinforcement learning's suboptimal performance and generalizability issues.",
        "technologyreview":0.2938438597,
        "venturebeat":0.2566176386,
        "wired":0.0734080865,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.05940v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1644625093000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2201.11782v1",
        "predicted_newsworthiness":0.4700184517,
        "title":"An Empirical Analysis of Recurrent Learning Algorithms In Neural Lossy Image Compression Systems",
        "summary":"Recent advances in deep learning have resulted in image compression algorithms that outperform JPEG and JPEG 2000 on the standard Kodak benchmark. However, they are slow to train (due to backprop-through-time) and, to the best of our knowledge, have not been systematically evaluated on a large variety of datasets. In this paper, we perform the first large-scale comparison of recent state-of-the-art hybrid neural compression algorithms, while exploring the effects of alternative training strategies (when applicable). The hybrid recurrent neural decoder is a former state-of-the-art model (recently overtaken by a Google model) that can be trained using backprop-through-time (BPTT) or with alternative algorithms like sparse attentive backtracking (SAB), unbiased online recurrent optimization (UORO), and real-time recurrent learning (RTRL). We compare these training alternatives along with the Google models (GOOG and E2E) on 6 benchmark datasets. Surprisingly, we found that the model trained with SAB performs better (outperforming even BPTT), resulting in faster convergence and a better peak signal-to-noise ratio.",
        "completion1":"Backprop-through-time\" found to be slow in training image compression algorithms",
        "completion2":"New study finds alternative training strategies outperform backprop-through-time",
        "completion3":"Hybrid recurrent neural decoder found to be best performing image compression algorithm in large-scale comparison",
        "technologyreview":0.2650228126,
        "venturebeat":0.2113519304,
        "wired":0.0760972713,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.11782v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1643312871000,
        "code_mentioned":0,
        "readability":0.81
    },
    {
        "arxiv_id":"2207.09813v1",
        "predicted_newsworthiness":0.560983466,
        "title":"A Shared Autonomy Reconfigurable Control Framework for Telemanipulation of Multi-arm Systems",
        "summary":"Teleoperation is a widely adopted strategy to control robotic manipulators executing complex tasks that require highly dexterous movements and critical high-level intelligence. Classical teleoperation schemes are based on either joystick control, or on more intuitive interfaces which map directly the user arm motions into one robot arm's motions. These approaches have limits when the execution of a given task requires reconfigurable multiple robotic arm systems. Indeed, the simultaneous teleoperation of two or more robot arms could extend the workspace of the manipulation cell, or increase its total payload, or afford other advantages. In different phases of a reconfigurable multi-arm system, each robot could act as an independent arm, or as one of a pair of cooperating arms, or as one of the fingers of a virtual, large robot hand. This manuscript proposes a novel telemanipulation framework that enables both the individual and combined control of any number of robotic arms. Thanks to the designed control architecture, the human operator can intuitively choose the proposed control modalities and the manipulators that make the task convenient to execute through the user interface. Moreover, through the tele-impedance paradigm, the system can address complex tasks that require physical interaction by letting the robot mimic the arm impedance and position references of the human operator. The proposed framework is validated with 8 subjects controlling 4 Franka Emika Panda robots with 7-DoFs to execute a telemanipulation task. Qualitative results of the experiments show us the promising applicability of our framework.",
        "completion1":"A Shared Autonomy Reconfigurable Control Framework for Telemanipulation of Multi-arm Systems",
        "completion2":"A Novel Telemanipulation Framework That Enables Both the Individual and Combined Control of Any Number of Robotic Arms",
        "completion3":"The Proposed Framework is Validated with 8 Subjects Controlling 4 Franka Emika Panda Robots With 7-DoFs to Execute a Telemanipulation Task",
        "technologyreview":0.2629719834,
        "venturebeat":0.1975029085,
        "wired":0.042036124,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.09813v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658314510000,
        "code_mentioned":0,
        "readability":0.82
    },
    {
        "arxiv_id":"2203.00902v1",
        "predicted_newsworthiness":0.4897734239,
        "title":"Do Prompts Solve NLP Tasks Using Natural Language?",
        "summary":"Thanks to the advanced improvement of large pre-trained language models, prompt-based fine-tuning is shown to be effective on a variety of downstream tasks. Though many prompting methods have been investigated, it remains unknown which type of prompts are the most effective among three types of prompts (i.e., human-designed prompts, schema prompts and null prompts). In this work, we empirically compare the three types of prompts under both few-shot and fully-supervised settings. Our experimental results show that schema prompts are the most effective in general. Besides, the performance gaps tend to diminish when the scale of training data grows large.",
        "completion1":"Do Prompts Solve NLP Tasks Using Natural Language?",
        "completion2":"Prompt-Based Fine-Tuning Effective on Variety of Downstream Tasks",
        "completion3":"Schema Prompts Most Effective Among Three Types of Prompts",
        "technologyreview":0.2399372141,
        "venturebeat":0.2182011105,
        "wired":0.0608761401,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.00902v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1646205659000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2204.03017v1",
        "predicted_newsworthiness":0.3388411548,
        "title":"Learning from Untrimmed Videos: Self-Supervised Video Representation Learning with Hierarchical Consistency",
        "summary":"Natural videos provide rich visual contents for self-supervised learning. Yet most existing approaches for learning spatio-temporal representations rely on manually trimmed videos, leading to limited diversity in visual patterns and limited performance gain. In this work, we aim to learn representations by leveraging more abundant information in untrimmed videos. To this end, we propose to learn a hierarchy of consistencies in videos, i.e., visual consistency and topical consistency, corresponding respectively to clip pairs that tend to be visually similar when separated by a short time span and share similar topics when separated by a long time span. Specifically, a hierarchical consistency learning framework HiCo is presented, where the visually consistent pairs are encouraged to have the same representation through contrastive learning, while the topically consistent pairs are coupled through a topical classifier that distinguishes whether they are topic related. Further, we impose a gradual sampling algorithm for proposed hierarchical consistency learning, and demonstrate its theoretical superiority. Empirically, we show that not only HiCo can generate stronger representations on untrimmed videos, it also improves the representation quality when applied to trimmed videos. This is in contrast to standard contrastive learning that fails to learn appropriate representations from untrimmed videos.",
        "completion1":"Learning from Untrimmed Videos: Self-Supervised Video Representation Learning with Hierarchical Consistency",
        "completion2":"A hierarchical consistency learning framework HiCo is presented which encourages visually consistent pairs to have the same representation through contrastive learning",
        "completion3":"HiCo improves the representation quality when applied to trimmed videos in contrast to standard contrastive learning",
        "technologyreview":0.1745863361,
        "venturebeat":0.1210864758,
        "wired":0.0376567803,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.03017v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1649268294000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2202.08985v1",
        "predicted_newsworthiness":0.5127322951,
        "title":"Out of Distribution Data Detection Using Dropout Bayesian Neural Networks",
        "summary":"We explore the utility of information contained within a dropout based Bayesian neural network (BNN) for the task of detecting out of distribution (OOD) data. We first show how previous attempts to leverage the randomized embeddings induced by the intermediate layers of a dropout BNN can fail due to the distance metric used. We introduce an alternative approach to measuring embedding uncertainty, justify its use theoretically, and demonstrate how incorporating embedding uncertainty improves OOD data identification across three tasks: image classification, language classification, and malware detection.",
        "completion1":"Out of Distribution Detection Using Dropout Bayesian Neural Networks",
        "completion2":"Improved Out of Distribution Detection with Dropout Bayesian Neural Networks",
        "completion3":"Out of Distribution Data Detection with Dropout Bayesian Neural Networks",
        "technologyreview":0.2381201655,
        "venturebeat":0.2076822997,
        "wired":0.0506333802,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.08985v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1645151023000,
        "code_mentioned":1,
        "readability":0.76
    },
    {
        "arxiv_id":"2201.07877v2",
        "predicted_newsworthiness":0.3957934255,
        "title":"PDE-Based Optimal Strategy for Unconstrained Online Learning",
        "summary":"Unconstrained Online Linear Optimization (OLO) is a practical problem setting to study the training of machine learning models. Existing works proposed a number of potential-based algorithms, but in general the design of these potential functions relies heavily on guessing. To streamline this workflow, we present a framework that generates new potential functions by solving a Partial Differential Equation (PDE). Specifically, when losses are 1-Lipschitz, our framework produces a novel algorithm with anytime regret bound $C\\sqrt{T}+||u||\\sqrt{2T}[\\sqrt{\\log(1+||u||\/C)}+2]$, where $C$ is a user-specified constant and $u$ is any comparator unknown and unbounded a priori. Such a bound attains an optimal loss-regret trade-off without the impractical doubling trick. Moreover, a matching lower bound shows that the leading order term, including the constant multiplier $\\sqrt{2}$, is tight. To our knowledge, the proposed algorithm is the first to achieve such optimalities.",
        "completion1":"A New Way to Train Machine Learning Models",
        "completion2":"A More Efficient Method for Online Learning",
        "completion3":"A Framework for Generating Potential Functions",
        "technologyreview":0.2104955406,
        "venturebeat":0.172495215,
        "wired":0.0324619987,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.07877v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1642630881000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2201.12320v1",
        "predicted_newsworthiness":0.4533540272,
        "title":"Generative Cooperative Networks for Natural Language Generation",
        "summary":"Generative Adversarial Networks (GANs) have known a tremendous success for many continuous generation tasks, especially in the field of image generation. However, for discrete outputs such as language, optimizing GANs remains an open problem with many instabilities, as no gradient can be properly back-propagated from the discriminator output to the generator parameters. An alternative is to learn the generator network via reinforcement learning, using the discriminator signal as a reward, but such a technique suffers from moving rewards and vanishing gradient problems. Finally, it often falls short compared to direct maximum-likelihood approaches. In this paper, we introduce Generative Cooperative Networks, in which the discriminator architecture is cooperatively used along with the generation policy to output samples of realistic texts for the task at hand. We give theoretical guarantees of convergence for our approach, and study various efficient decoding schemes to empirically achieve state-of-the-art results in two main NLG tasks.",
        "completion1":"New Approach Achieves State-of-the-Art Results in Natural Language Generation",
        "completion2":"Generative Cooperative Networks outperform GANs for Natural Language Generation",
        "completion3":"Researchers find more efficient way to generate realistic texts",
        "technologyreview":0.2396802485,
        "venturebeat":0.1877094638,
        "wired":0.0617159122,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.12320v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cl"
        ],
        "published":1643395017000,
        "code_mentioned":0,
        "readability":0.82
    },
    {
        "arxiv_id":"2201.07540v2",
        "predicted_newsworthiness":0.4483582879,
        "title":"Virtual Coil Augmentation Technology for MR Coil Extrapolation via Deep Learning",
        "summary":"Magnetic resonance imaging (MRI) is a widely used medical imaging modality. However, due to the limitations in hardware, scan time, and throughput, it is often clinically challenging to obtain high-quality MR images. In this article, we propose a method of using artificial intelligence to expand the channel to achieve the goal of generating the virtual coils. The main characteristic of our work is utilizing dummy variable technology to expand\/extrapolate the receive coils in both image and k-space domains. The high-dimensional information formed by channel expansion is used as the prior information to improve the reconstruction effect of parallel imaging. Two main components are incorporated into the network design, namely variable augmentation technology and sum of squares (SOS) objective function. Variable augmentation provides the network with more high-dimensional prior information, which is helpful for the network to extract the deep feature information of the data. The SOS objective function is employed to solve the deficiency of k-space data training while speeding up convergence. Experimental results demonstrated its great potentials in super-resolution of MR images and accelerated parallel imaging reconstruction.",
        "completion1":"Artificial intelligence used to expand MR coils",
        "completion2":"Deep learning used to improve MR image quality",
        "completion3":"New technique speeds up convergence of parallel imaging",
        "technologyreview":0.2221231401,
        "venturebeat":0.1640811858,
        "wired":0.040685998,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.07540v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1642592018000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2207.08982v1",
        "predicted_newsworthiness":0.5339638044,
        "title":"Selection Bias Induced Spurious Correlations in Large Language Models",
        "summary":"In this work we show how large language models (LLMs) can learn statistical dependencies between otherwise unconditionally independent variables due to dataset selection bias. To demonstrate the effect, we developed a masked gender task that can be applied to BERT-family models to reveal spurious correlations between predicted gender pronouns and a variety of seemingly gender-neutral variables like date and location, on pre-trained (unmodified) BERT and RoBERTa large models. Finally, we provide an online demo, inviting readers to experiment further.",
        "completion1":"Selection Bias Induced Spurious Correlations in Large Language Models",
        "completion2":"How Large Language Models Can Learn Statistical Dependencies Between Otherwise Unconditionally Independent Variables",
        "completion3":"How to Use a Masked Gender Task to Reveal Spurious Correlations inBERT-Family Models",
        "technologyreview":0.2758019697,
        "venturebeat":0.2208645483,
        "wired":0.0702718348,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.08982v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1658187832000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2205.01904v1",
        "predicted_newsworthiness":0.5972139714,
        "title":"ImAiR : Airwriting Recognition framework using Image Representation of IMU Signals",
        "summary":"The problem of Airwriting Recognition is focused on identifying letters written by movement of finger in free space. It is a type of gesture recognition where the dictionary corresponds to letters in a specific language. In particular, airwriting recognition using sensor data from wrist-worn devices can be used as a medium of user input for applications in Human-Computer Interaction (HCI). Recognition of in-air trajectories using such wrist-worn devices is limited in literature and forms the basis of the current work. In this paper, we propose an airwriting recognition framework by first encoding the time-series data obtained from a wearable Inertial Measurement Unit (IMU) on the wrist as images and then utilizing deep learning-based models for identifying the written alphabets. The signals recorded from 3-axis accelerometer and gyroscope in IMU are encoded as images using different techniques such as Self Similarity Matrix (SSM), Gramian Angular Field (GAF) and Markov Transition Field (MTF) to form two sets of 3-channel images. These are then fed to two separate classification models and letter prediction is made based on an average of the class conditional probabilities obtained from the two models. Several standard model architectures for image classification such as variants of ResNet, DenseNet, VGGNet, AlexNet and GoogleNet have been utilized. Experiments performed on two publicly available datasets demonstrate the efficacy of the proposed strategy. The code for our implementation will be made available at https:\/\/github.com\/ayushayt\/ImAiR.",
        "completion1":"Airwriting Recognition framework using Image Representation of IMU Signals",
        "completion2":"In-air trajectories using such wrist-worn devices is limited in literature and forms the basis of the current work",
        "completion3":"The code for our implementation will be made available",
        "technologyreview":0.2963744503,
        "venturebeat":0.2591645896,
        "wired":0.0776882584,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.01904v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1651644634000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2204.06127v2",
        "predicted_newsworthiness":0.4065019639,
        "title":"Reinforcement learning on graphs: A survey",
        "summary":"Graph mining tasks arise from many different application domains, ranging from social networks, transportation to E-commerce, etc., which have been receiving great attention from the theoretical and algorithmic design communities in recent years, and there has been some pioneering work employing the research-rich Reinforcement Learning (RL) techniques to address graph data mining tasks. However, these graph mining methods and RL models are dispersed in different research areas, which makes it hard to compare them. In this survey, we provide a comprehensive overview of RL and graph mining methods and generalize these methods to Graph Reinforcement Learning (GRL) as a unified formulation. We further discuss the applications of GRL methods across various domains and summarize the method descriptions, open-source codes, and benchmark datasets of GRL methods. Furthermore, we propose important directions and challenges to be solved in the future. As far as we know, this is the latest work on a comprehensive survey of GRL, this work provides a global view and a learning resource for scholars. In addition, we create an online open-source for both interested scholars who want to enter this rapidly developing domain and experts who would like to compare GRL methods.",
        "completion1":"Reinforcement learning on graphs: A survey",
        "completion2":"Graph mining tasks from many different application domains",
        "completion3":"The importance of Reinforcement Learning in graph data mining",
        "technologyreview":0.2211858536,
        "venturebeat":0.2096592059,
        "wired":0.0388500949,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.06127v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1649813158000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2202.07376v1",
        "predicted_newsworthiness":0.4041856245,
        "title":"Deep-QPP: A Pairwise Interaction-based Deep Learning Model for Supervised Query Performance Prediction",
        "summary":"Motivated by the recent success of end-to-end deep neural models for ranking tasks, we present here a supervised end-to-end neural approach for query performance prediction (QPP). In contrast to unsupervised approaches that rely on various statistics of document score distributions, our approach is entirely data-driven. Further, in contrast to weakly supervised approaches, our method also does not rely on the outputs from different QPP estimators. In particular, our model leverages information from the semantic interactions between the terms of a query and those in the top-documents retrieved with it. The architecture of the model comprises multiple layers of 2D convolution filters followed by a feed-forward layer of parameters. Experiments on standard test collections demonstrate that our proposed supervised approach outperforms other state-of-the-art supervised and unsupervised approaches.",
        "completion1":"Deep-QPP: A Pairwise Interaction-based Deep Learning Model for Supervised Query Performance Prediction",
        "completion2":"Motivated by the recent success of end-to-end deep neural models for ranking tasks, we present here a supervised end-to-end neural approach for query performance prediction",
        "completion3":"In contrast to unsupervised approaches that rely on various statistics of document score distributions, our approach is entirely data-driven",
        "technologyreview":0.2206280533,
        "venturebeat":0.2107927182,
        "wired":0.0510330826,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.07376v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1644930256000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2207.08083v1",
        "predicted_newsworthiness":0.517403075,
        "title":"Towards Explainability in NLP: Analyzing and Calculating Word Saliency through Word Properties",
        "summary":"The wide use of black-box models in natural language processing brings great challenges to the understanding of the decision basis, the trustworthiness of the prediction results, and the improvement of the model performance. The words in text samples have properties that reflect their semantics and contextual information, such as the part of speech, the position, etc. These properties may have certain relationships with the word saliency, which is of great help for studying the explainability of the model predictions. In this paper, we explore the relationships between the word saliency and the word properties. According to the analysis results, we further establish a mapping model, Seq2Saliency, from the words in a text sample and their properties to the saliency values based on the idea of sequence tagging. In addition, we establish a new dataset called PrSalM, which contains each word in the text samples, the word properties, and the word saliency values. The experimental evaluations are conducted to analyze the saliency of words with different properties. The effectiveness of the Seq2Saliency model is verified.",
        "completion1":"New method to explain the 'black-box' of NLP models.",
        "completion2":"ow word properties affect a model's predictions.",
        "completion3":"he creation of a new dataset to help with explainability in NLP models.",
        "technologyreview":0.2395459769,
        "venturebeat":0.2184437391,
        "wired":0.0655623643,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.08083v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658037768000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2202.10581v1",
        "predicted_newsworthiness":0.3246565532,
        "title":"Unleashing the Power of Transformer for Graphs",
        "summary":"Despite recent successes in natural language processing and computer vision, Transformer suffers from the scalability problem when dealing with graphs. The computational complexity is unacceptable for large-scale graphs, e.g., knowledge graphs. One solution is to consider only the near neighbors, which, however, will lose the key merit of Transformer to attend to the elements at any distance. In this paper, we propose a new Transformer architecture, named dual-encoding Transformer (DET). DET has a structural encoder to aggregate information from connected neighbors and a semantic encoder to focus on semantically useful distant nodes. In comparison with resorting to multi-hop neighbors, DET seeks the desired distant neighbors via self-supervised training. We further find these two encoders can be incorporated to boost each others' performance. Our experiments demonstrate DET has achieved superior performance compared to the respective state-of-the-art methods in dealing with molecules, networks and knowledge graphs with various sizes.",
        "completion1":"Researchers Develop 'Dual-Encoding Transformer' to Improve Graph Scalability.",
        "completion2":"New Transformer Architecture Achieves Superior Performance on Molecules, Networks and Knowledge Graphs.",
        "completion3":"Researchers Unleash the Power of Transformer for Graphs with New Architecture.",
        "technologyreview":0.2280045094,
        "venturebeat":0.1913117667,
        "wired":0.053977603,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.10581v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1645166451000,
        "code_mentioned":0,
        "readability":0.78
    },
    {
        "arxiv_id":"2205.04638v2",
        "predicted_newsworthiness":0.5791942603,
        "title":"Using Frequency Attention to Make Adversarial Patch Powerful Against Person Detector",
        "summary":"Deep neural networks (DNNs) are vulnerable to adversarial attacks. In particular, object detectors may be attacked by applying a particular adversarial patch to the image. However, because the patch shrinks during preprocessing, most existing approaches that employ adversarial patches to attack object detectors would diminish the attack success rate on small and medium targets. This paper proposes a Frequency Module(FRAN), a frequency-domain attention module for guiding patch generation. This is the first study to introduce frequency domain attention to optimize the attack capabilities of adversarial patches. Our method increases the attack success rates of small and medium targets by 4.18% and 3.89%, respectively, over the state-of-the-art attack method for fooling the human detector while assaulting YOLOv3 without reducing the attack success rate of big targets.",
        "completion1":"Using Frequency Attention to Make Adversarial Patch Powerful Against Person Detector",
        "completion2":"Deep neural networks (DNNs) are vulnerable to adversarial attacks",
        "completion3":"Object detectors may be attacked by applying a particular adversarial patch to the image",
        "technologyreview":0.2883451904,
        "venturebeat":0.2144944012,
        "wired":0.0793705559,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.04638v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1652151263000,
        "code_mentioned":0,
        "readability":0.82
    },
    {
        "arxiv_id":"2207.04997v2",
        "predicted_newsworthiness":0.3988585571,
        "title":"A Closer Look at Invariances in Self-supervised Pre-training for 3D Vision",
        "summary":"Self-supervised pre-training for 3D vision has drawn increasing research interest in recent years. In order to learn informative representations, a lot of previous works exploit invariances of 3D features, e.g., perspective-invariance between views of the same scene, modality-invariance between depth and RGB images, format-invariance between point clouds and voxels. Although they have achieved promising results, previous researches lack a systematic and fair comparison of these invariances. To address this issue, our work, for the first time, introduces a unified framework, under which various pre-training methods can be investigated. We conduct extensive experiments and provide a closer look at the contributions of different invariances in 3D pre-training. Also, we propose a simple but effective method that jointly pre-trains a 3D encoder and a depth map encoder using contrastive learning. Models pre-trained with our method gain significant performance boost in downstream tasks. For instance, a pre-trained VoteNet outperforms previous methods on SUN RGB-D and ScanNet object detection benchmarks with a clear margin.",
        "completion1":"A Unified Framework for 3D Pre-training.",
        "completion2":"The Importance of Invariances in 3D Pre-training.",
        "completion3":"A Simple but Effective Method for Joint Pre-training of 3D Encoders and Depth Map Encoders.",
        "technologyreview":0.214383973,
        "venturebeat":0.1870160448,
        "wired":0.0590747132,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.04997v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1657557855000,
        "code_mentioned":0,
        "readability":0.85
    },
    {
        "arxiv_id":"2207.04163v1",
        "predicted_newsworthiness":0.4377551722,
        "title":"Optimizing Bipedal Maneuvers of Single Rigid-Body Models for Reinforcement Learning",
        "summary":"In this work, we propose a method to generate reduced-order model reference trajectories for general classes of highly dynamic maneuvers for bipedal robots for use in sim-to-real reinforcement learning. Our approach is to utilize a single rigid-body model (SRBM) to optimize libraries of trajectories offline to be used as expert references in the reward function of a learned policy. This method translates the model's dynamically rich rotational and translational behaviour to a full-order robot model and successfully transfers to real hardware. The SRBM's simplicity allows for fast iteration and refinement of behaviors, while the robustness of learning-based controllers allows for highly dynamic motions to be transferred to hardware. % Within this work we introduce a set of transferability constraints that amend the SRBM dynamics to actual bipedal robot hardware, our framework for creating optimal trajectories for dynamic stepping, turning maneuvers and jumps as well as our approach to integrating reference trajectories to a reinforcement learning policy. Within this work we introduce a set of transferability constraints that amend the SRBM dynamics to actual bipedal robot hardware, our framework for creating optimal trajectories for a variety of highly dynamic maneuvers as well as our approach to integrating reference trajectories for a high-speed running reinforcement learning policy. We validate our methods on the bipedal robot Cassie on which we were successfully able to demonstrate highly dynamic grounded running gaits up to 3.0 m\/s.",
        "completion1":"Optimizing Bipedal Maneuvers of Single Rigid-Body Models for Reinforcement Learning",
        "completion2":"Generating Reduced-Order Model Reference Trajectories for Reinforcement Learning",
        "completion3":"Transferring Highly Dynamic Motions to Hardware Using a Learning-Based Controller",
        "technologyreview":0.2061335444,
        "venturebeat":0.1625251938,
        "wired":0.032952508,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.04163v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1657326319000,
        "code_mentioned":1,
        "readability":0.77
    },
    {
        "arxiv_id":"2201.13033v1",
        "predicted_newsworthiness":0.4311189605,
        "title":"Integrated Decision Control Approach for Cooperative Safety-Critical Payload Transport in a Cluttered Environment",
        "summary":"In this paper, the problem of coordinated transportation of heavy payload by a team of UAVs in a cluttered environment is addressed. The payload is modeled as a rigid body and is assumed to track a pre-computed global flight trajectory from a start point to a goal point. Due to the presence of local dynamic obstacles in the environment, the UAVs must ensure that there is no collision between the payload and these obstacles while ensuring that the payload oscillations are kept minimum. An Integrated Decision Controller (IDC) is proposed, that integrates the optimal tracking control law given by a centralized Model Predictive Controller with safety-critical constraints provided by the Exponential Control Barrier Functions. The entire payload-UAV system is enclosed by a safe convex hull boundary, and the IDC ensures that no obstacle enters this boundary. To evaluate the performance of the IDC, the results for a numerical simulation as well as a high-fidelity Gazebo simulation are presented. An ablation study is conducted to analyze the robustness of the proposed IDC against practical dubieties like noisy state values, relative obstacle safety margin, and payload mass uncertainty. The results clearly show that the IDC achieves both trajectory tracking and obstacle avoidance successfully while restricting the payload oscillations within a safe limit.",
        "completion1":"New Approach for Cooperative Safety-Critical Payload Transport in a Cluttered Environment",
        "completion2":"IDC Achieves Trajectory Tracking and Obstacle Avoidance Successfully",
        "completion3":"Ablation Study Analyzes Robustness of IDC Against Practical Dubieties",
        "technologyreview":0.154797284,
        "venturebeat":0.1063548475,
        "wired":0.0042004291,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.13033v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1643614040000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2201.01409v1",
        "predicted_newsworthiness":0.4804325233,
        "title":"Towards Understanding Quality Challenges of the Federated Learning: A First Look from the Lens of Robustness",
        "summary":"Federated learning (FL) is a widely adopted distributed learning paradigm in practice, which intends to preserve users' data privacy while leveraging the entire dataset of all participants for training. In FL, multiple models are trained independently on the users and aggregated centrally to update a global model in an iterative process. Although this approach is excellent at preserving privacy by design, FL still tends to suffer from quality issues such as attacks or byzantine faults. Some recent attempts have been made to address such quality challenges on the robust aggregation techniques for FL. However, the effectiveness of state-of-the-art (SOTA) robust FL techniques is still unclear and lacks a comprehensive study. Therefore, to better understand the current quality status and challenges of these SOTA FL techniques in the presence of attacks and faults, in this paper, we perform a large-scale empirical study to investigate the SOTA FL's quality from multiple angles of attacks, simulated faults (via mutation operators), and aggregation (defense) methods. In particular, we perform our study on two generic image datasets and one real-world federated medical image dataset. We also systematically investigate the effect of the distribution of attacks\/faults over users and the independent and identically distributed (IID) factors, per dataset, on the robustness results. After a large-scale analysis with 496 configurations, we find that most mutators on each individual user have a negligible effect on the final model. Moreover, choosing the most robust FL aggregator depends on the attacks and datasets. Finally, we illustrate that it is possible to achieve a generic solution that works almost as well or even better than any single aggregator on all attacks and configurations with a simple ensemble model of aggregators.",
        "completion1":"Towards Understanding Quality Challenges of the Federated Learning: A First Look from the Lens of Robustness",
        "completion2":"Federated learning still suffers from quality issues such as attacks or byzantine faults",
        "completion3":"Most mutators on each individual user have a negligible effect on the final model",
        "technologyreview":0.2815730114,
        "venturebeat":0.2505113953,
        "wired":0.0596122778,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.01409v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1641348399000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2203.08243v1",
        "predicted_newsworthiness":0.4337916495,
        "title":"Unified Visual Transformer Compression",
        "summary":"Vision transformers (ViTs) have gained popularity recently. Even without customized image operators such as convolutions, ViTs can yield competitive performance when properly trained on massive data. However, the computational overhead of ViTs remains prohibitive, due to stacking multi-head self-attention modules and else. Compared to the vast literature and prevailing success in compressing convolutional neural networks, the study of Vision Transformer compression has also just emerged, and existing works focused on one or two aspects of compression. This paper proposes a unified ViT compression framework that seamlessly assembles three effective techniques: pruning, layer skipping, and knowledge distillation. We formulate a budget-constrained, end-to-end optimization framework, targeting jointly learning model weights, layer-wise pruning ratios\/masks, and skip configurations, under a distillation loss. The optimization problem is then solved using the primal-dual algorithm. Experiments are conducted with several ViT variants, e.g. DeiT and T2T-ViT backbones on the ImageNet dataset, and our approach consistently outperforms recent competitors. For example, DeiT-Tiny can be trimmed down to 50\\% of the original FLOPs almost without losing accuracy. Codes are available online:~\\url{https:\/\/github.com\/VITA-Group\/UVC}.",
        "completion1":"Unified ViT Compression Framework Outperforms Competitors",
        "completion2":"DeiT-Tiny Trimmed Down to 50% FLOPs With No Accuracy Loss",
        "completion3":"New Approach to Vision Transformer Compression",
        "technologyreview":0.2121139795,
        "venturebeat":0.1786562151,
        "wired":0.0580729079,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.08243v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1647376702000,
        "code_mentioned":1,
        "readability":0.75
    },
    {
        "arxiv_id":"2204.13630v2",
        "predicted_newsworthiness":0.4040074456,
        "title":"Rotationally Equivariant 3D Object Detection",
        "summary":"Rotation equivariance has recently become a strongly desired property in the 3D deep learning community. Yet most existing methods focus on equivariance regarding a global input rotation while ignoring the fact that rotation symmetry has its own spatial support. Specifically, we consider the object detection problem in 3D scenes, where an object bounding box should be equivariant regarding the object pose, independent of the scene motion. This suggests a new desired property we call object-level rotation equivariance. To incorporate object-level rotation equivariance into 3D object detectors, we need a mechanism to extract equivariant features with local object-level spatial support while being able to model cross-object context information. To this end, we propose Equivariant Object detection Network (EON) with a rotation equivariance suspension design to achieve object-level equivariance. EON can be applied to modern point cloud object detectors, such as VoteNet and PointRCNN, enabling them to exploit object rotation symmetry in scene-scale inputs. Our experiments on both indoor scene and autonomous driving datasets show that significant improvements are obtained by plugging our EON design into existing state-of-the-art 3D object detectors.",
        "completion1":"Rotationally Equivariant 3D Object Detection Proves Effective for Indoor Scene and Autonomous Driving Datasets",
        "completion2":"New 'Equivariant Object Detection Network' Achieves Object-Level Rotation Equivariance",
        "completion3":"EON Design Plugged Into Existing State-of-the-Art 3D Object Detectors Show Significant Improvements",
        "technologyreview":0.2064100326,
        "venturebeat":0.1739470015,
        "wired":0.0478841466,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.13630v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1651164530000,
        "code_mentioned":1,
        "readability":0.78
    },
    {
        "arxiv_id":"2206.05394v1",
        "predicted_newsworthiness":0.4481365021,
        "title":"Applications of Deep Learning in Fish Habitat Monitoring: A Tutorial and Survey",
        "summary":"Marine ecosystems and their fish habitats are becoming increasingly important due to their integral role in providing a valuable food source and conservation outcomes. Due to their remote and difficult to access nature, marine environments and fish habitats are often monitored using underwater cameras. These cameras generate a massive volume of digital data, which cannot be efficiently analysed by current manual processing methods, which involve a human observer. DL is a cutting-edge AI technology that has demonstrated unprecedented performance in analysing visual data. Despite its application to a myriad of domains, its use in underwater fish habitat monitoring remains under explored. In this paper, we provide a tutorial that covers the key concepts of DL, which help the reader grasp a high-level understanding of how DL works. The tutorial also explains a step-by-step procedure on how DL algorithms should be developed for challenging applications such as underwater fish monitoring. In addition, we provide a comprehensive survey of key deep learning techniques for fish habitat monitoring including classification, counting, localization, and segmentation. Furthermore, we survey publicly available underwater fish datasets, and compare various DL techniques in the underwater fish monitoring domains. We also discuss some challenges and opportunities in the emerging field of deep learning for fish habitat processing. This paper is written to serve as a tutorial for marine scientists who would like to grasp a high-level understanding of DL, develop it for their applications by following our step-by-step tutorial, and see how it is evolving to facilitate their research efforts. At the same time, it is suitable for computer scientists who would like to survey state-of-the-art DL-based methodologies for fish habitat monitoring.",
        "completion1":"Deep Learning Achieves Unprecedented Performance in Marine Habitat Monitoring",
        "completion2":"New Tutorial Explains How to Use Deep Learning for Marine Habitat Monitoring",
        "completion3":"Comprehensive Survey of Key Deep Learning Techniques for Fish Habitat Monitoring",
        "technologyreview":0.2792573713,
        "venturebeat":0.2282781891,
        "wired":0.04999489,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.05394v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1654912794000,
        "code_mentioned":1,
        "readability":0.88
    },
    {
        "arxiv_id":"2204.02285v1",
        "predicted_newsworthiness":0.4231400087,
        "title":"SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in Visual Question Answering",
        "summary":"While Visual Question Answering (VQA) has progressed rapidly, previous works raise concerns about robustness of current VQA models. In this work, we study the robustness of VQA models from a novel perspective: visual context. We suggest that the models over-rely on the visual context, i.e., irrelevant objects in the image, to make predictions. To diagnose the model's reliance on visual context and measure their robustness, we propose a simple yet effective perturbation technique, SwapMix. SwapMix perturbs the visual context by swapping features of irrelevant context objects with features from other objects in the dataset. Using SwapMix we are able to change answers to more than 45 % of the questions for a representative VQA model. Additionally, we train the models with perfect sight and find that the context over-reliance highly depends on the quality of visual representations. In addition to diagnosing, SwapMix can also be applied as a data augmentation strategy during training in order to regularize the context over-reliance. By swapping the context object features, the model reliance on context can be suppressed effectively. Two representative VQA models are studied using SwapMix: a co-attention model MCAN and a large-scale pretrained model LXMERT. Our experiments on the popular GQA dataset show the effectiveness of SwapMix for both diagnosing model robustness and regularizing the over-reliance on visual context. The code for our method is available at https:\/\/github.com\/vipulgupta1011\/swapmix",
        "completion1":"A new technique called SwapMix can change answers to more than 45% of questions for a representative VQA model, by perturbing the visual context.",
        "completion2":"SwapMix can be applied as a data augmentation strategy during training in order to regularize the context over-reliance.",
        "completion3":"The code for our method is available.",
        "technologyreview":0.2395848826,
        "venturebeat":0.1872679937,
        "wired":0.0557827716,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.02285v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.cl",
            "cs.lg"
        ],
        "published":1649172745000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2201.04895v1",
        "predicted_newsworthiness":0.3825267382,
        "title":"Solving Dynamic Graph Problems with Multi-Attention Deep Reinforcement Learning",
        "summary":"Graph problems such as traveling salesman problem, or finding minimal Steiner trees are widely studied and used in data engineering and computer science. Typically, in real-world applications, the features of the graph tend to change over time, thus, finding a solution to the problem becomes challenging. The dynamic version of many graph problems are the key for a plethora of real-world problems in transportation, telecommunication, and social networks. In recent years, using deep learning techniques to find heuristic solutions for NP-hard graph combinatorial problems has gained much interest as these learned heuristics can find near-optimal solutions efficiently. However, most of the existing methods for learning heuristics focus on static graph problems. The dynamic nature makes NP-hard graph problems much more challenging to learn, and the existing methods fail to find reasonable solutions. In this paper, we propose a novel architecture named Graph Temporal Attention with Reinforcement Learning (GTA-RL) to learn heuristic solutions for graph-based dynamic combinatorial optimization problems. The GTA-RL architecture consists of an encoder capable of embedding temporal features of a combinatorial problem instance and a decoder capable of dynamically focusing on the embedded features to find a solution to a given combinatorial problem instance. We then extend our architecture to learn heuristics for the real-time version of combinatorial optimization problems where all input features of a problem are not known a prior, but rather learned in real-time. Our experimental results against several state-of-the-art learning-based algorithms and optimal solvers demonstrate that our approach outperforms the state-of-the-art learning-based approaches in terms of effectiveness and optimal solvers in terms of efficiency on dynamic and real-time graph combinatorial optimization.",
        "completion1":"GTA-RL: A novel deep learning architecture for solving dynamic graph problems",
        "completion2":"Deep learning methods for finding heuristic solutions to NP-hard graph problems",
        "completion3":"Graph Temporal Attention with Reinforcement Learning (GTA-RL) outperforms state-of-the-art methods in terms of effectiveness and efficiency.",
        "technologyreview":0.2579702228,
        "venturebeat":0.2315594934,
        "wired":0.061100325,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.04895v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1642073765000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2204.06407v1",
        "predicted_newsworthiness":0.4658705301,
        "title":"Flexible Multiple-Objective Reinforcement Learning for Chip Placement",
        "summary":"Recently, successful applications of reinforcement learning to chip placement have emerged. Pretrained models are necessary to improve efficiency and effectiveness. Currently, the weights of objective metrics (e.g., wirelength, congestion, and timing) are fixed during pretraining. However, fixed-weighed models cannot generate the diversity of placements required for engineers to accommodate changing requirements as they arise. This paper proposes flexible multiple-objective reinforcement learning (MORL) to support objective functions with inference-time variable weights using just a single pretrained model. Our macro placement results show that MORL can generate the Pareto frontier of multiple objectives effectively.",
        "completion1":"Reinforcement Learning Achieves Success in Chip Placement",
        "completion2":"Pretrained Models Improve Efficiency and Effectiveness of Chip Placement",
        "completion3":"Flexible Multiple-Objective Reinforcement Learning Generates Pareto Frontier for Multiple Objectives",
        "technologyreview":0.203026395,
        "venturebeat":0.1790849224,
        "wired":0.028188104,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.06407v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1649859259000,
        "code_mentioned":0,
        "readability":0.81
    },
    {
        "arxiv_id":"2204.12143v2",
        "predicted_newsworthiness":0.4427219454,
        "title":"Deeper Insights into ViTs Robustness towards Common Corruptions",
        "summary":"Recent literature have shown design strategies from Convolutions Neural Networks (CNNs) benefit Vision Transformers (ViTs) in various vision tasks. However, it remains unclear how these design choices impact on robustness when transferred to ViTs. In this paper, we make the first attempt to investigate how CNN-like architectural designs and CNN-based data augmentation strategies impact on ViTs' robustness towards common corruptions through an extensive and rigorous benchmarking. We demonstrate that overlapping patch embedding and convolutional Feed-Forward Network (FFN) boost performance on robustness. Furthermore, adversarial noise training is powerful on ViTs while fourier-domain augmentation fails. Moreover, we introduce a novel conditional method enabling input-varied augmentations from two angles: (1) Generating dynamic augmentation parameters conditioned on input images. It conduces to state-of-the-art performance on robustness through conditional convolutions; (2) Selecting most suitable augmentation strategy by an extra predictor helps to achieve the best trade-off between clean accuracy and robustness.",
        "completion1":"ViTs Robustness Boosted by CNN-Like Architectural Designs",
        "completion2":"ViTs Robustness Boosted by CNN-Based Data Augmentation Strategies",
        "completion3":"ViTs Robustness Boosted by Conditional Method",
        "technologyreview":0.2828217128,
        "venturebeat":0.2150872803,
        "wired":0.073598712,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.12143v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1650961354000,
        "code_mentioned":1,
        "readability":0.75
    },
    {
        "arxiv_id":"2204.10080v1",
        "predicted_newsworthiness":0.7089304956,
        "title":"Identifying and Characterizing Active Citizens who Refute Misinformation in Social Media",
        "summary":"The phenomenon of misinformation spreading in social media has developed a new form of active citizens who focus on tackling the problem by refuting posts that might contain misinformation. Automatically identifying and characterizing the behavior of such active citizens in social media is an important task in computational social science for complementing studies in misinformation analysis. In this paper, we study this task across different social media platforms (i.e., Twitter and Weibo) and languages (i.e., English and Chinese) for the first time. To this end, (1) we develop and make publicly available a new dataset of Weibo users mapped into one of the two categories (i.e., misinformation posters or active citizens); (2) we evaluate a battery of supervised models on our new Weibo dataset and an existing Twitter dataset which we repurpose for the task; and (3) we present an extensive analysis of the differences in language use between the two user categories.",
        "completion1":"A new study finds that there is a distinct difference in the way that social media users who spread misinformation vs. those who refute it communicate.",
        "completion2":"The study, which analyzed data from Twitter and Weibo, found that active citizens who refute misinformation are more likely to use certain language cues than those who spread it.",
        "completion3":"This is the first study to analyze the behavior of active citizens across different social media platforms and languages.",
        "technologyreview":0.3319978246,
        "venturebeat":0.2421850752,
        "wired":0.0694170501,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.10080v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.si"
        ],
        "published":1650547368000,
        "code_mentioned":1,
        "readability":0.87
    },
    {
        "arxiv_id":"2203.04446v1",
        "predicted_newsworthiness":0.5150889315,
        "title":"Tune your Place Recognition: Self-Supervised Domain Calibration via Robust SLAM",
        "summary":"Visual place recognition techniques based on deep learning, which have imposed themselves as the state-of-the-art in recent years, do not always generalize well to environments that are visually different from the training set. Thus, to achieve top performance, it is sometimes necessary to fine-tune the networks to the target environment. To this end, we propose a completely self-supervised domain calibration procedure based on robust pose graph estimation from Simultaneous Localization and Mapping (SLAM) as the supervision signal without requiring GPS or manual labeling. We first show that the training samples produced by our technique are sufficient to train a visual place recognition system from a pre-trained classification model. Then, we show that our approach can improve the performance of a state-of-the-art technique on a target environment dissimilar from the training set. We believe that this approach will help practitioners to deploy more robust place recognition solutions in real-world applications.",
        "completion1":"New technique allows for self-supervised domain calibration of visual place recognition systems.",
        "completion2":"State-of-the-art performance in visual place recognition can be achieved without GPS or manual labeling.",
        "completion3":"Robust pose graph estimation from Simultaneous Localization and Mapping can be used to improve performance in visually dissimilar environments.",
        "technologyreview":0.2144483466,
        "venturebeat":0.1716331287,
        "wired":0.0462719928,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.04446v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ro"
        ],
        "published":1646782504000,
        "code_mentioned":0,
        "readability":0.9
    },
    {
        "arxiv_id":"2203.01927v1",
        "predicted_newsworthiness":0.4017718029,
        "title":"As Little as Possible, as Much as Necessary: Detecting Over- and Undertranslations with Contrastive Conditioning",
        "summary":"Omission and addition of content is a typical issue in neural machine translation. We propose a method for detecting such phenomena with off-the-shelf translation models. Using contrastive conditioning, we compare the likelihood of a full sequence under a translation model to the likelihood of its parts, given the corresponding source or target sequence. This allows to pinpoint superfluous words in the translation and untranslated words in the source even in the absence of a reference translation. The accuracy of our method is comparable to a supervised method that requires a custom quality estimation model.",
        "completion1":"As Little as Possible, as Much as Necessary: Detecting Over- and Undertranslations with Contrastive Conditioning.",
        "completion2":"Omission and addition of content is a typical issue in neural machine translation.",
        "completion3":"We propose a method for detecting such phenomena with off-the-shelf translation models.",
        "technologyreview":0.2006986046,
        "venturebeat":0.1497088469,
        "wired":0.0506179452,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.01927v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1646333942000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2202.09747v1",
        "predicted_newsworthiness":0.3301832011,
        "title":"PGE: Robust Product Graph Embedding Learning for Error Detection",
        "summary":"Although product graphs (PGs) have gained increasing attentions in recent years for their successful applications in product search and recommendations, the extensive power of PGs can be limited by the inevitable involvement of various kinds of errors. Thus, it is critical to validate the correctness of triples in PGs to improve their reliability. Knowledge graph (KG) embedding methods have strong error detection abilities. Yet, existing KG embedding methods may not be directly applicable to a PG due to its distinct characteristics: (1) PG contains rich textual signals, which necessitates a joint exploration of both text information and graph structure; (2) PG contains a large number of attribute triples, in which attribute values are represented by free texts. Since free texts are too flexible to define entities in KGs, traditional way to map entities to their embeddings using ids is no longer appropriate for attribute value representation; (3) Noisy triples in a PG mislead the embedding learning and significantly hurt the performance of error detection. To address the aforementioned challenges, we propose an end-to-end noise-tolerant embedding learning framework, PGE, to jointly leverage both text information and graph structure in PG to learn embeddings for error detection. Experimental results on real-world product graph demonstrate the effectiveness of the proposed framework comparing with the state-of-the-art approaches.",
        "completion1":"PGE: A Robust Product Graph Embedding Learning Framework for Error Detection",
        "completion2":"PGE Learns Embeddings for Error Detection from Text and Graph Structure",
        "completion3":"PGE: A Framework for Detecting Errors in Product Graphs",
        "technologyreview":0.2070802102,
        "venturebeat":0.2005825429,
        "wired":0.0545929425,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.09747v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1645341369000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2204.05231v2",
        "predicted_newsworthiness":0.4165534199,
        "title":"Towards Generalizable Semantic Product Search by Text Similarity Pre-training on Search Click Logs",
        "summary":"Recently, semantic search has been successfully applied to e-commerce product search and the learned semantic space(s) for query and product encoding are expected to generalize to unseen queries or products. Yet, whether generalization can conveniently emerge has not been thoroughly studied in the domain thus far. In this paper, we examine several general-domain and domain-specific pre-trained Roberta variants and discover that general-domain fine-tuning does not help generalization, which aligns with the discovery of prior art. Proper domain-specific fine-tuning with clickstream data can lead to better model generalization, based on a bucketed analysis of a publicly available manual annotated query-product pair da",
        "completion1":"Towards Generalizable Semantic Product Search by Text Similarity Pre-training on Search Click Logs",
        "completion2":"General-domain Fine-tuning Does Not Help Generalization",
        "completion3":"Domain-specific Fine-tuning with Clickstream Data Can Lead to Better Model Generalization",
        "technologyreview":0.2076188976,
        "venturebeat":0.2035409283,
        "wired":0.039500696,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.05231v2",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.ai",
            "cs.cl",
            "cs.lg"
        ],
        "published":1649694215000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2201.02489v2",
        "predicted_newsworthiness":0.3425441172,
        "title":"Semantic-based Data Augmentation for Math Word Problems",
        "summary":"It's hard for neural MWP solvers to deal with tiny local variances. In MWP task, some local changes conserve the original semantic while the others may totally change the underlying logic. Currently, existing datasets for MWP task contain limited samples which are key for neural models to learn to disambiguate different kinds of local variances in questions and solve the questions correctly. In this paper, we propose a set of novel data augmentation approaches to supplement existing datasets with such data that are augmented with different kinds of local variances, and help to improve the generalization ability of current neural models. New samples are generated by knowledge guided entity replacement, and logic guided problem reorganization. The augmentation approaches are ensured to keep the consistency between the new data and their labels. Experimental results have shown the necessity and the effectiveness of our methods.",
        "completion1":"Semantic-based Data Augmentation for Math Word Problems could improve neural models' performance",
        "completion2":"Researchers propose novel data augmentation approaches to supplement existing datasets",
        "completion3":"Experimental results show the necessity and effectiveness of proposed methods",
        "technologyreview":0.2014467711,
        "venturebeat":0.1698317925,
        "wired":0.0275480623,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.02489v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1641568076000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2204.06699v1",
        "predicted_newsworthiness":0.4386926441,
        "title":"SNP2Vec: Scalable Self-Supervised Pre-Training for Genome-Wide Association Study",
        "summary":"Self-supervised pre-training methods have brought remarkable breakthroughs in the understanding of text, image, and speech. Recent developments in genomics has also adopted these pre-training methods for genome understanding. However, they focus only on understanding haploid sequences, which hinders their applicability towards understanding genetic variations, also known as single nucleotide polymorphisms (SNPs), which is crucial for genome-wide association study. In this paper, we introduce SNP2Vec, a scalable self-supervised pre-training approach for understanding SNP. We apply SNP2Vec to perform long-sequence genomics modeling, and we evaluate the effectiveness of our approach on predicting Alzheimer's disease risk in a Chinese cohort. Our approach significantly outperforms existing polygenic risk score methods and all other baselines, including the model that is trained entirely with haploid sequences. We release our code and dataset on https:\/\/github.com\/HLTCHKUST\/snp2vec.",
        "completion1":"New method for understanding single nucleotide polymorphisms could help with predicting Alzheimer's disease risk.",
        "completion2":"SNP2Vec: Scalable self-supervised pre-training approach for understanding single nucleotide polymorphisms.",
        "completion3":"New method for understanding single nucleotide polymorphisms could help predict risk for various diseases.",
        "technologyreview":0.2585940038,
        "venturebeat":0.2000927072,
        "wired":0.0517230883,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.06699v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1649901238000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2202.12162v2",
        "predicted_newsworthiness":0.5722031171,
        "title":"Measuring CLEVRness: Blackbox testing of Visual Reasoning Models",
        "summary":"How can we measure the reasoning capabilities of intelligence systems? Visual question answering provides a convenient framework for testing the model's abilities by interrogating the model through questions about the scene. However, despite scores of various visual QA datasets and architectures, which sometimes yield even a super-human performance, the question of whether those architectures can actually reason remains open to debate. To answer this, we extend the visual question answering framework and propose the following behavioral test in the form of a two-player game. We consider black-box neural models of CLEVR. These models are trained on a diagnostic dataset benchmarking reasoning. Next, we train an adversarial player that re-configures the scene to fool the CLEVR model. We show that CLEVR models, which otherwise could perform at a human level, can easily be fooled by our agent. Our results put in doubt whether data-driven approaches can do reasoning without exploiting the numerous biases that are often present in those datasets. Finally, we also propose a controlled experiment measuring the efficiency of such models to learn and perform reasoning.",
        "completion1":"Can data-driven approaches really do reasoning?",
        "completion2":"Do black-box neural models of CLEVR have any reasoning abilities?",
        "completion3":"What is the efficiency of such models to learn and perform reasoning?",
        "technologyreview":0.3597690458,
        "venturebeat":0.2760139948,
        "wired":0.0921174314,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.12162v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cv"
        ],
        "published":1645718369000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2206.00557v1",
        "predicted_newsworthiness":0.4107698368,
        "title":"A Near-Optimal Best-of-Both-Worlds Algorithm for Online Learning with Feedback Graphs",
        "summary":"We consider online learning with feedback graphs, a sequential decision-making framework where the learner's feedback is determined by a directed graph over the action set. We present a computationally efficient algorithm for learning in this framework that simultaneously achieves near-optimal regret bounds in both stochastic and adversarial environments. The bound against oblivious adversaries is $\\tilde{O} (\\sqrt{\\alpha T})$, where $T$ is the time horizon and $\\alpha$ is the independence number of the feedback graph. The bound against stochastic environments is $O\\big( (\\ln T)^2 \\max_{S\\in \\mathcal I(G)} \\sum_{i \\in S} \\Delta_i^{-1}\\big)$ where $\\mathcal I(G)$ is the family of all independent sets in a suitably defined undirected version of the graph and $\\Delta_i$ are the suboptimality gaps. The algorithm combines ideas from the EXP3++ algorithm for stochastic and adversarial bandits and the EXP3.G algorithm for feedback graphs with a novel exploration scheme. The scheme, which exploits the structure of the graph to reduce exploration, is key to obtain best-of-both-worlds guarantees with feedback graphs. We also extend our algorithm and results to a setting where the feedback graphs are allowed to change over time.",
        "completion1":"A Near-Optimal Best-of-Both-Worlds Algorithm for Online Learning with Feedback Graphs",
        "completion2":"New algorithm offers 'best of both worlds' for online learning with feedback graphs",
        "completion3":"Best of both worlds' online learning algorithm achieves near-optimal regret bounds in stochastic and adversarial environments",
        "technologyreview":0.2317399839,
        "venturebeat":0.1907777667,
        "wired":0.0549703436,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.00557v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1654096472000,
        "code_mentioned":0,
        "readability":0.8
    },
    {
        "arxiv_id":"2207.05228v1",
        "predicted_newsworthiness":0.5933402385,
        "title":"Uncertainty-Aware Online Merge Planning with Learned Driver Behavior",
        "summary":"Safe and reliable autonomy solutions are a critical component of next-generation intelligent transportation systems. Autonomous vehicles in such systems must reason about complex and dynamic driving scenes in real time and anticipate the behavior of nearby drivers. Human driving behavior is highly nuanced and specific to individual traffic participants. For example, drivers might display cooperative or non-cooperative behaviors in the presence of merging vehicles. These behaviors must be estimated and incorporated in the planning process for safe and efficient driving. In this work, we present a framework for estimating the cooperation level of drivers on a freeway and plan merging maneuvers with the drivers' latent behaviors explicitly modeled. The latent parameter estimation problem is solved using a particle filter to approximate the probability distribution over the cooperation level. A partially observable Markov decision process (POMDP) that includes the latent state estimate is solved online to extract a policy for a merging vehicle. We evaluate our method in a high-fidelity automotive simulator against methods that are agnostic to latent states or rely on $\\textit{a priori}$ assumptions about actor behavior.",
        "completion1":"Uncertainty-Aware Online Merge Planning with Learned Driver Behavior",
        "completion2":"Autonomous vehicles must reason about complex and dynamic driving scenes in real time and anticipate the behavior of nearby drivers",
        "completion3":"Human driving behavior is highly nuanced and specific to individual traffic participants",
        "technologyreview":0.2251957003,
        "venturebeat":0.1705263118,
        "wired":0.0351895605,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.05228v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1657583328000,
        "code_mentioned":0,
        "readability":0.81
    },
    {
        "arxiv_id":"2203.08568v2",
        "predicted_newsworthiness":0.4103551818,
        "title":"In-Context Learning for Few-Shot Dialogue State Tracking",
        "summary":"Collecting and annotating task-oriented dialogues is time-consuming and costly. Thus, zero and few shot learning for dialogue tasks presents an exciting opportunity. In this work, we propose an in-context (IC) learning framework for zero-shot and few-shot learning dialogue state tracking (DST), where a large pretrained language model (LM) takes a test instance and a few exemplars as input, and directly decodes the dialogue state without any parameter updates. This approach is more flexible and scalable than prior DST work when adapting to new domains and scenarios. To better leverage a tabular domain description in the LM prompt, we reformulate DST into a text-to-SQL problem. We also propose a novel approach to retrieve annotated dialogues as exemplars. Empirical results on MultiWOZ show that our method IC-DST substantially outperforms previous fine-tuned state-of-the-art models in few-shot settings. In addition, we test IC-DST in zero-shot settings, in which the model only takes a fixed task instruction as input, finding that it outperforms previous zero-shot methods by a large margin on MultiWOZ.",
        "completion1":"In-Context Learning for Few-Shot Dialogue State Tracking outperforms previous methods by large margin.",
        "completion2":"In-Context Learning for Few-Shot Dialogue State Tracking more flexible and scalable than prior DST work.",
        "completion3":"In-Context Learning for Few-ShotDialogue State Tracking reformulates DST into a text-to-SQL problem.",
        "technologyreview":0.2017064344,
        "venturebeat":0.1954350081,
        "wired":0.0538950526,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.08568v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1647431904000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2201.09612v1",
        "predicted_newsworthiness":0.3497118982,
        "title":"Learning Geometric Constraints in Task and Motion Planning",
        "summary":"Searching for bindings of geometric parameters in task and motion planning (TAMP) is a finite-horizon stochastic planning problem with high-dimensional decision spaces. A robot manipulator can only move in a subspace of its whole range that is subjected to many geometric constraints. A TAMP solver usually takes many explorations before finding a feasible binding set for each task. It is favorable to learn those constraints once and then transfer them over different tasks within the same workspace. We address this problem by representing constraint knowledge with transferable primitives and using Bayesian optimization (BO) based on these primitives to guide binding search in further tasks. Via semantic and geometric backtracking in TAMP, we construct constraint primitives to encode the geometric constraints respectively in a reusable form. Then we devise a BO approach to efficiently utilize the accumulated constraints for guiding node expansion of an MCTS-based binding planner. We further compose a transfer mechanism to enable free knowledge flow between TAMP tasks. Results indicate that our approach reduces the expensive exploration calls in binding search by 43.60to 71.69 when compared to the baseline unguided planner.",
        "completion1":"New approach reduces expensive exploration calls in binding search by up to 71.69%\".",
        "completion2":"Bayesian optimization helps guide node expansion of an MCTS-based binding planner\".",
        "completion3":"Transfer mechanism enables free flow of knowledge between TAMP tasks\".",
        "technologyreview":0.1748431547,
        "venturebeat":0.1384921187,
        "wired":0.014391345,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.09612v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1643024324000,
        "code_mentioned":0,
        "readability":0.77
    },
    {
        "arxiv_id":"2201.06999v1",
        "predicted_newsworthiness":0.8083580918,
        "title":"Industry 4.0 in Health care: A systematic review",
        "summary":"Industry 4.0 in health care has evolved drastically over the past century. In fact, it is evolving every day, with new tools and strategies being developed by physicians and researchers alike. Health care and technology have been intertwined together with the advancement of cloud computing and big data. This study aims to analyze the impact of industry 4.0 in health care systems. To do so, a systematic literature review was carried out considering peer-reviewed articles extracted from the two popular databases: Scopus and Web of Science (WoS). PRISMA statement 2015 was used to include and exclude that data. At first, a bibliometric analysis was carried out using 346 articles considering the following factors: publication by year, journal, authors, countries, institutions, authors' keywords, and citations. Finally, qualitative analysis was carried out based on selected 32 articles considering the following factors: a conceptual framework, schedule problems, security, COVID-19, digital supply chain, and blockchain technology. Study finding suggests that during the onset of COVID-19, health care and industry 4.0 has been merged and evolved jointly, considering various crisis such as data security, resource allocation, and data transparency. Industry 4.0 enables many technologies such as the internet of things (IoT), blockchain, big data, cloud computing, machine learning, deep learning, information, and communication technologies (ICT) to track patients' records and helps reduce social transmission COVID-19 and so on. The study findings will give future researchers and practitioners some insights regarding the integration of health care and Industry 4.0.",
        "completion1":"Industry 4.0 in health care: A systematic review",
        "completion2":"Study finds industry 4.0 enables technologies to help reduce social transmission of COVID-19",
        "completion3":"Industry 4.0 and health care merge during COVID-19 crisis",
        "technologyreview":0.4159294928,
        "venturebeat":0.3836781531,
        "wired":0.1421593022,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.06999v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1642079330000,
        "code_mentioned":0,
        "readability":0.88
    },
    {
        "arxiv_id":"2206.03799v2",
        "predicted_newsworthiness":0.3910684557,
        "title":"Dyna-DM: Dynamic Object-aware Self-supervised Monocular Depth Maps",
        "summary":"Self-supervised monocular depth estimation has been a subject of intense study in recent years, because of its applications in robotics and autonomous driving. Much of the recent work focuses on improving depth estimation by increasing architecture complexity. This paper shows that state-of-the-art performance can also be achieved by improving the learning process rather than increasing model complexity. More specifically, we propose (i) only using invariant pose loss for the first few epochs during training, (ii) disregarding small potentially dynamic objects when training, and (iii) employing an appearance-based approach to separately estimate object pose for truly dynamic objects. We demonstrate that these simplifications reduce GPU memory usage by 29% and result in qualitatively and quantitatively improved depth maps. The code is available at https:\/\/github.com\/kieran514\/Dyna-DM.",
        "completion1":"New Method for Self-supervised Monocular Depth Estimation Achieves State-of-the-art Performance",
        "completion2":"Researchers Improve Learning Process for Self-supervised Monocular Depth Estimation",
        "completion3":"New Approach to Depth Estimation Reduces GPU Memory Usage by 29%",
        "technologyreview":0.2145791315,
        "venturebeat":0.1901931031,
        "wired":0.0436255764,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.03799v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1654684951000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2205.08770v1",
        "predicted_newsworthiness":0.3509552984,
        "title":"Relation Extraction with Weighted Contrastive Pre-training on Distant Supervision",
        "summary":"Contrastive pre-training on distant supervision has shown remarkable effectiveness for improving supervised relation extraction tasks. However, the existing methods ignore the intrinsic noise of distant supervision during the pre-training stage. In this paper, we propose a weighted contrastive learning method by leveraging the supervised data to estimate the reliability of pre-training instances and explicitly reduce the effect of noise. Experimental results on three supervised datasets demonstrate the advantages of our proposed weighted contrastive learning approach, compared to two state-of-the-art non-weighted baselines.",
        "completion1":"Contrastive Pre-training on Distant Supervision Yields Better Results for Relation Extraction",
        "completion2":"Weighting Contrastive Learning can Help Reduce Noisy Pre-training Data",
        "completion3":"New Approach to Contrastive Learning Show Promise for Effective Relation Extraction",
        "technologyreview":0.1639267556,
        "venturebeat":0.131207534,
        "wired":0.0146376784,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.08770v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1652859959000,
        "code_mentioned":0,
        "readability":0.81
    },
    {
        "arxiv_id":"2207.02671v1",
        "predicted_newsworthiness":0.4643070041,
        "title":"Low-Level Force-Control of MR-Hydrostatic Actuators",
        "summary":"Precise and high-fidelity force control is critical for new generations of robots that interact with humans and unknown environments. Mobile robots, such as wearable devices and legged robots, must also be lightweight to accomplish their function. Hydrostatic transmissions have been proposed as a promising strategy for meeting these two challenging requirements. In previous publications, it was shown that using magnetorheological (MR) actuators coupled with hydrostatic transmissions provides high power density and great open-loop human-robot interactions. Still, the open-loop force fidelity at low and high frequencies are decreased by the transmission's dynamics and by nonlinear friction. This letter compares control strategies for MR-hydrostatic actuator systems to increase its torque fidelity, defined as the bandwidth (measured vs desired torque reference) and transparency (minimizing the undesired forces reflected to the end effector when backdriving the robot). Four control approaches are developed and compared experimentally: (1) Open-loop control with friction compensation; (2) non-collocated pressure feedback; (3) collocated pressure feedback; (4) LQGI state feedback. A dither strategy is also implemented to smoothen ball screw friction. Results show that approaches (1), (2) and (3) can increase the performances but are facing compromises, while approach (4) can simultaneously improve all metrics. These results show the potential of using control schemes for improving the force control performance of robots using tethered architectures, addressing issues such as transmission dynamics and friction.",
        "completion1":"Low-Level Force-Control of MR-Hydrostatic Actuators Improves Torque Fidelity",
        "completion2":"Open-Loop Control with Friction Compensation Increases Bandwidth and Transparency for MR-Hydrostatic Actuators",
        "completion3":"LQGI State Feedback Provides Best Control Strategy for MR-Hydrostatic Actuators",
        "technologyreview":0.1547564332,
        "venturebeat":0.0842939601,
        "wired":0.0061642074,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.02671v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1657114671000,
        "code_mentioned":0,
        "readability":0.8
    },
    {
        "arxiv_id":"2205.15492v1",
        "predicted_newsworthiness":0.5397150831,
        "title":"Sepsis Prediction with Temporal Convolutional Networks",
        "summary":"We design and implement a temporal convolutional network model to predict sepsis onset. Our model is trained on data extracted from MIMIC III database, based on a retrospective analysis of patients admitted to intensive care unit who did not fall under the definition of sepsis at the time of admission. Benchmarked with several machine learning models, our model is superior on this binary classification task, demonstrates the prediction power of convolutional networks for temporal patterns, also shows the significant impact of having longer look back time on sepsis prediction.",
        "completion1":"New Machine Learning Model Can Predict Sepsis Onset",
        "completion2":"Temporal Convolutional Networks Show Promise for Sepsis Prediction",
        "completion3":"Longer Look Back Time Key to Sepsis Prediction, Study Finds",
        "technologyreview":0.2322971304,
        "venturebeat":0.1987602895,
        "wired":0.0471740528,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.15492v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1653959678000,
        "code_mentioned":0,
        "readability":0.81
    },
    {
        "arxiv_id":"2206.15228v1",
        "predicted_newsworthiness":0.6082507471,
        "title":"Signed ego network model and its application to Twitter",
        "summary":"The Ego Network Model (ENM) describes how individuals organise their social relations in concentric circles (typically five) of decreasing intimacy, and it has been found almost ubiquitously in social networks, both offline and online. The ENM gauges the tie strength between peers in terms of interaction frequency, which is easy to measure and provides a good proxy for the time spent nurturing the relationship. However, advances in signed network analysis have shown that positive and negative relations play very different roles in network dynamics. For this reason, this work sets out to investigate the ENM when including signed relations. The main contributions of this paper are twofold: firstly, a novel method of signing relationships between individuals using sentiment analysis and, secondly, an investigation of the properties of Signed Ego Networks (Ego Networks with signed connections). Signed Ego Networks are then extracted for the users of eight different Twitter datasets composed of both specialised users (e.g. journalists) and generic users. We find that negative links are over-represented in the active part of the Ego Networks of all types of users, suggesting that Twitter users tend to engage regularly with negative connections. Further, we observe that negative relationships are overwhelmingly predominant in the Ego Network circles of specialised users, hinting at very polarised online interactions for this category of users. In addition, negative relationships are found disproportionately more at the more intimate levels of the ENM for journalists, while their percentages are stable across the circles of the other Twitter users",
        "completion1":"Signed Ego Networks: A novel method of signing relationships between individuals using sentiment analysis",
        "completion2":"The role of negative relations in Signed Ego Networks: Twitter users tend to engage regularly with negative connections",
        "completion3":"Signed Ego Networks of journalists: Negative relationships are found disproportionately more at the more intimate levels",
        "technologyreview":0.2966936362,
        "venturebeat":0.241564519,
        "wired":0.1052883934,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.15228v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1656591581000,
        "code_mentioned":0,
        "readability":0.9
    },
    {
        "arxiv_id":"2202.00515v1",
        "predicted_newsworthiness":0.5272965435,
        "title":"Comparing Community-aware Centrality Measures in Online Social Networks",
        "summary":"Identifying key nodes is crucial for accelerating or impeding dynamic spreading in a network. Community-aware centrality measures tackle this problem by exploiting the community structure of a network. Although there is a growing trend to design new community-aware centrality measures, there is no systematic investigation of the proposed measures' effectiveness. This study performs an extensive comparative evaluation of prominent community-aware centrality measures using the Susceptible-Infected-Recovered (SIR) model on real-world online social networks. Overall, results show that K-shell with Community and Community-based Centrality measures are the most accurate in identifying influential nodes under a single-spreader problem. Additionally, the epidemic transmission rate doesn't significantly affect the behavior of the community-aware centrality measures.",
        "completion1":"Community-aware centrality measures are the most accurate in identifying influential nodes under a single-spreader problem.",
        "completion2":"K-shell with Community and Community-based Centrality measures are the most accurate in identifying influential nodes.",
        "completion3":"The epidemic transmission rate doesn't significantly affect the behavior of the community-aware centrality measures.",
        "technologyreview":0.2639044896,
        "venturebeat":0.2097248365,
        "wired":0.0593610125,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.00515v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1643578987000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2201.03941v2",
        "predicted_newsworthiness":0.6786900095,
        "title":"Sentiment Analysis with Deep Learning Models: A Comparative Study on a Decade of Sinhala Language Facebook Data",
        "summary":"The relationship between Facebook posts and the corresponding reaction feature is an interesting subject to explore and understand. To achieve this end, we test state-of-the-art Sinhala sentiment analysis models against a data set containing a decade worth of Sinhala posts with millions of reactions. For the purpose of establishing benchmarks and with the goal of identifying the best model for Sinhala sentiment analysis, we also test, on the same data set configuration, other deep learning models catered for sentiment analysis. In this study we report that the 3 layer Bidirectional LSTM model achieves an F1 score of 84.58% for Sinhala sentiment analysis, surpassing the current state-of-the-art model; Capsule B, which only manages to get an F1 score of 82.04%. Further, since all the deep learning models show F1 scores above 75% we conclude that it is safe to claim that Facebook reactions are suitable to predict the sentiment of a text.",
        "completion1":"Sentiment Analysis with Deep Learning Models: A Comparative Study on a Decade of Sinhala Language Facebook Data",
        "completion2":"The relationship between Facebook posts and the corresponding reaction feature is an interesting subject to explore and understand.",
        "completion3":"To achieve this end, we test state-of-the-art Sinhala sentiment analysis models against a data set containing a decade worth of Sinhala posts with millions of reactions.",
        "technologyreview":0.2923732366,
        "venturebeat":0.2465789534,
        "wired":0.0963781034,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.03941v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1641907875000,
        "code_mentioned":0,
        "readability":0.9
    },
    {
        "arxiv_id":"2203.16487v4",
        "predicted_newsworthiness":0.4081719416,
        "title":"Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding",
        "summary":"Different from previous work accelerating translation at the cost of quality loss, we propose Generalized Aggressive Decoding (GAD) -- a novel decoding paradigm for lossless speedup of autoregressive translation, through the collaboration of autoregressive and non-autoregressive translation (NAT) of the Transformer. At each decoding iteration, GAD aggressively decodes a number of tokens with NAT as a draft and then verifies them in the autoregressive manner, where only the tokens that pass the verification are accepted as decoded tokens. GAD can achieve the same results as autoregressive translation but much more efficiently because both NAT drafting and autoregressive verification compute in parallel. We conduct experiments in four standard WMT benchmarks and confirm that the vanilla GAD yields exactly the same results as greedy decoding with an around $3\\times$ speedup, and that its variant (GAD++) with an advanced verification strategy not only outperforms the greedy translation and even achieves the comparable translation quality with the beam search result, but also further improves the decoding speed, resulting in an around $5\\times$ speedup over autoregressive translation. Moreover, GAD can be easily generalized for lossless speedup of other seq2seq tasks like Abstractive Summarization, and benefit more from stronger computing devices, demonstrating its potential to become a de facto decoding paradigm in the future. Our models and codes are available at https:\/\/github.com\/hemingkx\/GAD.",
        "completion1":"Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding",
        "completion2":"eneralized Aggressive Decoding - a novel decoding paradigm for lossless speedup of autoregressive translation",
        "completion3":"AD can achieve the same results as autoregressive translation but much more efficiently because both NAT drafting and autoregressive verification compute in parallel",
        "technologyreview":0.1759620673,
        "venturebeat":0.1624442282,
        "wired":0.0602980409,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.16487v4",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1648661229000,
        "code_mentioned":1,
        "readability":0.69
    },
    {
        "arxiv_id":"2203.06569v1",
        "predicted_newsworthiness":0.4352026254,
        "title":"SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization",
        "summary":"Sequence-to-sequence neural networks have recently achieved great success in abstractive summarization, especially through fine-tuning large pre-trained language models on the downstream dataset. These models are typically decoded with beam search to generate a unique summary. However, the search space is very large, and with the exposure bias, such decoding is not optimal. In this paper, we show that it is possible to directly train a second-stage model performing re-ranking on a set of summary candidates. Our mixture-of-experts SummaReranker learns to select a better candidate and consistently improves the performance of the base model. With a base PEGASUS, we push ROUGE scores by 5.44% on CNN-DailyMail (47.16 ROUGE-1), 1.31% on XSum (48.12 ROUGE-1) and 9.34% on Reddit TIFU (29.83 ROUGE-1), reaching a new state-of-the-art. Our code and checkpoints will be available at https:\/\/github.com\/ntunlp\/SummaReranker.",
        "completion1":"SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization",
        "completion2":"Sequence-to-sequence neural networks have recently achieved great success in abstractive summarization",
        "completion3":"With a base PEGASUS, we push ROUGE scores by 5.44% on CNN-DailyMail (47.16 ROUGE-1), 1.31% on XSum (48.12 ROUGE-1) and 9.34% on Reddit TIFU (29.83 ROUGE-1), reaching a new state-of-the-art.",
        "technologyreview":0.2281967635,
        "venturebeat":0.1955534254,
        "wired":0.0728941176,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.06569v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1647147910000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2204.07651v1",
        "predicted_newsworthiness":0.3470254161,
        "title":"Learning time-dependent PDE solver using Message Passing Graph Neural Networks",
        "summary":"One of the main challenges in solving time-dependent partial differential equations is to develop computationally efficient solvers that are accurate and stable. Here, we introduce a graph neural network approach to finding efficient PDE solvers through learning using message-passing models. We first introduce domain invariant features for PDE-data inspired by classical PDE solvers for an efficient physical representation. Next, we use graphs to represent PDE-data on an unstructured mesh and show that message passing graph neural networks (MPGNN) can parameterize governing equations, and as a result, efficiently learn accurate solver schemes for linear\/nonlinear PDEs. We further show that the solvers are independent of the initial trained geometry, i.e. the trained solver can find PDE solution on different complex domains. Lastly, we show that a recurrent graph neural network approach can find a temporal sequence of solutions to a PDE.",
        "completion1":"Graph Neural Networks developed to efficiently find accurate solvers for linear\/nonlinear Partial Differential Equations.",
        "completion2":"New approach to learning time-dependent PDE solvers using message passing graph neural networks.",
        "completion3":"Recurrent graph neural network finds temporal sequence of solutions to a PDE.",
        "technologyreview":0.1949406226,
        "venturebeat":0.1393580952,
        "wired":0.0304083564,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.07651v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1650057032000,
        "code_mentioned":1,
        "readability":0.75
    },
    {
        "arxiv_id":"2201.12462v2",
        "predicted_newsworthiness":0.616982538,
        "title":"Explaining Reinforcement Learning Policies through Counterfactual Trajectories",
        "summary":"In order for humans to confidently decide where to employ RL agents for real-world tasks, a human developer must validate that the agent will perform well at test-time. Some policy interpretability methods facilitate this by capturing the policy's decision making in a set of agent rollouts. However, even the most informative trajectories of training time behavior may give little insight into the agent's behavior out of distribution. In contrast, our method conveys how the agent performs under distribution shifts by showing the agent's behavior across a wider trajectory distribution. We generate these trajectories by guiding the agent to more diverse unseen states and showing the agent's behavior there. In a user study, we demonstrate that our method enables users to score better than baseline methods on one of two agent validation tasks.",
        "completion1":"Reinforcement Learning Policies Explained Through Counterfactual Trajectories",
        "completion2":"Human Developers Validate RL Agents' Performance at Test-Time",
        "completion3":"Method Conveys How Agent Performs Under Distribution Shifts",
        "technologyreview":0.3326198967,
        "venturebeat":0.2857927733,
        "wired":0.1127754598,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.12462v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.hc",
            "cs.ro"
        ],
        "published":1643417557000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2204.10202v1",
        "predicted_newsworthiness":0.4643597922,
        "title":"Unsupervised Numerical Reasoning to Extract Phenotypes from Clinical Text by Leveraging External Knowledge",
        "summary":"Extracting phenotypes from clinical text has been shown to be useful for a variety of clinical use cases such as identifying patients with rare diseases. However, reasoning with numerical values remains challenging for phenotyping in clinical text, for example, temperature 102F representing Fever. Current state-of-the-art phenotyping models are able to detect general phenotypes, but perform poorly when they detect phenotypes requiring numerical reasoning. We present a novel unsupervised methodology leveraging external knowledge and contextualized word embeddings from ClinicalBERT for numerical reasoning in a variety of phenotypic contexts. Comparing against unsupervised benchmarks, it shows a substantial performance improvement with absolute gains on generalized Recall and F1 scores up to 79% and 71%, respectively. In the supervised setting, it also surpasses the performance of alternative approaches with absolute gains on generalized Recall and F1 scores up to 70% and 44%, respectively.",
        "completion1":"A new unsupervised methodology for numerical reasoning in phenotypic contexts has been developed, showing substantial performance improvements.",
        "completion2":"This new methodology outperforms existing approaches in the supervised setting, with absolute gains on generalized Recall and F1 scores up to 70% and 44%, respectively.",
        "completion3":"The new methodology is based on ClinicalBERT and external knowledge, and could have a range of applications in clinical use cases such as identifying patients with rare diseases.",
        "technologyreview":0.2182927728,
        "venturebeat":0.1607774205,
        "wired":0.0286803566,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.10202v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1650372272000,
        "code_mentioned":0,
        "readability":0.8
    },
    {
        "arxiv_id":"2204.07714v2",
        "predicted_newsworthiness":0.428853114,
        "title":"Pushing the Performance Limit of Scene Text Recognizer without Human Annotation",
        "summary":"Scene text recognition (STR) attracts much attention over the years because of its wide application. Most methods train STR model in a fully supervised manner which requires large amounts of labeled data. Although synthetic data contributes a lot to STR, it suffers from the real-tosynthetic domain gap that restricts model performance. In this work, we aim to boost STR models by leveraging both synthetic data and the numerous real unlabeled images, exempting human annotation cost thoroughly. A robust consistency regularization based semi-supervised framework is proposed for STR, which can effectively solve the instability issue due to domain inconsistency between synthetic and real images. A character-level consistency regularization is designed to mitigate the misalignment between characters in sequence recognition. Extensive experiments on standard text recognition benchmarks demonstrate the effectiveness of the proposed method. It can steadily improve existing STR models, and boost an STR model to achieve new state-of-the-art results. To our best knowledge, this is the first consistency regularization based framework that applies successfully to STR.",
        "completion1":"Pushing the Performance Limit of Scene Text Recognizer without Human Annotation",
        "completion2":"Scene text recognition attracts much attention over the years because of its wide application",
        "completion3":"Most methods train STR model in a fully supervised manner which requires large amounts of labeled data",
        "technologyreview":0.2066061631,
        "venturebeat":0.1675287727,
        "wired":0.0523167377,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.07714v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1650084122000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2205.06130v1",
        "predicted_newsworthiness":0.4488243931,
        "title":"Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models",
        "summary":"Massively Multilingual Transformer based Language Models have been observed to be surprisingly effective on zero-shot transfer across languages, though the performance varies from language to language depending on the pivot language(s) used for fine-tuning. In this work, we build upon some of the existing techniques for predicting the zero-shot performance on a task, by modeling it as a multi-task learning problem. We jointly train predictive models for different tasks which helps us build more accurate predictors for tasks where we have test data in very few languages to measure the actual performance of the model. Our approach also lends us the ability to perform a much more robust feature selection and identify a common set of features that influence zero-shot performance across a variety of tasks.",
        "completion1":"Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models",
        "completion2":"Massively Multilingual Transformer based Language Models",
        "completion3":"Jointly training predictive models for different tasks",
        "technologyreview":0.2096469997,
        "venturebeat":0.1797254923,
        "wired":0.0461199112,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.06130v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1652366823000,
        "code_mentioned":1,
        "readability":0.91
    },
    {
        "arxiv_id":"2205.00366v1",
        "predicted_newsworthiness":0.3944027003,
        "title":"Fractional Vegetation Cover Estimation using Hough Lines and Linear Iterative Clustering",
        "summary":"A common requirement of plant breeding programs across the country is companion planting -- growing different species of plants in close proximity so they can mutually benefit each other. However, the determination of companion plants requires meticulous monitoring of plant growth. The technique of ocular monitoring is often laborious and error prone. The availability of image processing techniques can be used to address the challenge of plant growth monitoring and provide robust solutions that assist plant scientists to identify companion plants. This paper presents a new image processing algorithm to determine the amount of vegetation cover present in a given area, called fractional vegetation cover. The proposed technique draws inspiration from the trusted Daubenmire method for vegetation cover estimation and expands upon it. Briefly, the idea is to estimate vegetation cover from images containing multiple rows of plant species growing in close proximity separated by a multi-segment PVC frame of known size. The proposed algorithm applies a Hough Transform and Simple Linear Iterative Clustering (SLIC) to estimate the amount of vegetation cover within each segment of the PVC frame. The analysis when repeated over images captured at regular intervals of time provides crucial insights into plant growth. As a means of comparison, the proposed algorithm is compared with SamplePoint and Canopeo, two trusted applications used for vegetation cover estimation. The comparison shows a 99% similarity with both SamplePoint and Canopeo demonstrating the accuracy and feasibility of the algorithm for fractional vegetation cover estimation.",
        "completion1":"New algorithm for fractional vegetation cover estimation is 99% accurate.",
        "completion2":"Algorithm simplifies ocular monitoring of plant growth.",
        "completion3":"New image processing technique assists plant scientists in determining companion plants.",
        "technologyreview":0.1645882246,
        "venturebeat":0.1284592985,
        "wired":0.009001643,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.00366v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1651361611000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2201.05984v1",
        "predicted_newsworthiness":0.420052149,
        "title":"In Situ Answer Sentence Selection at Web-scale",
        "summary":"Current answer sentence selection (AS2) applied in open-domain question answering (ODQA) selects answers by ranking a large set of possible candidates, i.e., sentences, extracted from the retrieved text. In this paper, we present Passage-based Extracting Answer Sentence In-place (PEASI), a novel design for AS2 optimized for Web-scale setting, that, instead, computes such answer without processing each candidate individually. Specifically, we design a Transformer-based framework that jointly (i) reranks passages retrieved for a question and (ii) identifies a probable answer from the top passages in place. We train PEASI in a multi-task learning framework that encourages feature sharing between the components: passage reranker and passage-based answer sentence extractor. To facilitate our development, we construct a new Web-sourced large-scale QA dataset consisting of 800,000+ labeled passages\/sentences for 60,000+ questions. The experiments show that our proposed design effectively outperforms the current state-of-the-art setting for AS2, i.e., a point-wise model for ranking sentences independently, by 6.51% in accuracy, from 48.86% to 55.37%. In addition, PEASI is exceptionally efficient in computing answer sentences, requiring only ~20% inferences compared to the standard setting, i.e., reranking all possible candidates. We believe the release of PEASI, both the dataset and our proposed design, can contribute to advancing the research and development in deploying question answering services at Web scale.",
        "completion1":"In Situ Answer Sentence Selection at Web-scale",
        "completion2":"Transformer-based framework for AS2",
        "completion3":"New Dataset consisting of 800,000+ labeled passages\/sentences",
        "technologyreview":0.2207616983,
        "venturebeat":0.2141822122,
        "wired":0.0623445463,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.05984v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1642314960000,
        "code_mentioned":0,
        "readability":0.84
    },
    {
        "arxiv_id":"2205.08824v1",
        "predicted_newsworthiness":0.6412326452,
        "title":"Automating In-Network Machine Learning",
        "summary":"Using programmable network devices to aid in-network machine learning has been the focus of significant research. However, most of the research was of a limited scope, providing a proof of concept or describing a closed-source algorithm. To date, no general solution has been provided for mapping machine learning algorithms to programmable network devices. In this paper, we present Planter, an open-source, modular framework for mapping trained machine learning models to programmable devices. Planter supports a wide range of machine learning models, multiple targets and can be easily extended. The evaluation of Planter compares different mapping approaches, and demonstrates the feasibility, performance, and resource efficiency for applications such as anomaly detection, financial transactions, and quality of experience. The results show that Planter-based in-network machine learning algorithms can run at line rate, have a negligible effect on latency, coexist with standard switching functionality, and have no or minor accuracy trade-offs.",
        "completion1":"Planter: An Open-Source Modular Framework for Mapping Trained Machine Learning Models to Programmable Devices",
        "completion2":"Planter: A Framework for In-Network Machine Learning",
        "completion3":"Planter: An Evaluation of Mapping Approaches for In-Network Machine Learning",
        "technologyreview":0.3627208408,
        "venturebeat":0.3567196823,
        "wired":0.0903365625,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.08824v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni",
            "cs.lg"
        ],
        "published":1652866942000,
        "code_mentioned":1,
        "readability":0.87
    },
    {
        "arxiv_id":"2203.04541v1",
        "predicted_newsworthiness":0.3869013308,
        "title":"PUTN: A Plane-fitting based Uneven Terrain Navigation Framework",
        "summary":"Autonomous navigation of ground robots has been widely used in indoor structured 2D environments, but there are still many challenges in outdoor 3D unstructured environments, especially in rough, uneven terrains. This paper proposed a plane-fitting based uneven terrain navigation framework (PUTN) to solve this problem. The implementation of PUTN is divided into three steps. First, based on Rapidly-exploring Random Trees (RRT), an improved sample-based algorithm called Plane Fitting RRT* (PF-RRT*) is proposed to obtain a sparse trajectory. Each sampling point corresponds to a custom traversability index and a fitted plane on the point cloud. These planes are connected in series to form a traversable strip. Second, Gaussian Process Regression is used to generate traversability of the dense trajectory interpolated from the sparse trajectory, and the sampling tree is used as the training set. Finally, local planning is performed using nonlinear model predictive control (NMPC). By adding the traversability index and uncertainty to the cost function, and adding obstacles generated by the real-time point cloud to the constraint function, a safe motion planning algorithm with smooth speed and strong robustness is available. Experiments in real scenarios are conducted to verify the effectiveness of the method.",
        "completion1":"A new framework for autonomous navigation of ground robots in outdoor 3D unstructured environments",
        "completion2":"Improved sample-based algorithm for autonomous navigation in outdoor environments",
        "completion3":"New method for safe and smooth motion planning in outdoor environments",
        "technologyreview":0.1919476069,
        "venturebeat":0.1354075921,
        "wired":0.0104054102,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.04541v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1646806934000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2206.01718v1",
        "predicted_newsworthiness":0.5491356434,
        "title":"A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge",
        "summary":"The Visual Question Answering (VQA) task aspires to provide a meaningful testbed for the development of AI models that can jointly reason over visual and natural language inputs. Despite a proliferation of VQA datasets, this goal is hindered by a set of common limitations. These include a reliance on relatively simplistic questions that are repetitive in both concepts and linguistic structure, little world knowledge needed outside of the paired image, and limited reasoning required to arrive at the correct answer. We introduce A-OKVQA, a crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer. In contrast to the existing knowledge-based VQA datasets, the questions generally cannot be answered by simply querying a knowledge base, and instead require some form of commonsense reasoning about the scene depicted in the image. We demonstrate the potential of this new dataset through a detailed analysis of its contents and baseline performance measurements over a variety of state-of-the-art vision-language models. Project page: http:\/\/a-okvqa.allenai.org\/",
        "completion1":"A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge",
        "completion2":"The Visual Question Answering task aspires to provide a meaningful testbed for the development of AI models that can jointly reason over visual and natural language inputs",
        "completion3":"We introduce A-OKVQA, a crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer",
        "technologyreview":0.2581533784,
        "venturebeat":0.210692843,
        "wired":0.0683716607,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.01718v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.cl"
        ],
        "published":1654278747000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2204.03276v1",
        "predicted_newsworthiness":0.4759600224,
        "title":"PALBERT: Teaching ALBERT to Ponder",
        "summary":"Currently, pre-trained models can be considered the default choice for a wide range of NLP tasks. Despite their SoTA results, there is practical evidence that these models may require a different number of computing layers for different input sequences, since evaluating all layers leads to overconfidence on wrong predictions (namely overthinking). This problem can potentially be solved by implementing adaptive computation time approaches, which were first designed to improve inference speed. Recently proposed PonderNet may be a promising solution for performing an early exit by treating the exit layers index as a latent variable. However, the originally proposed exit criterion, relying on sampling from trained posterior distribution on the probability of exiting from i-th layer, introduces major variance in model outputs, significantly reducing the resulting models performance. In this paper, we propose Ponder ALBERT (PALBERT): an improvement to PonderNet with a novel deterministic Q-exit criterion and a revisited model architecture. We compared PALBERT with recent methods for performing an early exit. We observed that the proposed changes can be considered significant improvements on the original PonderNet architecture and outperform PABEE on a wide range of GLUE tasks. In addition, we also performed an in-depth ablation study of the proposed architecture to further understand Lambda layers and their performance.",
        "completion1":"PALBERT: A new method of early exit for pre-trained models",
        "completion2":"PALBERT: Outperforming recent methods of early exit",
        "completion3":"PALBERT: An in-depth ablation study",
        "technologyreview":0.2617989956,
        "venturebeat":0.2279555114,
        "wired":0.0745530609,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.03276v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cl"
        ],
        "published":1649318473000,
        "code_mentioned":0,
        "readability":0.83
    },
    {
        "arxiv_id":"2203.04363v1",
        "predicted_newsworthiness":0.4635955211,
        "title":"On the Fitness Landscapes of Interdependency Models in the Travelling Thief Problem",
        "summary":"Since its inception in 2013, the Travelling Thief Problem (TTP) has been widely studied as an example of problems with multiple interconnected sub-problems. The dependency in this model arises when tying the travelling time of the \"thief\" to the weight of the knapsack. However, other forms of dependency as well as combinations of dependencies should be considered for investigation, as they are often found in complex real-world problems. Our goal is to study the impact of different forms of dependency in the TTP using a simple local search algorithm. To achieve this, we use Local Optima Networks, a technique for analysing the fitness landscape.",
        "completion1":"On the Fitness Landscapes of Interdependency Models in the Travelling Thief Problem",
        "completion2":"The Impact of Different Forms of Dependency in the TTP",
        "completion3":"Using a Simple Local Search Algorithm to Study the TTP",
        "technologyreview":0.1781285437,
        "venturebeat":0.1435732944,
        "wired":0.0339834788,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.04363v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1646054802000,
        "code_mentioned":0,
        "readability":0.91
    },
    {
        "arxiv_id":"2205.06938v1",
        "predicted_newsworthiness":0.6004874445,
        "title":"Generating Literal and Implied Subquestions to Fact-check Complex Claims",
        "summary":"Verifying complex political claims is a challenging task, especially when politicians use various tactics to subtly misrepresent the facts. Automatic fact-checking systems fall short here, and their predictions like \"half-true\" are not very useful in isolation, since we have no idea which parts of the claim are true and which are not. In this work, we focus on decomposing a complex claim into a comprehensive set of yes-no subquestions whose answers influence the veracity of the claim. We present ClaimDecomp, a dataset of decompositions for over 1000 claims. Given a claim and its verification paragraph written by fact-checkers, our trained annotators write subquestions covering both explicit propositions of the original claim and its implicit facets, such as asking about additional political context that changes our view of the claim's veracity. We study whether state-of-the-art models can generate such subquestions, showing that these models generate reasonable questions to ask, but predicting the comprehensive set of subquestions from the original claim without evidence remains challenging. We further show that these subquestions can help identify relevant evidence to fact-check the full claim and derive the veracity through their answers, suggesting that they can be useful pieces of a fact-checking pipeline.",
        "completion1":"Can automatic fact-checking systems really help verify complex political claims?",
        "completion2":"What is the ' ClaimDecomp' dataset and how can it help with verifying complex claims?",
        "completion3":"How do state-of-the-art models fare when it comes to generating subquestions about complex claims?",
        "technologyreview":0.2698126477,
        "venturebeat":0.2049372956,
        "wired":0.07166489,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.06938v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1652488857000,
        "code_mentioned":0,
        "readability":0.87
    },
    {
        "arxiv_id":"2204.00401v1",
        "predicted_newsworthiness":0.4892699377,
        "title":"CTAB-GAN+: Enhancing Tabular Data Synthesis",
        "summary":"While data sharing is crucial for knowledge development, privacy concerns and strict regulation (e.g., European General Data Protection Regulation (GDPR)) limit its full effectiveness. Synthetic tabular data emerges as alternative to enable data sharing while fulfilling regulatory and privacy constraints. State-of-the-art tabular data synthesizers draw methodologies from Generative Adversarial Networks (GAN). As GANs improve the synthesized data increasingly resemble the real data risking to leak privacy. Differential privacy (DP) provides theoretical guarantees on privacy loss but degrades data utility. Striking the best trade-off remains yet a challenging research question. We propose CTAB-GAN+ a novel conditional tabular GAN. CTAB-GAN+ improves upon state-of-the-art by (i) adding downstream losses to conditional GANs for higher utility synthetic data in both classification and regression domains; (ii) using Wasserstein loss with gradient penalty for better training convergence; (iii) introducing novel encoders targeting mixed continuous-categorical variables and variables with unbalanced or skewed data; and (iv) training with DP stochastic gradient descent to impose strict privacy guarantees. We extensively evaluate CTAB-GAN+ on data similarity and analysis utility against state-of-the-art tabular GANs. The results show that CTAB-GAN+ synthesizes privacy-preserving data with at least 48.16% higher utility across multiple datasets and learning tasks under different privacy budgets.",
        "completion1":"CTAB-GAN+: Enhancing Tabular Data Synthesis",
        "completion2":"State-of-the-art tabular data synthesizers draw methodologies from Generative Adversarial Networks",
        "completion3":"We propose CTAB-GAN+ a novel conditional tabular GAN.",
        "technologyreview":0.2915656087,
        "venturebeat":0.2458672129,
        "wired":0.079978499,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.00401v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1648817550000,
        "code_mentioned":1,
        "readability":0.75
    },
    {
        "arxiv_id":"2206.06484v1",
        "predicted_newsworthiness":0.497551228,
        "title":"On Image Segmentation With Noisy Labels: Characterization and Volume Properties of the Optimal Solutions to Accuracy and Dice",
        "summary":"We study two of the most popular performance metrics in medical image segmentation, Accuracy and Dice, when the target labels are noisy. For both metrics, several statements related to characterization and volume properties of the set of optimal segmentations are proved, and associated experiments are provided. Our main insights are: (i) the volume of the solutions to both metrics may deviate significantly from the expected volume of the target, (ii) the volume of a solution to Accuracy is always less than or equal to the volume of a solution to Dice and (iii) the optimal solutions to both of these metrics coincide when the set of feasible segmentations is constrained to the set of segmentations with the volume equal to the expected volume of the target.",
        "completion1":"Study Shows That Optimal Solutions to Accuracy and Dice May Deviate Significantly From Expected Volume of Target",
        "completion2":"Volume of Solution to Accuracy Always Less Than or Equal to Volume of Solution to Dice",
        "completion3":"Optimal Solutions to Both Metrics Coincide When Feasible Segmentations Constrained to Set With Equal Volume",
        "technologyreview":0.1739101081,
        "venturebeat":0.124083526,
        "wired":0.0094763401,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.06484v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1655155829000,
        "code_mentioned":1,
        "readability":0.87
    },
    {
        "arxiv_id":"2202.08406v1",
        "predicted_newsworthiness":0.6795934353,
        "title":"The Unboxing Experience: Exploration and Design of Initial Interactions Between Children and Social Robots",
        "summary":"Social robots are increasingly introduced into children's lives as educational and social companions, yet little is known about how these products might best be introduced to their environments. The emergence of the \"unboxing\" phenomenon in media suggests that introduction is key to technology adoption where initial impressions are made. To better understand this phenomenon toward designing a positive unboxing experience in the context of social robots for children, we conducted three field studies with families of children aged 8 to 13: (1) an exploratory free-play activity ($n=12$); (2) a co-design session ($n=11$) that informed the development of a prototype box and a curated unboxing experience; and (3) a user study ($n=9$) that evaluated children's experiences. Our findings suggest the unboxing experience of social robots can be improved through the design of a creative aesthetic experience that engages the child socially to guide initial interactions and foster a positive child-robot relationship.",
        "completion1":"Social Robots Are Increasingly Being Introduced Into Children's Lives",
        "completion2":"The Unboxing Experience is Key to Technology Adoption",
        "completion3":"Designing a Positive Unboxing Experience for Social Robots",
        "technologyreview":0.3506839451,
        "venturebeat":0.3192629114,
        "wired":0.1598833179,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.08406v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.hc"
        ],
        "published":1645064149000,
        "code_mentioned":0,
        "readability":0.88
    },
    {
        "arxiv_id":"2202.04124v3",
        "predicted_newsworthiness":0.4177224342,
        "title":"An Adaptive Mini-Block Fisher Method for Deep Neural Networks",
        "summary":"Deep neural networks (DNNs) are currently predominantly trained using first-order methods. Some of these methods (e.g., Adam, AdaGrad, and RMSprop, and their variants) incorporate a small amount of curvature information by using a diagonal matrix to precondition the stochastic gradient. Recently, effective second-order methods, such as KFAC, K-BFGS, Shampoo, and TNT, have been developed for training DNNs, by preconditioning the stochastic gradient by layer-wise block-diagonal matrices. Here we propose an adaptive \"mini-block Fisher (MBF)\" preconditioned gradient method, that lies in between these two classes of methods. Specifically, our method uses a block-diagonal approximation to the empirical Fisher matrix, where for each layer in the DNN, whether it is convolutional or feed-forward and fully connected, the associated diagonal block is itself block-diagonal and is composed of a large number of mini-blocks of modest size. Our novel approach utilizes the parallelism of GPUs to efficiently perform computations on the large number of matrices in each layer. Consequently, MBF's per-iteration computational cost is only slightly higher than it is for first-order methods. The performance of our proposed method is compared to that of several baseline methods, on both autoencoder and CNN problems, to validate its effectiveness both in terms of time efficiency and generalization power. Finally, prove that an idealized version of MBF converges linearly.",
        "completion1":"Adaptive Mini-Block Fisher Method Proven More Effective Than Adam, AdaGrad, and RMSprop for Deep Neural Networks",
        "completion2":"New 'Mini-Block Fisher' Method Offers Hope for Accelerating Training of Deep Neural Networks",
        "completion3":"Mini-Block Fisher' May Be Key to Achieving Human-Level AI",
        "technologyreview":0.2110399448,
        "venturebeat":0.1697172825,
        "wired":0.0497328737,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.04124v3",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1644350508000,
        "code_mentioned":1,
        "readability":0.79
    },
    {
        "arxiv_id":"2203.03923v1",
        "predicted_newsworthiness":0.3971832453,
        "title":"ROLL: Long-Term Robust LiDAR-based Localization With Temporary Mapping in Changing Environments",
        "summary":"Long-term scene changes present challenges to localization systems using a pre-built map. This paper presents a LiDAR-based system that can provide robust localization against those challenges. Our method starts with activation of a mapping process temporarily when global matching towards the pre-built map is unreliable. The temporary map will be merged onto the pre-built map for later localization runs once reliable matching is obtained again. We further integrate a LiDAR inertial odometry (LIO) to provide motion-compensated LiDAR scans and a reliable initial pose guess for the global matching module. To generate a smooth real-time trajectory for navigation purposes, we fuse poses from odometry and global matching by solving a pose graph optimization problem. We evaluate our localization system with extensive experiments on the NCLT dataset including a variety of changing indoor and outdoor environments, and the results demonstrate a robust and accurate localization performance for over a year. The implementations are open sourced on GitHub.",
        "completion1":"Roll: A Long-Term, Robust Localization System for Changing Environments",
        "completion2":"oll: A New LiDAR-based Localization System for Changing Environments",
        "completion3":"oll: A Revolutionary New Localization System for Changing Environments",
        "technologyreview":0.188568565,
        "venturebeat":0.159732719,
        "wired":0.035458627,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.03923v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1646728982000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2203.00710v1",
        "predicted_newsworthiness":0.7554659264,
        "title":"Understanding Effects of Algorithmic vs. Community Label on Perceived Accuracy of Hyper-partisan Misinformation",
        "summary":"Hyper-partisan misinformation has become a major public concern. In order to examine what type of misinformation label can mitigate hyper-partisan misinformation sharing on social media, we conducted a 4 (label type: algorithm, community, third-party fact-checker, and no label) X 2 (post ideology: liberal vs. conservative) between-subjects online experiment (N = 1,677) in the context of COVID-19 health information. The results suggest that for liberal users, all labels reduced the perceived accuracy and believability of fake posts regardless of the posts' ideology. In contrast, for conservative users, the efficacy of the labels depended on whether the posts were ideologically consistent: algorithmic labels were more effective in reducing the perceived accuracy and believability of fake conservative posts compared to community labels, whereas all labels were effective in reducing their belief in liberal posts. Our results shed light on the differing effects of various misinformation labels dependent on people's political ideology.",
        "completion1":"Different types of misinformation labels have different effects on people's perception of accuracy, depending on their political ideology.",
        "completion2":"Algorithmic labels are more effective in reducing the perceived accuracy and believability of fake conservative posts compared to community labels.",
        "completion3":"All labels are effective in reducing the belief in liberal posts.",
        "technologyreview":0.3409122906,
        "venturebeat":0.2332493939,
        "wired":0.0967344175,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.00710v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.si"
        ],
        "published":1646162116000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2205.04617v1",
        "predicted_newsworthiness":0.4820992735,
        "title":"CoDo: Contrastive Learning with Downstream Background Invariance for Detection",
        "summary":"The prior self-supervised learning researches mainly select image-level instance discrimination as pretext task. It achieves a fantastic classification performance that is comparable to supervised learning methods. However, with degraded transfer performance on downstream tasks such as object detection. To bridge the performance gap, we propose a novel object-level self-supervised learning method, called Contrastive learning with Downstream background invariance (CoDo). The pretext task is converted to focus on instance location modeling for various backgrounds, especially for downstream datasets. The ability of background invariance is considered vital for object detection. Firstly, a data augmentation strategy is proposed to paste the instances onto background images, and then jitter the bounding box to involve background information. Secondly, we implement architecture alignment between our pretraining network and the mainstream detection pipelines. Thirdly, hierarchical and multi views contrastive learning is designed to improve performance of visual representation learning. Experiments on MSCOCO demonstrate that the proposed CoDo with common backbones, ResNet50-FPN, yields strong transfer learning results for object detection.",
        "completion1":"A new self-supervised learning method called CoDo could improve transfer performance on downstream tasks such as object detection.",
        "completion2":"The pretext task in CoDo is converted to focus on instance location modeling for various backgrounds.",
        "completion3":"Hierarchical and multi views contrastive learning is designed to improve performance of visual representation learning in CoDo.",
        "technologyreview":0.2434984152,
        "venturebeat":0.1883049114,
        "wired":0.0445429863,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.04617v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1652145975000,
        "code_mentioned":1,
        "readability":0.78
    },
    {
        "arxiv_id":"2204.08078v2",
        "predicted_newsworthiness":0.8125201845,
        "title":"A Psycho-linguistic Analysis of BitChute",
        "summary":"In order to better support researchers, journalist, and practitioners in their use of the MeLa-BitChute dataset for exploration and investigative reporting, we provide new psycho-linguistic metadata for the videos, comments, and channels in the dataset using LIWC22. This paper describes that metadata and methods to filter the data using the metadata. In addition, we provide basic analysis and comparison of the language on BitChute to other social media platforms. The MeLa-BitChute dataset and LIWC metadata described in this paper can be found at: https:\/\/dataverse.harvard.edu\/dataset.xhtml?persistentId=doi:10.7910\/DVN\/KRD1VS.",
        "completion1":"New psycho-linguistic metadata for the videos, comments, and channels in the MeLa-BitChute dataset.",
        "completion2":"Methods to filter the data using the new metadata.",
        "completion3":"Basic analysis and comparison of the language on BitChute to other social media platforms.",
        "technologyreview":0.3627355532,
        "venturebeat":0.3036509458,
        "wired":0.1466807884,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.08078v2",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy",
            "cs.cl",
            "cs.si"
        ],
        "published":1650226202000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2203.02227v1",
        "predicted_newsworthiness":0.3414856596,
        "title":"Partial Wasserstein Adversarial Network for Non-rigid Point Set Registration",
        "summary":"Given two point sets, the problem of registration is to recover a transformation that matches one set to the other. This task is challenging due to the presence of the large number of outliers, the unknown non-rigid deformations and the large sizes of point sets. To obtain strong robustness against outliers, we formulate the registration problem as a partial distribution matching (PDM) problem, where the goal is to partially match the distributions represented by point sets in a metric space. To handle large point sets, we propose a scalable PDM algorithm by utilizing the efficient partial Wasserstein-1 (PW) discrepancy. Specifically, we derive the Kantorovich-Rubinstein duality for the PW discrepancy, and show its gradient can be explicitly computed. Based on these results, we propose a partial Wasserstein adversarial network (PWAN), which is able to approximate the PW discrepancy by a neural network, and minimize it by gradient descent. In addition, it also incorporates an efficient coherence regularizer for non-rigid transformations to avoid unrealistic deformations. We evaluate PWAN on practical point set registration tasks, and show that the proposed PWAN is robust, scalable and performs more favorably than the state-of-the-art methods.",
        "completion1":"New AI algorithm can handle large point sets and unknown non-rigid deformations for robust registration",
        "completion2":"Kantorovich-Rubinstein duality used to develop Partial Wasserstein Adversarial Network",
        "completion3":"PWAN more favorable than state-of-the-art methods for practical point set registration tasks",
        "technologyreview":0.1844820193,
        "venturebeat":0.1394794919,
        "wired":0.0294627385,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.02227v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1646389428000,
        "code_mentioned":1,
        "readability":0.77
    },
    {
        "arxiv_id":"2207.01178v1",
        "predicted_newsworthiness":0.3269445707,
        "title":"An Improved Probability Propagation Algorithm for Density Peak Clustering Based on Natural Nearest Neighborhood",
        "summary":"Clustering by fast search and find of density peaks (DPC) (Since, 2014) has been proven to be a promising clustering approach that efficiently discovers the centers of clusters by finding the density peaks. The accuracy of DPC depends on the cutoff distance ($d_c$), the cluster number ($k$) and the selection of the centers of clusters. Moreover, the final allocation strategy is sensitive and has poor fault tolerance. The shortcomings above make the algorithm sensitive to parameters and only applicable for some specific datasets. To overcome the limitations of DPC, this paper presents an improved probability propagation algorithm for density peak clustering based on the natural nearest neighborhood (DPC-PPNNN). By introducing the idea of natural nearest neighborhood and probability propagation, DPC-PPNNN realizes the nonparametric clustering process and makes the algorithm applicable for more complex datasets. In experiments on several datasets, DPC-PPNNN is shown to outperform DPC, K-means and DBSCAN.",
        "completion1":"An Improved Probability Propagation Algorithm for Density Peak Clustering Based on Natural Nearest Neighborhood.",
        "completion2":"The accuracy of DPC depends on the cutoff distance () and the selection of the centers of clusters.",
        "completion3":"By introducing the idea of natural nearest neighborhood and probability propagation, DPC-PPNNN realizes the nonparametric clustering process and makes the algorithm applicable for more complex datasets.",
        "technologyreview":0.1256738857,
        "venturebeat":0.1147471933,
        "wired":-0.0037968662,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.01178v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1656905817000,
        "code_mentioned":0,
        "readability":0.72
    },
    {
        "arxiv_id":"2207.03692v1",
        "predicted_newsworthiness":0.5757530881,
        "title":"Mining Discriminative Food Regions for Accurate Food Recognition",
        "summary":"Automatic food recognition is the very first step towards passive dietary monitoring. In this paper, we address the problem of food recognition by mining discriminative food regions. Taking inspiration from Adversarial Erasing, a strategy that progressively discovers discriminative object regions for weakly supervised semantic segmentation, we propose a novel network architecture in which a primary network maintains the base accuracy of classifying an input image, an auxiliary network adversarially mines discriminative food regions, and a region network classifies the resulting mined regions. The global (the original input image) and the local (the mined regions) representations are then integrated for the final prediction. The proposed architecture denoted as PAR-Net is end-to-end trainable, and highlights discriminative regions in an online fashion. In addition, we introduce a new fine-grained food dataset named as Sushi-50, which consists of 50 different sushi categories. Extensive experiments have been conducted to evaluate the proposed approach. On three food datasets chosen (Food-101, Vireo-172, and Sushi-50), our approach performs consistently and achieves state-of-the-art results (top-1 testing accuracy of $90.4\\%$, $90.2\\%$, $92.0\\%$, respectively) compared with other existing approaches. Dataset and code are available at https:\/\/github.com\/Jianing-Qiu\/PARNet",
        "completion1":"Mining Discriminative Food Regions for Accurate Food Recognition",
        "completion2":"A Novel Network Architecture for Food Recognition",
        "completion3":"PAR-Net: A New Approach to Food Recognition",
        "technologyreview":0.252769305,
        "venturebeat":0.1856847677,
        "wired":0.0414018423,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.03692v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1657256964000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2206.02633v1",
        "predicted_newsworthiness":0.4665224621,
        "title":"Towards Fair Federated Recommendation Learning: Characterizing the Inter-Dependence of System and Data Heterogeneity",
        "summary":"Federated learning (FL) is an effective mechanism for data privacy in recommender systems by running machine learning model training on-device. While prior FL optimizations tackled the data and system heterogeneity challenges faced by FL, they assume the two are independent of each other. This fundamental assumption is not reflective of real-world, large-scale recommender systems -- data and system heterogeneity are tightly intertwined. This paper takes a data-driven approach to show the inter-dependence of data and system heterogeneity in real-world data and quantifies its impact on the overall model quality and fairness. We design a framework, RF^2, to model the inter-dependence and evaluate its impact on state-of-the-art model optimization techniques for federated recommendation tasks. We demonstrate that the impact on fairness can be severe under realistic heterogeneity scenarios, by up to 15.8--41x compared to a simple setup assumed in most (if not all) prior work. It means when realistic system-induced data heterogeneity is not properly modeled, the fairness impact of an optimization can be downplayed by up to 41x. The result shows that modeling realistic system-induced data heterogeneity is essential to achieving fair federated recommendation learning. We plan to open-source RF^2 to enable future design and evaluation of FL innovations.",
        "completion1":"Towards Fair Federated Recommendation Learning: Characterizing the Inter-Dependence of System and Data Heterogeneity",
        "completion2":"Federated learning is an effective mechanism for data privacy in recommender systems by running machine learning model training on-device.",
        "completion3":"RF^2 is a framework to model the inter-dependence and evaluate its impact on state-of-the-art model optimization techniques for federated recommendation tasks.",
        "technologyreview":0.2953306444,
        "venturebeat":0.2652599299,
        "wired":0.0802908338,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.02633v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.lg"
        ],
        "published":1653944375000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2201.02850v1",
        "predicted_newsworthiness":0.6456850436,
        "title":"Image-based Automatic Dial Meter Reading in Unconstrained Scenarios",
        "summary":"The replacement of analog meters with smart meters is costly, laborious, and far from complete in developing countries. The Energy Company of Parana (Copel) (Brazil) performs more than 4 million meter readings (almost entirely of non-smart devices) per month, and we estimate that 850 thousand of them are from dial meters. Therefore, an image-based automatic reading system can reduce human errors, create a proof of reading, and enable the customers to perform the reading themselves through a mobile application. We propose novel approaches for Automatic Dial Meter Reading (ADMR) and introduce a new dataset for ADMR in unconstrained scenarios, called UFPR-ADMR-v2. Our best-performing method combines YOLOv4 with a novel regression approach (AngReg), and explores several postprocessing techniques. Compared to previous works, it decreased the Mean Absolute Error (MAE) from 1,343 to 129 and achieved a meter recognition rate (MRR) of 98.90% -- with an error tolerance of 1 Kilowatt-hour (kWh).",
        "completion1":"Smart Meter Replacement in Developing Countries Is Costly, Laborious",
        "completion2":"New Approach to Automatic Dial Meter Reading Decreases Mean Absolute Error by 90%",
        "completion3":"YOLOv4 Combined with Novel Regression Approach Achieves 98.90% Meter Recognition Rate",
        "technologyreview":0.2435625403,
        "venturebeat":0.225353131,
        "wired":0.0443599808,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.02850v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1641657826000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2207.09685v1",
        "predicted_newsworthiness":0.36191254,
        "title":"BigColor: Colorization using a Generative Color Prior for Natural Images",
        "summary":"For realistic and vivid colorization, generative priors have recently been exploited. However, such generative priors often fail for in-the-wild complex images due to their limited representation space. In this paper, we propose BigColor, a novel colorization approach that provides vivid colorization for diverse in-the-wild images with complex structures. While previous generative priors are trained to synthesize both image structures and colors, we learn a generative color prior to focus on color synthesis given the spatial structure of an image. In this way, we reduce the burden of synthesizing image structures from the generative prior and expand its representation space to cover diverse images. To this end, we propose a BigGAN-inspired encoder-generator network that uses a spatial feature map instead of a spatially-flattened BigGAN latent code, resulting in an enlarged representation space. Our method enables robust colorization for diverse inputs in a single forward pass, supports arbitrary input resolutions, and provides multi-modal colorization results. We demonstrate that BigColor significantly outperforms existing methods especially on in-the-wild images with complex structures.",
        "completion1":"BigColor: A novel colorization approach that provides vivid colorization for diverse in-the-wild images with complex structures.",
        "completion2":"BigColor outperforms existing methods especially on in-the-wild images with complex structures.",
        "completion3":"BigColor supports arbitrary input resolutions and provides multi-modal colorization results.",
        "technologyreview":0.1760092486,
        "venturebeat":0.132876939,
        "wired":0.0496177943,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.09685v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658299006000,
        "code_mentioned":1,
        "readability":0.77
    },
    {
        "arxiv_id":"2207.04673v1",
        "predicted_newsworthiness":0.3714734157,
        "title":"Learning Spatial and Temporal Variations for 4D Point Cloud Segmentation",
        "summary":"LiDAR-based 3D scene perception is a fundamental and important task for autonomous driving. Most state-of-the-art methods on LiDAR-based 3D recognition tasks focus on single frame 3D point cloud data, and the temporal information is ignored in those methods. We argue that the temporal information across the frames provides crucial knowledge for 3D scene perceptions, especially in the driving scenario. In this paper, we focus on spatial and temporal variations to better explore the temporal information across the 3D frames. We design a temporal variation-aware interpolation module and a temporal voxel-point refiner to capture the temporal variation in the 4D point cloud. The temporal variation-aware interpolation generates local features from the previous and current frames by capturing spatial coherence and temporal variation information. The temporal voxel-point refiner builds a temporal graph on the 3D point cloud sequences and captures the temporal variation with a graph convolution module. The temporal voxel-point refiner also transforms the coarse voxel-level predictions into fine point-level predictions. With our proposed modules, the new network TVSN achieves state-of-the-art performance on SemanticKITTI and SemantiPOSS. Specifically, our method achieves 52.5\\% in mIoU (+5.5% against previous best approaches) on the multiple scan segmentation task on SemanticKITTI, and 63.0% on SemanticPOSS (+2.8% against previous best approaches).",
        "completion1":"Learning Spatial and Temporal Variations for 4D Point Cloud Segmentation",
        "completion2":"LiDAR-based 3D scene perception is a fundamental and important task for autonomous driving",
        "completion3":"Most state-of-the-art methods on LiDAR-based 3D recognition tasks focus on single frame 3D point cloud data, ignoring the temporal information",
        "technologyreview":0.2020356711,
        "venturebeat":0.1802339701,
        "wired":0.0497042106,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.04673v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1657524986000,
        "code_mentioned":0,
        "readability":0.76
    },
    {
        "arxiv_id":"2201.03291v1",
        "predicted_newsworthiness":0.5310066299,
        "title":"A novel interpretable machine learning system to generate clinical risk scores: An application for predicting early mortality or unplanned readmission in a retrospective cohort study",
        "summary":"Risk scores are widely used for clinical decision making and commonly generated from logistic regression models. Machine-learning-based methods may work well for identifying important predictors, but such 'black box' variable selection limits interpretability, and variable importance evaluated from a single model can be biased. We propose a robust and interpretable variable selection approach using the recently developed Shapley variable importance cloud (ShapleyVIC) that accounts for variability across models. Our approach evaluates and visualizes overall variable contributions for in-depth inference and transparent variable selection, and filters out non-significant contributors to simplify model building steps. We derive an ensemble variable ranking from variable contributions, which is easily integrated with an automated and modularized risk score generator, AutoScore, for convenient implementation. In a study of early death or unplanned readmission, ShapleyVIC selected 6 of 41 candidate variables to create a well-performing model, which had similar performance to a 16-variable model from machine-learning-based ranking.",
        "completion1":"A novel machine learning system can generate clinical risk scores that are more accurate than existing models.",
        "completion2":"This new system, called ShapleyVIC, is able to identify important predictors of early mortality and unplanned readmission.",
        "completion3":"The system has the potential to simplify model building steps and improve transparency in clinical decision making.",
        "technologyreview":0.2760900171,
        "venturebeat":0.243353382,
        "wired":0.0749488853,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.03291v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1641813726000,
        "code_mentioned":1,
        "readability":0.82
    }
]