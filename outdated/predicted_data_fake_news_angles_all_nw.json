[
    {
        "arxiv_id":"2207.14086v1",
        "predicted_newsworthiness":0.8860131419,
        "title":"Ever heard of ethical AI? Investigating the salience of ethical AI issues among the German population",
        "summary":"Building and implementing ethical AI systems that benefit the whole society is cost-intensive and a multi-faceted task fraught with potential problems. While computer science focuses mostly on the technical questions to mitigate social issues, social science addresses citizens' perceptions to elucidate social and political demands that influence the societal implementation of AI systems. Thus, in this study, we explore the salience of AI issues in the public with an emphasis on ethical criteria to investigate whether it is likely that ethical AI is actively requested by the population. Between May 2020 and April 2021, we conducted 15 surveys asking the German population about the most important AI-related issues (total of N=14,988 respondents). Our results show that the majority of respondents were not concerned with AI at all. However, it can be seen that general interest in AI and a higher educational level are predictive of some engagement with AI. Among those, who reported having thought about AI, specific applications (e.g., autonomous driving) were by far the most mentioned topics. Ethical issues are voiced only by a small subset of citizens with fairness, accountability, and transparency being the least mentioned ones. These have been identified in several ethical guidelines (including the EU Commission's proposal) as key elements for the development of ethical AI. The salience of ethical issues affects the behavioral intentions of citizens in the way that they 1) tend to avoid AI technology and 2) engage in public discussions about AI. We conclude that the low level of ethical implications may pose a serious problem for the actual implementation of ethical AI for the Common Good and emphasize that those who are presumably most affected by ethical issues of AI are especially unaware of ethical risks. Yet, once ethical AI is top of the mind, there is some potential for activism.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.3196319641,
        "newsscientist":0.3196175518,
        "technologyreview":0.4894837359,
        "venturebeat":0.4248078926,
        "wired":0.4004314725,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14086v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659015973000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.01305v1",
        "predicted_newsworthiness":0.8625239629,
        "title":"Humble Machines: Attending to the Underappreciated Costs of Misplaced Distrust",
        "summary":"It is curious that AI increasingly outperforms human decision makers, yet much of the public distrusts AI to make decisions affecting their lives. In this paper we explore a novel theory that may explain one reason for this. We propose that public distrust of AI is a moral consequence of designing systems that prioritize reduction of costs of false positives over less tangible costs of false negatives. We show that such systems, which we characterize as 'distrustful', are more likely to miscategorize trustworthy individuals, with cascading consequences to both those individuals and the overall human-AI trust relationship. Ultimately, we argue that public distrust of AI stems from well-founded concern about the potential of being miscategorized. We propose that restoring public trust in AI will require that systems are designed to embody a stance of 'humble trust', whereby the moral costs of the misplaced distrust associated with false negatives is weighted appropriately during development and use.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2930865734,
        "newsscientist":0.3178910533,
        "technologyreview":0.4846068623,
        "venturebeat":0.4255554812,
        "wired":0.3997664167,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01305v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659428669000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.11521v1",
        "predicted_newsworthiness":0.8402257629,
        "title":"Vaccine Discourse on Twitter During the COVID-19 Pandemic",
        "summary":"Since the onset of the COVID-19 pandemic, vaccines have been an important topic in public discourse. The discussions around vaccines are polarized as some see them as an important measure to end the pandemic, and others are hesitant or find them harmful. This study investigates posts related to COVID-19 vaccines on Twitter and focuses on those which have a negative stance toward vaccines. A dataset of 16,713,238 English tweets related to COVID-19 vaccines was collected covering the period from March 1, 2020, to July 31, 2021. We used the Scikit-learn Python library to apply a support vector machine (SVM) classifier to identify the tweets with a negative stance toward the COVID-19 vaccines. A total of 5,163 tweets were used to train the classifier, out of which a subset of 2,484 tweets were manually annotated by us and made publicly available. We used the BERTtopic model to extract and investigate the topics discussed within the negative tweets and how they changed over time. We show that the negativity with respect to COVID-19 vaccines has decreased over time along with the vaccine roll-outs. We identify 37 topics of discussion and present their respective importance over time. We show that popular topics consist of conspiratorial discussions such as 5G towers and microchips, but also contain legitimate concerns around vaccination safety and side effects as well as concerns about policies. Our study shows that even unpopular opinions or conspiracy theories can become widespread when paired with a widely popular discussion topic such as COVID-19 vaccines. Understanding the concerns and the discussed topics and how they change over time is essential for policymakers and public health authorities to provide better and in-time information and policies, to facilitate vaccination of the population in future similar crises.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.3844258275,
        "newsscientist":0.3448199254,
        "technologyreview":0.4124848021,
        "venturebeat":0.3181777582,
        "wired":0.3744791755,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11521v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy",
            "cs.cl",
            "cs.si"
        ],
        "published":1658584251000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.00681v1",
        "predicted_newsworthiness":0.8233200001,
        "title":"The Many Facets of Trust in AI: Formalizing the Relation Between Trust and Fairness, Accountability, and Transparency",
        "summary":"Efforts to promote fairness, accountability, and transparency are assumed to be critical in fostering Trust in AI (TAI), but extant literature is frustratingly vague regarding this 'trust'. The lack of exposition on trust itself suggests that trust is commonly understood, uncomplicated, or even uninteresting. But is it? Our analysis of TAI publications reveals numerous orientations which differ in terms of who is doing the trusting (agent), in what (object), on the basis of what (basis), in order to what (objective), and why (impact). We develop an ontology that encapsulates these key axes of difference to a) illuminate seeming inconsistencies across the literature and b) more effectively manage a dizzying number of TAI considerations. We then reflect this ontology through a corpus of publications exploring fairness, accountability, and transparency to examine the variety of ways that TAI is considered within and between these approaches to promoting trust.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2252426563,
        "newsscientist":0.2432175128,
        "technologyreview":0.418128042,
        "venturebeat":0.3744825468,
        "wired":0.3257343687,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00681v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659342417000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.12555v1",
        "predicted_newsworthiness":0.8124202163,
        "title":"Ethics for social robotics: A critical analysis",
        "summary":"Social robotics development for the practice of care and European prospects to incorporate these AI-based systems in institutional healthcare contexts call for an urgent ethical reflection to (re)configurate our practical life according to human values and rights. Despite the growing attention to the ethical implications of social robotics, the current debate on one of its central branches, social assistive robotics (SAR), rests upon an impoverished ethical approach. This paper presents and examines some tendencies of this prevailing approach, which have been identified as a result of a critical literature review. Based on this analysis of a representative case of how ethical reflection is being led towards social robotics, some future research lines are outlined, which may help reframe and deepen in its ethical implications.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2697229953,
        "newsscientist":0.274276975,
        "technologyreview":0.3750523503,
        "venturebeat":0.3043923991,
        "wired":0.3041642676,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12555v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy",
            "cs.ro"
        ],
        "published":1658787480000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.00280v1",
        "predicted_newsworthiness":0.8063317058,
        "title":"How to Make Users Adopt More Sustainable Cryptocurrencies: Evidence from Nigeria",
        "summary":"Some of the most popular decentralised cryptocurrency networks have drawn widespread criticism for consuming vast amounts of electricity and have thus become targets of regulatory interest. Attempts to influence cryptocurrency network operations via policy in the pursuit of sustainability in the past, however, have been widely unsuccessful. Some were abandoned out of fear of jeopardising innovation while others failed due to the highly globalised nature of decentralised systems. Considering Bitcoin as an archetype for cryptocurrencies with high energy demand, this study takes a bottom-up approach by analysing statements made by Nigerian cryptocurrency users ($N = 158$) concerning their perception of sustainability issues. Three main findings emerged: 1) Despite self-reporting as highly knowledgeable, most participants significantly underestimate the energy demand of Bitcoin. 2) Those who accurately assess the energy demand of Bitcoin are more likely to support measures targeting its energy demand than those who misestimate it. 3) Those who support measures predominantly hold private actors responsible. In light of these findings, it is concluded that the primary task of policy makers in the context of cryptocurrency sustainability is to enforce consumer education.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.3041697502,
        "newsscientist":0.278828296,
        "technologyreview":0.3551121535,
        "venturebeat":0.2747786999,
        "wired":0.2836655742,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00280v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659202245000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.02056v1",
        "predicted_newsworthiness":0.8006555235,
        "title":"Fast or Accurate? Governing Conflicting Goals in Highly Autonomous Vehicles",
        "summary":"The tremendous excitement around the deployment of autonomous vehicles (AVs) comes from their purported promise. In addition to decreasing accidents, AVs are projected to usher in a new era of equity in human autonomy by providing affordable, accessible, and widespread mobility for disabled, elderly, and low-income populations. However, to realize this promise, it is necessary to ensure that AVs are safe for deployment, and to contend with the risks AV technology poses, which threaten to eclipse its benefits. In this Article, we focus on an aspect of AV engineering currently unexamined in the legal literature, but with critical implications for safety, accountability, liability, and power. Specifically, we explain how understanding the fundamental engineering trade-off between accuracy and speed in AVs is critical for policymakers to regulate the uncertainty and risk inherent in AV systems. We discuss how understanding the trade-off will help create tools that will enable policymakers to assess how the trade-off is being implemented. Such tools will facilitate opportunities for developing concrete, ex ante AV safety standards and conclusive mechanisms for ex post determination of accountability after accidents occur. This will shift the balance of power from manufacturers to the public by facilitating effective regulation, reducing barriers to tort recovery, and ensuring that public values like safety and accountability are appropriately balanced.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.252143492,
        "newsscientist":0.2494928565,
        "technologyreview":0.4119373094,
        "venturebeat":0.3531646938,
        "wired":0.3730778011,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02056v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659533065000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.01350v1",
        "predicted_newsworthiness":0.7819377184,
        "title":"Application of Blockchain Smart Contracts in E-Commerce and Government",
        "summary":"With technological advances and the establishment of e-commerce models, business challenges have shifted to online platforms. The promise of embedding self-executing and autonomous programs into blockchain technologies has attracted increased interest and its use in niche solutions. Using qualitative interviews, this paper sought the opinions of the eleven industry leaders regarding smart contracts. Findings reveal that the technology is gaining momentum in e-commerce, particularly in financial transfer, record-keeping, real estate, and property management, insurance, mortgage, supply chain management, data storage, authorization of credit, denaturalized intelligence, aviation sector, shipping of products, invoice financing and other domains. The significant benefits of widespread adoption and deployment of smart contracts include their capability to deliver decentralization, efficacy, cost-effectiveness, transparency, speed, autonomy, transparency, privacy, and security, encouraging the emergence of novel business models. Albeit these benefits that revolutionize online transactions, the technology faced multifaceted challenges. Smart technologies are only a decade old and are not advanced in security, transparency, cost-effectiveness, and regulatory framework. Furthermore, organizational, and technical challenges limit their deployment: incompatibility with legacy systems, scalability, bugs, speed, and lack of talent and understanding regarding smart contracts. Consequently, policymakers, developers, researchers, practitioners, and other stakeholders need to invest effort and time to foster the technologies and address pertinent issues to enable the global adoption of smart contracts by small and big businesses.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2569041657,
        "newsscientist":0.2023218446,
        "technologyreview":0.4087943456,
        "venturebeat":0.3932755114,
        "wired":0.3312630576,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01350v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659436441000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.11474v1",
        "predicted_newsworthiness":0.7758885165,
        "title":"Investigating the Validity of Botometer-based Social Bot Studies",
        "summary":"The idea that social media platforms like Twitter are inhabited by vast numbers of social bots has become widely accepted in recent years. Social bots are assumed to be automated social media accounts operated by malicious actors with the goal of manipulating public opinion. They are credited with the ability to produce content autonomously and to interact with human users. Social bot activity has been reported in many different political contexts, including the U.S. presidential elections, discussions about migration, climate change, and COVID-19. However, the relevant publications either use crude and questionable heuristics to discriminate between supposed social bots and humans or -- in the vast majority of the cases -- fully rely on the output of automatic bot detection tools, most commonly Botometer. In this paper, we point out a fundamental theoretical flaw in the widely-used study design for estimating the prevalence of social bots. Furthermore, we empirically investigate the validity of peer-reviewed Botometer-based studies by closely and systematically inspecting hundreds of accounts that had been counted as social bots. We were unable to find a single social bot. Instead, we found mostly accounts undoubtedly operated by human users, the vast majority of them using Twitter in an inconspicuous and unremarkable fashion without the slightest traces of automation. We conclude that studies claiming to investigate the prevalence, properties, or influence of social bots based on Botometer have, in reality, just investigated false positives and artifacts of this approach.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2931209385,
        "newsscientist":0.2629624584,
        "technologyreview":0.4318042705,
        "venturebeat":0.3666187813,
        "wired":0.4049302556,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11474v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.cy",
            "cs.hc"
        ],
        "published":1658568690000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.14677v1",
        "predicted_newsworthiness":0.7655344929,
        "title":"Big Data and Analytics Implementation in Tertiary Institutions to Predict Students Performance in Nigeria",
        "summary":"The term Big Data has been coined to refer to the gargantuan bulk of data that cannot be dealt with by traditional data-handling techniques. Big Data is still a novel concept, and in the following literature, we intend to elaborate on it in a palpable fashion. It commences with the concept of the subject in itself, along with its properties and the two general approaches to dealing with it. Big Data provides an opportunity for educational Institutions to use their Information Technology resources strategically to improve educational quality, guide students to higher completion rates and improve student persistence and outcomes. This paper explores the attributes of big data that are relevant to educational institutions, investigates the factors influencing the adoption of big data and analytics in learning institutions, and seeks to establish the limiting factors hindering the use of big data in Institutions of higher learning. A survey research design was adopted in conducting this research, and Questionnaires were the instrument employed for data collection.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.227607648,
        "newsscientist":0.1863571064,
        "technologyreview":0.2939845726,
        "venturebeat":0.2825912849,
        "wired":0.2206322776,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14677v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy",
            "cs.ai",
            "cs.lg"
        ],
        "published":1659102744000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.01509v1",
        "predicted_newsworthiness":0.7646605236,
        "title":"Characterizing Vaccination Movements on YouTube in the United States and Brazil",
        "summary":"In the context of COVID-19 pandemic, social networks such as Twitter and YouTube stand out as important sources of information. YouTube, as the largest and most engaging online media consumption platform, has a large influence in the spread of information and misinformation, which makes it important to study how it deals with the problems that arise from disinformation, as well as how its users interact with different types of content. Considering that United States (USA) and Brazil (BR) are two countries with the highest COVID-19 death tolls, we asked the following question: What are the nuances of vaccination campaigns in the two countries? With that in mind, we engage in a comparative analysis of pro and anti-vaccine movements on YouTube. We also investigate the role of YouTube in countering online vaccine misinformation in USA and BR. For this means, we monitored the removal of vaccine related content on the platform and also applied various techniques to analyze the differences in discourse and engagement in pro and anti-vaccine \"comment sections\". We found that American anti-vaccine content tend to lead to considerably more toxic and negative discussion than their pro-vaccine counterparts while also leading to 18% higher user-user engagement, while Brazilian anti-vaccine content was significantly less engaging. We also found that pro-vaccine and anti-vaccine discourses are considerably different as the former is associated with conspiracy theories (e.g. ccp), misinformation and alternative medicine (e.g. hydroxychloroquine), while the latter is associated with protective measures. Finally, it was observed that YouTube content removals are still insufficient, with only approximately 16% of the anti-vaccine content being removed by the end of the studied period, with the USA registering the highest percentage of removed anti-vaccine content(34%) and BR registering the lowest(9.8%).",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.3838318719,
        "newsscientist":0.3288438803,
        "technologyreview":0.4125312808,
        "venturebeat":0.3112443754,
        "wired":0.3988916917,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01509v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1659452061000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.13913v1",
        "predicted_newsworthiness":0.7580802271,
        "title":"A health telemonitoring platform based on data integration from different sources",
        "summary":"The management of people with long-term or chronic illness is one of the biggest challenges for national health systems. In fact, these diseases are among the leading causes of hospitalization, especially for the elderly, and huge amount of resources required to monitor them leads to problems with sustainability of the healthcare systems. The increasing diffusion of portable devices and new connectivity technologies allows the implementation of telemonitoring system capable of providing support to health care providers and lighten the burden on hospitals and clinics. In this paper, we present the implementation of a telemonitoring platform for healthcare, designed to capture several types of physiological health parameters from different consumer mobile and custom devices. Consumer medical devices can be integrated into the platform via the Google Fit ecosystem that supports hundreds of devices, while custom devices can directly interact with the platform with standard communication protocols. The platform is designed to process the acquired data using machine learning algorithms, and to provide patients and physicians the physiological health parameters with a user-friendly, comprehensive, and easy to understand dashboard which monitors the parameters through time. Preliminary usability tests show a good user satisfaction in terms of functionality and usefulness.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2091557729,
        "newsscientist":0.2464104139,
        "technologyreview":0.3597847185,
        "venturebeat":0.3782604527,
        "wired":0.3159808618,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13913v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy",
            "cs.ai",
            "cs.hc"
        ],
        "published":1658992384000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.11569v1",
        "predicted_newsworthiness":0.7527614891,
        "title":"Robots Enact Malignant Stereotypes",
        "summary":"Stereotypes, bias, and discrimination have been extensively documented in Machine Learning (ML) methods such as Computer Vision (CV) [18, 80], Natural Language Processing (NLP) [6], or both, in the case of large image and caption models such as OpenAI CLIP [14]. In this paper, we evaluate how ML bias manifests in robots that physically and autonomously act within the world. We audit one of several recently published CLIP-powered robotic manipulation methods, presenting it with objects that have pictures of human faces on the surface which vary across race and gender, alongside task descriptions that contain terms associated with common stereotypes. Our experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy, at scale. Furthermore, the audited methods are less likely to recognize Women and People of Color. Our interdisciplinary sociotechnical analysis synthesizes across fields and applications such as Science Technology and Society (STS), Critical Studies, History, Safety, Robotics, and AI. We find that robots powered by large datasets and Dissolution Models (sometimes called \"foundation models\", e.g. CLIP) that contain humans risk physically amplifying malignant stereotypes in general; and that merely correcting disparities will be insufficient for the complexity and scale of the problem. Instead, we recommend that robot learning methods that physically manifest stereotypes or other harmful outcomes be paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just. Finally, we discuss comprehensive policy changes and the potential of new interdisciplinary research on topics like Identity Safety Assessment Frameworks and Design Justice to better understand and address these harms.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2938802498,
        "newsscientist":0.3127555077,
        "technologyreview":0.467283079,
        "venturebeat":0.3986650812,
        "wired":0.3901454749,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11569v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai",
            "cs.cv",
            "cs.cy",
            "cs.lg"
        ],
        "published":1658599692000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.11602v1",
        "predicted_newsworthiness":0.7505031382,
        "title":"Challenges Faced by Teaching Assistants in Computer Science Education Across Europe",
        "summary":"Teaching assistants (TAs) are heavily used in computer science courses as a way to handle high enrollment and still being able to offer students individual tutoring and detailed assessments. TAs are themselves students who take on this additional role in parallel with their own studies at the same institution. Previous research has shown that being a TA can be challenging but has mainly been conducted on TAs from a single institution or within a single course. This paper offers a multi-institutional, multi-national perspective of challenges that TAs in computer science face. This has been done by conducting a thematic analysis of 180 reflective essays written by TAs from three institutions across Europe. The thematic analysis resulted in five main challenges: becoming a professional TA, student focused challenges, assessment, defining and using best practice, and threats to best practice. In addition, these challenges were all identified within the essays from all three institutions, indicating that the identified challenges are not particularly context-dependent. Based on these findings, we also outline implications for educators involved in TA training and coordinators of computer science courses with TAs.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.219857159,
        "newsscientist":0.1635914758,
        "technologyreview":0.2398843092,
        "venturebeat":0.2019276759,
        "wired":0.1889864019,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11602v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1658610269000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.00249v1",
        "predicted_newsworthiness":0.7467260496,
        "title":"Cause-and-Effect Analysis of ADAS: A Comparison Study between Literature Review and Complaint Data",
        "summary":"Advanced driver assistance systems (ADAS) are designed to improve vehicle safety. However, it is difficult to achieve such benefits without understanding the causes and limitations of the current ADAS and their possible solutions. This study 1) investigated the limitations and solutions of ADAS through a literature review, 2) identified the causes and effects of ADAS through consumer complaints using natural language processing models, and 3) compared the major differences between the two. These two lines of research identified similar categories of ADAS causes, including human factors, environmental factors, and vehicle factors. However, academic research focused more on human factors of ADAS issues and proposed advanced algorithms to mitigate such issues while drivers complained more of vehicle factors of ADAS failures, which led to associated top consequences. The findings from these two sources tend to complement each other and provide important implications for the improvement of ADAS in the future.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2027812493,
        "newsscientist":0.2270131969,
        "technologyreview":0.3739325604,
        "venturebeat":0.347231182,
        "wired":0.3165305471,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00249v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659194158000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00721v1",
        "predicted_newsworthiness":0.7326938567,
        "title":"Domain Analysis of Ethical, Social and Environmental Accounting Methods",
        "summary":"Ethical, social and environmental accounting is the practice of assessing and reporting organisations' performance on environmental, social and governance topics. There are ample methods that describe how to perform such sustainability assessments. This report presents a domain analysis of ethical, social and environmental accounting methods. Our analysis contains 21 methods. Each method is modelled as a process deliverable diagram. The diagrams have been validated by experts in the methods. The diagrams lay the foundation for further analysis and software development. In this report, we touch upon the ethical, social and environmental accounting method ontology that has been created based on the domain analysis.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2303333567,
        "newsscientist":0.1771144121,
        "technologyreview":0.2011265489,
        "venturebeat":0.1848841107,
        "wired":0.144423744,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00721v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659348721000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.01112v1",
        "predicted_newsworthiness":0.7233488516,
        "title":"VacciNet: Towards a Smart Framework for Learning the Distribution Chain Optimization of Vaccines for a Pandemic",
        "summary":"Vaccinations against viruses have always been the need of the hour since long past. However, it is hard to efficiently distribute the vaccines (on time) to all the corners of a country, especially during a pandemic. Considering the vastness of the population, diversified communities, and demands of a smart society, it is an important task to optimize the vaccine distribution strategy in any country\/state effectively. Although there is a profusion of data (Big Data) from various vaccine administration sites that can be mined to gain valuable insights about mass vaccination drives, very few attempts has been made towards revolutionizing the traditional mass vaccination campaigns to mitigate the socio-economic crises of pandemic afflicted countries. In this paper, we bridge this gap in studies and experimentation. We collect daily vaccination data which is publicly available and carefully analyze it to generate meaning-full insights and predictions. We put forward a novel framework leveraging Supervised Learning and Reinforcement Learning (RL) which we call VacciNet, that is capable of learning to predict the demand of vaccination in a state of a country as well as suggest optimal vaccine allocation in the state for minimum cost of procurement and supply. At the present, our framework is trained and tested with vaccination data of the USA.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.3392498895,
        "newsscientist":0.3202406679,
        "technologyreview":0.3893165145,
        "venturebeat":0.3417112838,
        "wired":0.274602864,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01112v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cy"
        ],
        "published":1659382653000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01355v1",
        "predicted_newsworthiness":0.7232059955,
        "title":"A Comparative Study on COVID-19 Fake News Detection Using Different Transformer Based Models",
        "summary":"The rapid advancement of social networks and the convenience of internet availability have accelerated the rampant spread of false news and rumors on social media sites. Amid the COVID 19 epidemic, this misleading information has aggravated the situation by putting peoples mental and physical lives in danger. To limit the spread of such inaccuracies, identifying the fake news from online platforms could be the first and foremost step. In this research, the authors have conducted a comparative analysis by implementing five transformer based models such as BERT, BERT without LSTM, ALBERT, RoBERTa, and a Hybrid of BERT & ALBERT in order to detect the fraudulent news of COVID 19 from the internet. COVID 19 Fake News Dataset has been used for training and testing the models. Among all these models, the RoBERTa model has performed better than other models by obtaining an F1 score of 0.98 in both real and fake classes.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2382013794,
        "newsscientist":0.2409579808,
        "technologyreview":0.3279640589,
        "venturebeat":0.2880117414,
        "wired":0.282955269,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01355v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1659437416000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.13644v1",
        "predicted_newsworthiness":0.7214489131,
        "title":"Using Deep Learning to Detecting Deepfakes",
        "summary":"In the recent years, social media has grown to become a major source of information for many online users. This has given rise to the spread of misinformation through deepfakes. Deepfakes are videos or images that replace one persons face with another computer-generated face, often a more recognizable person in society. With the recent advances in technology, a person with little technological experience can generate these videos. This enables them to mimic a power figure in society, such as a president or celebrity, creating the potential danger of spreading misinformation and other nefarious uses of deepfakes. To combat this online threat, researchers have developed models that are designed to detect deepfakes. This study looks at various deepfake detection models that use deep learning algorithms to combat this looming threat. This survey focuses on providing a comprehensive overview of the current state of deepfake detection models and the unique approaches many researchers take to solving this problem. The benefits, limitations, and suggestions for future work will be thoroughly discussed throughout this paper.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2247113403,
        "newsscientist":0.2476348717,
        "technologyreview":0.4010318635,
        "venturebeat":0.3384334261,
        "wired":0.3426181491,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13644v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1658941516000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11713v1",
        "predicted_newsworthiness":0.7101547768,
        "title":"Discovering adoption barriers of Clinical Decision Support Systems in primary health care sector",
        "summary":"Adopting a good health information system (HIS) is essential for providing high-quality healthcare. With rapid advances in technology in the healthcare industry in recent years, healthcare providers seek effective options to deal with numerous diseases and a growing number of patients, adopting advanced HIS such as for clinical decision support. While the clinical decision support systems (CDSS) can help medical personnel make better decisions, they may bring negative results due to a lack of understanding of the elements that influence GP's adoption of CDSS. This paper focuses on discovering obstacles that may contribute to the problems surrounding CDSS adoption. Thirty general practitioners were interviewed from different primary health centers in Saudi Arabia in order to determine the challenges and obstacles in the sector. While the outcome confirms that there are obstacles that affect the aspects, such as time risk, quality of the system used, slow Internet speed, user interface, lack of training, high costs, patient satisfaction, multiple systems used, technical support, computer skills, lack of flexibility, system update, professional skills and knowledge, computer efficiency and quality and accuracy of data.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2127504143,
        "newsscientist":0.1885409565,
        "technologyreview":0.2691177098,
        "venturebeat":0.2697390451,
        "wired":0.187423126,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11713v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1658659775000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.02187v1",
        "predicted_newsworthiness":0.7067764891,
        "title":"On the independence between phenomenal consciousness and computational intelligence",
        "summary":"Consciousness and intelligence are properties commonly understood as dependent by folk psychology and society in general. The term artificial intelligence and the kind of problems that it managed to solve in the recent years has been shown as an argument to establish that machines experience some sort of consciousness. Following the analogy of Russell, if a machine is able to do what a conscious human being does, the likelihood that the machine is conscious increases. However, the social implications of this analogy are catastrophic. Concretely, if rights are given to entities that can solve the kind of problems that a neurotypical person can, does the machine have potentially more rights that a person that has a disability? For example, the autistic syndrome disorder spectrum can make a person unable to solve the kind of problems that a machine solves. We believe that the obvious answer is no, as problem solving does not imply consciousness. Consequently, we will argue in this paper how phenomenal consciousness and, at least, computational intelligence are independent and why machines do not possess phenomenal consciousness, although they can potentially develop a higher computational intelligence that human beings. In order to do so, we try to formulate an objective measure of computational intelligence and study how it presents in human beings, animals and machines. Analogously, we study phenomenal consciousness as a dichotomous variable and how it is distributed in humans, animals and machines. As phenomenal consciousness and computational intelligence are independent, this fact has critical implications for society that we also analyze in this work.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2595174929,
        "newsscientist":0.3119430525,
        "technologyreview":0.4074786229,
        "venturebeat":0.3362680605,
        "wired":0.3267331472,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02187v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1659543431000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.11490v1",
        "predicted_newsworthiness":0.7066856645,
        "title":"Towards Smart Fake News Detection Through Explainable AI",
        "summary":"People now see social media sites as their sole source of information due to their popularity. The Majority of people get their news through social media. At the same time, fake news has grown exponentially on social media platforms in recent years. Several artificial intelligence-based solutions for detecting fake news have shown promising results. On the other hand, these detection systems lack explanation capabilities, i.e., the ability to explain why they made a prediction. This paper highlights the current state of the art in explainable fake news detection. We discuss the pitfalls in the current explainable AI-based fake news detection models and present our ongoing research on multi-modal explainable fake news detection model.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1533141012,
        "newsscientist":0.1751418184,
        "technologyreview":0.3154371545,
        "venturebeat":0.2840607785,
        "wired":0.2500151834,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11490v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.ir",
            "cs.si"
        ],
        "published":1658573325000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.14740v1",
        "predicted_newsworthiness":0.705135514,
        "title":"Rating the Crisis of Online Public Opinion Using a Multi-Level Index System",
        "summary":"Online public opinion usually spreads rapidly and widely, thus a small incident probably evolves into a large social crisis in a very short time, and results in a heavy loss in credit or economic aspects. We propose a method to rate the crisis of online public opinion based on a multi-level index system to evaluate the impact of events objectively. Firstly, the dissemination mechanism of online public opinion is explained from the perspective of information ecology. According to the mechanism, some evaluation indexes are selected through correlation analysis and principal component analysis. Then, a classification model of text emotion is created via the training by deep learning to achieve the accurate quantification of the emotional indexes in the index system. Finally, based on the multi-level evaluation index system and grey correlation analysis, we propose a method to rate the crisis of online public opinion. The experiment with the real-time incident show that this method can objectively evaluate the emotional tendency of Internet users and rate the crisis in different dissemination stages of online public opinion. It is helpful to realizing the crisis warning of online public opinion and timely blocking the further spread of the crisis.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2672835477,
        "newsscientist":0.2340631085,
        "technologyreview":0.3356827374,
        "venturebeat":0.2990269235,
        "wired":0.3111443545,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14740v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.ai",
            "cs.cl"
        ],
        "published":1659108336000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.14394v2",
        "predicted_newsworthiness":0.7018481872,
        "title":"Logic and Accuracy Testing: A Fifty-State Review",
        "summary":"Pre-election logic and accuracy (L&A) testing is a process in which election officials validate the behavior of voting equipment by casting a known set of test ballots and confirming the expected results. Ideally, such testing can serve to detect certain forms of human error or fraud and help bolster voter confidence. We present the first detailed analysis of L&A testing practices across the United States. We find that while all states require L&A testing before every election, their implementations vary dramatically in scope, transparency, and rigorousness. We summarize each state's requirements and score them according to uniform criteria. We also highlight best practices and flag opportunities for improvement, in hopes of encouraging broader adoption of more effective L&A processes.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2443560787,
        "newsscientist":0.2053425207,
        "technologyreview":0.2978911318,
        "venturebeat":0.2435656952,
        "wired":0.2599704942,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14394v2",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659046897000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.11603v1",
        "predicted_newsworthiness":0.6969148219,
        "title":"Experience with Abrupt Transition to Remote Teaching of Embedded Systems",
        "summary":"Due to the pandemic of COVID-19, many university courses had to abruptly transform to enable remote teaching. Adjusting courses on embedded systems and micro-controllers was extra challenging since interaction with real hardware is their integral part. We start by comparing our experience with four basic alternatives of teaching embedded systems: 1) interacting with hardware at school, 2) having remote access to hardware, 3) lending hardware to students for at-home work and 4) virtualizing hardware. Afterward, we evaluate in detail our experience of the fast transition from traditional, offline at-school hardware programming course to using remote access to real hardware present in the lab. The somewhat unusual remote hardware access approach turned out to be a fully viable alternative for teaching embedded systems, enabling a relatively low-effort transition. Our setup is based on existing solutions and stable open technologies without the need for custom-developed applications that require high maintenance. We evaluate the experience of both the students and teachers and condense takeaways for future courses. The specific environment setup is available online as an inspiration for others.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1737747892,
        "newsscientist":0.1954015854,
        "technologyreview":0.2884420841,
        "venturebeat":0.2950198368,
        "wired":0.3006420304,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11603v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy",
            "cs.ro"
        ],
        "published":1658610306000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.13941v1",
        "predicted_newsworthiness":0.6963390132,
        "title":"A Civil Protection Early Warning System to Improve the Resilience of Adriatic-Ionian Territories to Natural and Man-made Risk",
        "summary":"We are currently witnessing an increased occurrence of extreme weather events, causing a great deal of disruption and distress across the globe. In this setting, the importance and utility of Early Warning Systems is becoming increasingly obvious. In this work, we present the design of an early warning system called TransCPEarlyWarning, aimed at seven countries in the Adriatic-Ionian area in Europe. The overall objective is to increase the level of cooperation among national civil protection institutions in these countries, addressing natural and man-made risks from the early warning stage and improving the intervention capabilities of civil protection mechanisms. The system utilizes an innovative approach with a lever effect, while also aiming to support the whole system of Civil Protection.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2814792266,
        "newsscientist":0.2601614791,
        "technologyreview":0.2732509101,
        "venturebeat":0.2391736776,
        "wired":0.2397302445,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13941v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1658995537000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.11897v1",
        "predicted_newsworthiness":0.6811131876,
        "title":"AI Powered Anti-Cyber Bullying System using Machine Learning Algorithm of Multinomial Naive Bayes and Optimized Linear Support Vector Machine",
        "summary":"\"Unless and until our society recognizes cyber bullying for what it is, the suffering of thousands of silent victims will continue.\" ~ Anna Maria Chavez. There had been series of research on cyber bullying which are unable to provide reliable solution to cyber bullying. In this research work, we were able to provide a permanent solution to this by developing a model capable of detecting and intercepting bullying incoming and outgoing messages with 92% accuracy. We also developed a chatbot automation messaging system to test our model leading to the development of Artificial Intelligence powered anti-cyber bullying system using machine learning algorithm of Multinomial Naive Bayes (MNB) and optimized linear Support Vector Machine (SVM). Our model is able to detect and intercept bullying outgoing and incoming bullying messages and take immediate action.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1855827184,
        "newsscientist":0.2180865804,
        "technologyreview":0.3274025914,
        "venturebeat":0.3166124584,
        "wired":0.2662303096,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11897v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1658721722000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.11500v1",
        "predicted_newsworthiness":0.6778673303,
        "title":"Catch Me If You Can: Deceiving Stance Detection and Geotagging Models to Protect Privacy of Individuals on Twitter",
        "summary":"The recent advances in natural language processing have yielded many exciting developments in text analysis and language understanding models; however, these models can also be used to track people, bringing severe privacy concerns. In this work, we investigate what individuals can do to avoid being detected by those models while using social media platforms. We ground our investigation in two exposure-risky tasks, stance detection and geotagging. We explore a variety of simple techniques for modifying text, such as inserting typos in salient words, paraphrasing, and adding dummy social media posts. Our experiments show that the performance of BERT-based models fined tuned for stance detection decreases significantly due to typos, but it is not affected by paraphrasing. Moreover, we find that typos have minimal impact on state-of-the-art geotagging models due to their increased reliance on social networks; however, we show that users can deceive those models by interacting with different users, reducing their performance by almost 50%.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2179474553,
        "newsscientist":0.2123886857,
        "technologyreview":0.3427253041,
        "venturebeat":0.30195669,
        "wired":0.3271118943,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11500v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.cy"
        ],
        "published":1658577318000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.12589v1",
        "predicted_newsworthiness":0.6734360902,
        "title":"Folk Models of Misinformation on Social Media",
        "summary":"In this paper we investigate what folk models of misinformation exist through semi-structured interviews with a sample of 235 social media users. Work on social media misinformation does not investigate how ordinary users - the target of misinformation - deal with it; rather, the focus is mostly on the anxiety, tensions, or divisions misinformation creates. Studying the aspects of creation, diffusion and amplification also overlooks how misinformation is internalized by users on social media and thus is quick to prescribe \"inoculation\" strategies for the presumed lack of immunity to misinformation. How users grapple with social media content to develop \"natural immunity\" as a precursor to misinformation resilience remains an open question. We have identified at least five folk models that conceptualize misinformation as either: political (counter)argumentation, out-of-context narratives, inherently fallacious information, external propaganda, or simply entertainment. We use the rich conceptualizations embodied in these folk models to uncover how social media users minimize adverse reactions to misinformation encounters in their everyday lives.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.3260661933,
        "newsscientist":0.2571649636,
        "technologyreview":0.3530517288,
        "venturebeat":0.2598362848,
        "wired":0.3809045008,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12589v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.cy"
        ],
        "published":1658796026000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.14681v1",
        "predicted_newsworthiness":0.6732919625,
        "title":"Unfolding Values through Systematic Guidance: Conducting a Value-Centered Participatory Workshop for a Patient-Oriented Data Donation",
        "summary":"Routinely collected clinical patient data posits a valuable resource for data-driven medical innovation. Such secondary data use for medical research purposes is dependent on the patient's consent. To gain an understanding of the patients' values and needs regarding medical data donations, we developed a participatory workshop method, integrating approaches from value-sensitive and reflective design to explore patients' values and translate them into hypothetical, ideal design solutions. The data gathered in the workshop are used to derive practicable design requirements for patient-oriented data donation technologies. In this paper, we introduce the workshop process and evaluate its application.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2703233935,
        "newsscientist":0.2666623907,
        "technologyreview":0.3541892508,
        "venturebeat":0.3251561215,
        "wired":0.3137170128,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14681v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1659102975000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.13394v2",
        "predicted_newsworthiness":0.6713335466,
        "title":"Statistical Keystroke Synthesis for Improved Bot Detection",
        "summary":"This work proposes two statistical approaches for the synthesis of keystroke biometric data based on Universal and User-dependent Models. Both approaches are validated on the bot detection task, using the keystroke synthetic data to better train the systems. Our experiments include a dataset with 136 million keystroke events from 168,000 subjects. We have analyzed the performance of the two synthesis approaches through qualitative and quantitative experiments. Different bot detectors are considered based on two supervised classifiers (Support Vector Machine and Long Short-Term Memory network) and a learning framework including human and generated samples. Our results prove that the proposed statistical approaches are able to generate realistic human-like synthetic keystroke samples. Also, the classification results suggest that in scenarios with large labeled data, these synthetic samples can be detected with high accuracy. However, in few-shot learning scenarios it represents an important challenge.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1363732192,
        "newsscientist":0.2268406563,
        "technologyreview":0.3277191741,
        "venturebeat":0.3088736276,
        "wired":0.2639149127,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13394v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1658913975000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12829v1",
        "predicted_newsworthiness":0.6697050947,
        "title":"Visualization Design Practices in a Crisis: Behind the Scenes with COVID-19 Dashboard Creators",
        "summary":"During the COVID-19 pandemic, a number of data visualizations were created to inform the public about the rapidly evolving crisis. Data dashboards, a form of information dissemination used during the pandemic, have facilitated this process by visualizing statistics regarding the number of COVID-19 cases over time. In this research, we conducted a qualitative interview study among dashboard creators from federal agencies, state health departments, mainstream news media outlets, and other organizations that created (often widely-used) COVID-19 dashboards to answer the following questions: how did visualization creators engage in COVID-19 dashboard design, and what tensions, conflicts, and challenges arose during this process? Our findings detail the trajectory of design practices -- from creation to expansion, maintenance, and termination -- that are shaped by the complex interplay between design goals, tools and technologies, labor, emerging crisis contexts, and public engagement. We particularly examined the tensions between designers and the general public involved in these processes. These conflicts, which often materialized due to a divergence between public demands and standing policies, centered around the type and amount of information to be visualized, how public perceptions shape and are shaped by visualization design, and the strategies utilized to deal with (potential) misinterpretations and misuse of visualizations. Our findings and lessons learned shed light on new ways of thinking in visualization design, focusing on the bundled activities that are invariably involved in human and nonhuman participation throughout the entire trajectory of design practice.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.3630463624,
        "newsscientist":0.3089610604,
        "technologyreview":0.3839500459,
        "venturebeat":0.3354065959,
        "wired":0.3655656891,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12829v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.cy"
        ],
        "published":1658836355000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.13834v1",
        "predicted_newsworthiness":0.6684354556,
        "title":"Toward Supporting Perceptual Complementarity in Human-AI Collaboration via Reflection on Unobservables",
        "summary":"In many real world contexts, successful human-AI collaboration requires humans to productively integrate complementary sources of information into AI-informed decisions. However, in practice human decision-makers often lack understanding of what information an AI model has access to in relation to themselves. There are few available guidelines regarding how to effectively communicate about unobservables: features that may influence the outcome, but which are unavailable to the model. In this work, we conducted an online experiment to understand whether and how explicitly communicating potentially relevant unobservables influences how people integrate model outputs and unobservables when making predictions. Our findings indicate that presenting prompts about unobservables can change how humans integrate model outputs and unobservables, but do not necessarily lead to improved performance. Furthermore, the impacts of these prompts can vary depending on decision-makers' prior domain expertise. We conclude by discussing implications for future research and design of AI-based decision support tools.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2124801589,
        "newsscientist":0.2595795922,
        "technologyreview":0.3993887907,
        "venturebeat":0.3793583619,
        "wired":0.2947500751,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13834v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ai"
        ],
        "published":1658966714000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.12196v1",
        "predicted_newsworthiness":0.6605923557,
        "title":"On the Relation Between Opinion Change and Information Consumption on Reddit",
        "summary":"While much attention has been devoted to the causes of opinion change, little is known about its consequences. Our study sheds a light on the relationship between one user's opinion change episode and subsequent behavioral change on an online social media, Reddit. In particular, we look at r\/ChangeMyView, an online community dedicated to debating one's own opinions. Interestingly, this forum adopts a well-codified schema for explicitly self-reporting opinion change. Starting from this ground truth, we analyze changes in future online information consumption behavior that arise after a self-reported opinion change on sociopolitical topics; and in particular, operationalized in this work as the participation to sociopolitical subreddits. Such participation profile is important as it represents one's information diet, and is a reliable proxy for, e.g., political affiliation or health choices. We find that people who report an opinion change are significantly more likely to change their future participation in a specific subset of online communities. We characterize which communities are more likely to be abandoned after opinion change, and find a significant association (r=0.46) between propaganda-like language used in a community and the increase in chances of leaving it. We find comparable results (r=0.39) for the opposite direction, i.e., joining a community. This finding suggests how propagandistic communities act as a first gateway to internalize a shift in one's sociopolitical opinion. Finally, we show that the textual content of the discussion associated with opinion change is indicative of which communities are going to be subject to a participation change. In fact, a predictive model based only on the opinion change post is able to pinpoint these communities with an AP@5 of 0.20, similar to what can be reached by using all the past history of participation in communities.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2885333961,
        "newsscientist":0.2379209511,
        "technologyreview":0.3404313654,
        "venturebeat":0.2758260009,
        "wired":0.3524482464,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12196v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.cy"
        ],
        "published":1658756056000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.11528v1",
        "predicted_newsworthiness":0.6593762865,
        "title":"Supporting peace negotiations in the Yemen war through machine learning",
        "summary":"Today's conflicts are becoming increasingly complex, fluid and fragmented, often involving a host of national and international actors with multiple and often divergent interests. This development poses significant challenges for conflict mediation, as mediators struggle to make sense of conflict dynamics, such as the range of conflict parties and the evolution of their political positions, the distinction between relevant and less relevant actors in peace-making, or the identification of key conflict issues and their interdependence. International peace efforts appear ill-equipped to successfully address these challenges. While technology is already being experimented with and used in a range of conflict related fields, such as conflict predicting or information gathering, less attention has been given to how technology can contribute to conflict mediation. This case study contributes to emerging research on the use of state-of-the-art machine learning technologies and techniques in conflict mediation processes. Using dialogue transcripts from peace negotiations in Yemen, this study shows how machine-learning can effectively support mediating teams by providing them with tools for knowledge management, extraction and conflict analysis. Apart from illustrating the potential of machine learning tools in conflict mediation, the paper also emphasises the importance of interdisciplinary and participatory, co-creation methodology for the development of context-sensitive and targeted tools and to ensure meaningful and responsible implementation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2770948453,
        "newsscientist":0.225041465,
        "technologyreview":0.3638059283,
        "venturebeat":0.3311332626,
        "wired":0.3044240478,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11528v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.cy",
            "cs.lg"
        ],
        "published":1658586278000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11893v1",
        "predicted_newsworthiness":0.657495982,
        "title":"Overview of the Shared Task on Fake News Detection in Urdu at FIRE 2020",
        "summary":"This overview paper describes the first shared task on fake news detection in Urdu language. The task was posed as a binary classification task, in which the goal is to differentiate between real and fake news. We provided a dataset divided into 900 annotated news articles for training and 400 news articles for testing. The dataset contained news in five domains: (i) Health, (ii) Sports, (iii) Showbiz, (iv) Technology, and (v) Business. 42 teams from 6 different countries (India, China, Egypt, Germany, Pakistan, and the UK) registered for the task. 9 teams submitted their experimental results. The participants used various machine learning methods ranging from feature-based traditional machine learning to neural networks techniques. The best performing system achieved an F-score value of 0.90, showing that the BERT-based approach outperforms other machine learning techniques",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2147536585,
        "newsscientist":0.195985429,
        "technologyreview":0.3117521784,
        "venturebeat":0.2742314685,
        "wired":0.2732099562,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11893v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658720492000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00433v1",
        "predicted_newsworthiness":0.6540955563,
        "title":"The Who in Code-Switching: A Case Study for Predicting Egyptian Arabic-English Code-Switching Levels based on Character Profiles",
        "summary":"Code-switching (CS) is a common linguistic phenomenon exhibited by multilingual individuals, where they tend to alternate between languages within one single conversation. CS is a complex phenomenon that not only encompasses linguistic challenges, but also contains a great deal of complexity in terms of its dynamic behaviour across speakers. Given that the factors giving rise to CS vary from one country to the other, as well as from one person to the other, CS is found to be a speaker-dependant behaviour, where the frequency by which the foreign language is embedded differs across speakers. While several researchers have looked into predicting CS behaviour from a linguistic point of view, research is still lacking in the task of predicting user CS behaviour from sociological and psychological perspectives. We provide an empirical user study, where we investigate the correlations between users' CS levels and character traits. We conduct interviews with bilinguals and gather information on their profiles, including their demographics, personality traits, and traveling experiences. We then use machine learning (ML) to predict users' CS levels based on their profiles, where we identify the main influential factors in the modeling process. We experiment with both classification as well as regression tasks. Our results show that the CS behaviour is affected by the relation between speakers, travel experiences as well as Neuroticism and Extraversion personality traits.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2121034445,
        "newsscientist":0.2109356285,
        "technologyreview":0.2861730907,
        "venturebeat":0.2849080632,
        "wired":0.2579108808,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00433v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659275255000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.12406v1",
        "predicted_newsworthiness":0.6539820246,
        "title":"UrduFake@FIRE2020: Shared Track on Fake News Identification in Urdu",
        "summary":"This paper gives the overview of the first shared task at FIRE 2020 on fake news detection in the Urdu language. This is a binary classification task in which the goal is to identify fake news using a dataset composed of 900 annotated news articles for training and 400 news articles for testing. The dataset contains news in five domains: (i) Health, (ii) Sports, (iii) Showbiz, (iv) Technology, and (v) Business. 42 teams from 6 different countries (India, China, Egypt, Germany, Pakistan, and the UK) registered for the task. 9 teams submitted their experimental results. The participants used various machine learning methods ranging from feature-based traditional machine learning to neural network techniques. The best performing system achieved an F-score value of 0.90, showing that the BERT-based approach outperforms other machine learning classifiers.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2218470732,
        "newsscientist":0.2013635417,
        "technologyreview":0.3133568942,
        "venturebeat":0.2773478417,
        "wired":0.2826276421,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12406v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658720811000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.13749v1",
        "predicted_newsworthiness":0.6515948139,
        "title":"Nutzungsverhalten und Funktionsanforderungen digitaler Trainingsanwendungen w\u00e4hrend der Pandemie",
        "summary":"Due to contact restrictions, closure of fitness centers and quarantine measures, the SARS-CoV-2 pandemic led to a considerable decline of sporting activities. The first relaxation of these restrictions allowed German citizens to mostly return to their normal training and exercise behavior, yet the long-term impact of the recurring measures (i.e. the \"Lockdown\", \"Lockdown light\" as well as the \"Corona Emergency Break\" in the case of Germany) remain rather under-investigated. Using a survey of (n=108) German sportspersons, we measured a significant decline of sporting activities even within the intermediary phases without major pandemic constraints. To evaluate the capabilities of digital training applications in countering these effects, we additionally recorded the usage of, among others, apps, trackers, videos and conferencing systems and identified the most important as well as missing and\/or essential features with regards to their capabilities of facilitating individual sport and training in times without access to facilities or social contacts. Effectively, the usage of smart watches, online videos and conferences increased significantly when compared to before the pandemic; and especially online videos and conferences contributed to higher training frequencies. Data-driven or individual feedback, motivation and collaboration revealed to be the most important or even necessary functions for users of digital training applications to counter the decline of social components of training.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.3429847692,
        "newsscientist":0.2899181469,
        "technologyreview":0.354094497,
        "venturebeat":0.3356411816,
        "wired":0.3301646766,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13749v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1658948376000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.00479v1",
        "predicted_newsworthiness":0.650269421,
        "title":"The impact of Twitter on political influence on the choice of a running mate: Social Network Analysis and Semantic Analysis -- A Review",
        "summary":"In this new era of social media, social networks are becoming increasingly important sources of user-generated content on the internet. These kinds of information resources, which include a lot of people's feelings, opinions, feedback, and reviews, are very useful for big businesses, markets, politics, journalism, and many other fields. Politics is one of the most talked-about and popular topics on social media networks right now. Many politicians use micro-blogging services like Twitter because they have a large number of followers and supporters on those networks. Politicians, political parties, political organizations, and foundations use social media networks to communicate with citizens ahead of time. Today, social media is used by hundreds of thousands of political groups and politicians. On these social media networks, every politician and political party has millions of followers, and politicians find new and innovative ways to urge individuals to participate in politics. Furthermore, social media assists politicians in various decision-making processes by providing recommendations, such as developing policies and strategies based on previous experiences, recommending and selecting suitable candidates for a particular constituency, recommending a suitable person for a particular position in the party, and launching a political campaign based on citizen sentiments on various issues and controversies, among other things. This research is a review on the use of social network analysis (SNA) and semantic analysis (SA) on the Twitter platform to study the supporters networks of political leaders because it can help in decision-making when predicting their political futures.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2716114038,
        "newsscientist":0.1937511297,
        "technologyreview":0.3324583585,
        "venturebeat":0.2843187941,
        "wired":0.3461680868,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00479v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.si"
        ],
        "published":1659289497000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00176v2",
        "predicted_newsworthiness":0.6487458938,
        "title":"ELF22: A Context-based Counter Trolling Dataset to Combat Internet Trolls",
        "summary":"Online trolls increase social costs and cause psychological damage to individuals. With the proliferation of automated accounts making use of bots for trolling, it is difficult for targeted individual users to handle the situation both quantitatively and qualitatively. To address this issue, we focus on automating the method to counter trolls, as counter responses to combat trolls encourage community users to maintain ongoing discussion without compromising freedom of expression. For this purpose, we propose a novel dataset for automatic counter response generation. In particular, we constructed a pair-wise dataset that includes troll comments and counter responses with labeled response strategies, which enables models fine-tuned on our dataset to generate responses by varying counter responses according to the specified strategy. We conducted three tasks to assess the effectiveness of our dataset and evaluated the results through both automatic and human evaluation. In human evaluation, we demonstrate that the model fine-tuned on our dataset shows a significantly improved performance in strategy-controlled sentence generation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1991123947,
        "newsscientist":0.1937065253,
        "technologyreview":0.3179598239,
        "venturebeat":0.2945075391,
        "wired":0.2876244283,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00176v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659176081000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.13658v1",
        "predicted_newsworthiness":0.6438311438,
        "title":"BotBuster: Multi-platform Bot Detection Using A Mixture of Experts",
        "summary":"Despite rapid development, current bot detection models still face challenges in dealing with incomplete data and cross-platform applications. In this paper, we propose BotBuster, a social bot detector built with the concept of a mixture of experts approach. Each expert is trained to analyze a portion of account information, e.g. username, and are combined to estimate the probability that the account is a bot. Experiments on 10 Twitter datasets show that BotBuster outperforms popular bot-detection baselines (avg F1=73.54 vs avg F1=45.12). This is accompanied with F1=60.04 on a Reddit dataset and F1=60.92 on an external evaluation set. Further analysis shows that only 36 posts is required for a stable bot classification. Investigation shows that bot post features have changed across the years and can be difficult to differentiate from human features, making bot detection a difficult and ongoing problem.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.167184137,
        "newsscientist":0.1964428491,
        "technologyreview":0.3449257349,
        "venturebeat":0.3333170075,
        "wired":0.2735056731,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13658v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1658942769000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.13222v1",
        "predicted_newsworthiness":0.6434960127,
        "title":"Information We Can Extract About a User From 'One Minute Mobile Application Usage'",
        "summary":"Understanding human behavior is an important task and has applications in many domains such as targeted advertisement, health analytics, security, and entertainment, etc. For this purpose, designing a system for activity recognition (AR) is important. However, since every human can have different behaviors, understanding and analyzing common patterns become a challenging task. Since smartphones are easily available to every human being in the modern world, using them to track the human activities becomes possible. In this paper, we extracted different human activities using accelerometer, magnetometer, and gyroscope sensors of android smartphones by building an android mobile applications. Using different social media applications, such as Facebook, Instagram, Whatsapp, and Twitter, we extracted the raw sensor values along with the attributes of $29$ subjects along with their attributes (class labels) such as age, gender, and left\/right\/both hands application usage. We extract features from the raw signals and use them to perform classification using different machine learning (ML) algorithms. Using statistical analysis, we show the importance of different features towards the prediction of class labels. In the end, we use the trained ML model on our data to extract unknown features from a well known activity recognition data from UCI repository, which highlights the potential of privacy breach using ML models. This security analysis could help researchers in future to take appropriate steps to preserve the privacy of human subjects.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1835834782,
        "newsscientist":0.2319685795,
        "technologyreview":0.3349760017,
        "venturebeat":0.3300883502,
        "wired":0.3190461539,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13222v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cy"
        ],
        "published":1658881391000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01157v1",
        "predicted_newsworthiness":0.6432820404,
        "title":"Performance Disparities Between Accents in Automatic Speech Recognition",
        "summary":"Automatic speech recognition (ASR) services are ubiquitous, transforming speech into text for systems like Amazon's Alexa, Google's Assistant, and Microsoft's Cortana. However, researchers have identified biases in ASR performance between particular English language accents by racial group and by nationality. In this paper, we expand this discussion both qualitatively by relating it to historical precedent and quantitatively through a large-scale audit. Standardization of language and the use of language to maintain global and political power have played an important role in history, which we explain to show the parallels in the ways in which ASR services act on English language speakers today. Then, using a large and global data set of speech from The Speech Accent Archive which includes over 2,700 speakers of English born in 171 different countries, we perform an international audit of some of the most popular English ASR services. We show that performance disparities exist as a function of whether or not a speaker's first language is English and, even when controlling for multiple linguistic covariates, that these disparities have a statistically significant relationship to the political alignment of the speaker's birth country with respect to the United States' geopolitical power.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.235050667,
        "newsscientist":0.2168448898,
        "technologyreview":0.3455641025,
        "venturebeat":0.3186473076,
        "wired":0.2867880365,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01157v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.cy"
        ],
        "published":1659391821000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11562v1",
        "predicted_newsworthiness":0.6422687289,
        "title":"Better Reasoning Behind Classification Predictions with BERT for Fake News Detection",
        "summary":"Fake news detection has become a major task to solve as there has been an increasing number of fake news on the internet in recent years. Although many classification models have been proposed based on statistical learning methods showing good results, reasoning behind the classification performances may not be enough. In the self-supervised learning studies, it has been highlighted that a quality of representation (embedding) space matters and directly affects a downstream task performance. In this study, a quality of the representation space is analyzed visually and analytically in terms of linear separability for different classes on a real and fake news dataset. To further add interpretability to a classification model, a modification of Class Activation Mapping (CAM) is proposed. The modified CAM provides a CAM score for each word token, where the CAM score on a word token denotes a level of focus on that word token to make the prediction. Finally, it is shown that the naive BERT model topped with a learnable linear layer is enough to achieve robust performance while being compatible with CAM.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1835238711,
        "newsscientist":0.2024831467,
        "technologyreview":0.3311862468,
        "venturebeat":0.2909554145,
        "wired":0.2707858785,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11562v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658598888000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14526v1",
        "predicted_newsworthiness":0.6417106397,
        "title":"Leveraging Explanations in Interactive Machine Learning: An Overview",
        "summary":"Explanations have gained an increasing level of interest in the AI and Machine Learning (ML) communities in order to improve model transparency and allow users to form a mental model of a trained ML model. However, explanations can go beyond this one way communication as a mechanism to elicit user control, because once users understand, they can then provide feedback. The goal of this paper is to present an overview of research where explanations are combined with interactive capabilities as a mean to learn new models from scratch and to edit and debug existing ones. To this end, we draw a conceptual map of the state-of-the-art, grouping relevant approaches based on their intended purpose and on how they structure the interaction, highlighting similarities and differences between them. We also discuss open research issues and outline possible directions forward, with the hope of spurring further research on this blooming research topic.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.15830719,
        "newsscientist":0.201991534,
        "technologyreview":0.3462702371,
        "venturebeat":0.3264676858,
        "wired":0.2466518568,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14526v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659080771000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01273v1",
        "predicted_newsworthiness":0.6404476538,
        "title":"Industry 4.0 Asset Administration Shell (AAS): Interoperable Skill-Based Service-Robots",
        "summary":"This paper describes our use of Industry 4.0 Asset Administration Shells (AASs) in the context of service robots. We use AASs with software components of service robots and with complete service robot systems. The AAS for a software component serves as a standardized digital data sheet. It helps sysem builders at design time in finding and selecting software components that match system-level requirements of the systems to be built. The AAS for a system comprises a data sheet for the system and furthermore collects at runtime operational data and it allows for skill-level commanding of the service robot. AASs are generated and filled as part of our model-driven development and composition workflow for service robotics. AASs can serve as a key enabler for a standardized integration and interaction with service robots.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1046392223,
        "newsscientist":0.1606648869,
        "technologyreview":0.3110231635,
        "venturebeat":0.3066105144,
        "wired":0.2473367472,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01273v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659422663000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.12761v1",
        "predicted_newsworthiness":0.6397705074,
        "title":"The Human in the Infinite Loop: A Case Study on Revealing and Explaining Human-AI Interaction Loop Failures",
        "summary":"Interactive AI systems increasingly employ a human-in-the-loop strategy. This creates new challenges for the HCI community when designing such systems. We reveal and investigate some of these challenges in a case study with an industry partner, and developed a prototype human-in-the-loop system for preference-guided 3D model processing. Two 3D artists used it in their daily work for 3 months. We found that the human-AI loop often did not converge towards a satisfactory result and designed a lab study (N=20) to investigate this further. We analyze interaction data and user feedback through the lens of theories of human judgment to explain the observed human-in-the-loop failures with two key insights: 1) optimization using preferential choices lacks mechanisms to deal with inconsistent and contradictory human judgments; 2) machine outcomes, in turn, influence future user inputs via heuristic biases and loss aversion. To mitigate these problems, we propose descriptive UI design guidelines. Our case study draws attention to challenging and practically relevant imperfections in human-AI loops that need to be considered when designing human-in-the-loop systems.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1940931621,
        "newsscientist":0.2495430646,
        "technologyreview":0.3762120154,
        "venturebeat":0.3535348298,
        "wired":0.3281805353,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12761v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1658826674000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.12551v1",
        "predicted_newsworthiness":0.6397453176,
        "title":"DialCrowd 2.0: A Quality-Focused Dialog System Crowdsourcing Toolkit",
        "summary":"Dialog system developers need high-quality data to train, fine-tune and assess their systems. They often use crowdsourcing for this since it provides large quantities of data from many workers. However, the data may not be of sufficiently good quality. This can be due to the way that the requester presents a task and how they interact with the workers. This paper introduces DialCrowd 2.0 to help requesters obtain higher quality data by, for example, presenting tasks more clearly and facilitating effective communication with workers. DialCrowd 2.0 guides developers in creating improved Human Intelligence Tasks (HITs) and is directly applicable to the workflows used currently by developers and researchers.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1772571285,
        "newsscientist":0.2143828074,
        "technologyreview":0.3642648231,
        "venturebeat":0.3807594572,
        "wired":0.3076354796,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12551v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658786780000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.02007v1",
        "predicted_newsworthiness":0.6389625458,
        "title":"Maintaining Performance with Less Data",
        "summary":"We propose a novel method for training a neural network for image classification to reduce input data dynamically, in order to reduce the costs of training a neural network model. As Deep Learning tasks become more popular, their computational complexity increases, leading to more intricate algorithms and models which have longer runtimes and require more input data. The result is a greater cost on time, hardware, and environmental resources. By using data reduction techniques, we reduce the amount of work performed, and therefore the environmental impact of AI techniques, and with dynamic data reduction we show that accuracy may be maintained while reducing runtime by up to 50%, and reducing carbon emission proportionally.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1685615701,
        "newsscientist":0.2288991013,
        "technologyreview":0.3480001276,
        "venturebeat":0.3227379419,
        "wired":0.2328671249,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02007v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1659529338000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01696v1",
        "predicted_newsworthiness":0.6375634717,
        "title":"Measuring Commonality in Recommendation of Cultural Content: Recommender Systems to Enhance Cultural Citizenship",
        "summary":"Recommender systems have become the dominant means of curating cultural content, significantly influencing the nature of individual cultural experience. While the majority of research on recommender systems optimizes for personalized user experience, this paradigm does not capture the ways that recommender systems impact cultural experience in the aggregate, across populations of users. Although existing novelty, diversity, and fairness studies probe how systems relate to the broader social role of cultural content, they do not adequately center culture as a core concept and challenge. In this work, we introduce commonality as a new measure that reflects the degree to which recommendations familiarize a given user population with specified categories of cultural content. Our proposed commonality metric responds to a set of arguments developed through an interdisciplinary dialogue between researchers in computer science and the social sciences and humanities. With reference to principles underpinning non-profit, public service media systems in democratic societies, we identify universality of address and content diversity in the service of strengthening cultural citizenship as particularly relevant goals for recommender systems delivering cultural content. Taking diversity in movie recommendation as a case study in enhancing pluralistic cultural experience, we empirically compare systems' performance using commonality and existing utility, diversity, and fairness metrics. Our results demonstrate that commonality captures a property of system behavior complementary to existing metrics and suggest the need for alternative, non-personalized interventions in recommender systems oriented to strengthening cultural citizenship across populations of users. In this way, commonality contributes to a growing body of scholarship developing 'public good' rationales for digital media and ML systems.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2678207729,
        "newsscientist":0.1993060673,
        "technologyreview":0.3201509678,
        "venturebeat":0.3060444426,
        "wired":0.3100962501,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01696v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy",
            "cs.ir"
        ],
        "published":1659467689000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.11953v1",
        "predicted_newsworthiness":0.6364616646,
        "title":"Deep Learning for Forecasting the Energy Consumption in Public Buildings",
        "summary":"In this paper we propose a Long Short-Term Memory Network based method to forecast the energy consumption in public buildings, based on past measurements. Our approach consists of three main steps: data processing step, training and validation step, and finally the forecasting step. We tested our method on a data set consisting of measurements taken every half an hour from the main building of the National Archives of the United Kingdom, in Kew and as evaluation metrics we have used Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE).",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1608783183,
        "newsscientist":0.1926354725,
        "technologyreview":0.2450218039,
        "venturebeat":0.2335310574,
        "wired":0.1830661309,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11953v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658734975000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12054v1",
        "predicted_newsworthiness":0.6342318599,
        "title":"A Reference Data Model for Process-Related User Interaction Logs",
        "summary":"User interaction (UI) logs are high-resolution event logs that record low-level activities performed by a user during the execution of a task in an information system. Each event in a UI log corresponds to a single interaction between the user and the interface, such as clicking a button or entering a string into a text field. UI logs are used for purposes like task mining or robotic process automation (RPA), but each study and tool relies on a different conceptualization and implementation of the elements and attributes that constitute user interactions. This lack of standardization makes it difficult to integrate UI logs from different sources and to combine tools for UI data collection with downstream analytics or automation solutions. To address this, we propose a universally applicable reference data model for process-related UI logs. Based on a review of scientific literature and industry solutions, this model includes the core attributes of UI logs, but remains flexible with regard to the scope, level of abstraction, and case notion. We provide an implementation of the model as an extension to the XES interchange standard for event logs and demonstrate its practical applicability in a real-life RPA scenario.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.112064822,
        "newsscientist":0.1386772244,
        "technologyreview":0.2325377081,
        "venturebeat":0.2761316306,
        "wired":0.2057988504,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12054v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1658746067000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2208.01802v1",
        "predicted_newsworthiness":0.6327826423,
        "title":"Mutual Information Scoring: Increasing Interpretability in Categorical Clustering Tasks with Applications to Child Welfare Data",
        "summary":"Youth in the American foster care system are significantly more likely than their peers to face a number of negative life outcomes, from homelessness to incarceration. Administrative data on these youth have the potential to provide insights that can help identify ways to improve their path towards a better life. However, such data also suffer from a variety of biases, from missing data to reflections of systemic inequality. The present work proposes a novel, prescriptive approach to using these data to provide insights about both data biases and the systems and youth they track. Specifically, we develop a novel categorical clustering and cluster summarization methodology that allows us to gain insights into subtle biases in existing data on foster youth, and to provide insight into where further (often qualitative) research is needed to identify potential ways of assisting youth.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2680982662,
        "newsscientist":0.1808820486,
        "technologyreview":0.2547669494,
        "venturebeat":0.2246162433,
        "wired":0.2137680507,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01802v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659489069000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.00780v2",
        "predicted_newsworthiness":0.6313206373,
        "title":"Visual correspondence-based explanations improve AI robustness and human-AI team accuracy",
        "summary":"Explaining artificial intelligence (AI) predictions is increasingly important and even imperative in many high-stakes applications where humans are the ultimate decision-makers. In this work, we propose two novel architectures of self-interpretable image classifiers that first explain, and then predict (as opposed to post-hoc explanations) by harnessing the visual correspondences between a query image and exemplars. Our models consistently improve (by 1 to 4 points) on out-of-distribution (OOD) datasets while performing marginally worse (by 1 to 2 points) on in-distribution tests than ResNet-50 and a $k$-nearest neighbor classifier (kNN). Via a large-scale, human study on ImageNet and CUB, our correspondence-based explanations are found to be more useful to users than kNN explanations. Our explanations help users more accurately reject AI's wrong decisions than all other tested methods. Interestingly, for the first time, we show that it is possible to achieve complementary human-AI team accuracy (i.e., that is higher than either AI-alone or human-alone), in ImageNet and CUB image classification tasks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1408994668,
        "newsscientist":0.2178582052,
        "technologyreview":0.389216765,
        "venturebeat":0.362375405,
        "wired":0.2561048009,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00780v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.hc",
            "cs.lg"
        ],
        "published":1658833182000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14382v2",
        "predicted_newsworthiness":0.629950362,
        "title":"Large Language Models and the Reverse Turing Test",
        "summary":"Large Language Models (LLMs) have been transformative. They are pre-trained foundational models that can be adapted with fine tuning to many different natural language tasks, each of which previously would have required a separate network model. This is one step closer to the extraordinary versatility of human language. GPT-3 and more recently LaMDA can carry on dialogs with humans on many topics after minimal priming with a few examples. However, there has been a wide range of reactions on whether these LLMs understand what they are saying or exhibit signs of intelligence. This high variance is exhibited in three interviews with LLMs reaching wildly different conclusions. A new possibility was uncovered that could explain this divergence. What appears to be intelligence in LLMs may in fact be a mirror that reflects the intelligence of the interviewer, a remarkable twist that could be considered a Reverse Turing Test. If so, then by studying interviews we may be learning more about the intelligence and beliefs of the interviewer than the intelligence of the LLMs. As LLMs become more capable they may transform the way we access and use information.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2012542224,
        "newsscientist":0.2558554005,
        "technologyreview":0.4039185237,
        "venturebeat":0.3798393361,
        "wired":0.2983199451,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14382v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai",
            "cs.lg"
        ],
        "published":1659043367000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14636v1",
        "predicted_newsworthiness":0.6263294377,
        "title":"Detecting Spam Reviews on Vietnamese E-commerce Websites",
        "summary":"The reviews of customers play an essential role in online shopping. People often refer to reviews or comments of previous customers to decide whether to buy a new product. Catching up with this behavior, some people create untruths and illegitimate reviews to hoax customers about the fake quality of products. These reviews are called spam reviews, which confuse consumers on online shopping platforms and negatively affect online shopping behaviors. We propose the dataset called ViSpamReviews, which has a strict annotation procedure for detecting spam reviews on e-commerce platforms. Our dataset consists of two tasks: the binary classification task for detecting whether a review is a spam or not and the multi-class classification task for identifying the type of spam. The PhoBERT obtained the highest results on both tasks, 88.93% and 72.17%, respectively, by macro average F1 score.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1473920418,
        "newsscientist":0.1536726668,
        "technologyreview":0.2632584047,
        "venturebeat":0.265843473,
        "wired":0.2203753241,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14636v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1658918234000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.01312v1",
        "predicted_newsworthiness":0.6257730767,
        "title":"BEIKE NLP at SemEval-2022 Task 4: Prompt-Based Paragraph Classification for Patronizing and Condescending Language Detection",
        "summary":"PCL detection task is aimed at identifying and categorizing language that is patronizing or condescending towards vulnerable communities in the general media.Compared to other NLP tasks of paragraph classification, the negative language presented in the PCL detection task is usually more implicit and subtle to be recognized, making the performance of common text-classification approaches disappointed. Targeting the PCL detection problem in SemEval-2022 Task 4, in this paper, we give an introduction to our team's solution, which exploits the power of prompt-based learning on paragraph classification. We reformulate the task as an appropriate cloze prompt and use pre-trained Masked Language Models to fill the cloze slot. For the two subtasks, binary classification and multi-label classification, DeBERTa model is adopted and fine-tuned to predict masked label words of task-specific prompts. On the evaluation dataset, for binary classification, our approach achieves an F1-score of 0.6406; for multi-label classification, our approach achieves an macro-F1-score of 0.4689 and ranks first in the leaderboard.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2057628977,
        "newsscientist":0.1734312238,
        "technologyreview":0.2734468165,
        "venturebeat":0.2549071726,
        "wired":0.2565036979,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01312v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659429527000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00516v1",
        "predicted_newsworthiness":0.6238324532,
        "title":"Learning an Interpretable Model for Driver Behavior Prediction with Inductive Biases",
        "summary":"To plan safe maneuvers and act with foresight, autonomous vehicles must be capable of accurately predicting the uncertain future. In the context of autonomous driving, deep neural networks have been successfully applied to learning predictive models of human driving behavior from data. However, the predictions suffer from cascading errors, resulting in large inaccuracies over long time horizons. Furthermore, the learned models are black boxes, and thus it is often unclear how they arrive at their predictions. In contrast, rule-based models, which are informed by human experts, maintain long-term coherence in their predictions and are human-interpretable. However, such models often lack the sufficient expressiveness needed to capture complex real-world dynamics. In this work, we begin to close this gap by embedding the Intelligent Driver Model, a popular hand-crafted driver model, into deep neural networks. Our model's transparency can offer considerable advantages, e.g., in debugging the model and more easily interpreting its predictions. We evaluate our approach on a simulated merging scenario, showing that it yields a robust model that is end-to-end trainable and provides greater transparency at no cost to the model's predictive accuracy.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1576596113,
        "newsscientist":0.2046662373,
        "technologyreview":0.3638483093,
        "venturebeat":0.3308262351,
        "wired":0.2877272691,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00516v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659300762000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.12669v2",
        "predicted_newsworthiness":0.6220166024,
        "title":"EEG-Based Detection of Braking Intention During Simulated Driving",
        "summary":"Accurately detecting and identifying drivers' braking intention is the basis of man-machine driving. In this paper, we proposed an electroencephalographic (EEG)-based braking intention measurement strategy. We used the Car Learning to Act (Carla) platform to build the simulated driving environment. 11 subjects participated in our study, and each subject drove a simulated vehicle to complete emergency braking and normal braking tasks. We compared the EEG topographic maps in different braking situations and used three different classifiers to predict the subjects' braking intention through EEG signals. The experimental results showed that the average response time of subjects in emergency braking was 762 ms; emergency braking and no braking can be well distinguished, while normal braking and no braking were not easy to be classified; for the two different types of braking, emergency braking and normal braking had obvious differences in EEG topographic maps, and the classification results also showed that the two were highly distinguishable. This study provides a user-centered driver-assistance system and a good framework to combine with advanced shared control algorithms, which has the potential to be applied to achieve a more friendly interaction between the driver and vehicle in real driving environment.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1534864271,
        "newsscientist":0.20180478,
        "technologyreview":0.2815313687,
        "venturebeat":0.2478267953,
        "wired":0.2284777174,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12669v2",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1658816340000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.12200v1",
        "predicted_newsworthiness":0.6195867567,
        "title":"Aveiro Tech City Living Lab: A Communication, Sensing and Computing Platform for City Environments",
        "summary":"This article presents the deployment and experimentation architecture of the Aveiro Tech City Living Lab (ATCLL) in Aveiro, Portugal. This platform comprises a large number of Internet-of-Things devices with communication, sensing and computing capabilities. The communication infrastructure, built on fiber and Millimeter-wave (mmWave) links, integrates a communication network with radio terminals (WiFi, ITS-G5, C-V2X, 5G and LoRa(WAN)), multiprotocol, spread throughout 44 connected points of access in the city. Additionally, public transportation has also been equipped with communication and sensing units. All these points combine and interconnect a set of sensors, such as mobility (Radars, Lidars, video cameras) and environmental sensors. Combining edge computing and cloud management to deploy the services and manage the platform, and a data platform to gather and process the data, the living lab supports a wide range of services and applications: IoT, intelligent transportation systems and assisted driving, environmental monitoring, emergency and safety, among others. This article describes the architecture, implementation and deployment to make the overall platform to work and integrate researchers and citizens. Moreover, it showcases some examples of the performance metrics achieved in the city infrastructure, the data that can be collected, visualized and used to build services and applications to the cities, and, finally, different use cases in the mobility and safety scenarios.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2131557684,
        "newsscientist":0.2216667105,
        "technologyreview":0.3289703874,
        "venturebeat":0.3523933929,
        "wired":0.3193273768,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12200v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658756529000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.13842v1",
        "predicted_newsworthiness":0.6194139704,
        "title":"Dive into Machine Learning Algorithms for Influenza Virus Host Prediction with Hemagglutinin Sequences",
        "summary":"Influenza viruses mutate rapidly and can pose a threat to public health, especially to those in vulnerable groups. Throughout history, influenza A viruses have caused pandemics between different species. It is important to identify the origin of a virus in order to prevent the spread of an outbreak. Recently, there has been increasing interest in using machine learning algorithms to provide fast and accurate predictions for viral sequences. In this study, real testing data sets and a variety of evaluation metrics were used to evaluate machine learning algorithms at different taxonomic levels. As hemagglutinin is the major protein in the immune response, only hemagglutinin sequences were used and represented by position-specific scoring matrix and word embedding. The results suggest that the 5-grams-transformer neural network is the most effective algorithm for predicting viral sequence origins, with approximately 99.54% AUCPR, 98.01% F1 score and 96.60% MCC at a higher classification level, and approximately 94.74% AUCPR, 87.41% F1 score and 80.79% MCC at a lower classification level.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2413945152,
        "newsscientist":0.2642443926,
        "technologyreview":0.3019726223,
        "venturebeat":0.2672433371,
        "wired":0.1934682644,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13842v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658969694000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13382v2",
        "predicted_newsworthiness":0.619384507,
        "title":"Exploration and Application of AI in 6G Field",
        "summary":"The recent upsurge of diversified mobile applications, especially those supported by AI, is spurring heated discussions on the future evolution of wireless communications. While 5G is being deployed around the world, efforts from industry and academia have started to look beyond 5G and conceptualize 6G. We envision 6G to experience an unprecedented transformation that will make it completely different from the previous generations of wireless systems. In particular, 6G will go beyond mobile Internet and will be required to support AI services. Meanwhile, AI will play a critical role in designing and optimizing 6G architectures, protocols and operations. In this article, we discuss the features of 6G, and the difficulties of carrying out 6G, and AI-enabled methods for 6G network design and optimization.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.103492876,
        "newsscientist":0.1746755012,
        "technologyreview":0.2876421233,
        "venturebeat":0.2978678156,
        "wired":0.2422304015,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13382v2",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658913065000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.12935v1",
        "predicted_newsworthiness":0.6176413193,
        "title":"\"I Used To Carry A Wallet, Now I Just Need To Carry My Phone\": Understanding Current Banking Practices and Challenges Among Older Adults in China",
        "summary":"Managing finances is crucial for older adults who are retired and may rely on savings to ensure their life quality. As digital banking platforms (e.g., mobile apps, electronic payment) gradually replace physical ones, it is critical to understand how they adapt to digital banking and the potential frictions they experience. We conducted semi-structured interviews with 16 older adults in China, where the aging population is the largest and digital banking grows fast. We also interviewed bank employees to gain complementary perspectives of these help givers. Our findings show that older adults used both physical and digital platforms as an ecosystem based on perceived pros and cons. Perceived usefulness, self-confidence, and social influence were key motivators for learning digital banking. They experienced app-related (e.g., insufficient error-recovery support) and user-related challenges (e.g., trust, security and privacy concerns, low perceived self-efficacy) and developed coping strategies. We discuss design considerations to improve their banking experiences.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2418798507,
        "newsscientist":0.1967234975,
        "technologyreview":0.3188853161,
        "venturebeat":0.3114103957,
        "wired":0.3011475677,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12935v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.cy"
        ],
        "published":1658846649000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.12021v1",
        "predicted_newsworthiness":0.6157869541,
        "title":"Neural Generation Meets Real People: Building a Social, Informative Open-Domain Dialogue Agent",
        "summary":"We present Chirpy Cardinal, an open-domain social chatbot. Aiming to be both informative and conversational, our bot chats with users in an authentic, emotionally intelligent way. By integrating controlled neural generation with scaffolded, hand-written dialogue, we let both the user and bot take turns driving the conversation, producing an engaging and socially fluent experience. Deployed in the fourth iteration of the Alexa Prize Socialbot Grand Challenge, Chirpy Cardinal handled thousands of conversations per day, placing second out of nine bots with an average user rating of 3.58\/5.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1753650693,
        "newsscientist":0.2279155323,
        "technologyreview":0.367229987,
        "venturebeat":0.3622158377,
        "wired":0.3048123857,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12021v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658743043000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.02058v1",
        "predicted_newsworthiness":0.6126444709,
        "title":"Robots with Different Embodiments Can Express and Influence Carefulness in Object Manipulation",
        "summary":"Humans have an extraordinary ability to communicate and read the properties of objects by simply watching them being carried by someone else. This level of communicative skills and interpretation, available to humans, is essential for collaborative robots if they are to interact naturally and effectively. For example, suppose a robot is handing over a fragile object. In that case, the human who receives it should be informed of its fragility in advance, through an immediate and implicit message, i.e., by the direct modulation of the robot's action. This work investigates the perception of object manipulations performed with a communicative intent by two robots with different embodiments (an iCub humanoid robot and a Baxter robot). We designed the robots' movements to communicate carefulness or not during the transportation of objects. We found that not only this feature is correctly perceived by human observers, but it can elicit as well a form of motor adaptation in subsequent human object manipulations. In addition, we get an insight into which motion features may induce to manipulate an object more or less carefully.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1602035381,
        "newsscientist":0.2346764662,
        "technologyreview":0.3147115834,
        "venturebeat":0.2482707888,
        "wired":0.2416229545,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02058v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.lg"
        ],
        "published":1659533212000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.02052v1",
        "predicted_newsworthiness":0.6093986306,
        "title":"Large scale analysis of gender bias and sexism in song lyrics",
        "summary":"We employ Natural Language Processing techniques to analyse 377808 English song lyrics from the \"Two Million Song Database\" corpus, focusing on the expression of sexism across five decades (1960-2010) and the measurement of gender biases. Using a sexism classifier, we identify sexist lyrics at a larger scale than previous studies using small samples of manually annotated popular songs. Furthermore, we reveal gender biases by measuring associations in word embeddings learned on song lyrics. We find sexist content to increase across time, especially from male artists and for popular songs appearing in Billboard charts. Songs are also shown to contain different language biases depending on the gender of the performer, with male solo artist songs containing more and stronger biases. This is the first large scale analysis of this type, giving insights into language usage in such an influential part of popular culture.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2459197387,
        "newsscientist":0.1857582784,
        "technologyreview":0.2786261178,
        "venturebeat":0.2484277722,
        "wired":0.2865302123,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02052v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659532722000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.12144v1",
        "predicted_newsworthiness":0.608243917,
        "title":"Personalised Robot Behaviour Modelling for Robot-Assisted Therapy in the Context of Autism Spectrum Disorder",
        "summary":"In robot-assisted therapy for individuals with Autism Spectrum Disorder, the workload of therapists during a therapeutic session is increased if they have to control the robot manually. To allow therapists to focus on the interaction with the person instead, the robot should be more autonomous, namely it should be able to interpret the person's state and continuously adapt its actions according to their behaviour. In this paper, we develop a personalised robot behaviour model that can be used in the robot decision-making process during an activity; this behaviour model is trained with the help of a user model that has been learned from real interaction data. We use Q-learning for this task, such that the results demonstrate that the policy requires about 10,000 iterations to converge. We thus investigate policy transfer for improving the convergence speed; we show that this is a feasible solution, but an inappropriate initial policy can lead to a suboptimal final return.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1482741575,
        "newsscientist":0.2074360228,
        "technologyreview":0.3312426326,
        "venturebeat":0.302432806,
        "wired":0.2405301197,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12144v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658753257000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00033v1",
        "predicted_newsworthiness":0.6080551433,
        "title":"Personalised recommendations of sleep behaviour with neural networks using sleep diaries captured in Sleepio",
        "summary":"SleepioTM is a digital mobile phone and web platform that uses techniques from cognitive behavioural therapy (CBT) to improve sleep in people with sleep difficulty. As part of this process, Sleepio captures data about the sleep behaviour of the users that have consented to such data being processed. For neural networks, the scale of the data is an opportunity to train meaningful models translatable to actual clinical practice. In collaboration with Big Health, the therapeutics company that created and utilizes Sleepio, we have analysed data from a random sample of 401,174 sleep diaries and built a neural network to model sleep behaviour and sleep quality of each individual in a personalised manner. We demonstrate that this neural network is more accurate than standard statistical methods in predicting the sleep quality of an individual based on his\/her behaviour from the last 10 days. We compare model performance in a wide range of hyperparameter settings representing various scenarios. We further show that the neural network can be used to produce personalised recommendations of what sleep habits users should follow to maximise sleep quality, and show that these recommendations are substantially better than the ones generated by standard methods. We finally show that the neural network can explain the recommendation given to each participant and calculate confidence intervals for each prediction, all of which are essential for clinicians to be able to adopt such a tool in clinical practice.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2143595986,
        "newsscientist":0.2572722749,
        "technologyreview":0.3232593099,
        "venturebeat":0.3148876242,
        "wired":0.2724662824,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00033v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659119345000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00496v1",
        "predicted_newsworthiness":0.6068030131,
        "title":"Wigglite: Low-cost Information Collection and Triage",
        "summary":"Consumers conducting comparison shopping, researchers making sense of competitive space, and developers looking for code snippets online all face the challenge of capturing the information they find for later use without interrupting their current flow. In addition, during many learning and exploration tasks, people need to externalize their mental context, such as estimating how urgent a topic is to follow up on, or rating a piece of evidence as a \"pro\" or \"con,\" which helps scaffold subsequent deeper exploration. However, current approaches incur a high cost, often requiring users to select, copy, context switch, paste, and annotate information in a separate document without offering specific affordances that capture their mental context. In this work, we explore a new interaction technique called \"wiggling,\" which can be used to fluidly collect, organize, and rate information during early sensemaking stages with a single gesture. Wiggling involves rapid back-and-forth movements of a pointer or up-and-down scrolling on a smartphone, which can indicate the information to be collected and its valence, using a single, light-weight gesture that does not interfere with other interactions that are already available. Through implementation and user evaluation, we found that wiggling helped participants accurately collect information and encode their mental context with a 58% reduction in operational cost while being 24% faster compared to a common baseline.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1964544102,
        "newsscientist":0.2385445399,
        "technologyreview":0.3040505829,
        "venturebeat":0.312840807,
        "wired":0.3260079223,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00496v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1659295439000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.14690v1",
        "predicted_newsworthiness":0.6067616132,
        "title":"Renting Edge Computing Resources for Service Hosting",
        "summary":"We consider the setting where a service is hosted on a third-party edge server deployed close to the users and a cloud server at a greater distance from the users. Due to the proximity of the edge servers to the users, requests can be served at the edge with low latency. However, as the computation resources at the edge are limited, some requests must be routed to the cloud for service and incur high latency. The system's overall performance depends on the rent cost incurred to use the edge server, the latency experienced by the users, and the cost incurred to change the amount of edge computation resources rented over time. The algorithmic challenge is to determine the amount of edge computation power to rent over time. We propose a deterministic online policy and characterize its performance for adversarial and stochastic i.i.d. request arrival processes. We also characterize a fundamental bound on the performance of any deterministic online policy. Further, we compare the performance of our policy with suitably modified versions of existing policies to conclude that our policy is robust to temporal changes in the intensity of request arrivals.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1624461451,
        "newsscientist":0.1888848714,
        "technologyreview":0.3075147891,
        "venturebeat":0.3475661612,
        "wired":0.284968784,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14690v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1659103217000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.12863v1",
        "predicted_newsworthiness":0.605905741,
        "title":"FRIB: Low-poisoning Rate Invisible Backdoor Attack based on Feature Repair",
        "summary":"During the generation of invisible backdoor attack poisoned data, the feature space transformation operation tends to cause the loss of some poisoned features and weakens the mapping relationship between source images with triggers and target labels, resulting in the need for a higher poisoning rate to achieve the corresponding backdoor attack success rate. To solve the above problems, we propose the idea of feature repair for the first time and introduce the blind watermark technique to repair the poisoned features lost during the generation of poisoned data. Under the premise of ensuring consistent labeling, we propose a low-poisoning rate invisible backdoor attack based on feature repair, named FRIB. Benefiting from the above design concept, the new method enhances the mapping relationship between the source images with triggers and the target labels, and increases the degree of misleading DNNs, thus achieving a high backdoor attack success rate with a very low poisoning rate. Ultimately, the detailed experimental results show that the goal of achieving a high success rate of backdoor attacks with a very low poisoning rate is achieved on all MNIST, CIFAR10, GTSRB, and ImageNet datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1231367408,
        "newsscientist":0.1883778522,
        "technologyreview":0.2845657532,
        "venturebeat":0.2381790069,
        "wired":0.2336900552,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12863v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658839737000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00824v1",
        "predicted_newsworthiness":0.6055895584,
        "title":"Safe Perception -- A Hierarchical Monitor Approach",
        "summary":"Our transportation world is rapidly transforming induced by an ever increasing level of autonomy. However, to obtain license of fully automated vehicles for widespread public use, it is necessary to assure safety of the entire system, which is still a challenge. This holds in particular for AI-based perception systems that have to handle a diversity of environmental conditions and road users, and at the same time should robustly detect all safety relevant objects (i.e no detection misses should occur). Yet, limited training and validation data make a proof of fault-free operation hardly achievable, as the perception system might be exposed to new, yet unknown objects or conditions on public roads. Hence, new safety approaches for AI-based perception systems are required. For this reason we propose in this paper a novel hierarchical monitoring approach that is able to validate the object list from a primary perception system, can reliably detect detection misses, and at the same time has a very low false alarm rate.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1321895354,
        "newsscientist":0.1985251529,
        "technologyreview":0.314154752,
        "venturebeat":0.2808762054,
        "wired":0.2373785715,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00824v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659359364000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01714v1",
        "predicted_newsworthiness":0.6047820737,
        "title":"An Open-Source Cultural Consensus Approach to Name-Based Gender Classification",
        "summary":"Name-based gender classification has enabled hundreds of otherwise infeasible scientific studies of gender. Yet, the lack of standardization, proliferation of ad hoc methods, reliance on paid services, understudied limitations, and conceptual debates cast a shadow over many applications. To address these problems we develop and evaluate an ensemble-based open-source method built on publicly available data of empirical name-gender associations. Our method integrates 36 distinct sources-spanning over 150 countries and more than a century-via a meta-learning algorithm inspired by Cultural Consensus Theory (CCT). We also construct a taxonomy with which names themselves can be classified. We find that our method's performance is competitive with paid services and that our method, and others, approach the upper limits of performance; we show that conditioning estimates on additional metadata (e.g. cultural context), further combining methods, or collecting additional name-gender association data is unlikely to meaningfully improve performance. This work definitively shows that name-based gender classification can be a reliable part of scientific research and provides a pair of tools, a classification method and a taxonomy of names, that realize this potential.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.264163919,
        "newsscientist":0.2314242762,
        "technologyreview":0.2911121585,
        "venturebeat":0.2550183604,
        "wired":0.2542435362,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01714v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1659470132000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2208.00913v1",
        "predicted_newsworthiness":0.6030387137,
        "title":"Effective Gesture Based Framework for Capturing User Input",
        "summary":"Computers today aren't just confined to laptops and desktops. Mobile gadgets like mobile phones and laptops also make use of it. However, one input device that hasn't changed in the last 50 years is the QWERTY keyboard. Users of virtual keyboards can type on any surface as if it were a keyboard thanks to sensor technology and artificial intelligence. In this research, we use the idea of image processing to create an application for seeing a computer keyboard using a novel framework which can detect hand gestures with precise accuracy while also being sustainable and financially viable. A camera is used to capture keyboard images and finger movements which subsequently acts as a virtual keyboard. In addition, a visible virtual mouse that accepts finger coordinates as input is also described in this study. This system has a direct benefit of reducing peripheral cost, reducing electronics waste generated due to external devices and providing accessibility to people who cannot use the traditional keyboard and mouse.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1419232839,
        "newsscientist":0.215694402,
        "technologyreview":0.2780867603,
        "venturebeat":0.2839675693,
        "wired":0.2792969329,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00913v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ai"
        ],
        "published":1659365897000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.00565v1",
        "predicted_newsworthiness":0.6028664419,
        "title":"Modeling Human Response to Robot Errors for Timely Error Detection",
        "summary":"In human-robot collaboration, robot errors are inevitable -- damaging user trust, willingness to work together, and task performance. Prior work has shown that people naturally respond to robot errors socially and that in social interactions it is possible to use human responses to detect errors. However, there is little exploration in the domain of non-social, physical human-robot collaboration such as assembly and tool retrieval. In this work, we investigate how people's organic, social responses to robot errors may be used to enable timely automatic detection of errors in physical human-robot interactions. We conducted a data collection study to obtain facial responses to train a real-time detection algorithm and a case study to explore the generalizability of our method with different task settings and errors. Our results show that natural social responses are effective signals for timely detection and localization of robot errors even in non-social contexts and that our method is robust across a variety of task contexts, robot errors, and user responses. This work contributes to robust error detection without detailed task specifications.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1607751771,
        "newsscientist":0.2337208123,
        "technologyreview":0.3418795494,
        "venturebeat":0.3022640648,
        "wired":0.2658101914,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00565v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659318931000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.10733v2",
        "predicted_newsworthiness":0.6015565061,
        "title":"GreenDB -- A Dataset and Benchmark for Extraction of Sustainability Information of Consumer Goods",
        "summary":"The production, shipping, usage, and disposal of consumer goods have a substantial impact on greenhouse gas emissions and the depletion of resources. Machine Learning (ML) can help to foster sustainable consumption patterns by accounting for sustainability aspects in product search or recommendations of modern retail platforms. However, the lack of large high quality publicly available product data with trustworthy sustainability information impedes the development of ML technology that can help to reach our sustainability goals. Here we present GreenDB, a database that collects products from European online shops on a weekly basis. As proxy for the products' sustainability, it relies on sustainability labels, which are evaluated by experts. The GreenDB schema extends the well-known schema.org Product definition and can be readily integrated into existing product catalogs. We present initial results demonstrating that ML models trained with our data can reliably (F1 score 96%) predict the sustainability label of products. These contributions can help to complement existing e-commerce experiences and ultimately encourage users to more sustainable consumption patterns.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2458293826,
        "newsscientist":0.2569610897,
        "technologyreview":0.3049504562,
        "venturebeat":0.318686359,
        "wired":0.2560869078,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.10733v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cy"
        ],
        "published":1658433582000,
        "published_hr":"Jul 21, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12850v1",
        "predicted_newsworthiness":0.6004034687,
        "title":"Towards Smart City Security: Violence and Weaponized Violence Detection using DCNN",
        "summary":"In this ever connected society, CCTVs have had a pivotal role in enforcing safety and security of the citizens by recording unlawful activities for the authorities to take actions. In a smart city context, using Deep Convolutional Neural Networks (DCNN) to detection violence and weaponized violence from CCTV videos will provide an additional layer of security by ensuring real-time detection around the clock. In this work, we introduced a new specialised dataset by gathering real CCTV footage of both weaponized and non-weaponized violence as well as non-violence videos from YouTube. We also proposed a novel approach in merging consecutive video frames into a single salient image which will then be the input to the DCNN. Results from multiple DCNN architectures have proven the effectiveness of our method by having the highest accuracy of 99\\%. We also take into consideration the efficiency of our methods through several parameter trade-offs to ensure smart city sustainability.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2056860105,
        "newsscientist":0.197733954,
        "technologyreview":0.3150183008,
        "venturebeat":0.2748524357,
        "wired":0.262664892,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12850v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658838661000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11774v1",
        "predicted_newsworthiness":0.5975414597,
        "title":"Towards a Sentiment-Aware Conversational Agent",
        "summary":"In this paper, we propose an end-to-end sentiment-aware conversational agent based on two models: a reply sentiment prediction model, which leverages the context of the dialogue to predict an appropriate sentiment for the agent to express in its reply; and a text generation model, which is conditioned on the predicted sentiment and the context of the dialogue, to produce a reply that is both context and sentiment appropriate. Additionally, we propose to use a sentiment classification model to evaluate the sentiment expressed by the agent during the development of the model. This allows us to evaluate the agent in an automatic way. Both automatic and human evaluation results show that explicitly guiding the text generation model with a pre-defined set of sentences leads to clear improvements, both regarding the expressed sentiment and the quality of the generated text.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1446073429,
        "newsscientist":0.1681107051,
        "technologyreview":0.289464755,
        "venturebeat":0.306238556,
        "wired":0.2274729189,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11774v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658681984000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14388v1",
        "predicted_newsworthiness":0.5974673005,
        "title":"Data Integrity Verification in Network Slicing using Oracles and Smart Contracts",
        "summary":"The fifth-generation (5G) wireless networks are expected to provide various services compared to the 4G and previous generations of networks. The Quality of Service requirements can be quite different in terms of latency, bandwidth, reliability, and availability. 5G technology allows the fragmentation of the network into small pieces, known as network slices. This network slicing is done by specific tools and the configuration must be protected from attacks that may be performed by malicious users. Thus in this paper, a solution to protect and prevent these failures from happening is addressed. For this solution to be carried out, a study was conducted on the Blockchain technology, as well as the use of Oracles in order to implement an integrity verification system, a system capable of assuring 5G network slices configuration integrity through a complete architecture involving Blockchain, Smart Contracts and Oracles.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1446568838,
        "newsscientist":0.1178001307,
        "technologyreview":0.2415191506,
        "venturebeat":0.2515291441,
        "wired":0.1979635043,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14388v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1659044926000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2208.00508v1",
        "predicted_newsworthiness":0.5973461984,
        "title":"Deep Active Learning with Budget Annotation",
        "summary":"Digital data collected over the decades and data currently being produced with use of information technology is vastly the unlabeled data or data without description. The unlabeled data is relatively easy to acquire but expensive to label even with use of domain experts. Most of the recent works focus on use of active learning with uncertainty metrics measure to address this problem. Although most uncertainty selection strategies are very effective, they fail to take informativeness of the unlabeled instances into account and are prone to querying outliers. In order to address these challenges we propose an hybrid approach of computing both the uncertainty and informativeness of an instance, then automaticaly label the computed instances using budget annotator. To reduce the annotation cost, we employ the state-of-the-art pre-trained models in order to avoid querying information already contained in those models. Our extensive experiments on different sets of datasets demonstrate the efficacy of the proposed approach.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1191720853,
        "newsscientist":0.1637060408,
        "technologyreview":0.282991263,
        "venturebeat":0.2849863817,
        "wired":0.1980208121,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00508v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1659298844000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12497v1",
        "predicted_newsworthiness":0.5964225606,
        "title":"Estimating and Controlling for Fairness via Sensitive Attribute Predictors",
        "summary":"Although machine learning classifiers have been increasingly used in high-stakes decision making (e.g., cancer diagnosis, criminal prosecution decisions), they have demonstrated biases against underrepresented groups. Standard definitions of fairness require access to sensitive attributes of interest (e.g., gender and race), which are often unavailable. In this work we demonstrate that in these settings where sensitive attributes are unknown, one can still reliably estimate and ultimately control for fairness by using proxy sensitive attributes derived from a sensitive attribute predictor. Specifically, we first show that with just a little knowledge of the complete data distribution, one may use a sensitive attribute predictor to obtain upper and lower bounds of the classifier's true fairness metric. Second, we demonstrate how one can provably control for fairness with respect to the true sensitive attributes by controlling for fairness with respect to the proxy sensitive attributes. Our results hold under assumptions that are significantly milder than previous works. We illustrate our results on a series of synthetic and real datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2240253472,
        "newsscientist":0.1975021406,
        "technologyreview":0.3082665465,
        "venturebeat":0.2731400945,
        "wired":0.2420186802,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12497v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cy"
        ],
        "published":1658778962000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11623v1",
        "predicted_newsworthiness":0.5937153887,
        "title":"A Simplistic and Cost-Effective Design for Real-World Development of an Ambient Assisted Living System for Fall Detection and Indoor Localization: Proof of Concept",
        "summary":"Falls, highly common in the constantly increasing global aging population, can have a variety of negative effects on their health, well-being, and quality of life, including restricting their capabilities to conduct Activities of Daily Living (ADLs), which are crucial for one's sustenance. Timely assistance during falls is highly necessary, which involves tracking the indoor location of the elderly during their diverse navigational patterns associated with ADLs to detect the precise location of a fall. With the decreasing caregiver population on a global scale, it is important that the future of intelligent living environments can detect falls during ADLs while being able to track the indoor location of the elderly in the real world. To address these challenges, this work proposes a cost-effective and simplistic design paradigm for an Ambient Assisted Living system that can capture multimodal components of user behaviors during ADLs that are necessary for performing fall detection and indoor localization in a simultaneous manner in the real world. Proof of concept results from real-world experiments are presented to uphold the effective working of the system. The findings from two comparison studies with prior works in this field are also presented to uphold the novelty of this work. The first comparison study shows how the proposed system outperforms prior works in the areas of indoor localization and fall detection in terms of the effectiveness of its software design and hardware design. The second comparison study shows that the cost for the development of this system is the least as compared to prior works in these fields, which involved real-world development of the underlining systems, thereby upholding its cost-effective nature.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1661219334,
        "newsscientist":0.2017007767,
        "technologyreview":0.2688229607,
        "venturebeat":0.2783546896,
        "wired":0.256433805,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11623v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ai",
            "cs.cv",
            "cs.lg"
        ],
        "published":1658621612000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.00953v1",
        "predicted_newsworthiness":0.5934128632,
        "title":"What do Deep Neural Networks Learn in Medical Images?",
        "summary":"Deep learning is increasingly gaining rapid adoption in healthcare to help improve patient outcomes. This is more so in medical image analysis which requires extensive training to gain the requisite expertise to become a trusted practitioner. However, while deep learning techniques have continued to provide state-of-the-art predictive performance, one of the primary challenges that stands to hinder this progress in healthcare is the opaque nature of the inference mechanism of these models. So, attribution has a vital role in building confidence in stakeholders for the predictions made by deep learning models to inform clinical decisions. This work seeks to answer the question: what do deep neural network models learn in medical images? In that light, we present a novel attribution framework using adaptive path-based gradient integration techniques. Results show a promising direction of building trust in domain experts to improve healthcare outcomes by allowing them to understand the input-prediction correlative structures, discover new bio-markers, and reveal potential model biases.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1876649119,
        "newsscientist":0.2336633841,
        "technologyreview":0.3660774932,
        "venturebeat":0.3247031814,
        "wired":0.2462888929,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00953v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659369914000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01099v1",
        "predicted_newsworthiness":0.592880969,
        "title":"Parsimonious Argument Annotations for Hate Speech Counter-narratives",
        "summary":"We present an enrichment of the Hateval corpus of hate speech tweets (Basile et. al 2019) aimed to facilitate automated counter-narrative generation. Comparably to previous work (Chung et. al. 2019), manually written counter-narratives are associated to tweets. However, this information alone seems insufficient to obtain satisfactory language models for counter-narrative generation. That is why we have also annotated tweets with argumentative information based on Wagemanns (2016), that we believe can help in building convincing and effective counter-narratives for hate speech against particular groups. We discuss adequacies and difficulties of this annotation process and present several baselines for automatic detection of the annotated elements. Preliminary results show that automatic annotators perform close to human annotators to detect some aspects of argumentation, while others only reach low or moderate level of inter-annotator agreement.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2504424789,
        "newsscientist":0.1831159708,
        "technologyreview":0.3155609052,
        "venturebeat":0.2597715799,
        "wired":0.3125025539,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01099v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659380312000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.01483v1",
        "predicted_newsworthiness":0.5927349226,
        "title":"Label Sleuth: From Unlabeled Text to a Classifier in a Few Hours",
        "summary":"Text classification can be useful in many real-world scenarios, saving a lot of time for end users. However, building a custom classifier typically requires coding skills and ML knowledge, which poses a significant barrier for many potential users. To lift this barrier, we introduce Label Sleuth, a free open source system for labeling and creating text classifiers. This system is unique for (a) being a no-code system, making NLP accessible to non-experts, (b) guiding users through the entire labeling process until they obtain a custom classifier, making the process efficient -- from cold start to classifier in a few hours, and (c) being open for configuration and extension by developers. By open sourcing Label Sleuth we hope to build a community of users and developers that will broaden the utilization of NLP models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1499730518,
        "newsscientist":0.185472672,
        "technologyreview":0.3079998955,
        "venturebeat":0.3255503459,
        "wired":0.2657030951,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01483v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.hc"
        ],
        "published":1659450703000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14355v1",
        "predicted_newsworthiness":0.5896754198,
        "title":"Multiple Attribute Fairness: Application to Fraud Detection",
        "summary":"We propose a fairness measure relaxing the equality conditions in the popular equal odds fairness regime for classification. We design an iterative, model-agnostic, grid-based heuristic that calibrates the outcomes per sensitive attribute value to conform to the measure. The heuristic is designed to handle high arity attribute values and performs a per attribute sanitization of outcomes across different protected attribute values. We also extend our heuristic for multiple attributes. Highlighting our motivating application, fraud detection, we show that the proposed heuristic is able to achieve fairness across multiple values of a single protected attribute, multiple protected attributes. When compared to current fairness techniques, that focus on two groups, we achieve comparable performance across several public data sets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1663546374,
        "newsscientist":0.1506001934,
        "technologyreview":0.2583538499,
        "venturebeat":0.2493049762,
        "wired":0.2063566349,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14355v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cy"
        ],
        "published":1659035985000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01103v1",
        "predicted_newsworthiness":0.5889485615,
        "title":"Safe and Efficient Exploration of Human Models During Human-Robot Interaction",
        "summary":"Many collaborative human-robot tasks require the robot to stay safe and work efficiently around humans. Since the robot can only stay safe with respect to its own model of the human, we want the robot to learn a good model of the human in order to act both safely and efficiently. This paper studies methods that enable a robot to safely explore the space of a human-robot system to improve the robot's model of the human, which will consequently allow the robot to access a larger state space and better work with the human. In particular, we introduce active exploration under the framework of energy-function based safe control, investigate the effect of different active exploration strategies, and finally analyze the effect of safe active exploration on both analytical and neural network human models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1151709931,
        "newsscientist":0.1764800853,
        "technologyreview":0.275015494,
        "venturebeat":0.2134338899,
        "wired":0.190918019,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01103v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659381058000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00301v1",
        "predicted_newsworthiness":0.588201329,
        "title":"Towards Visualization of Time-Series Ecological Momentary Assessment (EMA) Data on Standalone Voice-First Virtual Assistants",
        "summary":"Population aging is an increasingly important consideration for health care in the 21th century, and continuing to have access and interact with digital health information is a key challenge for aging populations. Voice-based Intelligent Virtual Assistants (IVAs) are promising to improve the Quality of Life (QoL) of older adults, and coupled with Ecological Momentary Assessments (EMA) they can be effective to collect important health information from older adults, especially when it comes to repeated time-based events. However, this same EMA data is hard to access for the older adult: although the newest IVAs are equipped with a display, the effectiveness of visualizing time-series based EMA data on standalone IVAs has not been explored. To investigate the potential opportunities for visualizing time-series based EMA data on standalone IVAs, we designed a prototype system, where older adults are able to query and examine the time-series EMA data on Amazon Echo Show - a widely used commercially available standalone screen-based IVA. We conducted a preliminary semi-structured interview with a geriatrician and an older adult, and identified three findings that should be carefully considered when designing such visualizations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2377391899,
        "newsscientist":0.2583974141,
        "technologyreview":0.3361023905,
        "venturebeat":0.3428812562,
        "wired":0.3181354999,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00301v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.cy"
        ],
        "published":1659211395000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.00724v1",
        "predicted_newsworthiness":0.5859847897,
        "title":"Safe Policy Improvement Approaches and their Limitations",
        "summary":"Safe Policy Improvement (SPI) is an important technique for offline reinforcement learning in safety critical applications as it improves the behavior policy with a high probability. We classify various SPI approaches from the literature into two groups, based on how they utilize the uncertainty of state-action pairs. Focusing on the Soft-SPIBB (Safe Policy Improvement with Soft Baseline Bootstrapping) algorithms, we show that their claim of being provably safe does not hold. Based on this finding, we develop adaptations, the Adv-Soft-SPIBB algorithms, and show that they are provably safe. A heuristic adaptation, Lower-Approx-Soft-SPIBB, yields the best performance among all SPIBB algorithms in extensive experiments on two benchmarks. We also check the safety guarantees of the provably safe algorithms and show that huge amounts of data are necessary such that the safety bounds become useful in practice.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1200451694,
        "newsscientist":0.13982109,
        "technologyreview":0.2294574289,
        "venturebeat":0.2078918549,
        "wired":0.1745978055,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00724v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659348783000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12943v2",
        "predicted_newsworthiness":0.5857258439,
        "title":"Unique in what sense? Heterogeneous relationships between multiple types of uniqueness and popularity in music",
        "summary":"How does our society appreciate the uniqueness of cultural products? This fundamental puzzle has intrigued scholars in many fields, including psychology, sociology, anthropology, and marketing. It has been theorized that cultural products that balance familiarity and novelty are more likely to become popular. However, a cultural product's novelty is typically multifaceted. This paper uses songs as a case study to study the multiple facets of uniqueness and their relationship with success. We first unpack the multiple facets of a song's novelty or uniqueness and, next, measure its impact on a song's popularity. We employ a series of statistical models to study the relationship between a song's popularity and novelty associated with its lyrics, chord progressions, or audio properties. Our analyses performed on a dataset of over fifty thousand songs find a consistently negative association between all types of song novelty and popularity. Overall we found a song's lyrics uniqueness to have the most significant association with its popularity. However, audio uniqueness was the strongest predictor of a song's popularity, conditional on the song's genre. We further found the theme and repetitiveness of a song's lyrics to mediate the relationship between the song's popularity and novelty. Broadly, our results contradict the \"optimal distinctiveness theory\" (balance between novelty and familiarity) and call for an investigation into the multiple dimensions along which a cultural product's uniqueness could manifest.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2274462595,
        "newsscientist":0.1907814732,
        "technologyreview":0.2229907711,
        "venturebeat":0.2604363645,
        "wired":0.2645114727,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12943v2",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1658846998000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.12319v2",
        "predicted_newsworthiness":0.5846908507,
        "title":"OpenFilter: A Framework to Democratize Research Access to Social Media AR Filters",
        "summary":"Augmented Reality or AR filters on selfies have become very popular on social media platforms for a variety of applications, including marketing, entertainment and aesthetics. Given the wide adoption of AR face filters and the importance of faces in our social structures and relations, there is increased interest by the scientific community to analyze the impact of such filters from a psychological, artistic and sociological perspective. However, there are few quantitative analyses in this area mainly due to a lack of publicly available datasets of facial images with applied AR filters. The proprietary, close nature of most social media platforms does not allow users, scientists and practitioners to access the code and the details of the available AR face filters. Scraping faces from these platforms to collect data is ethically unacceptable and should, therefore, be avoided in research. In this paper, we present OpenFilter, a flexible framework to apply AR filters available in social media platforms on existing large collections of human faces. Moreover, we share FairBeauty and B-LFW, two beautified versions of the publicly available FairFace and LFW datasets and we outline insights derived from the analysis of these beautified datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2506021742,
        "newsscientist":0.2672009726,
        "technologyreview":0.3713647026,
        "venturebeat":0.3562708487,
        "wired":0.367514132,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12319v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658250325000,
        "published_hr":"Jul 19, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01366v1",
        "predicted_newsworthiness":0.5834891125,
        "title":"Detecting Individual Decision-Making Style: Exploring Behavioral Stylometry in Chess",
        "summary":"The advent of machine learning models that surpass human decision-making ability in complex domains has initiated a movement towards building AI systems that interact with humans. Many building blocks are essential for this activity, with a central one being the algorithmic characterization of human behavior. While much of the existing work focuses on aggregate human behavior, an important long-range goal is to develop behavioral models that specialize to individual people and can differentiate among them. To formalize this process, we study the problem of behavioral stylometry, in which the task is to identify a decision-maker from their decisions alone. We present a transformer-based approach to behavioral stylometry in the context of chess, where one attempts to identify the player who played a set of games. Our method operates in a few-shot classification framework, and can correctly identify a player from among thousands of candidate players with 98% accuracy given only 100 labeled games. Even when trained on amateur play, our method generalises to out-of-distribution samples of Grandmaster players, despite the dramatic differences between amateur and world-class players. Finally, we consider more broadly what our resulting embeddings reveal about human style in chess, as well as the potential ethical implications of powerful methods for identifying individuals from behavioral data.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1758649798,
        "newsscientist":0.2220722492,
        "technologyreview":0.3253884101,
        "venturebeat":0.2981005798,
        "wired":0.24363344,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01366v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.lg"
        ],
        "published":1659439096000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.12958v1",
        "predicted_newsworthiness":0.5824845815,
        "title":"From Interpretable Filters to Predictions of Convolutional Neural Networks with Explainable Artificial Intelligence",
        "summary":"Convolutional neural networks (CNN) are known for their excellent feature extraction capabilities to enable the learning of models from data, yet are used as black boxes. An interpretation of the convolutional filtres and associated features can help to establish an understanding of CNN to distinguish various classes. In this work, we focus on the explainability of a CNN model called as cnnexplain that is used for Covid-19 and non-Covid-19 classification with a focus on the interpretability of features by the convolutional filters, and how these features contribute to classification. Specifically, we have used various explainable artificial intelligence (XAI) methods, such as visualizations, SmoothGrad, Grad-CAM, and LIME to provide interpretation of convolutional filtres, and relevant features, and their role in classification. We have analyzed the explanation of these methods for Covid-19 detection using dry cough spectrograms. Explanation results obtained from the LIME, SmoothGrad, and Grad-CAM highlight important features of different spectrograms and their relevance to classification.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2164358101,
        "newsscientist":0.2462367896,
        "technologyreview":0.3308011065,
        "venturebeat":0.2930028271,
        "wired":0.2200277485,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12958v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658847925000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00085v1",
        "predicted_newsworthiness":0.5818427216,
        "title":"Machine Learning and Computer Vision Techniques in Bee Monitoring Applications",
        "summary":"Machine learning and computer vision are dynamically growing fields, which have proven to be able to solve very complex tasks. They could also be used for the monitoring of the honeybee colonies and for the inspection of their health state, which could identify potentially dangerous states before the situation is critical, or to better plan periodic bee colony inspections and therefore save significant costs. In this paper, we present an overview of the state-of-the-art computer vision and machine learning applications used for bee monitoring. We also demonstrate the potential of those methods as an example of an automated bee counter algorithm. The paper is aimed at veterinary and apidology professionals and experts, who might not be familiar with machine learning to introduce to them its possibilities, therefore each family of applications is opened by a brief theoretical introduction and motivation related to its base method. We hope that this paper will inspire other scientists to use the machine learning techniques for other applications in bee monitoring.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1559195681,
        "newsscientist":0.2307653817,
        "technologyreview":0.2932458172,
        "venturebeat":0.2690665232,
        "wired":0.223265959,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00085v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659131819000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01901v1",
        "predicted_newsworthiness":0.58167722,
        "title":"Asynchronous Federated Learning for Edge-assisted Vehicular Networks",
        "summary":"Vehicular networks enable vehicles support real-time vehicular applications through training data. Due to the limited computing capability, vehicles usually transmit data to a road side unit (RSU) at the network edge to process data. However, vehicles are usually reluctant to share data with each other due to the privacy issue. For the traditional federated learning (FL), vehicles train the data locally to obtain a local model and then upload the local model to the RSU to update the global model, thus the data privacy can be protected through sharing model parameters instead of data. The traditional FL updates the global model synchronously, i.e., the RSU needs to wait for all vehicles to upload their models for the global model updating. However, vehicles may usually drive out of the coverage of the RSU before they obtain their local models through training, which reduces the accuracy of the global model. It is necessary to propose an asynchronous federated learning (AFL) to solve this problem, where the RSU updates the global model once it receives a local model from a vehicle. However, the amount of data, computing capability and vehicle mobility may affect the accuracy of the global model. In this paper, we jointly consider the amount of data, computing capability and vehicle mobility to design an AFL scheme to improve the accuracy of the global model. Extensive simulation experiments have demonstrated that our scheme outperforms the FL scheme",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1177417816,
        "newsscientist":0.1359132424,
        "technologyreview":0.2616295667,
        "venturebeat":0.2686851282,
        "wired":0.2389197579,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01901v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ni",
            "cs.ro"
        ],
        "published":1659513902000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01211v1",
        "predicted_newsworthiness":0.5811409562,
        "title":"Gesture-aware Interactive Machine Teaching with In-situ Object Annotations",
        "summary":"Interactive Machine Teaching (IMT) systems allow non-experts to easily create Machine Learning (ML) models. However, existing vision-based IMT systems either ignore annotations on the objects of interest or require users to annotate in a post-hoc manner. Without the annotations on objects, the model may misinterpret the objects using unrelated features. Post-hoc annotations cause additional workload, which diminishes the usability of the overall model building process. In this paper, we develop LookHere, which integrates in-situ object annotations into vision-based IMT. LookHere exploits users' deictic gestures to segment the objects of interest in real time. This segmentation information can be additionally used for training. To achieve the reliable performance of this object segmentation, we utilize our custom dataset called HuTics, including 2040 front-facing images of deictic gestures toward various objects by 170 people. The quantitative results of our user study showed that participants were 16.3 times faster in creating a model with our system compared to a standard IMT system with a post-hoc annotation process while demonstrating comparable accuracies. Additionally, models created by our system showed a significant accuracy improvement ($\\Delta mIoU=0.466$) in segmenting the objects of interest compared to those without annotations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.129929023,
        "newsscientist":0.1996679028,
        "technologyreview":0.3112068637,
        "venturebeat":0.2962655206,
        "wired":0.2628251254,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01211v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1659407015000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.11609v1",
        "predicted_newsworthiness":0.5798205283,
        "title":"Exploring the Impact of Temporal Bias in Point-of-Interest Recommendation",
        "summary":"Recommending appropriate travel destinations to consumers based on contextual information such as their check-in time and location is a primary objective of Point-of-Interest (POI) recommender systems. However, the issue of contextual bias (i.e., how much consumers prefer one situation over another) has received little attention from the research community. This paper examines the effect of temporal bias, defined as the difference between users' check-in hours, leisure vs.~work hours, on the consumer-side fairness of context-aware recommendation algorithms. We believe that eliminating this type of temporal (and geographical) bias might contribute to a drop in traffic-related air pollution, noting that rush-hour traffic may be more congested. To surface effective POI recommendations, we evaluated the sensitivity of state-of-the-art context-aware models to the temporal bias contained in users' check-in activities on two POI datasets, namely Gowalla and Yelp. The findings show that the examined context-aware recommendation models prefer one group of users over another based on the time of check-in and that this preference persists even when users have the same amount of interactions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.194053263,
        "newsscientist":0.1754905219,
        "technologyreview":0.2579307449,
        "venturebeat":0.2836481412,
        "wired":0.2521613465,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11609v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1658611519000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.12574v1",
        "predicted_newsworthiness":0.5773601186,
        "title":"Enabling a Cooperative Driver Messenger System for Lane Change Assistance Application",
        "summary":"Sensor data and Vehicle-to-Everything (V2X) communication can greatly assist Connected and Autonomous Vehicles (CAVs) in situational awareness and provide a safer driving experience. While sensor data recorded from devices such as radar and camera can assist in local awareness in the close vicinity of the Host Vehicle (HV), the information obtained is useful solely for the HV itself. On the other hand, V2X communication can allow CAVs to communicate with each other and transceive basic and\/or advanced safety information, allowing each CAV to create a sophisticated local object map for situational awareness. This paper introduces a point-to-point Driver Messenger System (DMS) that regularly maintains a local object map of the HV and uses it to convey HV's Over-the-Air (OTA) Driver Intent Messages (DIMs) to nearby identified Target Vehicle(s) (TV(s)) based on a list of pre-defined common traffic applications. The focus of this paper is on the lane change application where DMS can use the local object map to automatically identify closest TV in adjacent lane in the direction of HV's intended lane change and inform the TV via a DIM. Within DMS, the paper proposes a TV recognition algorithm for lane change application that utilizes the HV's Path History (PH) to accurately determine the closest TV that could potentially benefit from receiving a DIM from HV. Finally, DMS is also shown to act as an advanced warning system by providing extra time and space headway measurements between the HV and TVs upon a number of simulated lane change scenarios.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1075626949,
        "newsscientist":0.1529995787,
        "technologyreview":0.2453977373,
        "venturebeat":0.2438772355,
        "wired":0.2258340174,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12574v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658792737000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2208.01230v1",
        "predicted_newsworthiness":0.5770929992,
        "title":"A Multifaceted Benchmarking of Synthetic Electronic Health Record Generation Models",
        "summary":"Synthetic health data have the potential to mitigate privacy concerns when sharing data to support biomedical research and the development of innovative healthcare applications. Modern approaches for data generation based on machine learning, generative adversarial networks (GAN) methods in particular, continue to evolve and demonstrate remarkable potential. Yet there is a lack of a systematic assessment framework to benchmark methods as they emerge and determine which methods are most appropriate for which use cases. In this work, we introduce a generalizable benchmarking framework to appraise key characteristics of synthetic health data with respect to utility and privacy metrics. We apply the framework to evaluate synthetic data generation methods for electronic health records (EHRs) data from two large academic medical centers with respect to several use cases. The results illustrate that there is a utility-privacy tradeoff for sharing synthetic EHR data. The results further indicate that no method is unequivocally the best on all criteria in each use case, which makes it evident why synthetic data generation methods need to be assessed in context.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1848411792,
        "newsscientist":0.2290353566,
        "technologyreview":0.3386027669,
        "venturebeat":0.3146391739,
        "wired":0.2584440518,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01230v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cy"
        ],
        "published":1659411885000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12283v1",
        "predicted_newsworthiness":0.5768518273,
        "title":"MedML: Fusing Medical Knowledge and Machine Learning Models for Early Pediatric COVID-19 Hospitalization and Severity Prediction",
        "summary":"The COVID-19 pandemic has caused devastating economic and social disruption, straining the resources of healthcare institutions worldwide. This has led to a nationwide call for models to predict hospitalization and severe illness in patients with COVID-19 to inform distribution of limited healthcare resources. We respond to one of these calls specific to the pediatric population. To address this challenge, we study two prediction tasks for the pediatric population using electronic health records: 1) predicting which children are more likely to be hospitalized, and 2) among hospitalized children, which individuals are more likely to develop severe symptoms. We respond to the national Pediatric COVID-19 data challenge with a novel machine learning model, MedML. MedML extracts the most predictive features based on medical knowledge and propensity scores from over 6 million medical concepts and incorporates the inter-feature relationships between heterogeneous medical features via graph neural networks (GNN). We evaluate MedML across 143,605 patients for the hospitalization prediction task and 11,465 patients for the severity prediction task using data from the National Cohort Collaborative (N3C) dataset. We also report detailed group-level and individual-level feature importance analyses to evaluate the model interpretability. MedML achieves up to a 7% higher AUROC score and up to a 14% higher AUPRC score compared to the best baseline machine learning models and performs well across all nine national geographic regions and over all three-month spans since the start of the pandemic. Our cross-disciplinary research team has developed a method of incorporating clinical domain knowledge as the framework for a new type of machine learning model that is more predictive and explainable than current state-of-the-art data-driven feature selection methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2855330402,
        "newsscientist":0.2747580262,
        "technologyreview":0.3391821469,
        "venturebeat":0.3031228672,
        "wired":0.2402361998,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12283v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658764574000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01721v1",
        "predicted_newsworthiness":0.5762970463,
        "title":"Rumor Stance Classification in Online Social Networks: A Survey on the State-of-the-Art, Prospects, and Future Challenges",
        "summary":"The emergence of the Internet as a ubiquitous technology has facilitated the rapid evolution of social media as the leading virtual platform for communication, content sharing, and information dissemination. In spite of revolutionizing the way news used to be delivered to people, this technology has also brought along with itself inevitable demerits. One such drawback is the spread of rumors facilitated by social media platforms which may provoke doubt and fear upon people. Therefore, the need to debunk rumors before their wide spread has become essential all the more. Over the years, many studies have been conducted to develop effective rumor verification systems. One aspect of such studies focuses on rumor stance classification, which concerns the task of utilizing users' viewpoints about a rumorous post to better predict the veracity of a rumor. Relying on users' stances in rumor verification task has gained great importance, for it has shown significant improvements in the model performances. In this paper, we conduct a comprehensive literature review on rumor stance classification in complex social networks. In particular, we present a thorough description of the approaches and mark the top performances. Moreover, we introduce multiple datasets available for this purpose and highlight their limitations. Finally, some challenges and future directions are discussed to stimulate further relevant research efforts.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1807007154,
        "newsscientist":0.1473778766,
        "technologyreview":0.2735242204,
        "venturebeat":0.2287610352,
        "wired":0.2756509532,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01721v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.ni"
        ],
        "published":1659470869000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.13596v1",
        "predicted_newsworthiness":0.5759903258,
        "title":"Fairness and Randomness in Machine Learning: Statistical Independence and Relativization",
        "summary":"Fair Machine Learning endeavors to prevent unfairness arising in the context of machine learning applications embedded in society. Despite the variety of definitions of fairness and proposed \"fair algorithms\", there remain unresolved conceptual problems regarding fairness. In this paper, we argue that randomness and fairness can be considered equivalent concepts in machine learning. We obtain a relativized notion of randomness expressed as statistical independence by appealing to Von Mises' century-old foundations for probability. Via fairness notions in machine learning, which are expressed as statistical independence as well, we then link the ante randomness assumptions about the data to the ex post requirements for fair predictions. This connection proves fruitful: we use it to argue that randomness and fairness are essentially relative and that randomness should reflect its nature as a modeling assumption in machine learning.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1995620497,
        "newsscientist":0.2138320223,
        "technologyreview":0.3318361545,
        "venturebeat":0.2807895334,
        "wired":0.2508386032,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13596v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cy"
        ],
        "published":1658937305000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13500v1",
        "predicted_newsworthiness":0.5757403526,
        "title":"Modelling Social Context for Fake News Detection: A Graph Neural Network Based Approach",
        "summary":"Detection of fake news is crucial to ensure the authenticity of information and maintain the news ecosystems reliability. Recently, there has been an increase in fake news content due to the recent proliferation of social media and fake content generation techniques such as Deep Fake. The majority of the existing modalities of fake news detection focus on content based approaches. However, most of these techniques fail to deal with ultra realistic synthesized media produced by generative models. Our recent studies find that the propagation characteristics of authentic and fake news are distinguishable, irrespective of their modalities. In this regard, we have investigated the auxiliary information based on social context to detect fake news. This paper has analyzed the social context of fake news detection with a hybrid graph neural network based approach. This hybrid model is based on integrating a graph neural network on the propagation of news and bi directional encoder representations from the transformers model on news content to learn the text features. Thus this proposed approach learns the content as well as the context features and hence able to outperform the baseline models with an f1 score of 0.91 on PolitiFact and 0.93 on the Gossipcop dataset, respectively",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1779667335,
        "newsscientist":0.1664850936,
        "technologyreview":0.3155684852,
        "venturebeat":0.2518061833,
        "wired":0.2731233693,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13500v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.cl",
            "cs.ir"
        ],
        "published":1658926713000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2208.01890v1",
        "predicted_newsworthiness":0.5749370546,
        "title":"High stable and accurate vehicle selection scheme based on federated edge learning in vehicular networks",
        "summary":"Federated edge learning (FEEL) technology for vehicular networks is considered as a promising technology to reduce the computation workload while keep the privacy of users. In the FEEL system, vehicles upload data to the edge servers, which train the vehicles' data to update local models and then return the result to vehicles to avoid sharing the original data. However, the cache queue in the edge is limited and the channel between edge server and each vehicle is a time varying wireless channel, which makes a challenge to select a suitable number of vehicles to upload data to keep a stable cache queue in edge server and maximize the learning accuracy. Moreover, selecting vehicles with different resource statuses to update data will affect the total amount of data involved in training, which further affects the model accuracy. In this paper, we propose a vehicle selection scheme, which maximizes the learning accuracy while ensuring the stability of the cache queue, where the statuses of all the vehicles in the coverage of edge server are taken into account. The performance of this scheme is evaluated through simulation experiments, which indicates that our proposed scheme can perform better than the known benchmark scheme.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.106740813,
        "newsscientist":0.1418873414,
        "technologyreview":0.2585772897,
        "venturebeat":0.2679240627,
        "wired":0.2408274963,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01890v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1659512377000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.12310v1",
        "predicted_newsworthiness":0.5743936543,
        "title":"Estimaci\u00f3n de \u00e1reas de cultivo mediante Deep Learning y programaci\u00f3n convencional",
        "summary":"Artificial Intelligence has enabled the implementation of more accurate and efficient solutions to problems in various areas. In the agricultural sector, one of the main needs is to know at all times the extent of land occupied or not by crops in order to improve production and profitability. The traditional methods of calculation demand the collection of data manually and in person in the field, causing high labor costs, execution times, and inaccuracy in the results. The present work proposes a new method based on Deep Learning techniques complemented with conventional programming for the determination of the area of populated and unpopulated crop areas. We have considered as a case study one of the most recognized companies in the planting and harvesting of sugar cane in Ecuador. The strategy combines a Generative Adversarial Neural Network (GAN) that is trained on a dataset of aerial photographs of natural and urban landscapes to improve image resolution; a Convolutional Neural Network (CNN) trained on a dataset of aerial photographs of sugar cane plots to distinguish populated or unpopulated crop areas; and a standard image processing module for the calculation of areas in a percentage manner. The experiments performed demonstrate a significant improvement in the quality of the aerial photographs as well as a remarkable differentiation between populated and unpopulated crop areas, consequently, a more accurate result of cultivated and uncultivated areas. The proposed method can be extended to the detection of possible pests, areas of weed vegetation, dynamic crop development, and both qualitative and quantitative quality control.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1767280157,
        "newsscientist":0.2094851058,
        "technologyreview":0.3000365382,
        "venturebeat":0.2559352802,
        "wired":0.1991353277,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12310v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658766175000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12052v1",
        "predicted_newsworthiness":0.5738538747,
        "title":"Designing an AI-Driven Talent Intelligence Solution: Exploring Big Data to extend the TOE Framework",
        "summary":"AI has the potential to improve approaches to talent management enabling dynamic provisions through implementing advanced automation. This study aims to identify the new requirements for developing AI-oriented artifacts to address talent management issues. Focusing on enhancing interactions between professional assessment and planning attributes, the design artifact is an intelligent employment automation solution for career guidance that is largely dependent on a talent intelligent module and an individuals growth needs. A design science method is adopted for conducting the experimental study with structured machine learning techniques which is the primary element of a comprehensive AI solution framework informed through a proposed moderation of the technology-organization-environment theory.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1988549684,
        "newsscientist":0.2021595418,
        "technologyreview":0.3577264057,
        "venturebeat":0.3476413625,
        "wired":0.2668891036,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12052v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1658745770000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.14145v1",
        "predicted_newsworthiness":0.5738049586,
        "title":"A Probabilistic Framework for Estimating the Risk of Pedestrian-Vehicle Conflicts at Intersections",
        "summary":"Pedestrian safety has become an important research topic among various studies due to the increased number of pedestrian-involved crashes. To evaluate pedestrian safety proactively, surrogate safety measures (SSMs) have been widely used in traffic conflict-based studies as they do not require historical crashes as inputs. However, most existing SSMs were developed based on the assumption that road users would maintain constant velocity and direction. Risk estimations based on this assumption are less unstable, more likely to be exaggerated, and unable to capture the evasive maneuvers of drivers. Considering the limitations among existing SSMs, this study proposes a probabilistic framework for estimating the risk of pedestrian-vehicle conflicts at intersections. The proposed framework loosen restrictions of constant speed by predicting trajectories using a Gaussian Process Regression and accounts for the different possible driver maneuvers with a Random Forest model. Real-world LiDAR data collected at an intersection was used to evaluate the performance of the proposed framework. The newly developed framework is able to identify all pedestrian-vehicle conflicts. Compared to the Time-to-Collision, the proposed framework provides a more stable risk estimation and captures the evasive maneuvers of vehicles. Moreover, the proposed framework does not require expensive computation resources, which makes it an ideal choice for real-time proactive pedestrian safety solutions at intersections.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1492288764,
        "newsscientist":0.1561689441,
        "technologyreview":0.2208128547,
        "venturebeat":0.1930273856,
        "wired":0.2027872132,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14145v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659020921000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11776v1",
        "predicted_newsworthiness":0.5730857232,
        "title":"Incorporating Heterogeneous User Behaviors and Social Influences for Predictive Analysis",
        "summary":"Behavior prediction based on historical behavioral data have practical real-world significance. It has been applied in recommendation, predicting academic performance, etc. With the refinement of user data description, the development of new functions, and the fusion of multiple data sources, heterogeneous behavioral data which contain multiple types of behaviors become more and more common. In this paper, we aim to incorporate heterogeneous user behaviors and social influences for behavior predictions. To this end, this paper proposes a variant of Long-Short Term Memory (LSTM) which can consider context information while modeling a behavior sequence, a projection mechanism which can model multi-faceted relationships among different types of behaviors, and a multi-faceted attention mechanism which can dynamically find out informative periods from different facets. Many kinds of behavioral data belong to spatio-temporal data. An unsupervised way to construct a social behavior graph based on spatio-temporal data and to model social influences is proposed. Moreover, a residual learning-based decoder is designed to automatically construct multiple high-order cross features based on social behavior representation and other types of behavior representations. Qualitative and quantitative experiments on real-world datasets have demonstrated the effectiveness of this model.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1687300913,
        "newsscientist":0.1953483795,
        "technologyreview":0.2739036613,
        "venturebeat":0.2760230858,
        "wired":0.2374062738,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11776v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658682337000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12105v1",
        "predicted_newsworthiness":0.5722125133,
        "title":"Ego-graph Replay based Continual Learning for Misinformation Engagement Prediction",
        "summary":"Online social network platforms have a problem with misinformation. One popular way of addressing this problem is via the use of machine learning based automated misinformation detection systems to classify if a post is misinformation. Instead of post hoc detection, we propose to predict if a user will engage with misinformation in advance and design an effective graph neural network classifier based on ego-graphs for this task. However, social networks are highly dynamic, reflecting continual changes in user behaviour, as well as the content being posted. This is problematic for machine learning models which are typically trained on a static training dataset, and can thus become outdated when the social network changes. Inspired by the success of continual learning on such problems, we propose an ego-graphs replay strategy in continual learning (EgoCL) using graph neural networks to effectively address this issue. We have evaluated the performance of our method on user engagement with misinformation on two Twitter datasets across nineteen misinformation and conspiracy topics. Our experimental results show that our approach EgoCL has better performance in terms of predictive accuracy and computational resources than the state of the art.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1952891291,
        "newsscientist":0.1955312745,
        "technologyreview":0.3346114168,
        "venturebeat":0.2938136773,
        "wired":0.291156387,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12105v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1658751345000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2208.01466v1",
        "predicted_newsworthiness":0.5720420511,
        "title":"A Secure Dynamic Edge Resource Federation Architecture for Cross-Domain IoT Systems",
        "summary":"The fast integration of 5G communication, Artificial Intelligence (AI), and Internet-of-Things (IoT) technologies is envisioned to enable Next Generation Networks (NGNs) for diverse smart services and user-defined applications for Smart Cities. However, it is still challenging to build a scalable and efficient infrastructure that satisfies the various performance, security, and management demands by heterogeneous IoT applications across multiple administrative domains. This paper presents a dynamic edge resource federation architecture, which integrates the concept of network slicing (NS) and blockchain to improve scalability, dynamicity, and security for multi-domain IoT applications. A NS-enabled dynamic edge resource federation framework adopts intelligent mechanisms to support efficient multi-domain service coordination that satisfies diverse Quality of Service (QoS) and security requirements. We propose a Hierarchical Integrated Federated Ledger (HIFL), which aims to guarantee decentralized security and privacy-preserving properties in multi-domain resource orchestration and service re-adjustment. As a secure-by-design solution, HIFL is promising to support efficient, trust and secured end-to-end IoT services. A preliminary proof-of-concept prototype has been implemented for comparing intra- and inter-domain performance expectations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1320575649,
        "newsscientist":0.1244537466,
        "technologyreview":0.2524679404,
        "venturebeat":0.2744376483,
        "wired":0.2073816101,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01466v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1659448826000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.13339v1",
        "predicted_newsworthiness":0.5714219476,
        "title":"ALBench: A Framework for Evaluating Active Learning in Object Detection",
        "summary":"Active learning is an important technology for automated machine learning systems. In contrast to Neural Architecture Search (NAS) which aims at automating neural network architecture design, active learning aims at automating training data selection. It is especially critical for training a long-tailed task, in which positive samples are sparsely distributed. Active learning alleviates the expensive data annotation issue through incrementally training models powered with efficient data selection. Instead of annotating all unlabeled samples, it iteratively selects and annotates the most valuable samples. Active learning has been popular in image classification, but has not been fully explored in object detection. Most of current approaches on object detection are evaluated with different settings, making it difficult to fairly compare their performance. To facilitate the research in this field, this paper contributes an active learning benchmark framework named as ALBench for evaluating active learning in object detection. Developed on an automatic deep model training system, this ALBench framework is easy-to-use, compatible with different active learning algorithms, and ensures the same training and testing protocols. We hope this automated benchmark system help researchers to easily reproduce literature's performance and have objective comparisons with prior arts. The code will be release through Github.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0988314075,
        "newsscientist":0.1625161691,
        "technologyreview":0.2654192838,
        "venturebeat":0.2435741611,
        "wired":0.1691026268,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13339v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658907983000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13552v1",
        "predicted_newsworthiness":0.5705581788,
        "title":"iCub Being Social: Exploiting Social Cues for Interactive Object Detection Learning",
        "summary":"Performing joint interaction requires constant mutual monitoring of own actions and their effects on the other's behaviour. Such an action-effect monitoring is boosted by social cues and might result in an increasing sense of agency. Joint actions and joint attention are strictly correlated and both of them contribute to the formation of a precise temporal coordination. In human-robot interaction, the robot's ability to establish joint attention with a human partner and exploit various social cues to react accordingly is a crucial step in creating communicative robots. Along the social component, an effective human-robot interaction can be seen as a new method to improve and make the robot's learning process more natural and robust for a given task. In this work we use different social skills, such as mutual gaze, gaze following, speech and human face recognition, to develop an effective teacher-learner scenario tailored to visual object learning in dynamic environments. Experiments on the iCub robot demonstrate that the system allows the robot to learn new objects through a natural interaction with a human teacher in presence of distractors.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.132677165,
        "newsscientist":0.2029043682,
        "technologyreview":0.2953850898,
        "venturebeat":0.2504985999,
        "wired":0.2229213952,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13552v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658933009000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.12148v1",
        "predicted_newsworthiness":0.5699676867,
        "title":"Applying Spatiotemporal Attention to Identify Distracted and Drowsy Driving with Vision Transformers",
        "summary":"A 20% rise in car crashes in 2021 compared to 2020 has been observed as a result of increased distraction and drowsiness. Drowsy and distracted driving are the cause of 45% of all car crashes. As a means to decrease drowsy and distracted driving, detection methods using computer vision can be designed to be low-cost, accurate, and minimally invasive. This work investigated the use of the vision transformer to outperform state-of-the-art accuracy from 3D-CNNs. Two separate transformers were trained for drowsiness and distractedness. The drowsy video transformer model was trained on the National Tsing-Hua University Drowsy Driving Dataset (NTHU-DDD) with a Video Swin Transformer model for 10 epochs on two classes -- drowsy and non-drowsy simulated over 10.5 hours. The distracted video transformer was trained on the Driver Monitoring Dataset (DMD) with Video Swin Transformer for 50 epochs over 9 distraction-related classes. The accuracy of the drowsiness model reached 44% and a high loss value on the test set, indicating overfitting and poor model performance. Overfitting indicates limited training data and applied model architecture lacked quantifiable parameters to learn. The distracted model outperformed state-of-the-art models on DMD reaching 97.5%, indicating that with sufficient data and a strong architecture, transformers are suitable for unfit driving detection. Future research should use newer and stronger models such as TokenLearner to achieve higher accuracy and efficiency, merge existing datasets to expand to detecting drunk driving and road rage to create a comprehensive solution to prevent traffic crashes, and deploying a functioning prototype to revolutionize the automotive safety industry.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.165713469,
        "newsscientist":0.2096404662,
        "technologyreview":0.2940672752,
        "venturebeat":0.2720138851,
        "wired":0.2394317776,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12148v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658507808000,
        "published_hr":"Jul 22, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11564v1",
        "predicted_newsworthiness":0.5699240111,
        "title":"A general-purpose method for applying Explainable AI for Anomaly Detection",
        "summary":"The need for explainable AI (XAI) is well established but relatively little has been published outside of the supervised learning paradigm. This paper focuses on a principled approach to applying explainability and interpretability to the task of unsupervised anomaly detection. We argue that explainability is principally an algorithmic task and interpretability is principally a cognitive task, and draw on insights from the cognitive sciences to propose a general-purpose method for practical diagnosis using explained anomalies. We define Attribution Error, and demonstrate, using real-world labeled datasets, that our method based on Integrated Gradients (IG) yields significantly lower attribution errors than alternative methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1434734694,
        "newsscientist":0.1995143407,
        "technologyreview":0.316126865,
        "venturebeat":0.2946707724,
        "wired":0.2159589651,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11564v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658598961000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12782v1",
        "predicted_newsworthiness":0.5690550535,
        "title":"An Explainable Decision Support System for Predictive Process Analytics",
        "summary":"Predictive Process Analytics is becoming an essential aid for organizations, providing online operational support of their processes. However, process stakeholders need to be provided with an explanation of the reasons why a given process execution is predicted to behave in a certain way. Otherwise, they will be unlikely to trust the predictive monitoring technology and, hence, adopt it. This paper proposes a predictive analytics framework that is also equipped with explanation capabilities based on the game theory of Shapley Values. The framework has been implemented in the IBM Process Mining suite and commercialized for business users. The framework has been tested on real-life event data to assess the quality of the predictions and the corresponding evaluations. In particular, a user evaluation has been performed in order to understand if the explanations provided by the system were intelligible to process stakeholders.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1268725485,
        "newsscientist":0.1454201309,
        "technologyreview":0.2464597606,
        "venturebeat":0.2789787838,
        "wired":0.1817415847,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12782v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658829349000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13835v1",
        "predicted_newsworthiness":0.5684459116,
        "title":"Impactful Robots: Evaluating Visual and Audio Warnings to Help Users Brace for Impact in Human Robot Interaction",
        "summary":"Wearable robotic devices have potential to assist and protect their users. Toward design of a Smart Helmet, this article examines the effectiveness of audio and visual warnings to help participants brace for impacts. A user study examines different warnings and impacts applied to users while running. Perturbation forces scaled to user mass are applied from different directions and user displacement is measured to characterize effectiveness of the warning. This is accomplished using the TreadPort Active Wind Tunnel adapted to deliver forward, rearward, right, or left perturbation forces at precise moments during the locomotor cycle. The article presents an overview of the system and demonstrates the ability to precisely deliver consistent warnings and perturbations during gait. User study results highlight effectiveness of visual and audio warnings to help users brace for impact, resulting in guidelines that will inform future human-robot warning systems.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.169746901,
        "newsscientist":0.2278436609,
        "technologyreview":0.2769896771,
        "venturebeat":0.2645925194,
        "wired":0.2694264529,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13835v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658966985000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.11822v1",
        "predicted_newsworthiness":0.5680197941,
        "title":"Efficient Embedding VNFs in 5G Network Slicing: A Deep Reinforcement Learning Approach",
        "summary":"5G radio access network (RAN) slicing aims to logically split an infrastructure into a set of self-contained programmable RAN slices, with each slice built on top of the underlying physical RAN (substrate) is a separate logical mobile network, which delivers a set of services with similar characteristics. Each RAN slice is constituted by various virtual network functions (VNFs) distributed geographically in numerous substrate nodes. A key challenge in building a robust RAN slicing is, therefore, designing a RAN slicing (RS)-configuration scheme that can utilize information such as resource availability in substrate networks as well as the interdependent relationships among slices to map (embed) VNFs onto live substrate nodes. With such motivation, we propose a machine-learning-powered RAN slicing scheme that aims to accommodate maximum numbers of slices (a set of connected Virtual Network Functions - VNFs) within a given request set. More specifically, we present a deep reinforcement scheme that is called Deep Allocation Agent (DAA). In short, DAA utilizes an empirically designed deep neural network that observes the current states of the substrate network and the requested slices to schedule the slices of which VNFs are then mapped to substrate nodes using an optimization algorithm. DAA is trained towards the goal of maximizing the number of accommodated slices in the given set by using an explicitly designed reward function. Our experiment study shows that, on average, DAA is able to maintain a rate of successfully routed slices above 80% in a resource-limited substrate network, and about 60% in extreme conditions, i.e., the available resources are much less than the demands.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1038594737,
        "newsscientist":0.13462696,
        "technologyreview":0.2446354158,
        "venturebeat":0.2639058164,
        "wired":0.1955236741,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11822v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658197463000,
        "published_hr":"Jul 18, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.12886v1",
        "predicted_newsworthiness":0.5679123412,
        "title":"Detection of road traffic crashes based on collision estimation",
        "summary":"This paper introduces a framework based on computer vision that can detect road traffic crashes (RCTs) by using the installed surveillance\/CCTV camera and report them to the emergency in real-time with the exact location and time of occurrence of the accident. The framework is built of five modules. We start with the detection of vehicles by using YOLO architecture; The second module is the tracking of vehicles using MOSSE tracker, Then the third module is a new approach to detect accidents based on collision estimation. Then the fourth module for each vehicle, we detect if there is a car accident or not based on the violent flow descriptor (ViF) followed by an SVM classifier for crash prediction. Finally, in the last stage, if there is a car accident, the system will send a notification to the emergency by using a GPS module that provides us with the location, time, and date of the accident to be sent to the emergency with the help of the GSM module. The main objective is to achieve higher accuracy with fewer false alarms and to implement a simple system based on pipelining technique.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1393922207,
        "newsscientist":0.163468399,
        "technologyreview":0.2272984495,
        "venturebeat":0.2144921,
        "wired":0.1997168467,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12886v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658841675000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01436v1",
        "predicted_newsworthiness":0.5675551202,
        "title":"Predicting Future Mosquito Habitats Using Time Series Climate Forecasting and Deep Learning",
        "summary":"Mosquito habitat ranges are projected to expand due to climate change. This investigation aims to identify future mosquito habitats by analyzing preferred ecological conditions of mosquito larvae. After assembling a data set with atmospheric records and larvae observations, a neural network is trained to predict larvae counts from ecological inputs. Time series forecasting is conducted on these variables and climate projections are passed into the initial deep learning model to generate location-specific larvae abundance predictions. The results support the notion of regional ecosystem-driven changes in mosquito spread, with high-elevation regions in particular experiencing an increase in susceptibility to mosquito infestation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2360368856,
        "newsscientist":0.251147745,
        "technologyreview":0.2622518101,
        "venturebeat":0.2162682061,
        "wired":0.1980024226,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01436v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659374709000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00461v1",
        "predicted_newsworthiness":0.5665182099,
        "title":"Adaptive Temperature Scaling for Robust Calibration of Deep Neural Networks",
        "summary":"In this paper, we study the post-hoc calibration of modern neural networks, a problem that has drawn a lot of attention in recent years. Many calibration methods of varying complexity have been proposed for the task, but there is no consensus about how expressive these should be. We focus on the task of confidence scaling, specifically on post-hoc methods that generalize Temperature Scaling, we call these the Adaptive Temperature Scaling family. We analyse expressive functions that improve calibration and propose interpretable methods. We show that when there is plenty of data complex models like neural networks yield better performance, but are prone to fail when the amount of data is limited, a common situation in certain post-hoc calibration applications like medical diagnosis. We study the functions that expressive methods learn under ideal conditions and design simpler methods but with a strong inductive bias towards these well-performing functions. Concretely, we propose Entropy-based Temperature Scaling, a simple method that scales the confidence of a prediction according to its entropy. Results show that our method obtains state-of-the-art performance when compared to others and, unlike complex models, it is robust against data scarcity. Moreover, our proposed model enables a deeper interpretation of the calibration process.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1307587497,
        "newsscientist":0.1891185083,
        "technologyreview":0.3031076934,
        "venturebeat":0.2674991211,
        "wired":0.1954768248,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00461v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659284406000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12515v1",
        "predicted_newsworthiness":0.565110834,
        "title":"A Survey on Trustworthy Recommender Systems",
        "summary":"Recommender systems (RS), serving at the forefront of Human-centered AI, are widely deployed in almost every corner of the web and facilitate the human decision-making process. However, despite their enormous capabilities and potential, RS may also lead to undesired counter-effects on users, items, producers, platforms, or even the society at large, such as compromised user trust due to non-transparency, unfair treatment of different consumers, or producers, privacy concerns due to extensive use of user's private data for personalization, just to name a few. All of these create an urgent need for Trustworthy Recommender Systems (TRS) so as to mitigate or avoid such adverse impacts and risks. In this survey, we will introduce techniques related to trustworthy and responsible recommendation, including but not limited to explainable recommendation, fairness in recommendation, privacy-aware recommendation, robustness in recommendation, user controllable recommendation, as well as the relationship between these different perspectives in terms of trustworthy and responsible recommendation. Through this survey, we hope to deliver readers with a comprehensive view of the research area and raise attention to the community about the importance, existing research achievements, and future research directions on trustworthy recommendation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1653796788,
        "newsscientist":0.1913070024,
        "technologyreview":0.3158984812,
        "venturebeat":0.3172595566,
        "wired":0.2675439448,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12515v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1658780605000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.01876v1",
        "predicted_newsworthiness":0.5644298105,
        "title":"Leveraging Smartphone Sensors for Detecting Abnormal Gait for Smart Wearable Mobile Technologies",
        "summary":"Walking is one of the most common modes of terrestrial locomotion for humans. Walking is essential for humans to perform most kinds of daily activities. When a person walks, there is a pattern in it, and it is known as gait. Gait analysis is used in sports and healthcare. We can analyze this gait in different ways, like using video captured by the surveillance cameras or depth image cameras in the lab environment. It also can be recognized by wearable sensors. e.g., accelerometer, force sensors, gyroscope, flexible goniometer, magneto resistive sensors, electromagnetic tracking system, force sensors, and electromyography (EMG). Analysis through these sensors required a lab condition, or users must wear these sensors. For detecting abnormality in gait action of a human, we need to incorporate the sensors separately. We can know about one's health condition by abnormal human gait after detecting it. Understanding a regular gait vs. abnormal gait may give insights to the health condition of the subject using the smart wearable technologies. Therefore, in this paper, we proposed a way to analyze abnormal human gait through smartphone sensors. Though smart devices like smartphones and smartwatches are used by most of the person nowadays. So, we can track down their gait using sensors of these intelligent wearable devices.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1454689117,
        "newsscientist":0.1977642408,
        "technologyreview":0.2543879067,
        "venturebeat":0.2507090255,
        "wired":0.2501261856,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01876v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.cv",
            "cs.lg"
        ],
        "published":1659510016000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.11875v1",
        "predicted_newsworthiness":0.5644148599,
        "title":"Seeking Subjectivity in Visual Emotion Distribution Learning",
        "summary":"Visual Emotion Analysis (VEA), which aims to predict people's emotions towards different visual stimuli, has become an attractive research topic recently. Rather than a single label classification task, it is more rational to regard VEA as a Label Distribution Learning (LDL) problem by voting from different individuals. Existing methods often predict visual emotion distribution in a unified network, neglecting the inherent subjectivity in its crowd voting process. In psychology, the \\textit{Object-Appraisal-Emotion} model has demonstrated that each individual's emotion is affected by his\/her subjective appraisal, which is further formed by the affective memory. Inspired by this, we propose a novel \\textit{Subjectivity Appraise-and-Match Network (SAMNet)} to investigate the subjectivity in visual emotion distribution. To depict the diversity in crowd voting process, we first propose the \\textit{Subjectivity Appraising} with multiple branches, where each branch simulates the emotion evocation process of a specific individual. Specifically, we construct the affective memory with an attention-based mechanism to preserve each individual's unique emotional experience. A subjectivity loss is further proposed to guarantee the divergence between different individuals. Moreover, we propose the \\textit{Subjectivity Matching} with a matching loss, aiming at assigning unordered emotion labels to ordered individual predictions in a one-to-one correspondence with the Hungarian algorithm. Extensive experiments and comparisons are conducted on public visual emotion distribution datasets, and the results demonstrate that the proposed SAMNet consistently outperforms the state-of-the-art methods. Ablation study verifies the effectiveness of our method and visualization proves its interpretability.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1727630006,
        "newsscientist":0.2011515574,
        "technologyreview":0.2785784641,
        "venturebeat":0.2537963688,
        "wired":0.2054693882,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11875v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658715603000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13700v1",
        "predicted_newsworthiness":0.5636334555,
        "title":"Remote Medication Status Prediction for Individuals with Parkinson's Disease using Time-series Data from Smartphones",
        "summary":"Medication for neurological diseases such as the Parkinson's disease usually happens remotely at home, away from hospitals. Such out-of-lab environments pose challenges in collecting timely and accurate health status data using the limited professional care devices for health condition analysis, medication adherence measurement and future dose or treatment planning. Individual differences in behavioral signals collected from wearable sensors also lead to difficulties in adopting current general machine learning analysis pipelines. To address these challenges, we present a method for predicting medication status of Parkinson's disease patients using the public mPower dataset, which contains 62,182 remote multi-modal test records collected on smartphones from 487 patients. The proposed method shows promising results in predicting three medication status objectively: Before Medication (AUC=0.95), After Medication (AUC=0.958), and Another Time (AUC=0.976) by examining patient-wise historical records with the attention weights learned through a Transformer model. We believe our method provides an innovative way for personalized remote health sensing in a timely and objective fashion which could benefit a broad range of similar applications.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1863711666,
        "newsscientist":0.2332546817,
        "technologyreview":0.3149114477,
        "venturebeat":0.3084201604,
        "wired":0.2655134837,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13700v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658801288000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11808v1",
        "predicted_newsworthiness":0.5615667596,
        "title":"ArmanEmo: A Persian Dataset for Text-based Emotion Detection",
        "summary":"With the recent proliferation of open textual data on social media platforms, Emotion Detection (ED) from Text has received more attention over the past years. It has many applications, especially for businesses and online service providers, where emotion detection techniques can help them make informed commercial decisions by analyzing customers\/users' feelings towards their products and services. In this study, we introduce ArmanEmo, a human-labeled emotion dataset of more than 7000 Persian sentences labeled for seven categories. The dataset has been collected from different resources, including Twitter, Instagram, and Digikala (an Iranian e-commerce company) comments. Labels are based on Ekman's six basic emotions (Anger, Fear, Happiness, Hatred, Sadness, Wonder) and another category (Other) to consider any other emotion not included in Ekman's model. Along with the dataset, we have provided several baseline models for emotion classification focusing on the state-of-the-art transformer-based language models. Our best model achieves a macro-averaged F1 score of 75.39 percent across our test dataset. Moreover, we also conduct transfer learning experiments to compare our proposed dataset's generalization against other Persian emotion datasets. Results of these experiments suggest that our dataset has superior generalizability among the existing Persian emotion datasets. ArmanEmo is publicly available for non-commercial use at https:\/\/github.com\/Arman-Rayan-Sharif\/arman-text-emotion.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1900751871,
        "newsscientist":0.1795067125,
        "technologyreview":0.2727123782,
        "venturebeat":0.2660559015,
        "wired":0.2351077865,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11808v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1658694923000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00974v1",
        "predicted_newsworthiness":0.5614381448,
        "title":"Information Gain Sampling for Active Learning in Medical Image Classification",
        "summary":"Large, annotated datasets are not widely available in medical image analysis due to the prohibitive time, costs, and challenges associated with labelling large datasets. Unlabelled datasets are easier to obtain, and in many contexts, it would be feasible for an expert to provide labels for a small subset of images. This work presents an information-theoretic active learning framework that guides the optimal selection of images from the unlabelled pool to be labeled based on maximizing the expected information gain (EIG) on an evaluation dataset. Experiments are performed on two different medical image classification datasets: multi-class diabetic retinopathy disease scale classification and multi-class skin lesion classification. Results indicate that by adapting EIG to account for class-imbalances, our proposed Adapted Expected Information Gain (AEIG) outperforms several popular baselines including the diversity based CoreSet and uncertainty based maximum entropy sampling. Specifically, AEIG achieves ~95% of overall performance with only 19% of the training data, while other active learning approaches require around 25%. We show that, by careful design choices, our model can be integrated into existing deep learning classifiers.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.107952673,
        "newsscientist":0.1456406275,
        "technologyreview":0.2409283456,
        "venturebeat":0.222159047,
        "wired":0.1453413888,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00974v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659371153000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12043v1",
        "predicted_newsworthiness":0.5610392527,
        "title":"Representational Ethical Model Calibration",
        "summary":"Equity is widely held to be fundamental to the ethics of healthcare. In the context of clinical decision-making, it rests on the comparative fidelity of the intelligence -- evidence-based or intuitive -- guiding the management of each individual patient. Though brought to recent attention by the individuating power of contemporary machine learning, such epistemic equity arises in the context of any decision guidance, whether traditional or innovative. Yet no general framework for its quantification, let alone assurance, currently exists. Here we formulate epistemic equity in terms of model fidelity evaluated over learnt multi-dimensional representations of identity crafted to maximise the captured diversity of the population, introducing a comprehensive framework for Representational Ethical Model Calibration. We demonstrate use of the framework on large-scale multimodal data from UK Biobank to derive diverse representations of the population, quantify model performance, and institute responsive remediation. We offer our approach as a principled solution to quantifying and assuring epistemic equity in healthcare, with applications across the research, clinical, and regulatory domains.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2600238927,
        "newsscientist":0.2749077776,
        "technologyreview":0.3701465754,
        "venturebeat":0.3102627374,
        "wired":0.2769257615,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12043v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cy"
        ],
        "published":1658745219000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01705v1",
        "predicted_newsworthiness":0.5609611996,
        "title":"Success of Uncertainty-Aware Deep Models Depends on Data Manifold Geometry",
        "summary":"For responsible decision making in safety-critical settings, machine learning models must effectively detect and process edge-case data. Although existing works show that predictive uncertainty is useful for these tasks, it is not evident from literature which uncertainty-aware models are best suited for a given dataset. Thus, we compare six uncertainty-aware deep learning models on a set of edge-case tasks: robustness to adversarial attacks as well as out-of-distribution and adversarial detection. We find that the geometry of the data sub-manifold is an important factor in determining the success of various models. Our finding suggests an interesting direction in the study of uncertainty-aware deep learning models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1413547711,
        "newsscientist":0.1873353083,
        "technologyreview":0.314081221,
        "venturebeat":0.2847836536,
        "wired":0.2300510398,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01705v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659468979000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00636v1",
        "predicted_newsworthiness":0.5604310907,
        "title":"Studying writer-suggestion interaction: A qualitative study to understand writer interaction with aligned\/misaligned next-phrase suggestion",
        "summary":"We present an exploratory qualitative study to understand how writers interact with next-phrase suggestions. While there has been some quantitative research on the effects of suggestion systems on writing, there has been little qualitative work to understand how writers interact with suggestion systems and how it affects their writing process - specifically for a non-native but English writer. We conducted a study where amateur writers were asked to write two movie reviews each, one without suggestions and one with. We found writers interact with next-phrase suggestions in various complex ways - writers are able to abstract multiple parts of the suggestions and incorporate them within their writing - even when they disagree with the suggestion as a whole. The suggestion system also had various effects on the writing processes - contributing to different aspects of the writing process in unique ways. We propose a model of writer-suggestion interaction for writing with GPT-2 for a movie review writing task, followed by ways in which the model can be used for future research, along with outlining opportunities for research and design.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.179824659,
        "newsscientist":0.1699551235,
        "technologyreview":0.2235123778,
        "venturebeat":0.207978288,
        "wired":0.2481070439,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00636v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ai"
        ],
        "published":1659336547000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.12944v1",
        "predicted_newsworthiness":0.5582300476,
        "title":"AMF: Adaptable Weighting Fusion with Multiple Fine-tuning for Image Classification",
        "summary":"Fine-tuning is widely applied in image classification tasks as a transfer learning approach. It re-uses the knowledge from a source task to learn and obtain a high performance in target tasks. Fine-tuning is able to alleviate the challenge of insufficient training data and expensive labelling of new data. However, standard fine-tuning has limited performance in complex data distributions. To address this issue, we propose the Adaptable Multi-tuning method, which adaptively determines each data sample's fine-tuning strategy. In this framework, multiple fine-tuning settings and one policy network are defined. The policy network in Adaptable Multi-tuning can dynamically adjust to an optimal weighting to feed different samples into models that are trained using different fine-tuning strategies. Our method outperforms the standard fine-tuning approach by 1.69%, 2.79% on the datasets FGVC-Aircraft, and Describable Texture, yielding comparable performance on the datasets Stanford Cars, CIFAR-10, and Fashion-MNIST.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0824486656,
        "newsscientist":0.1197744169,
        "technologyreview":0.2246593302,
        "venturebeat":0.1994738186,
        "wired":0.1430334486,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12944v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658847003000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13280v1",
        "predicted_newsworthiness":0.5574856891,
        "title":"On-Device CPU Scheduling for Sense-React Systems",
        "summary":"Sense-react systems (e.g. robotics and AR\/VR) have to take highly responsive real-time actions, driven by complex decisions involving a pipeline of sensing, perception, planning, and reaction tasks. These tasks must be scheduled on resource-constrained devices such that the performance goals and the requirements of the application are met. This is a difficult scheduling problem that requires handling multiple scheduling dimensions, and variations in resource usage and availability. In practice, system designers manually tune parameters for their specific hardware and application, which results in poor generalization and increases the development burden. In this work, we highlight the emerging need for scheduling CPU resources at runtime in sense-react systems. We study three canonical applications (face tracking, robot navigation, and VR) to first understand the key scheduling requirements for such systems. Armed with this understanding, we develop a scheduling framework, Catan, that dynamically schedules compute resources across different components of an app so as to meet the specified application requirements. Through experiments with a prototype implemented on a widely-used robotics framework (ROS) and an open-source AR\/VR platform, we show the impact of system scheduling on meeting the performance goals for the three applications, how Catan is able to achieve better application performance than hand-tuned configurations, and how it dynamically adapts to runtime variations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1297753982,
        "newsscientist":0.1734414875,
        "technologyreview":0.2764280567,
        "venturebeat":0.3258267532,
        "wired":0.2803182125,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13280v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658894736000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01815v1",
        "predicted_newsworthiness":0.557050806,
        "title":"Effidit: Your AI Writing Assistant",
        "summary":"In this technical report, we introduce Effidit (Efficient and Intelligent Editing), a digital writing assistant that facilitates users to write higher-quality text more efficiently by using artificial intelligence (AI) technologies. Previous writing assistants typically provide the function of error checking (to detect and correct spelling and grammatical errors) and limited text-rewriting functionality. With the emergence of large-scale neural language models, some systems support automatically completing a sentence or a paragraph. In Effidit, we significantly expand the capacities of a writing assistant by providing functions in five categories: text completion, error checking, text polishing, keywords to sentences (K2S), and cloud input methods (cloud IME). In the text completion category, Effidit supports generation-based sentence completion, retrieval-based sentence completion, and phrase completion. In contrast, many other writing assistants so far only provide one or two of the three functions. For text polishing, we have three functions: (context-aware) phrase polishing, sentence paraphrasing, and sentence expansion, whereas many other writing assistants often support one or two functions in this category. The main contents of this report include major modules of Effidit, methods for implementing these modules, and evaluation results of some key methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.138715517,
        "newsscientist":0.1901662097,
        "technologyreview":0.3214061399,
        "venturebeat":0.3281696706,
        "wired":0.2459892932,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01815v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659493485000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.01844v1",
        "predicted_newsworthiness":0.5559257271,
        "title":"Multiclass ASMA vs Targeted PGD Attack in Image Segmentation",
        "summary":"Deep learning networks have demonstrated high performance in a large variety of applications, such as image classification, speech recognition, and natural language processing. However, there exists a major vulnerability exploited by the use of adversarial attacks. An adversarial attack imputes images by altering the input image very slightly, making it nearly undetectable to the naked eye, but results in a very different classification by the network. This paper explores the projected gradient descent (PGD) attack and the Adaptive Mask Segmentation Attack (ASMA) on the image segmentation DeepLabV3 model using two types of architectures: MobileNetV3 and ResNet50, It was found that PGD was very consistent in changing the segmentation to be its target while the generalization of ASMA to a multiclass target was not as effective. The existence of such attack however puts all of image classification deep learning networks in danger of exploitation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1098043665,
        "newsscientist":0.1498093862,
        "technologyreview":0.2773074604,
        "venturebeat":0.2344209822,
        "wired":0.1788754192,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01844v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659503130000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14694v1",
        "predicted_newsworthiness":0.5554076578,
        "title":"Design Methodology for Deep Out-of-Distribution Detectors in Real-Time Cyber-Physical Systems",
        "summary":"When machine learning (ML) models are supplied with data outside their training distribution, they are more likely to make inaccurate predictions; in a cyber-physical system (CPS), this could lead to catastrophic system failure. To mitigate this risk, an out-of-distribution (OOD) detector can run in parallel with an ML model and flag inputs that could lead to undesirable outcomes. Although OOD detectors have been well studied in terms of accuracy, there has been less focus on deployment to resource constrained CPSs. In this study, a design methodology is proposed to tune deep OOD detectors to meet the accuracy and response time requirements of embedded applications. The methodology uses genetic algorithms to optimize the detector's preprocessing pipeline and selects a quantization method that balances robustness and response time. It also identifies several candidate task graphs under the Robot Operating System (ROS) for deployment of the selected design. The methodology is demonstrated on two variational autoencoder based OOD detectors from the literature on two embedded platforms. Insights into the trade-offs that occur during the design process are provided, and it is shown that this design methodology can lead to a drastic reduction in response time in relation to an unoptimized OOD detector while maintaining comparable accuracy.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1010304559,
        "newsscientist":0.1811899461,
        "technologyreview":0.30586691,
        "venturebeat":0.298061432,
        "wired":0.2405717138,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14694v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659103587000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02043v1",
        "predicted_newsworthiness":0.5552532291,
        "title":"SmartControllerJS: A JavaScript library to turn smartphones into controllers for web-based interactive experiments",
        "summary":"We introduce SmartControllerJS, a new JavaScript library for fast, cost-effective designing of web applications controlled via everyday smartphones. At its core, SmartControllerJS establishes a connection between two webpages, one page running on a desktop browser and the other on the user's smartphone. The smartphone webpage loads a controller interface allowing users to control a web application running on their computer's browser. The SmartControllerJS framework enables fast iteration loops when designing interactive user experiments because it has minimal friction and allows for scaling, while having no running costs. We first describe how this library is built, how it can be used, and provide interactive examples. We then present two games designed for public screens along with results from user studies evaluating acceptability and ease of use. Finally, we implement a custom controller based on user feedback and introduce connection monitoring tools. We believe SmartControllerJS can accelerate the design of interactive experiments for researchers in Human-Computer Interaction, and be a useful tool for educational projects. To experience the various demos, we recommend reading this work on a desktop computer with your smartphone in hand. The library and the demos are available at https:\/\/github.com\/SmartControllerJS",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1927552918,
        "newsscientist":0.2419836362,
        "technologyreview":0.3185509387,
        "venturebeat":0.3517254379,
        "wired":0.3485027441,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02043v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1659532302000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.01514v1",
        "predicted_newsworthiness":0.5549465232,
        "title":"MBSE analysis for energy sustainability improvement in manufacturing industry",
        "summary":"With the ever increasing complexity of Industry 4.0 systems, plant energy management systems developed to improve energy sustainability become equally complex. Based on a Model-Based Systems Engineering analysis, this paper aims to provide a general approach to perform holistic development of an autonomous energy management system for manufacturing industries. This Energy Management System (EMS) will be capable of continuously improving its ability to assess, predict, and act, in order to improve by monitoring and controlling the energy sustainability of manufacturing systems. The approach was implemented with the System Modeling Language (SysML).",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1437681662,
        "newsscientist":0.1462115032,
        "technologyreview":0.2108468098,
        "venturebeat":0.1875767174,
        "wired":0.1545666831,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01514v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659452413000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14024v2",
        "predicted_newsworthiness":0.5544726374,
        "title":"Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer",
        "summary":"Large-scale deployment of autonomous vehicles has been continually delayed due to safety concerns. On the one hand, comprehensive scene understanding is indispensable, a lack of which would result in vulnerability to rare but complex traffic situations, such as the sudden emergence of unknown objects. However, reasoning from a global context requires access to sensors of multiple types and adequate fusion of multi-modal sensor signals, which is difficult to achieve. On the other hand, the lack of interpretability in learning models also hampers the safety with unverifiable failure causes. In this paper, we propose a safety-enhanced autonomous driving framework, named Interpretable Sensor Fusion Transformer(InterFuser), to fully process and fuse information from multi-modal multi-view sensors for achieving comprehensive scene understanding and adversarial event detection. Besides, intermediate interpretable features are generated from our framework, which provide more semantics and are exploited to better constrain actions to be within the safe sets. We conducted extensive experiments on CARLA benchmarks, where our model outperforms prior methods, ranking the first on the public CARLA Leaderboard. Our code will be made available at https:\/\/github.com\/opendilab\/InterFuser",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1438069832,
        "newsscientist":0.1849308145,
        "technologyreview":0.3193662743,
        "venturebeat":0.298663878,
        "wired":0.2481376776,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14024v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg",
            "cs.ro"
        ],
        "published":1659008181000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14218v1",
        "predicted_newsworthiness":0.5540606806,
        "title":"Gender In Gender Out: A Closer Look at User Attributes in Context-Aware Recommendation",
        "summary":"This paper studies user attributes in light of current concerns in the recommender system community: diversity, coverage, calibration, and data minimization. In experiments with a conventional context-aware recommender system that leverages side information, we show that user attributes do not always improve recommendation. Then, we demonstrate that user attributes can negatively impact diversity and coverage. Finally, we investigate the amount of information about users that ``survives'' from the training data into the recommendation lists produced by the recommender. This information is a weak signal that could in the future be exploited for calibration or studied further as a privacy leak.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2120512009,
        "newsscientist":0.2019230937,
        "technologyreview":0.3266766265,
        "venturebeat":0.3212983457,
        "wired":0.2965747674,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14218v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659026270000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13165v2",
        "predicted_newsworthiness":0.553731019,
        "title":"YOLO and Mask R-CNN for Vehicle Number Plate Identification",
        "summary":"License plate scanners have grown in popularity in parking lots during the past few years. In order to quickly identify license plates, traditional plate recognition devices used in parking lots employ a fixed source of light and shooting angles. For skewed angles, such as license plate images taken with ultra-wide angle or fisheye lenses, deformation of the license plate recognition plate can also be quite severe, impairing the ability of standard license plate recognition systems to identify the plate. Mask RCNN gadget that may be utilised for oblique pictures and various shooting angles. The results of the experiments show that the suggested design will be capable of classifying license plates with bevel angles larger than 0\/60. Character recognition using the suggested Mask R-CNN approach has advanced significantly as well. The proposed Mask R-CNN method has also achieved significant progress in character recognition, which is tilted more than 45 degrees as compared to the strategy of employing the YOLOv2 model. Experiment results also suggest that the methodology presented in the open data plate collecting is better than other techniques (known as the AOLP dataset).",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1054255415,
        "newsscientist":0.146258994,
        "technologyreview":0.2376843545,
        "venturebeat":0.2112087758,
        "wired":0.1868320208,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13165v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658864519000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13381v2",
        "predicted_newsworthiness":0.5529711526,
        "title":"Look Closer to Your Enemy: Learning to Attack via Teacher-student Mimicking",
        "summary":"This paper aims to generate realistic attack samples of person re-identification, ReID, by reading the enemy's mind (VM). In this paper, we propose a novel inconspicuous and controllable ReID attack baseline, LCYE, to generate adversarial query images. Concretely, LCYE first distills VM's knowledge via teacher-student memory mimicking in the proxy task. Then this knowledge prior acts as an explicit cipher conveying what is essential and realistic, believed by VM, for accurate adversarial misleading. Besides, benefiting from the multiple opposing task framework of LCYE, we further investigate the interpretability and generalization of ReID models from the view of the adversarial attack, including cross-domain adaption, cross-model consensus, and online learning process. Extensive experiments on four ReID benchmarks show that our method outperforms other state-of-the-art attackers with a large margin in white-box, black-box, and target attacks. Our code is now available at https:\/\/gitfront.io\/r\/user-3704489\/mKXusqDT4ffr\/LCYE\/.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1690120927,
        "newsscientist":0.226616155,
        "technologyreview":0.3383552174,
        "venturebeat":0.2834496737,
        "wired":0.2558294303,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13381v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658912988000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12236v1",
        "predicted_newsworthiness":0.5522349943,
        "title":"Personality-Driven Social Multimedia Content Recommendation",
        "summary":"Social media marketing plays a vital role in promoting brand and product values to wide audiences. In order to boost their advertising revenues, global media buying platforms such as Facebook Ads constantly reduce the reach of branded organic posts, pushing brands to spend more on paid media ads. In order to run organic and paid social media marketing efficiently, it is necessary to understand the audience, tailoring the content to fit their interests and online behaviours, which is impossible to do manually at a large scale. At the same time, various personality type categorization schemes such as the Myers-Briggs Personality Type indicator make it possible to reveal the dependencies between personality traits and user content preferences on a wider scale by categorizing audience behaviours in a unified and structured manner. This problem is yet to be studied in depth by the research community, while the level of impact of different personality traits on content recommendation accuracy has not been widely utilised and comprehensively evaluated so far. Specifically, in this work we investigate the impact of human personality traits on the content recommendation model by applying a novel personality-driven multi-view content recommender system called Personality Content Marketing Recommender Engine, or PersiC. Our experimental results and real-world case study demonstrate not just PersiC's ability to perform efficient human personality-driven multi-view content recommendation, but also allow for actionable digital ad strategy recommendations, which when deployed are able to improve digital advertising efficiency by over 420% as compared to the original human-guided approach.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1742798991,
        "newsscientist":0.1853434925,
        "technologyreview":0.2748295136,
        "venturebeat":0.3038574237,
        "wired":0.2653671835,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12236v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.ai"
        ],
        "published":1658759838000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.13243v2",
        "predicted_newsworthiness":0.5518195396,
        "title":"Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks",
        "summary":"The last decade of machine learning has seen drastic increases in scale and capabilities, and deep neural networks (DNNs) are increasingly being deployed across a wide range of domains. However, the inner workings of DNNs are generally difficult to understand, raising concerns about the safety of using these systems without a rigorous understanding of how they function. In this survey, we review literature on techniques for interpreting the inner components of DNNs, which we call \"inner\" interpretability methods. Specifically, we review methods for interpreting weights, neurons, subnetworks, and latent representations with a focus on how these techniques relate to the goal of designing safer, more trustworthy AI systems. We also highlight connections between interpretability and work in modularity, adversarial robustness, continual learning, network compression, and studying the human visual system. Finally, we discuss key challenges and argue for future work in interpretability for AI safety that focuses on diagnostics, benchmarking, and robustness.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1556291384,
        "newsscientist":0.2238895659,
        "technologyreview":0.3773251493,
        "venturebeat":0.3309627973,
        "wired":0.2618272271,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13243v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cl",
            "cs.cv"
        ],
        "published":1658887153000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13970v1",
        "predicted_newsworthiness":0.5516949712,
        "title":"PHEMEPlus: Enriching Social Media Rumour Verification with External Evidence",
        "summary":"Work on social media rumour verification utilises signals from posts, their propagation and users involved. Other lines of work target identifying and fact-checking claims based on information from Wikipedia, or trustworthy news articles without considering social media context. However works combining the information from social media with external evidence from the wider web are lacking. To facilitate research in this direction, we release a novel dataset, PHEMEPlus, an extension of the PHEME benchmark, which contains social media conversations as well as relevant external evidence for each rumour. We demonstrate the effectiveness of incorporating such evidence in improving rumour verification models. Additionally, as part of the evidence collection, we evaluate various ways of query formulation to identify the most effective method.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1933028425,
        "newsscientist":0.1698409802,
        "technologyreview":0.2611404484,
        "venturebeat":0.228314755,
        "wired":0.2726625024,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13970v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai",
            "cs.cy",
            "cs.lg"
        ],
        "published":1659000065000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11511v1",
        "predicted_newsworthiness":0.5513101036,
        "title":"SSBNet: Improving Visual Recognition Efficiency by Adaptive Sampling",
        "summary":"Downsampling is widely adopted to achieve a good trade-off between accuracy and latency for visual recognition. Unfortunately, the commonly used pooling layers are not learned, and thus cannot preserve important information. As another dimension reduction method, adaptive sampling weights and processes regions that are relevant to the task, and is thus able to better preserve useful information. However, the use of adaptive sampling has been limited to certain layers. In this paper, we show that using adaptive sampling in the building blocks of a deep neural network can improve its efficiency. In particular, we propose SSBNet which is built by inserting sampling layers repeatedly into existing networks like ResNet. Experiment results show that the proposed SSBNet can achieve competitive image classification and object detection performance on ImageNet and COCO datasets. For example, the SSB-ResNet-RS-200 achieved 82.6% accuracy on ImageNet dataset, which is 0.6% higher than the baseline ResNet-RS-152 with a similar complexity. Visualization shows the advantage of SSBNet in allowing different layers to focus on different positions, and ablation studies further validate the advantage of adaptive sampling over uniform methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1018265043,
        "newsscientist":0.1487614024,
        "technologyreview":0.2630983089,
        "venturebeat":0.2367037228,
        "wired":0.1642822536,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11511v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1658581315000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01468v1",
        "predicted_newsworthiness":0.5508639309,
        "title":"Unravelling Interlanguage Facts via Explainable Machine Learning",
        "summary":"Native language identification (NLI) is the task of training (via supervised machine learning) a classifier that guesses the native language of the author of a text. This task has been extensively researched in the last decade, and the performance of NLI systems has steadily improved over the years. We focus on a different facet of the NLI task, i.e., that of analysing the internals of an NLI classifier trained by an \\emph{explainable} machine learning algorithm, in order to obtain explanations of its classification decisions, with the ultimate goal of gaining insight into which linguistic phenomena ``give a speaker's native language away''. We use this perspective in order to tackle both NLI and a (much less researched) companion task, i.e., guessing whether a text has been written by a native or a non-native speaker. Using three datasets of different provenance (two datasets of English learners' essays and a dataset of social media posts), we investigate which kind of linguistic traits (lexical, morphological, syntactic, and statistical) are most effective for solving our two tasks, namely, are most indicative of a speaker's L1. We also present two case studies, one on Spanish and one on Italian learners of English, in which we analyse individual linguistic traits that the classifiers have singled out as most important for spotting these L1s. Overall, our study shows that the use of explainable machine learning can be a valuable tool for th",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1535505662,
        "newsscientist":0.1750299723,
        "technologyreview":0.2789111194,
        "venturebeat":0.259855201,
        "wired":0.1930790047,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01468v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1659449115000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.12763v1",
        "predicted_newsworthiness":0.5505483161,
        "title":"Using Abstraction for Interpretable Robot Programs in Stochastic Domains",
        "summary":"A robot's actions are inherently stochastic, as its sensors are noisy and its actions do not always have the intended effects. For this reason, the agent language Golog has been extended to models with degrees of belief and stochastic actions. While this allows more precise robot models, the resulting programs are much harder to comprehend, because they need to deal with the noise, e.g., by looping until some desired state has been reached with certainty, and because the resulting action traces consist of a large number of actions cluttered with sensor noise. To alleviate these issues, we propose to use abstraction. We define a high-level and nonstochastic model of the robot and then map the high-level model into the lower-level stochastic model. The resulting programs are much easier to understand, often do not require belief operators or loops, and produce much shorter action traces.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1002437657,
        "newsscientist":0.1640508549,
        "technologyreview":0.2832305058,
        "venturebeat":0.2330448801,
        "wired":0.215071436,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12763v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1658826937000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2208.00870v1",
        "predicted_newsworthiness":0.5477766909,
        "title":"Suggestion Lists vs. Continuous Generation: Interaction Design for Writing with Generative Models on Mobile Devices Affect Text Length, Wording and Perceived Authorship",
        "summary":"Neural language models have the potential to support human writing. However, questions remain on their integration and influence on writing and output. To address this, we designed and compared two user interfaces for writing with AI on mobile devices, which manipulate levels of initiative and control: 1) Writing with continuously generated text, the AI adds text word-by-word and user steers. 2) Writing with suggestions, the AI suggests phrases and user selects from a list. In a supervised online study (N=18), participants used these prototypes and a baseline without AI. We collected touch interactions, ratings on inspiration and authorship, and interview data. With AI suggestions, people wrote less actively, yet felt they were the author. Continuously generated text reduced this perceived authorship, yet increased editing behavior. In both designs, AI increased text length and was perceived to influence wording. Our findings add new empirical evidence on the impact of UI design decisions on user experience and output with co-creative systems.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1851254056,
        "newsscientist":0.2203224163,
        "technologyreview":0.3118962705,
        "venturebeat":0.3038881473,
        "wired":0.3033404133,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00870v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ai"
        ],
        "published":1659362231000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.02031v1",
        "predicted_newsworthiness":0.5463075475,
        "title":"Cross-lingual Approaches for the Detection of Adverse Drug Reactions in German from a Patient's Perspective",
        "summary":"In this work, we present the first corpus for German Adverse Drug Reaction (ADR) detection in patient-generated content. The data consists of 4,169 binary annotated documents from a German patient forum, where users talk about health issues and get advice from medical doctors. As is common in social media data in this domain, the class labels of the corpus are very imbalanced. This and a high topic imbalance make it a very challenging dataset, since often, the same symptom can have several causes and is not always related to a medication intake. We aim to encourage further multi-lingual efforts in the domain of ADR detection and provide preliminary experiments for binary classification using different methods of zero- and few-shot learning based on a multi-lingual model. When fine-tuning XLM-RoBERTa first on English patient forum data and then on the new German data, we achieve an F1-score of 37.52 for the positive class. We make the dataset and models publicly available for the community.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1860993123,
        "newsscientist":0.2017674958,
        "technologyreview":0.2397600007,
        "venturebeat":0.2264281508,
        "wired":0.1920507213,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02031v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1659531121000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14535v1",
        "predicted_newsworthiness":0.5454988573,
        "title":"SERCNN: Stacked Embedding Recurrent Convolutional Neural Network in Detecting Depression on Twitter",
        "summary":"Conventional approaches to identify depression are not scalable, and the public has limited awareness of mental health, especially in developing countries. As evident by recent studies, social media has the potential to complement mental health screening on a greater scale. The vast amount of first-person narrative posts in chronological order can provide insights into one's thoughts, feelings, behavior, or mood for some time, enabling a better understanding of depression symptoms reflected in the online space. In this paper, we propose SERCNN, which improves the user representation by (1) stacking two pretrained embeddings from different domains and (2) reintroducing the embedding context to the MLP classifier. Our SERCNN shows great performance over state-of-the-art and other baselines, achieving 93.7% accuracy in a 5-fold cross-validation setting. Since not all users share the same level of online activity, we introduced the concept of a fixed observation window that quantifies the observation period in a predefined number of posts. With as minimal as 10 posts per user, SERCNN performed exceptionally well with an 87% accuracy, which is on par with the BERT model, while having 98% less in the number of parameters. Our findings open up a promising direction for detecting depression on social media with a smaller number of posts for inference, towards creating solutions for a cost-effective and timely intervention. We hope that our work can bring this research area closer to real-world adoption in existing clinical practice.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2034351166,
        "newsscientist":0.2181764012,
        "technologyreview":0.3096434655,
        "venturebeat":0.2950245641,
        "wired":0.2726951038,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14535v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.cl",
            "cs.si"
        ],
        "published":1659082095000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2208.02121v1",
        "predicted_newsworthiness":0.5453956245,
        "title":"Pedestrian-Robot Interactions on Autonomous Crowd Navigation: Reactive Control Methods and Evaluation Metrics",
        "summary":"Autonomous navigation in highly populated areas remains a challenging task for robots because of the difficulty in guaranteeing safe interactions with pedestrians in unstructured situations. In this work, we present a crowd navigation control framework that delivers continuous obstacle avoidance and post-contact control evaluated on an autonomous personal mobility vehicle. We propose evaluation metrics for accounting efficiency, controller response and crowd interactions in natural crowds. We report the results of over 110 trials in different crowd types: sparse, flows, and mixed traffic, with low- (< 0.15 ppsm), mid- (< 0.65 ppsm), and high- (< 1 ppsm) pedestrian densities. We present comparative results between two low-level obstacle avoidance methods and a baseline of shared control. Results show a 10% drop in relative time to goal on the highest density tests, and no other efficiency metric decrease. Moreover, autonomous navigation showed to be comparable to shared-control navigation with a lower relative jerk and significantly higher fluency in commands indicating high compatibility with the crowd. We conclude that the reactive controller fulfils a necessary task of fast and continuous adaptation to crowd navigation, and it should be coupled with high-level planners for environmental and situational awareness.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1412962903,
        "newsscientist":0.1663028745,
        "technologyreview":0.2556067526,
        "venturebeat":0.2226286988,
        "wired":0.2225192948,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02121v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv",
            "cs.hc"
        ],
        "published":1659538563000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01076v1",
        "predicted_newsworthiness":0.5430570256,
        "title":"Rethinking Quality of Experience for Metaverse Services: A Consumer-based Economics Perspective",
        "summary":"The Metaverse is considered to be one prototype of the next-generation Internet, which contains people's expectations for the future world. However, the academic discussion of the Metaverse still mainly focused on the system technical design, and few research studied Metaverse challenges from the perspective of consumers, i.e., Metaverse users. One difficulty is that the analysis from the consumer's perspective requires interdisciplinary theoretical framework and quantifiable Quality of Experience (QoE) measurements. In this article, pioneering from consumers' point of view, we explore an interaction between Metaverse system design and consumer behaviors. Specifically, we rethink the QoE and propose an interdisciplinary framework that encompasses both the Metaverse service providers (MSPs) and consumer considerations. From the macro perspective, we introduce a joint optimization scheme that simultaneously considers the Metaverse system design, consumers' utility, and profitability of the MSPs. From the micro perspective, we advocate the Willingness-to-Pay (WTP) as an easy-to-implement QoE measurement for future Metaverse system studies. To illustrate the usability of the proposed integrated framework, a use case of Metaverse, i.e., virtual traveling, is presented. We show that our framework can benefit the MSPs in offering competitive and economical service design to consumers while maximizing the profit.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2200468846,
        "newsscientist":0.2031914381,
        "technologyreview":0.3033589388,
        "venturebeat":0.3665177363,
        "wired":0.3130784166,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01076v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1659377884000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.14227v1",
        "predicted_newsworthiness":0.5427373011,
        "title":"Visual Recognition by Request",
        "summary":"In this paper, we present a novel protocol of annotation and evaluation for visual recognition. Different from traditional settings, the protocol does not require the labeler\/algorithm to annotate\/recognize all targets (objects, parts, etc.) at once, but instead raises a number of recognition instructions and the algorithm recognizes targets by request. This mechanism brings two beneficial properties to reduce the burden of annotation, namely, (i) variable granularity: different scenarios can have different levels of annotation, in particular, object parts can be labeled only in large and clear instances, (ii) being open-domain: new concepts can be added to the database in minimal costs. To deal with the proposed setting, we maintain a knowledge base and design a query-based visual recognition framework that constructs queries on-the-fly based on the requests. We evaluate the recognition system on two mixed-annotated datasets, CPP and ADE20K, and demonstrate its promising ability of learning from partially labeled data as well as adapting to new concepts with only text labels.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0980291645,
        "newsscientist":0.1619076057,
        "technologyreview":0.2473719295,
        "venturebeat":0.2423442756,
        "wired":0.199075672,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14227v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659027311000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01136v1",
        "predicted_newsworthiness":0.5412973884,
        "title":"Exploring the GLIDE model for Human Action-effect Prediction",
        "summary":"We address the following action-effect prediction task. Given an image depicting an initial state of the world and an action expressed in text, predict an image depicting the state of the world following the action. The prediction should have the same scene context as the input image. We explore the use of the recently proposed GLIDE model for performing this task. GLIDE is a generative neural network that can synthesize (inpaint) masked areas of an image, conditioned on a short piece of text. Our idea is to mask-out a region of the input image where the effect of the action is expected to occur. GLIDE is then used to inpaint the masked region conditioned on the required action. In this way, the resulting image has the same background context as the input image, updated to show the effect of the action. We give qualitative results from experiments using the EPIC dataset of ego-centric videos labelled with actions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1466068276,
        "newsscientist":0.1975967602,
        "technologyreview":0.2724058143,
        "venturebeat":0.2306405358,
        "wired":0.2111268195,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01136v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659387099000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14447v1",
        "predicted_newsworthiness":0.5412245033,
        "title":"Dataset and Evaluation algorithm design for GOALS Challenge",
        "summary":"Glaucoma causes irreversible vision loss due to damage to the optic nerve, and there is no cure for glaucoma.OCT imaging modality is an essential technique for assessing glaucomatous damage since it aids in quantifying fundus structures. To promote the research of AI technology in the field of OCT-assisted diagnosis of glaucoma, we held a Glaucoma OCT Analysis and Layer Segmentation (GOALS) Challenge in conjunction with the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) 2022 to provide data and corresponding annotations for researchers studying layer segmentation from OCT images and the classification of glaucoma. This paper describes the released 300 circumpapillary OCT images, the baselines of the two sub-tasks, and the evaluation methodology. The GOALS Challenge is accessible at https:\/\/aistudio.baidu.com\/aistudio\/competition\/detail\/230.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1097079794,
        "newsscientist":0.1795701142,
        "technologyreview":0.2726644694,
        "venturebeat":0.2607292412,
        "wired":0.192480254,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14447v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659063086000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11637v1",
        "predicted_newsworthiness":0.541067721,
        "title":"Explored An Effective Methodology for Fine-Grained Snake Recognition",
        "summary":"Fine-Grained Visual Classification (FGVC) is a longstanding and fundamental problem in computer vision and pattern recognition, and underpins a diverse set of real-world applications. This paper describes our contribution at SnakeCLEF2022 with FGVC. Firstly, we design a strong multimodal backbone to utilize various meta-information to assist in fine-grained identification. Secondly, we provide new loss functions to solve the long tail distribution with dataset. Then, in order to take full advantage of unlabeled datasets, we use self-supervised learning and supervised learning joint training to provide pre-trained model. Moreover, some effective data process tricks also are considered in our experiments. Last but not least, fine-tuned in downstream task with hard mining, ensambled kinds of model performance. Extensive experiments demonstrate that our method can effectively improve the performance of fine-grained recognition. Our method can achieve a macro f1 score 92.7% and 89.4% on private and public dataset, respectively, which is the 1st place among the participators on private leaderboard.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0981301814,
        "newsscientist":0.1654162022,
        "technologyreview":0.2332973755,
        "venturebeat":0.2028008334,
        "wired":0.1658966362,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11637v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658629155000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00296v1",
        "predicted_newsworthiness":0.5404862744,
        "title":"ANOVA-based Automatic Attribute Selection and a Predictive Model for Heart Disease Prognosis",
        "summary":"Studies show that Studies that cardiovascular diseases (CVDs) are malignant for human health. Thus, it is important to have an efficient way of CVD prognosis. In response to this, the healthcare industry has adopted machine learning-based smart solutions to alleviate the manual process of CVD prognosis. Thus, this work proposes an information fusion technique that combines key attributes of a person through analysis of variance (ANOVA) and domain experts' knowledge. It also introduces a new collection of CVD data samples for emerging research. There are thirty-eight experiments conducted exhaustively to verify the performance of the proposed framework on four publicly available benchmark datasets and the newly created dataset in this work. The ablation study shows that the proposed approach can achieve a competitive mean average accuracy (mAA) of 99.2% and a mean average AUC of 97.9%.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1406075277,
        "newsscientist":0.1905893776,
        "technologyreview":0.2945475532,
        "venturebeat":0.2913497674,
        "wired":0.1797019458,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00296v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659209358000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13505v1",
        "predicted_newsworthiness":0.5401588569,
        "title":"Multi-Forgery Detection Challenge 2022: Push the Frontier of Unconstrained and Diverse Forgery Detection",
        "summary":"In this paper, we present the Multi-Forgery Detection Challenge held concurrently with the IEEE Computer Society Workshop on Biometrics at CVPR 2022. Our Multi-Forgery Detection Challenge aims to detect automatic image manipulations including but not limited to image editing, image synthesis, image generation, image photoshop, etc. Our challenge has attracted 674 teams from all over the world, with about 2000 valid result submission counts. We invited the Top 10 teams to present their solutions to the challenge, from which three teams are awarded prizes in the grand finale. In this paper, we present the solutions from the Top 3 teams, in order to boost the research work in the field of image forgery detection.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1234560077,
        "newsscientist":0.1812070137,
        "technologyreview":0.2508988135,
        "venturebeat":0.215432558,
        "wired":0.1966902275,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13505v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658927754000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13958v2",
        "predicted_newsworthiness":0.5400142489,
        "title":"Learning Based High-Level Decision Making for Abortable Overtaking in Autonomous Vehicles",
        "summary":"Autonomous vehicles are a growing technology that aims to enhance safety, accessibility, efficiency, and convenience through autonomous maneuvers ranging from lane change to overtaking. Overtaking is one of the most challenging maneuvers for autonomous vehicles, and current techniques for autonomous overtaking are limited to simple situations. This paper studies how to increase safety in autonomous overtaking by allowing the maneuver to be aborted. We propose a decision-making process based on a deep Q-Network to determine if and when the overtaking maneuver needs to be aborted. The proposed algorithm is empirically evaluated in simulation with varying traffic situations, indicating that the proposed method improves safety during overtaking maneuvers. Furthermore, the approach is demonstrated in real-world experiments using the autonomous shuttle iseAuto.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1144505333,
        "newsscientist":0.1548599824,
        "technologyreview":0.2818338893,
        "venturebeat":0.2436271937,
        "wired":0.2201176196,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13958v2",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658997988000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13976v1",
        "predicted_newsworthiness":0.5395035028,
        "title":"Federated Learning for IoUT: Concepts, Applications, Challenges and Opportunities",
        "summary":"Internet of Underwater Things (IoUT) have gained rapid momentum over the past decade with applications spanning from environmental monitoring and exploration, defence applications, etc. The traditional IoUT systems use machine learning (ML) approaches which cater the needs of reliability, efficiency and timeliness. However, an extensive review of the various studies conducted highlight the significance of data privacy and security in IoUT frameworks as a predominant factor in achieving desired outcomes in mission critical applications. Federated learning (FL) is a secured, decentralized framework which is a recent development in machine learning, that will help in fulfilling the challenges faced by conventional ML approaches in IoUT. This paper presents an overview of the various applications of FL in IoUT, its challenges, open issues and indicates direction of future research prospects.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1475204939,
        "newsscientist":0.1964857483,
        "technologyreview":0.2857755767,
        "venturebeat":0.2937138378,
        "wired":0.2464283098,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13976v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659001225000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12186v1",
        "predicted_newsworthiness":0.5394643523,
        "title":"On Binding Objects to Symbols: Learning Physical Concepts to Understand Real from Fake",
        "summary":"We revisit the classic signal-to-symbol barrier in light of the remarkable ability of deep neural networks to generate realistic synthetic data. DeepFakes and spoofing highlight the feebleness of the link between physical reality and its abstract representation, whether learned by a digital computer or a biological agent. Starting from a widely applicable definition of abstract concept, we show that standard feed-forward architectures cannot capture but trivial concepts, regardless of the number of weights and the amount of training data, despite being extremely effective classifiers. On the other hand, architectures that incorporate recursion can represent a significantly larger class of concepts, but may still be unable to learn them from a finite dataset. We qualitatively describe the class of concepts that can be \"understood\" by modern architectures trained with variants of stochastic gradient descent, using a (free energy) Lagrangian to measure information complexity. Even if a concept has been understood, however, a network has no means of communicating its understanding to an external agent, except through continuous interaction and validation. We then characterize physical objects as abstract concepts and use the previous analysis to show that physical objects can be encoded by finite architectures. However, to understand physical concepts, sensors must provide persistently exciting observations, for which the ability to control the data acquisition process is essential (active perception). The importance of control depends on the modality, benefiting visual more than acoustic or chemical perception. Finally, we conclude that binding physical entities to digital identities is possible in finite time with finite resources, solving in principle the signal-to-symbol barrier problem, but we highlight the need for continuous validation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1619593998,
        "newsscientist":0.2702454629,
        "technologyreview":0.3858641898,
        "venturebeat":0.316388102,
        "wired":0.2742350954,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12186v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cv"
        ],
        "published":1658769719000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00752v1",
        "predicted_newsworthiness":0.538827056,
        "title":"Data Collection and Analysis of French Dialects",
        "summary":"This paper discusses creating and analysing a new dataset for data mining and text analytics research, contributing to a joint Leeds University research project for the Corpus of National Dialects. This report investigates machine learning classifiers to classify samples of French dialect text across various French-speaking countries. Following the steps of the CRISP-DM methodology, this report explores the data collection process, data quality issues and data conversion for text analysis. Finally, after applying suitable data mining techniques, the evaluation methods, best overall features and classifiers and conclusions are discussed.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1801236575,
        "newsscientist":0.1364431853,
        "technologyreview":0.2130411357,
        "venturebeat":0.2205770938,
        "wired":0.1642751537,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00752v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1659352877000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.13247v1",
        "predicted_newsworthiness":0.5387754336,
        "title":"Concurrent Subsidiary Supervision for Unsupervised Source-Free Domain Adaptation",
        "summary":"The prime challenge in unsupervised domain adaptation (DA) is to mitigate the domain shift between the source and target domains. Prior DA works show that pretext tasks could be used to mitigate this domain shift by learning domain invariant representations. However, in practice, we find that most existing pretext tasks are ineffective against other established techniques. Thus, we theoretically analyze how and when a subsidiary pretext task could be leveraged to assist the goal task of a given DA problem and develop objective subsidiary task suitability criteria. Based on this criteria, we devise a novel process of sticker intervention and cast sticker classification as a supervised subsidiary DA problem concurrent to the goal task unsupervised DA. Our approach not only improves goal task adaptation performance, but also facilitates privacy-oriented source-free DA i.e. without concurrent source-target access. Experiments on the standard Office-31, Office-Home, DomainNet, and VisDA benchmarks demonstrate our superiority for both single-source and multi-source source-free DA. Our approach also complements existing non-source-free works, achieving leading performance.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1293591102,
        "newsscientist":0.1708506606,
        "technologyreview":0.2843204679,
        "venturebeat":0.2810349672,
        "wired":0.2235664606,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13247v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658888709000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13055v1",
        "predicted_newsworthiness":0.5386935601,
        "title":"Contextualizing Online Conversational Networks",
        "summary":"Online social connections occur within a specific conversational context. Prior work in network analysis of social media data attempts to contextualize data through filtering. We propose a method of contextualizing online conversational connections automatically and illustrate this method with Twitter data. Specifically, we detail a graph neural network model capable of representing tweets in a vector space based on their text, hashtags, URLs, and neighboring tweets. Once tweets are represented, clusters of tweets uncover conversational contexts. We apply our method to a dataset with 4.5 million tweets discussing the 2020 US election. We find that even filtered data contains many different conversational contexts, with users engaging in multiple contexts. Central users in the contextualized networks differ significantly from central users in the overall network. This result implies that standard network analysis on social media data can be unreliable in the face of multiple conversational contexts. We further demonstrate that dynamic analysis of conversational contexts gives a qualitative understanding of conversational flow.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1968428309,
        "newsscientist":0.1736489635,
        "technologyreview":0.3063584347,
        "venturebeat":0.2887419156,
        "wired":0.3075319917,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13055v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1658856308000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.14766v1",
        "predicted_newsworthiness":0.5386401478,
        "title":"Deep Reinforcement Learning for End-to-End Network Slicing: Challenges and Solutions",
        "summary":"5G and beyond is expected to enable various emerging use cases with diverse performance requirements from vertical industries. To serve these use cases cost-effectively, network slicing plays a key role in dynamically creating virtual end-to-end networks according to specific resource demands. A network slice may have hundreds of configurable parameters over multiple technical domains that define the performance of the network slice, which makes it impossible to use traditional model-based solutions to orchestrate resources for network slices. In this article, we discuss how to design and deploy deep reinforcement learning (DRL), a model-free approach, to address the network slicing problem. First, we analyze the network slicing problem and present a standard-compliant system architecture that enables DRL-based solutions in 5G and beyond networks. Second, we provide an in-depth analysis of the challenges in designing and deploying DRL in network slicing systems. Third, we explore multiple promising techniques, i.e., safety and distributed DRL, and imitation learning, for automating end-to-end network slicing.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1028696905,
        "newsscientist":0.1383916223,
        "technologyreview":0.2550053984,
        "venturebeat":0.2730721708,
        "wired":0.2064681285,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14766v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1659111426000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.11802v1",
        "predicted_newsworthiness":0.5385221275,
        "title":"On The Convexity Of The Effective Reproduction Number",
        "summary":"In this study we analyze the evolution of the effective reproduction number, $R$, through a SIR spreading process in heterogeneous networks; Characterizing its decay process allows to analytically study the effects of countermeasures on the progress of the virus under heterogeneity, and to optimize their policies. A striking result of recent studies has shown that heterogeneity across nodes\/individuals (or, super-spreading) may have a drastic effect on the spreading process progression, which may cause a non-linear decrease of $R$ in the number of infected individuals. We account for heterogeneity and analyze the stochastic progression of the spreading process. We show that the decrease of $R$ is, in fact, convex in the number of infected individuals, where this convexity stems from heterogeneity. The analysis is based on establishing stochastic monotonic relations between the susceptible populations in varying times of the spread. We demonstrate that the convex behavior of the effective reproduction number affects the performance of countermeasures used to fight a spread of a virus. The results are applicable to the control of virus and malware spreading in computer networks as well. We examine numerically the sensitivity of the Herd Immunity Threshold (HIT) to the heterogeneity level and to the chosen policy.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2143427653,
        "newsscientist":0.2003012337,
        "technologyreview":0.2229904875,
        "venturebeat":0.1532087712,
        "wired":0.1533733143,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11802v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1658693721000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.13438v1",
        "predicted_newsworthiness":0.5381213206,
        "title":"A Contact-Safe Reinforcement Learning Framework for Contact-Rich Robot Manipulation",
        "summary":"Reinforcement learning shows great potential to solve complex contact-rich robot manipulation tasks. However, the safety of using RL in the real world is a crucial problem, since unexpected dangerous collisions might happen when the RL policy is imperfect during training or in unseen scenarios. In this paper, we propose a contact-safe reinforcement learning framework for contact-rich robot manipulation, which maintains safety in both the task space and joint space. When the RL policy causes unexpected collisions between the robot arm and the environment, our framework is able to immediately detect the collision and ensure the contact force to be small. Furthermore, the end-effector is enforced to perform contact-rich tasks compliantly, while keeping robust to external disturbances. We train the RL policy in simulation and transfer it to the real robot. Real world experiments on robot wiping tasks show that our method is able to keep the contact force small both in task space and joint space even when the policy is under unseen scenario with unexpected collision, while rejecting the disturbances on the main task.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1008672683,
        "newsscientist":0.1468253646,
        "technologyreview":0.2417686888,
        "venturebeat":0.1948346911,
        "wired":0.1635544726,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13438v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai"
        ],
        "published":1658918144000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01234v1",
        "predicted_newsworthiness":0.5379968783,
        "title":"Flood Prediction Using Machine Learning Models",
        "summary":"Floods are one of nature's most catastrophic calamities which cause irreversible and immense damage to human life, agriculture, infrastructure and socio-economic system. Several studies on flood catastrophe management and flood forecasting systems have been conducted. The accurate prediction of the onset and progression of floods in real time is challenging. To estimate water levels and velocities across a large area, it is necessary to combine data with computationally demanding flood propagation models. This paper aims to reduce the extreme risks of this natural disaster and also contributes to policy suggestions by providing a prediction for floods using different machine learning models. This research will use Binary Logistic Regression, K-Nearest Neighbor (KNN), Support Vector Classifier (SVC) and Decision tree Classifier to provide an accurate prediction. With the outcome, a comparative analysis will be conducted to understand which model delivers a better accuracy.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1770167738,
        "newsscientist":0.1911458339,
        "technologyreview":0.2513171392,
        "venturebeat":0.243594665,
        "wired":0.1807147224,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01234v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659412783000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01248v1",
        "predicted_newsworthiness":0.537416847,
        "title":"Can Gaze Beat Touch? A Fitts' Law Evaluation of Gaze, Touch, and Mouse Inputs",
        "summary":"Gaze input has been a promising substitute for mouse input for point and select interactions. Individuals with severe motor and speech disabilities primarily rely on gaze input for communication. Gaze input also serves as a hands-free input modality in the scenarios of situationally-induced impairments and disabilities (SIIDs). Hence, the performance of gaze input has often been compared to mouse input through standardized performance evaluation procedure like the Fitts' Law. With the proliferation of touch-enabled devices such as smartphones, tablet PCs, or any computing device with a touch surface, it is also important to compare the performance of gaze input to touch input. In this study, we conducted ISO 9241-9 Fitts' Law evaluation to compare the performance of multimodal gaze and foot-based input to touch input in a standard desktop environment, while using mouse input as the baseline. From a study involving 12 participants, we found that the gaze input has the lowest throughput (2.55 bits\/s), and the highest movement time (1.04 s) of the three inputs. In addition, though touch input involves maximum physical movements, it achieved the highest throughput (6.67 bits\/s), the least movement time (0.5 s), and was the most preferred input. While there are similarities in how quickly pointing can be moved from source to target location when using both gaze and touch inputs, target selection consumes maximum time with gaze input. Hence, with a throughput that is over 160% higher than gaze, touch proves to be a superior input modality.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1338331177,
        "newsscientist":0.1785945828,
        "technologyreview":0.2328431058,
        "venturebeat":0.2448855048,
        "wired":0.2393780942,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01248v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1659415531000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.14243v1",
        "predicted_newsworthiness":0.5370718057,
        "title":"Combining human parsing with analytical feature extraction and ranking schemes for high-generalization person reidentification",
        "summary":"Person reidentification (re-ID) has been receiving increasing attention in recent years due to its importance for both science and society. Machine learning and particularly Deep Learning (DL) has become the main re-id tool that allowed researches to achieve unprecedented accuracy levels on benchmark datasets. However, there is a known problem of poor generalization of DL models. That is, models trained to achieve high accuracy on one dataset perform poorly on other ones and require re-training. To address this issue, we present a model without trainable parameters which shows great potential for high generalization. It combines a fully analytical feature extraction and similarity ranking scheme with DL-based human parsing used to obtain the initial subregion classification. We show that such combination to a high extent eliminates the drawbacks of existing analytical methods. We use interpretable color and texture features which have human-readable similarity measures associated with them. To verify the proposed method we conduct experiments on Market1501 and CUHK03 datasets achieving competitive rank-1 accuracy comparable with that of DL-models. Most importantly we show that our method achieves 63.9% and 93.5% rank-1 cross-domain accuracy when applied to transfer learning tasks. It is significantly higher than previously reported 30-50% transfer accuracy. We discuss the potential ways of adding new features to further improve the model. We also show the advantage of interpretable features for constructing human-generated queries from verbal description to conduct search without a query image.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1227750329,
        "newsscientist":0.1712681456,
        "technologyreview":0.2805816726,
        "venturebeat":0.2551919336,
        "wired":0.180295788,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14243v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659028968000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01575v1",
        "predicted_newsworthiness":0.5368123445,
        "title":"ferret: a Framework for Benchmarking Explainers on Transformers",
        "summary":"Many interpretability tools allow practitioners and researchers to explain Natural Language Processing systems. However, each tool requires different configurations and provides explanations in different forms, hindering the possibility of assessing and comparing them. A principled, unified evaluation benchmark will guide the users through the central question: which explanation method is more reliable for my use case? We introduce ferret, an easy-to-use, extensible Python library to explain Transformer-based models integrated with the Hugging Face Hub. It offers a unified benchmarking suite to test and compare a wide range of state-of-the-art explainers on any text or interpretability corpora. In addition, ferret provides convenient programming abstractions to foster the introduction of new explanation methods, datasets, or evaluation metrics.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1524510236,
        "newsscientist":0.1878532044,
        "technologyreview":0.2980190962,
        "venturebeat":0.2991559387,
        "wired":0.2357686107,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01575v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659457302000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00005v1",
        "predicted_newsworthiness":0.5360182169,
        "title":"Testing Relational Understanding in Text-Guided Image Generation",
        "summary":"Relations are basic building blocks of human cognition. Classic and recent work suggests that many relations are early developing, and quickly perceived. Machine models that aspire to human-level perception and reasoning should reflect the ability to recognize and reason generatively about relations. We report a systematic empirical examination of a recent text-guided image generation model (DALL-E 2), using a set of 15 basic physical and social relations studied or proposed in the literature, and judgements from human participants (N = 169). Overall, we find that only ~22% of images matched basic relation prompts. Based on a quantitative examination of people's judgments, we suggest that current image generation models do not yet have a grasp of even basic relations involving simple objects and agents. We examine reasons for model successes and failures, and suggest possible improvements based on computations observed in biological intelligence.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1741496224,
        "newsscientist":0.2321016776,
        "technologyreview":0.3090374397,
        "venturebeat":0.2655688343,
        "wired":0.237774681,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00005v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1659063578000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12194v2",
        "predicted_newsworthiness":0.5352404855,
        "title":"Domain Decorrelation with Potential Energy Ranking",
        "summary":"Machine learning systems, especially the methods based on deep learning, enjoy great success in modern computer vision tasks under experimental settings. Generally, these classic deep learning methods are built on the \\emph{i.i.d.} assumption, supposing the training and test data are drawn from a similar distribution independently and identically. However, the aforementioned \\emph{i.i.d.} assumption is in general unavailable in the real-world scenario, and as a result, leads to sharp performance decay of deep learning algorithms. Behind this, domain shift is one of the primary factors to be blamed. In order to tackle this problem, we propose using \\textbf{Po}tential \\textbf{E}nergy \\textbf{R}anking (PoER) to decouple the object feature and the domain feature (\\emph{i.e.,} appearance feature) in given images, promoting the learning of label-discriminative features while filtering out the irrelevant correlations between the objects and the background. PoER helps the neural networks to capture label-related features which contain the domain information first in shallow layers and then distills the label-discriminative representations out progressively, enforcing the neural networks to be aware of the characteristic of objects and background which is vital to the generation of domain-invariant features. PoER reports superior performance on domain generalization benchmarks, improving the average top-1 accuracy by at least 1.20\\% compared to the existing methods. Moreover, we use PoER in the ECCV 2022 NICO Challenge\\footnote{https:\/\/nicochallenge.com}, achieving top place with only a vanilla ResNet-18. The code has been made available at https:\/\/github.com\/ForeverPs\/PoER.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1169123749,
        "newsscientist":0.1751541402,
        "technologyreview":0.2983387727,
        "venturebeat":0.2671398417,
        "wired":0.1945805747,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12194v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658756033000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00386v1",
        "predicted_newsworthiness":0.5349449483,
        "title":"Robotic Dough Shaping",
        "summary":"We address the problem of shaping a piece of dough-like deformable material into a 2D target shape presented upfront. We use a 6 degree-of-freedom WidowX-250 Robot Arm equipped with a rolling pin and information collected from an RGB-D camera and a tactile sensor. We present and compare several control policies, including a dough shrinking action, in extensive experiments across three kinds of deformable materials and across three target dough shape sizes, achieving the intersection over union (IoU) of 0.90. Our results show that: i) rolling dough from the highest dough point is more efficient than from the 2D\/3D dough centroid; ii) it might be better to stop the roll movement at the current dough boundary as opposed to the target shape outline; iii) the shrink action might be beneficial only if properly tuned with respect to the exapand action; and iv) the Play-Doh material is easier to shape to a target shape as compared to Plasticine or Kinetic sand. Video demonstrations of our work are available at https:\/\/youtu.be\/ZzLMxuITdt4",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1030381065,
        "newsscientist":0.1862799168,
        "technologyreview":0.230410908,
        "venturebeat":0.1727787556,
        "wired":0.1849097593,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00386v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai",
            "cs.cv"
        ],
        "published":1659253854000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.11717v1",
        "predicted_newsworthiness":0.5342347766,
        "title":"A Priority Map for Vision-and-Language Navigation with Trajectory Plans and Feature-Location Cues",
        "summary":"In a busy city street, a pedestrian surrounded by distractions can pick out a single sign if it is relevant to their route. Artificial agents in outdoor Vision-and-Language Navigation (VLN) are also confronted with detecting supervisory signal on environment features and location in inputs. To boost the prominence of relevant features in transformer-based architectures without costly preprocessing and pretraining, we take inspiration from priority maps - a mechanism described in neuropsychological studies. We implement a novel priority map module and pretrain on auxiliary tasks using low-sample datasets with high-level representations of routes and environment-related references to urban features. A hierarchical process of trajectory planning - with subsequent parameterised visual boost filtering on visual inputs and prediction of corresponding textual spans - addresses the core challenges of cross-modal alignment and feature-level localisation. The priority map module is integrated into a feature-location framework that doubles the task completion rates of standalone transformers and attains state-of-the-art performance on the Touchdown benchmark for VLN. Code and data are referenced in Appendix C.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1377103677,
        "newsscientist":0.1978486379,
        "technologyreview":0.3005913357,
        "venturebeat":0.2864182961,
        "wired":0.2549107893,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11717v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658660985000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00147v1",
        "predicted_newsworthiness":0.5340256213,
        "title":"Few-Shot Class-Incremental Learning from an Open-Set Perspective",
        "summary":"The continual appearance of new objects in the visual world poses considerable challenges for current deep learning methods in real-world deployments. The challenge of new task learning is often exacerbated by the scarcity of data for the new categories due to rarity or cost. Here we explore the important task of Few-Shot Class-Incremental Learning (FSCIL) and its extreme data scarcity condition of one-shot. An ideal FSCIL model needs to perform well on all classes, regardless of their presentation order or paucity of data. It also needs to be robust to open-set real-world conditions and be easily adapted to the new tasks that always arise in the field. In this paper, we first reevaluate the current task setting and propose a more comprehensive and practical setting for the FSCIL task. Then, inspired by the similarity of the goals for FSCIL and modern face recognition systems, we propose our method -- Augmented Angular Loss Incremental Classification or ALICE. In ALICE, instead of the commonly used cross-entropy loss, we propose to use the angular penalty loss to obtain well-clustered features. As the obtained features not only need to be compactly clustered but also diverse enough to maintain generalization for future incremental classes, we further discuss how class augmentation, data augmentation, and data balancing affect classification performance. Experiments on benchmark datasets, including CIFAR100, miniImageNet, and CUB200, demonstrate the improved performance of ALICE over the state-of-the-art FSCIL methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.109305852,
        "newsscientist":0.1557837973,
        "technologyreview":0.2715791232,
        "venturebeat":0.2529669867,
        "wired":0.1952311677,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00147v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659159768000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14221v1",
        "predicted_newsworthiness":0.5336230313,
        "title":"Humans disagree with the IoU for measuring object detector localization error",
        "summary":"The localization quality of automatic object detectors is typically evaluated by the Intersection over Union (IoU) score. In this work, we show that humans have a different view on localization quality. To evaluate this, we conduct a survey with more than 70 participants. Results show that for localization errors with the exact same IoU score, humans might not consider that these errors are equal, and express a preference. Our work is the first to evaluate IoU with humans and makes it clear that relying on IoU scores alone to evaluate localization errors might not be sufficient.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.157319511,
        "newsscientist":0.2199185728,
        "technologyreview":0.2817947087,
        "venturebeat":0.24101285,
        "wired":0.2353469243,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14221v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.hc"
        ],
        "published":1659026611000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14116v1",
        "predicted_newsworthiness":0.5331645609,
        "title":"Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction",
        "summary":"We present Claim-Dissector: a novel latent variable model for fact-checking and fact-analysis, which given a claim and a set of retrieved provenances allows learning jointly: (i) what are the relevant provenances to this claim (ii) what is the veracity of this claim. We propose to disentangle the per-provenance relevance probability and its contribution to the final veracity probability in an interpretable way - the final veracity probability is proportional to a linear ensemble of per-provenance relevance probabilities. This way, it can be clearly identified the relevance of which sources contributes to what extent towards the final probability. We show that our system achieves state-of-the-art results on FEVER dataset comparable to two-stage systems typically used in traditional fact-checking pipelines, while it often uses significantly less parameters and computation. Our analysis shows that proposed approach further allows to learn not just which provenances are relevant, but also which provenances lead to supporting and which toward denying the claim, without direct supervision. This not only adds interpretability, but also allows to detect claims with conflicting evidence automatically. Furthermore, we study whether our model can learn fine-grained relevance cues while using coarse-grained supervision. We show that our model can achieve competitive sentence-recall while using only paragraph-level relevance supervision. Finally, traversing towards the finest granularity of relevance, we show that our framework is capable of identifying relevance at the token-level. To do this, we present a new benchmark focusing on token-level interpretability - humans annotate tokens in relevant provenances they considered essential when making their judgement. Then we measure how similar are these annotations to tokens our model is focusing on. Our code, and dataset will be released online.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1599738958,
        "newsscientist":0.1585935808,
        "technologyreview":0.2464367741,
        "venturebeat":0.2247809776,
        "wired":0.2072964741,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14116v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1659018606000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11837v2",
        "predicted_newsworthiness":0.5330239503,
        "title":"Inter-model Interpretability: Self-supervised Models as a Case Study",
        "summary":"Since early machine learning models, metrics such as accuracy and precision have been the de facto way to evaluate and compare trained models. However, a single metric number doesn't fully capture the similarities and differences between models, especially in the computer vision domain. A model with high accuracy on a certain dataset might provide a lower accuracy on another dataset, without any further insights. To address this problem we build on a recent interpretability technique called Dissect to introduce \\textit{inter-model interpretability}, which determines how models relate or complement each other based on the visual concepts they have learned (such as objects and materials). Towards this goal, we project 13 top-performing self-supervised models into a Learned Concepts Embedding (LCE) space that reveals proximities among models from the perspective of learned concepts. We further crossed this information with the performance of these models on four computer vision tasks and 15 datasets. The experiment allowed us to categorize the models into three categories and revealed for the first time the type of visual concepts different tasks requires. This is a step forward for designing cross-task learning algorithms.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1138763181,
        "newsscientist":0.1795912049,
        "technologyreview":0.3058263536,
        "venturebeat":0.2720432272,
        "wired":0.1841155891,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11837v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658703018000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12086v1",
        "predicted_newsworthiness":0.5319657324,
        "title":"Efficient Classification with Counterfactual Reasoning and Active Learning",
        "summary":"Data augmentation is one of the most successful techniques to improve the classification accuracy of machine learning models in computer vision. However, applying data augmentation to tabular data is a challenging problem since it is hard to generate synthetic samples with labels. In this paper, we propose an efficient classifier with a novel data augmentation technique for tabular data. Our method called CCRAL combines causal reasoning to learn counterfactual samples for the original training samples and active learning to select useful counterfactual samples based on a region of uncertainty. By doing this, our method can maximize our model's generalization on the unseen testing data. We validate our method analytically, and compare with the standard baselines. Our experimental results highlight that CCRAL achieves significantly better performance than those of the baselines across several real-world tabular datasets in terms of accuracy and AUC. Data and source code are available at: https:\/\/github.com\/nphdang\/CCRAL.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.119543718,
        "newsscientist":0.1791865109,
        "technologyreview":0.3143217285,
        "venturebeat":0.2870647259,
        "wired":0.1904888025,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12086v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658750620000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11838v1",
        "predicted_newsworthiness":0.5313058745,
        "title":"SAVCHOI: Detecting Suspicious Activities using Dense Video Captioning with Human Object Interactions",
        "summary":"Detecting suspicious activities in surveillance videos has been a longstanding problem, which can further lead to difficulties in detecting crimes. The authors propose a novel approach for detecting and summarizing the suspicious activities going on in the surveillance videos. They also create ground truth summaries for the UCF-Crime video dataset. Further, the authors test existing state-of-the-art algorithms for Dense Video Captioning for a subset of this dataset and propose a model for this task by leveraging Human-Object Interaction models for the Visual features. They observe that this formulation for Dense Captioning achieves large gains over earlier approaches by a significant margin. The authors also perform an ablative analysis of the dataset and the model and report their findings.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1400864952,
        "newsscientist":0.1555815213,
        "technologyreview":0.2438026851,
        "venturebeat":0.2213726692,
        "wired":0.2077076802,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11838v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658703203000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13799v1",
        "predicted_newsworthiness":0.5305552126,
        "title":"Network polarization, filter bubbles, and echo chambers: An annotated review of measures, models, and case studies",
        "summary":"Polarization arises when the underlying network connecting the members of a society is formed by highly connected groups with weak intergroup connectivity. The increasing polarization, the strengthening of echo chambers, and the isolation caused by information filters in social networks are increasingly attracting the attention of researchers from different areas of knowledge such as computer science, economics, social and political sciences. Despite hundreds of publications in this area, there was little effort to systematize or present the knowledge developed in the field in an organized way. This study presents an annotated review of network polarization measures, models used to handle existing polarization, their applications, and case studies. Altogether, 405 scientific articles and conference papers were examined, with 74 filtered for this review. Several approaches for measuring polarization in graphs and networks were identified, including those based on homophily, modularity, random walks, and balance theory. The models used for reducing polarization included methods that propose edge or node editions (including edge insertions or deletions, and edge weight modifications), changes in social network design, or changes in the recommendation systems embedded in these networks. This review will be helpful to researchers investigating polarized social networks from a theoretical and applied perspective.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2469384286,
        "newsscientist":0.2012241634,
        "technologyreview":0.3157900296,
        "venturebeat":0.2684110941,
        "wired":0.2962557388,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13799v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.cy"
        ],
        "published":1658957007000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.11786v1",
        "predicted_newsworthiness":0.5303388267,
        "title":"Physics-Informed Learning of Aerosol Microphysics",
        "summary":"Aerosol particles play an important role in the climate system by absorbing and scattering radiation and influencing cloud properties. They are also one of the biggest sources of uncertainty for climate modeling. Many climate models do not include aerosols in sufficient detail due to computational constraints. In order to represent key processes, aerosol microphysical properties and processes have to be accounted for. This is done in the ECHAM-HAM global climate aerosol model using the M7 microphysics, but high computational costs make it very expensive to run with finer resolution or for a longer time. We aim to use machine learning to emulate the microphysics model at sufficient accuracy and reduce the computational cost by being fast at inference time. The original M7 model is used to generate data of input-output pairs to train a neural network on it. We are able to learn the variables' tendencies achieving an average $R^2$ score of $77.1\\% $. We further explore methods to inform and constrain the neural network with physical knowledge to reduce mass violation and enforce mass positivity. On a GPU we achieve a speed-up of up to over 64x compared to the original model.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1587723629,
        "newsscientist":0.2150090327,
        "technologyreview":0.2488033154,
        "venturebeat":0.2094920586,
        "wired":0.1854052834,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11786v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658687212000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13626v1",
        "predicted_newsworthiness":0.5294693827,
        "title":"Towards Mapping and Assessing Sidewalk Accessibility Across Sociocultural and Geographic Contexts",
        "summary":"Despite the important role of sidewalks in supporting mobility, accessibility, and public health, there is a lack of high-quality datasets and corresponding analyses on sidewalk existence and condition. Our work explores a twofold vision: first, to develop scalable mechanisms to locate and assess sidewalks in cities across the world, and second, to use this data to support new urban analyses and mobility tools. We report on two preliminary urban science explorations enabled by our approach: exploring geo-spatial patterns and key correlates of sidewalk accessibility and examining differences in sidewalk infrastructure across regions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2488858404,
        "newsscientist":0.2093069901,
        "technologyreview":0.2519050034,
        "venturebeat":0.201090755,
        "wired":0.2367150839,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13626v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.cy"
        ],
        "published":1658939881000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.01329v1",
        "predicted_newsworthiness":0.5289096092,
        "title":"Self-Supervised Traversability Prediction by Learning to Reconstruct Safe Terrain",
        "summary":"Navigating off-road with a fast autonomous vehicle depends on a robust perception system that differentiates traversable from non-traversable terrain. Typically, this depends on a semantic understanding which is based on supervised learning from images annotated by a human expert. This requires a significant investment in human time, assumes correct expert classification, and small details can lead to misclassification. To address these challenges, we propose a method for predicting high- and low-risk terrains from only past vehicle experience in a self-supervised fashion. First, we develop a tool that projects the vehicle trajectory into the front camera image. Second, occlusions in the 3D representation of the terrain are filtered out. Third, an autoencoder trained on masked vehicle trajectory regions identifies low- and high-risk terrains based on the reconstruction error. We evaluated our approach with two models and different bottleneck sizes with two different training and testing sites with a fourwheeled off-road vehicle. Comparison with two independent test sets of semantic labels from similar terrain as training sites demonstrates the ability to separate the ground as low-risk and the vegetation as high-risk with 81.1% and 85.1% accuracy.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.121458074,
        "newsscientist":0.1757337167,
        "technologyreview":0.2554926553,
        "venturebeat":0.2260229451,
        "wired":0.2093012442,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01329v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv"
        ],
        "published":1659432279000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01708v1",
        "predicted_newsworthiness":0.5267667371,
        "title":"Autonomous Agriculture Robot for Smart Farming",
        "summary":"This project aims to develop and demonstrate a ground robot with intelligence capable of conducting semi-autonomous farm operations for different low-heights vegetable crops referred as Agriculture Application Robot(AAR). AAR is a lightweight, solar-electric powered robot that uses intelligent perception for conducting detection and classification of plants and their characteristics. The system also has a robotic arm for the autonomous weed cutting process. The robot can deliver fertilizer spraying, insecticide, herbicide, and other fluids to the targets such as crops, weeds, and other pests. Besides, it provides information for future research into higher-level tasks such as yield estimation, crop, and soil health monitoring. We present the design of robot and the associated experiments which show the promising results in real world environments.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1348881287,
        "newsscientist":0.2153413928,
        "technologyreview":0.2640037751,
        "venturebeat":0.2075399384,
        "wired":0.2141137837,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01708v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai",
            "cs.cv"
        ],
        "published":1659469128000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.02019v1",
        "predicted_newsworthiness":0.5263815389,
        "title":"YOLO-FaceV2: A Scale and Occlusion Aware Face Detector",
        "summary":"In recent years, face detection algorithms based on deep learning have made great progress. These algorithms can be generally divided into two categories, i.e. two-stage detector like Faster R-CNN and one-stage detector like YOLO. Because of the better balance between accuracy and speed, one-stage detectors have been widely used in many applications. In this paper, we propose a real-time face detector based on the one-stage detector YOLOv5, named YOLO-FaceV2. We design a Receptive Field Enhancement module called RFE to enhance receptive field of small face, and use NWD Loss to make up for the sensitivity of IoU to the location deviation of tiny objects. For face occlusion, we present an attention module named SEAM and introduce Repulsion Loss to solve it. Moreover, we use a weight function Slide to solve the imbalance between easy and hard samples and use the information of the effective receptive field to design the anchor. The experimental results on WiderFace dataset show that our face detector outperforms YOLO and its variants can be find in all easy, medium and hard subsets. Source code in https:\/\/github.com\/Krasjet-Yu\/YOLO-FaceV2",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1024859234,
        "newsscientist":0.1590754814,
        "technologyreview":0.2593807631,
        "venturebeat":0.2355415642,
        "wired":0.1857638663,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02019v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659530400000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00598v1",
        "predicted_newsworthiness":0.5260902355,
        "title":"A Real-time Edge-AI System for Reef Surveys",
        "summary":"Crown-of-Thorn Starfish (COTS) outbreaks are a major cause of coral loss on the Great Barrier Reef (GBR) and substantial surveillance and control programs are ongoing to manage COTS populations to ecologically sustainable levels. In this paper, we present a comprehensive real-time machine learning-based underwater data collection and curation system on edge devices for COTS monitoring. In particular, we leverage the power of deep learning-based object detection techniques, and propose a resource-efficient COTS detector that performs detection inferences on the edge device to assist marine experts with COTS identification during the data collection phase. The preliminary results show that several strategies for improving computational efficiency (e.g., batch-wise processing, frame skipping, model input size) can be combined to run the proposed detection model on edge hardware with low resource consumption and low information loss.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1610885717,
        "newsscientist":0.2408517172,
        "technologyreview":0.30852214,
        "venturebeat":0.3021755512,
        "wired":0.2317684393,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00598v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659326774000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01534v1",
        "predicted_newsworthiness":0.5258483517,
        "title":"Towards Psychologically-Grounded Dynamic Preference Models",
        "summary":"Designing recommendation systems that serve content aligned with time varying preferences requires proper accounting of the feedback effects of recommendations on human behavior and psychological condition. We argue that modeling the influence of recommendations on people's preferences must be grounded in psychologically plausible models. We contribute a methodology for developing grounded dynamic preference models. We demonstrate this method with models that capture three classic effects from the psychology literature: Mere-Exposure, Operant Conditioning, and Hedonic Adaptation. We conduct simulation-based studies to show that the psychological models manifest distinct behaviors that can inform system design. Our study has two direct implications for dynamic user modeling in recommendation systems. First, the methodology we outline is broadly applicable for psychologically grounding dynamic preference models. It allows us to critique recent contributions based on their limited discussion of psychological foundation and their implausible predictions. Second, we discuss implications of dynamic preference models for recommendation systems evaluation and design. In an example, we show that engagement and diversity metrics may be unable to capture desirable recommendation system performance.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1962011325,
        "newsscientist":0.1934225644,
        "technologyreview":0.2498364857,
        "venturebeat":0.2671843924,
        "wired":0.2496870627,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01534v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.ai",
            "cs.hc"
        ],
        "published":1659372838000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.13859v1",
        "predicted_newsworthiness":0.5256825629,
        "title":"Caching Scalable Videos in the Edge of Wireless Cellular Networks",
        "summary":"By pre-fetching popular videos into the local caches of edge nodes, wireless edge caching provides an effective means of reducing repeated content deliveries. To meet the various viewing quality requirements of multimedia users, scalable video coding (SVC) is integrated with edge caching, where the constituent layers of scalable videos are flexibly cached and transmitted to users. In this article, we discuss the challenges arising from the different content popularity and various viewing requirements of scalable videos, and present the diverse types of cached contents as well as the corresponding transmission schemes. We provide an overview of the existing caching schemes, and summarize the criteria of making caching decisions. A case study is then presented, where the transmission delay is quantified and used as the performance metric. Simulation results confirm that giving cognizance to the realistic requirements of end users is capable of significantly reducing the content transmission delay, compared to the existing caching schemes operating without SVC. The results also verify that the transmission delay of the proposed random caching scheme is lower than that of the caching scheme which only provides local caching gain.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1023475285,
        "newsscientist":0.1128821739,
        "technologyreview":0.1651791497,
        "venturebeat":0.2188429505,
        "wired":0.1955896001,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13859v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658974907000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.14380v1",
        "predicted_newsworthiness":0.5254774091,
        "title":"Webcam Eye Tracking: Study Conduction and Acceptance of Remote Tests with Gaze Analysis",
        "summary":"Webcam eye tracking for the collection of gaze data in the context of user studies is convenient - it can be used in remote tests where participants do not need special hardware. The approach has strong limitations, especially regarding the motion-free nature of the test persons during data recording and the quality of the gaze data obtained. Our study with 52 participants shows that usable eye tracking data can be obtained with commercially available webcams in a remote setting. However, a high drop off rate must be considered, which is why we recommend a high over-recruitment of 150%. We also show that the acceptance of the approach by the study participants is high despite the given limitations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1658846231,
        "newsscientist":0.2123433084,
        "technologyreview":0.2570652559,
        "venturebeat":0.2547240504,
        "wired":0.2307161677,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14380v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1659042522000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.13016v1",
        "predicted_newsworthiness":0.5253751419,
        "title":"Modeling the Social Influence of COVID-19 via Personalized Propagation with Deep Learning",
        "summary":"Social influence prediction has permeated many domains, including marketing, behavior prediction, recommendation systems, and more. However, traditional methods of predicting social influence not only require domain expertise,they also rely on extracting user features, which can be very tedious. Additionally, graph convolutional networks (GCNs), which deals with graph data in non-Euclidean space, are not directly applicable to Euclidean space. To overcome these problems, we extended DeepInf such that it can predict the social influence of COVID-19 via the transition probability of the page rank domain. Furthermore, our implementation gives rise to a deep learning-based personalized propagation algorithm, called DeepPP. The resulting algorithm combines the personalized propagation of a neural prediction model with the approximate personalized propagation of a neural prediction model from page rank analysis. Four social networks from different domains as well as two COVID-19 datasets were used to demonstrate the efficiency and effectiveness of the proposed algorithm. Compared to other baseline methods, DeepPP provides more accurate social influence predictions. Further, experiments demonstrate that DeepPP can be applied to real-world prediction data for COVID-19.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.263046666,
        "newsscientist":0.2670095642,
        "technologyreview":0.3578813393,
        "venturebeat":0.3302020208,
        "wired":0.295511151,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13016v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.lg"
        ],
        "published":1658852657000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.13857v1",
        "predicted_newsworthiness":0.5247746398,
        "title":"Measuring Difficulty of Novelty Reaction",
        "summary":"Current AI systems are designed to solve close-world problems with the assumption that the underlying world is remaining more or less the same. However, when dealing with real-world problems such assumptions can be invalid as sudden and unexpected changes can occur. To effectively deploy AI-powered systems in the real world, AI systems should be able to deal with open-world novelty quickly. Inevitably, dealing with open-world novelty raises an important question of novelty difficulty. Knowing whether one novelty is harder to deal with than another, can help researchers to train their systems systematically. In addition, it can also serve as a measurement of the performance of novelty robust AI systems. In this paper, we propose to define the novelty reaction difficulty as a relative difficulty of performing the known task after the introduction of the novelty. We propose a universal method that can be applied to approximate the difficulty. We present the approximations of the difficulty using our method and show how it aligns with the results of the evaluation of AI agents designed to deal with novelty.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1579555983,
        "newsscientist":0.2286103937,
        "technologyreview":0.3733674759,
        "venturebeat":0.331959113,
        "wired":0.2672934612,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13857v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1658974567000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.14205v1",
        "predicted_newsworthiness":0.5247590374,
        "title":"DoRO: Disambiguation of referred object for embodied agents",
        "summary":"Robotic task instructions often involve a referred object that the robot must locate (ground) within the environment. While task intent understanding is an essential part of natural language understanding, less effort is made to resolve ambiguity that may arise while grounding the task. Existing works use vision-based task grounding and ambiguity detection, suitable for a fixed view and a static robot. However, the problem magnifies for a mobile robot, where the ideal view is not known beforehand. Moreover, a single view may not be sufficient to locate all the object instances in the given area, which leads to inaccurate ambiguity detection. Human intervention is helpful only if the robot can convey the kind of ambiguity it is facing. In this article, we present DoRO (Disambiguation of Referred Object), a system that can help an embodied agent to disambiguate the referred object by raising a suitable query whenever required. Given an area where the intended object is, DoRO finds all the instances of the object by aggregating observations from multiple views while exploring & scanning the area. It then raises a suitable query using the information from the grounded object instances. Experiments conducted with the AI2Thor simulator show that DoRO not only detects the ambiguity more accurately but also raises verbose queries with more accurate information from the visual-language grounding.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1103279189,
        "newsscientist":0.1803137908,
        "technologyreview":0.2871290846,
        "venturebeat":0.2657341265,
        "wired":0.2240736056,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14205v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai"
        ],
        "published":1659025279000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.14192v1",
        "predicted_newsworthiness":0.5243896573,
        "title":"Mining Cross-Person Cues for Body-Part Interactiveness Learning in HOI Detection",
        "summary":"Human-Object Interaction (HOI) detection plays a crucial role in activity understanding. Though significant progress has been made, interactiveness learning remains a challenging problem in HOI detection: existing methods usually generate redundant negative H-O pair proposals and fail to effectively extract interactive pairs. Though interactiveness has been studied in both whole body- and part- level and facilitates the H-O pairing, previous works only focus on the target person once (i.e., in a local perspective) and overlook the information of the other persons. In this paper, we argue that comparing body-parts of multi-person simultaneously can afford us more useful and supplementary interactiveness cues. That said, to learn body-part interactiveness from a global perspective: when classifying a target person's body-part interactiveness, visual cues are explored not only from herself\/himself but also from other persons in the image. We construct body-part saliency maps based on self-attention to mine cross-person informative cues and learn the holistic relationships between all the body-parts. We evaluate the proposed method on widely-used benchmarks HICO-DET and V-COCO. With our new perspective, the holistic global-local body-part interactiveness learning achieves significant improvements over state-of-the-art. Our code is available at https:\/\/github.com\/enlighten0707\/Body-Part-Map-for-Interactiveness.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1061386389,
        "newsscientist":0.1472196238,
        "technologyreview":0.1940695444,
        "venturebeat":0.1803050436,
        "wired":0.1603501189,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14192v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659023871000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00031v1",
        "predicted_newsworthiness":0.5241805018,
        "title":"Paddy Leaf diseases identification on Infrared Images based on Convolutional Neural Networks",
        "summary":"Agriculture is the mainstay of human society because it is an essential need for every organism. Paddy cultivation is very significant so far as humans are concerned, largely in the Asian continent, and it is one of the staple foods. However, plant diseases in agriculture lead to depletion in productivity. Plant diseases are generally caused by pests, insects, and pathogens that decrease productivity to a large scale if not controlled within a particular time. Eventually, one cannot see an increase in paddy yield. Accurate and timely identification of plant diseases can help farmers mitigate losses due to pests and diseases. Recently, deep learning techniques have been used to identify paddy diseases and overcome these problems. This paper implements a convolutional neural network (CNN) based on a model and tests a public dataset consisting of 636 infrared image samples with five paddy disease classes and one healthy class. The proposed model proficiently identified and classified paddy diseases of five different types and achieved an accuracy of 88.28%",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1567038161,
        "newsscientist":0.1929568435,
        "technologyreview":0.2602771354,
        "venturebeat":0.2218560244,
        "wired":0.1533679934,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00031v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659119069000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13417v1",
        "predicted_newsworthiness":0.5233870059,
        "title":"Hardly Perceptible Trojan Attack against Neural Networks with Bit Flips",
        "summary":"The security of deep neural networks (DNNs) has attracted increasing attention due to their widespread use in various applications. Recently, the deployed DNNs have been demonstrated to be vulnerable to Trojan attacks, which manipulate model parameters with bit flips to inject a hidden behavior and activate it by a specific trigger pattern. However, all existing Trojan attacks adopt noticeable patch-based triggers (e.g., a square pattern), making them perceptible to humans and easy to be spotted by machines. In this paper, we present a novel attack, namely hardly perceptible Trojan attack (HPT). HPT crafts hardly perceptible Trojan images by utilizing the additive noise and per pixel flow field to tweak the pixel values and positions of the original images, respectively. To achieve superior attack performance, we propose to jointly optimize bit flips, additive noise, and flow field. Since the weight bits of the DNNs are binary, this problem is very hard to be solved. We handle the binary constraint with equivalent replacement and provide an effective optimization algorithm. Extensive experiments on CIFAR-10, SVHN, and ImageNet datasets show that the proposed HPT can generate hardly perceptible Trojan images, while achieving comparable or better attack performance compared to the state-of-the-art methods. The code is available at: https:\/\/github.com\/jiawangbai\/HPT.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1107124705,
        "newsscientist":0.1896669229,
        "technologyreview":0.2960693873,
        "venturebeat":0.2417754277,
        "wired":0.2348214278,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13417v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658915777000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13441v1",
        "predicted_newsworthiness":0.521592461,
        "title":"Time Series Forecasting Models Copy the Past: How to Mitigate",
        "summary":"Time series forecasting is at the core of important application domains posing significant challenges to machine learning algorithms. Recently neural network architectures have been widely applied to the problem of time series forecasting. Most of these models are trained by minimizing a loss function that measures predictions' deviation from the real values. Typical loss functions include mean squared error (MSE) and mean absolute error (MAE). In the presence of noise and uncertainty, neural network models tend to replicate the last observed value of the time series, thus limiting their applicability to real-world data. In this paper, we provide a formal definition of the above problem and we also give some examples of forecasts where the problem is observed. We also propose a regularization term penalizing the replication of previously seen values. We evaluate the proposed regularization term both on synthetic and real-world datasets. Our results indicate that the regularization term mitigates to some extent the aforementioned problem and gives rise to more robust models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1531580303,
        "newsscientist":0.1805913938,
        "technologyreview":0.2773448023,
        "venturebeat":0.2552703595,
        "wired":0.2012764878,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13441v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658918340000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11553v1",
        "predicted_newsworthiness":0.5214799246,
        "title":"High-Resolution Swin Transformer for Automatic Medical Image Segmentation",
        "summary":"The Resolution of feature maps is critical for medical image segmentation. Most of the existing Transformer-based networks for medical image segmentation are U-Net-like architecture that contains an encoder that utilizes a sequence of Transformer blocks to convert the input medical image from high-resolution representation into low-resolution feature maps and a decoder that gradually recovers the high-resolution representation from low-resolution feature maps. Unlike previous studies, in this paper, we utilize the network design style from the High-Resolution Network (HRNet), replace the convolutional layers with Transformer blocks, and continuously exchange information from the different resolution feature maps that are generated by Transformer blocks. The newly Transformer-based network presented in this paper is denoted as High-Resolution Swin Transformer Network (HRSTNet). Extensive experiments illustrate that HRSTNet can achieve comparable performance with the state-of-the-art Transformer-based U-Net-like architecture on Brain Tumor Segmentation(BraTS) 2021 and the liver dataset from Medical Segmentation Decathlon. The code of HRSTNet will be publicly available at https:\/\/github.com\/auroua\/HRSTNet.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0904488024,
        "newsscientist":0.1514538614,
        "technologyreview":0.22908144,
        "venturebeat":0.1984008393,
        "wired":0.1519388839,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11553v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658595337000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13267v1",
        "predicted_newsworthiness":0.520431287,
        "title":"Fault Detection and Classification of Aerospace Sensors using a VGG16-based Deep Neural Network",
        "summary":"Compared with traditional model-based fault detection and classification (FDC) methods, deep neural networks (DNN) prove to be effective for the aerospace sensors FDC problems. However, time being consumed in training the DNN is excessive, and explainability analysis for the FDC neural network is still underwhelming. A concept known as imagefication-based intelligent FDC has been studied in recent years. This concept advocates to stack the sensors measurement data into an image format, the sensors FDC issue is then transformed to abnormal regions detection problem on the stacked image, which may well borrow the recent advances in the machine vision vision realm. Although promising results have been claimed in the imagefication-based intelligent FDC researches, due to the low size of the stacked image, small convolutional kernels and shallow DNN layers were used, which hinders the FDC performance. In this paper, we first propose a data augmentation method which inflates the stacked image to a larger size (correspondent to the VGG16 net developed in the machine vision realm). The FDC neural network is then trained via fine-tuning the VGG16 directly. To truncate and compress the FDC net size (hence its running time), we perform model pruning on the fine-tuned net. Class activation mapping (CAM) method is also adopted for explainability analysis of the FDC net to verify its internal operations. Via data augmentation, fine-tuning from VGG16, and model pruning, the FDC net developed in this paper claims an FDC accuracy 98.90% across 4 aircraft at 5 flight conditions (running time 26 ms). The CAM results also verify the FDC net w.r.t. its internal operations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0853383347,
        "newsscientist":0.1605495832,
        "technologyreview":0.2650605805,
        "venturebeat":0.2412130901,
        "wired":0.177369817,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13267v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658891657000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01127v1",
        "predicted_newsworthiness":0.5202827041,
        "title":"Disparate Censorship & Undertesting: A Source of Label Bias in Clinical Machine Learning",
        "summary":"As machine learning (ML) models gain traction in clinical applications, understanding the impact of clinician and societal biases on ML models is increasingly important. While biases can arise in the labels used for model training, the many sources from which these biases arise are not yet well-studied. In this paper, we highlight disparate censorship (i.e., differences in testing rates across patient groups) as a source of label bias that clinical ML models may amplify, potentially causing harm. Many patient risk-stratification models are trained using the results of clinician-ordered diagnostic and laboratory tests of labels. Patients without test results are often assigned a negative label, which assumes that untested patients do not experience the outcome. Since orders are affected by clinical and resource considerations, testing may not be uniform in patient populations, giving rise to disparate censorship. Disparate censorship in patients of equivalent risk leads to undertesting in certain groups, and in turn, more biased labels for such groups. Using such biased labels in standard ML pipelines could contribute to gaps in model performance across patient groups. Here, we theoretically and empirically characterize conditions in which disparate censorship or undertesting affect model performance across subgroups. Our findings call attention to disparate censorship as a source of label bias in clinical ML models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2392597572,
        "newsscientist":0.2375458411,
        "technologyreview":0.3286281548,
        "venturebeat":0.2766207529,
        "wired":0.2470393215,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01127v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cy"
        ],
        "published":1659384931000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13991v1",
        "predicted_newsworthiness":0.5202451828,
        "title":"CoNet: Borderless and decentralized server cooperation in edge computing",
        "summary":"In edge computing (EC), by offloading tasks to edge server or remote cloud, the system performance can be improved greatly. However, since the traffic distribution in EC is heterogeneous and dynamic, it is difficult for an individual edge server to provide satisfactory computation service anytime and anywhere. This issue motivated the researchers to study the cooperation between edge servers. The previous server cooperation algorithms have disadvantages since the cooperated region is limited within one-hop. However, the performance of EC can be improved further by releasing the restriction of cooperation region. Even some works have extended the cooperated region to multi-hops, they fail to support the task offloading which is one of the core issues of edge computing. Therefore, we propose a new decentralized and borderless server cooperation algorithm for edge computing which takes task offloading strategy into account, named CoNet. In CoNet, the cooperation region is not limited. Each server forms its own basic cooperation unit (BCU) and calculates its announced capability based on BCU. The server's capability, the processing delay, the task and calculation result forwarding delay are considered during the calculation. The task division strategy bases on the real capability of host-server and the announced capability of cooperation-servers. This cooperation process is recursive and will be terminated once the terminal condition is satisfied. The simulation results demonstrate the advantages of CoNet over previous works.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0920490446,
        "newsscientist":0.1034299005,
        "technologyreview":0.1711513207,
        "venturebeat":0.2191993526,
        "wired":0.142072419,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13991v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1659003257000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.13050v1",
        "predicted_newsworthiness":0.5202071382,
        "title":"Efficient High-Resolution Deep Learning: A Survey",
        "summary":"Cameras in modern devices such as smartphones, satellites and medical equipment are capable of capturing very high resolution images and videos. Such high-resolution data often need to be processed by deep learning models for cancer detection, automated road navigation, weather prediction, surveillance, optimizing agricultural processes and many other applications. Using high-resolution images and videos as direct inputs for deep learning models creates many challenges due to their high number of parameters, computation cost, inference latency and GPU memory consumption. Simple approaches such as resizing the images to a lower resolution are common in the literature, however, they typically significantly decrease accuracy. Several works in the literature propose better alternatives in order to deal with the challenges of high-resolution data and improve accuracy and speed while complying with hardware limitations and time restrictions. This survey describes such efficient high-resolution deep learning methods, summarizes real-world applications of high-resolution deep learning, and provides comprehensive information about available high-resolution datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1185437183,
        "newsscientist":0.1640515165,
        "technologyreview":0.2649927491,
        "venturebeat":0.2459544078,
        "wired":0.198477145,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13050v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658855633000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01862v1",
        "predicted_newsworthiness":0.5198119342,
        "title":"MixNet: Structured Deep Neural Motion Prediction for Autonomous Racing",
        "summary":"Reliably predicting the motion of contestant vehicles surrounding an autonomous racecar is crucial for effective and performant planning. Although highly expressive, deep neural networks are black-box models, making their usage challenging in safety-critical applications, such as autonomous driving. In this paper, we introduce a structured way of forecasting the movement of opposing racecars with deep neural networks. The resulting set of possible output trajectories is constrained. Hence quality guarantees about the prediction can be given. We report the performance of the model by evaluating it together with an LSTM-based encoder-decoder architecture on data acquired from high-fidelity Hardware-in-the-Loop simulations. The proposed approach outperforms the baseline regarding the prediction accuracy but still fulfills the quality guarantees. Thus, a robust real-world application of the model is proven. The presented model was deployed on the racecar of the Technical University of Munich for the Indy Autonomous Challenge 2021. The code used in this research is available as open-source software at www.github.com\/TUMFTM\/MixNet.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1117210774,
        "newsscientist":0.1570461865,
        "technologyreview":0.2788813974,
        "venturebeat":0.2663964906,
        "wired":0.2344733161,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01862v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659507368000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13254v1",
        "predicted_newsworthiness":0.5194647781,
        "title":"Contextual Information and Commonsense Based Prompt for Emotion Recognition in Conversation",
        "summary":"Emotion recognition in conversation (ERC) aims to detect the emotion for each utterance in a given conversation. The newly proposed ERC models have leveraged pre-trained language models (PLMs) with the paradigm of pre-training and fine-tuning to obtain good performance. However, these models seldom exploit PLMs' advantages thoroughly, and perform poorly for the conversations lacking explicit emotional expressions. In order to fully leverage the latent knowledge related to the emotional expressions in utterances, we propose a novel ERC model CISPER with the new paradigm of prompt and language model (LM) tuning. Specifically, CISPER is equipped with the prompt blending the contextual information and commonsense related to the interlocutor's utterances, to achieve ERC more effectively. Our extensive experiments demonstrate CISPER's superior performance over the state-of-the-art ERC models, and the effectiveness of leveraging these two kinds of significant prompt information for performance gains. To reproduce our experimental results conveniently, CISPER's sourcecode and the datasets have been shared at https:\/\/github.com\/DeqingYang\/CISPER.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1235363032,
        "newsscientist":0.1361984969,
        "technologyreview":0.223885445,
        "venturebeat":0.2474899699,
        "wired":0.1898253687,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13254v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658889245000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.12065v1",
        "predicted_newsworthiness":0.5181265682,
        "title":"Dynamic Channel Selection in Self-Supervised Learning",
        "summary":"Whilst computer vision models built using self-supervised approaches are now commonplace, some important questions remain. Do self-supervised models learn highly redundant channel features? What if a self-supervised network could dynamically select the important channels and get rid of the unnecessary ones? Currently, convnets pre-trained with self-supervision have obtained comparable performance on downstream tasks in comparison to their supervised counterparts in computer vision. However, there are drawbacks to self-supervised models including their large numbers of parameters, computationally expensive training strategies and a clear need for faster inference on downstream tasks. In this work, our goal is to address the latter by studying how a standard channel selection method developed for supervised learning can be applied to networks trained with self-supervision. We validate our findings on a range of target budgets $t_{d}$ for channel computation on image classification task across different datasets, specifically CIFAR-10, CIFAR-100, and ImageNet-100, obtaining comparable performance to that of the original network when selecting all channels but at a significant reduction in computation reported in terms of FLOPs.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0900905998,
        "newsscientist":0.144102893,
        "technologyreview":0.2553196931,
        "venturebeat":0.2310981308,
        "wired":0.1791047431,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12065v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658747928000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01149v1",
        "predicted_newsworthiness":0.5178345615,
        "title":"A Feasibility Study on Image Inpainting for Non-cleft Lip Generation from Patients with Cleft Lip",
        "summary":"A Cleft lip is a congenital abnormality requiring surgical repair by a specialist. The surgeon must have extensive experience and theoretical knowledge to perform surgery, and Artificial Intelligence (AI) method has been proposed to guide surgeons in improving surgical outcomes. If AI can be used to predict what a repaired cleft lip would look like, surgeons could use it as an adjunct to adjust their surgical technique and improve results. To explore the feasibility of this idea while protecting patient privacy, we propose a deep learning-based image inpainting method that is capable of covering a cleft lip and generating a lip and nose without a cleft. Our experiments are conducted on two real-world cleft lip datasets and are assessed by expert cleft lip surgeons to demonstrate the feasibility of the proposed method.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1288660552,
        "newsscientist":0.1947324938,
        "technologyreview":0.2998737647,
        "venturebeat":0.2568026259,
        "wired":0.2004227425,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01149v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659390289000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12381v1",
        "predicted_newsworthiness":0.5176582483,
        "title":"LightX3ECG: A Lightweight and eXplainable Deep Learning System for 3-lead Electrocardiogram Classification",
        "summary":"Cardiovascular diseases (CVDs) are a group of heart and blood vessel disorders that is one of the most serious dangers to human health, and the number of such patients is still growing. Early and accurate detection plays a key role in successful treatment and intervention. Electrocardiogram (ECG) is the gold standard for identifying a variety of cardiovascular abnormalities. In clinical practices and most of the current research, standard 12-lead ECG is mainly used. However, using a lower number of leads can make ECG more prevalent as it can be conveniently recorded by portable or wearable devices. In this research, we develop a novel deep learning system to accurately identify multiple cardiovascular abnormalities by using only three ECG leads.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1160808863,
        "newsscientist":0.168705104,
        "technologyreview":0.2497101392,
        "venturebeat":0.2431838922,
        "wired":0.1731612755,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12381v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658771369000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01152v1",
        "predicted_newsworthiness":0.5174169259,
        "title":"Interpretable Time Series Clustering Using Local Explanations",
        "summary":"This study focuses on exploring the use of local interpretability methods for explaining time series clustering models. Many of the state-of-the-art clustering models are not directly explainable. To provide explanations for these clustering algorithms, we train classification models to estimate the cluster labels. Then, we use interpretability methods to explain the decisions of the classification models. The explanations are used to obtain insights into the clustering models. We perform a detailed numerical study to test the proposed approach on multiple datasets, clustering models, and classification models. The analysis of the results shows that the proposed approach can be used to explain time series clustering models, specifically when the underlying classification model is accurate. Lastly, we provide a detailed analysis of the results, discussing how our approach can be used in a real-life scenario.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1337459934,
        "newsscientist":0.1560056714,
        "technologyreview":0.2116808521,
        "venturebeat":0.2032838979,
        "wired":0.1644591147,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01152v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659390676000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13453v1",
        "predicted_newsworthiness":0.5171824491,
        "title":"Safe and Robust Experience Sharing for Deterministic Policy Gradient Algorithms",
        "summary":"Learning in high dimensional continuous tasks is challenging, mainly when the experience replay memory is very limited. We introduce a simple yet effective experience sharing mechanism for deterministic policies in continuous action domains for the future off-policy deep reinforcement learning applications in which the allocated memory for the experience replay buffer is limited. To overcome the extrapolation error induced by learning from other agents' experiences, we facilitate our algorithm with a novel off-policy correction technique without any action probability estimates. We test the effectiveness of our method in challenging OpenAI Gym continuous control tasks and conclude that it can achieve a safe experience sharing across multiple agents and exhibits a robust performance when the replay memory is strictly limited.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1020511441,
        "newsscientist":0.1394834708,
        "technologyreview":0.2387517368,
        "venturebeat":0.2106776919,
        "wired":0.1578056951,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13453v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658920250000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13430v1",
        "predicted_newsworthiness":0.5171385226,
        "title":"Concept Drift Challenge in Multimedia Anomaly Detection: A Case Study with Facial Datasets",
        "summary":"Anomaly detection in multimedia datasets is a widely studied area. Yet, the concept drift challenge in data has been ignored or poorly handled by the majority of the anomaly detection frameworks. The state-of-the-art approaches assume that the data distribution at training and deployment time will be the same. However, due to various real-life environmental factors, the data may encounter drift in its distribution or can drift from one class to another in the late future. Thus, a one-time trained model might not perform adequately. In this paper, we systematically investigate the effect of concept drift on various detection models and propose a modified Adaptive Gaussian Mixture Model (AGMM) based framework for anomaly detection in multimedia data. In contrast to the baseline AGMM, the proposed extension of AGMM remembers the past for a longer period in order to handle the drift better. Extensive experimental analysis shows that the proposed model better handles the drift in data as compared with the baseline AGMM. Further, to facilitate research and comparison with the proposed framework, we contribute three multimedia datasets constituting faces as samples. The face samples of individuals correspond to the age difference of more than ten years to incorporate a longer temporal context.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1096932123,
        "newsscientist":0.1526851711,
        "technologyreview":0.2305403063,
        "venturebeat":0.2100074326,
        "wired":0.1704110971,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13430v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658917104000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00881v1",
        "predicted_newsworthiness":0.5169789931,
        "title":"Computer vision-based analysis of buildings and built environments: A systematic review of current approaches",
        "summary":"Analysing 88 sources published from 2011 to 2021, this paper presents a first systematic review of the computer vision-based analysis of buildings and the built environments to assess its value to architectural and urban design studies. Following a multi-stage selection process, the types of algorithms and data sources used are discussed in respect to architectural applications such as a building classification, detail classification, qualitative environmental analysis, building condition survey, and building value estimation. This reveals current research gaps and trends, and highlights two main categories of research aims. First, to use or optimise computer vision methods for architectural image data, which can then help automate time-consuming, labour-intensive, or complex tasks of visual analysis. Second, to explore the methodological benefits of machine learning approaches to investigate new questions about the built environment by finding patterns and relationships between visual, statistical, and qualitative data, which can overcome limitations of conventional manual analysis. The growing body of research offers new methods to architectural and design studies, with the paper identifying future challenges and directions of research.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1921081473,
        "newsscientist":0.1922639758,
        "technologyreview":0.2530908756,
        "venturebeat":0.2335370096,
        "wired":0.2085745602,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00881v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659363471000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00403v1",
        "predicted_newsworthiness":0.5169459889,
        "title":"SG-Based Analysis of LEO Satellite-Relayed Communication Systems",
        "summary":"Due to their low latency, high capacity, and seamless worldwide coverage, low Earth orbit (LEO) satellites are essential to the equal access network. Stochastic geometry (SG) is an appropriate method for such a large and irregular system. The SG model can effectively assess and estimate the performance of the network as well as handle the growing network scale. In this article, a number of common satellite distribution models are examined. In the non-technical description, system-level metrics such as coverage probability are introduced. The impact of gateway density, as well as the quantity and height of satellites, on latency and likelihood of coverage, is then researched. This essay concludes by outlining potential uses for SG in the future.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0765814065,
        "newsscientist":0.1083936331,
        "technologyreview":0.1326261869,
        "venturebeat":0.1242892593,
        "wired":0.1087578142,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00403v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1659259274000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2208.01755v1",
        "predicted_newsworthiness":0.5158990277,
        "title":"Exploring Gender Bias in Retrieval Models",
        "summary":"Biases in culture, gender, ethnicity, etc. have existed for decades and have affected many areas of human social interaction. These biases have been shown to impact machine learning (ML) models, and for natural language processing (NLP), this can have severe consequences for downstream tasks. Mitigating gender bias in information retrieval (IR) is important to avoid propagating stereotypes. In this work, we employ a dataset consisting of two components: (1) relevance of a document to a query and (2) \"gender\" of a document, in which pronouns are replaced by male, female, and neutral conjugations. We definitively show that pre-trained models for IR do not perform well in zero-shot retrieval tasks when full fine-tuning of a large pre-trained BERT encoder is performed and that lightweight fine-tuning performed with adapter networks improves zero-shot retrieval performance almost by 20% over baseline. We also illustrate that pre-trained models have gender biases that result in retrieved articles tending to be more often male than female. We overcome this by introducing a debiasing technique that penalizes the model when it prefers males over females, resulting in an effective model that retrieves articles in a balanced fashion across genders.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2086284928,
        "newsscientist":0.1936892655,
        "technologyreview":0.3018410277,
        "venturebeat":0.286696036,
        "wired":0.256233734,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01755v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ir"
        ],
        "published":1659474725000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14131v1",
        "predicted_newsworthiness":0.5153862625,
        "title":"PencilNet: Zero-Shot Sim-to-Real Transfer Learning for Robust Gate Perception in Autonomous Drone Racing",
        "summary":"In autonomous and mobile robotics, one of the main challenges is the robust on-the-fly perception of the environment, which is often unknown and dynamic, like in autonomous drone racing. In this work, we propose a novel deep neural network-based perception method for racing gate detection -- PencilNet -- which relies on a lightweight neural network backbone on top of a pencil filter. This approach unifies predictions of the gates' 2D position, distance, and orientation in a single pose tuple. We show that our method is effective for zero-shot sim-to-real transfer learning that does not need any real-world training samples. Moreover, our framework is highly robust to illumination changes commonly seen under rapid flight compared to state-of-art methods. A thorough set of experiments demonstrates the effectiveness of this approach in multiple challenging scenarios, where the drone completes various tracks under different lighting conditions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0929617376,
        "newsscientist":0.1758755569,
        "technologyreview":0.2895331912,
        "venturebeat":0.2644726113,
        "wired":0.2424581066,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14131v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659019949000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01093v1",
        "predicted_newsworthiness":0.5151669081,
        "title":"EBOCA: Evidences for BiOmedical Concepts Association Ontology",
        "summary":"There is a large number of online documents data sources available nowadays. The lack of structure and the differences between formats are the main difficulties to automatically extract information from them, which also has a negative impact on its use and reuse. In the biomedical domain, the DISNET platform emerged to provide researchers with a resource to obtain information in the scope of human disease networks by means of large-scale heterogeneous sources. Specifically in this domain, it is critical to offer not only the information extracted from different sources, but also the evidence that supports it. This paper proposes EBOCA, an ontology that describes (i) biomedical domain concepts and associations between them, and (ii) evidences supporting these associations; with the objective of providing an schema to improve the publication and description of evidences and biomedical associations in this domain. The ontology has been successfully evaluated to ensure there are no errors, modelling pitfalls and that it meets the previously defined functional requirements. Test data coming from a subset of DISNET and automatic association extractions from texts has been transformed according to the proposed ontology to create a Knowledge Graph that can be used in real scenarios, and which has also been used for the evaluation of the presented ontology.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1988427053,
        "newsscientist":0.2110057481,
        "technologyreview":0.2466931626,
        "venturebeat":0.2227287996,
        "wired":0.1797410793,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01093v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1659379623000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.12035v1",
        "predicted_newsworthiness":0.5150423784,
        "title":"What makes you change your mind? An empirical investigation in online group decision-making conversations",
        "summary":"People leverage group discussions to collaborate in order to solve complex tasks, e.g. in project meetings or hiring panels. By doing so, they engage in a variety of conversational strategies where they try to convince each other of the best approach and ultimately reach a decision. In this work, we investigate methods for detecting what makes someone change their mind. To this end, we leverage a recently introduced dataset containing group discussions of people collaborating to solve a task. To find out what makes someone change their mind, we incorporate various techniques such as neural text classification and language-agnostic change point detection. Evaluation of these methods shows that while the task is not trivial, the best way to approach it is using a language-aware model with learning-to-rank training. Finally, we examine the cues that the models develop as indicative of the cause of a change of mind.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1753222953,
        "newsscientist":0.1911707535,
        "technologyreview":0.2788366145,
        "venturebeat":0.2858753674,
        "wired":0.23869537,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12035v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658744371000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11941v1",
        "predicted_newsworthiness":0.5142900823,
        "title":"GE-Grasp: Efficient Target-Oriented Grasping in Dense Clutter",
        "summary":"Grasping in dense clutter is a fundamental skill for autonomous robots. However, the crowdedness and occlusions in the cluttered scenario cause significant difficulties to generate valid grasp poses without collisions, which results in low efficiency and high failure rates. To address these, we present a generic framework called GE-Grasp for robotic motion planning in dense clutter, where we leverage diverse action primitives for occluded object removal and present the generator-evaluator architecture to avoid spatial collisions. Therefore, our GE-Grasp is capable of grasping objects in dense clutter efficiently with promising success rates. Specifically, we define three action primitives: target-oriented grasping for target capturing, pushing, and nontarget-oriented grasping to reduce the crowdedness and occlusions. The generators effectively provide various action candidates referring to the spatial information. Meanwhile, the evaluators assess the selected action primitive candidates, where the optimal action is implemented by the robot. Extensive experiments in simulated and real-world environments show that our approach outperforms the state-of-the-art methods of grasping in clutter with respect to motion efficiency and success rates. Moreover, we achieve comparable performance in the real world as that in the simulation environment, which indicates the strong generalization ability of our GE-Grasp. Supplementary material is available at: https:\/\/github.com\/CaptainWuDaoKou\/GE-Grasp.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0823164586,
        "newsscientist":0.137942259,
        "technologyreview":0.2108712462,
        "venturebeat":0.1758170202,
        "wired":0.1657646427,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11941v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658733522000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.12360v1",
        "predicted_newsworthiness":0.5140331631,
        "title":"Exploiting High Quality Tactile Sensors for Simplified Grasping",
        "summary":"Robots are expected to grasp a wide range of objects varying in shape, weight or material type. Providing robots with tactile capabilities similar to humans is thus essential for applications involving human-to-robot or robot-to-robot interactions, particularly in those situations where a robot is expected to grasp and manipulate complex objects not previously encountered. A critical aspect for successful object grasp and manipulation is the use of high-quality fingertips equipped with multiple high-performance sensors, distributed appropriately across a specific contact surface. In this paper, we present a detailed analysis of the use of two different types of commercially available robotic fingertips (BioTac and WTS-FT), each of which is equipped with multiple sensors distributed across the fingertips' contact surface. We further demonstrate that, due to the high performance of the fingertips, a complex adaptive grasping algorithm is not required for grasping of everyday objects. We conclude that a simple algorithm based on a proportional controller will suffice for many grasping applications, provided the relevant fingertips exhibit high sensitivity. In a quantified assessment, we also demonstrate that, due in part to the sensor distribution, the BioTac-based fingertip performs better than the WTS-FT device, in enabling lifting of loads up to 850g, and that the simple proportional controller can adapt the grasp even when the object is exposed to significant external vibrational challenges.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.071771771,
        "newsscientist":0.16071541,
        "technologyreview":0.185549658,
        "venturebeat":0.1410606988,
        "wired":0.1367506756,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12360v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv"
        ],
        "published":1658769577000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01252v1",
        "predicted_newsworthiness":0.5139418801,
        "title":"A Novel Transformer Network with Shifted Window Cross-Attention for Spatiotemporal Weather Forecasting",
        "summary":"Earth Observatory is a growing research area that can capitalize on the powers of AI for short time forecasting, a Now-casting scenario. In this work, we tackle the challenge of weather forecasting using a video transformer network. Vision transformer architectures have been explored in various applications, with major constraints being the computational complexity of Attention and the data hungry training. To address these issues, we propose the use of Video Swin-Transformer, coupled with a dedicated augmentation scheme. Moreover, we employ gradual spatial reduction on the encoder side and cross-attention on the decoder. The proposed approach is tested on the Weather4Cast2021 weather forecasting challenge data, which requires the prediction of 8 hours ahead future frames (4 per hour) from an hourly weather product sequence. The dataset was normalized to 0-1 to facilitate using the evaluation metrics across different datasets. The model results in an MSE score of 0.4750 when provided with training data, and 0.4420 during transfer learning without using training data, respectively.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.140679512,
        "newsscientist":0.1881992208,
        "technologyreview":0.2522688148,
        "venturebeat":0.2379658006,
        "wired":0.2085575497,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01252v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659416693000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12496v1",
        "predicted_newsworthiness":0.5136791795,
        "title":"NeuriCam: Video Super-Resolution and Colorization Using Key Frames",
        "summary":"We present NeuriCam, a key-frame video super-resolution and colorization based system, to achieve low-power video capture from dual-mode IOT cameras. Our idea is to design a dual-mode camera system where the first mode is low power (1.1~mW) but only outputs gray-scale, low resolution and noisy video and the second mode consumes much higher power (100~mW) but outputs color and higher resolution images. To reduce total energy consumption, we heavily duty cycle the high power mode to output an image only once every second. The data from this camera system is then wirelessly streamed to a nearby plugged-in gateway, where we run our real-time neural network decoder to reconstruct a higher resolution color video. To achieve this, we introduce an attention feature filter mechanism that assigns different weights to different features, based on the correlation between the feature map and contents of the input frame at each spatial location. We design a wireless hardware prototype using off-the-shelf cameras and address practical issues including packet loss and perspective mismatch. Our evaluation shows that our dual-camera hardware reduces camera energy consumption while achieving an average gray-scale PSNR gain of 3.7~dB over prior video super resolution methods and 5.6~dB RGB gain over existing color propagation methods. Open-source code: https:\/\/github.com\/vb000\/NeuriCam.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0949673441,
        "newsscientist":0.1526122736,
        "technologyreview":0.226149273,
        "venturebeat":0.252448602,
        "wired":0.2348476298,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12496v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658778897000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13771v1",
        "predicted_newsworthiness":0.5135834861,
        "title":"CompText: Visualizing, Comparing & Understanding Text Corpus",
        "summary":"A common practice in Natural Language Processing (NLP) is to visualize the text corpus without reading through the entire literature, still grasping the central idea and key points described. For a long time, researchers focused on extracting topics from the text and visualizing them based on their relative significance in the corpus. However, recently, researchers started coming up with more complex systems that not only expose the topics of the corpus but also word closely related to the topic to give users a holistic view. These detailed visualizations spawned research on comparing text corpora based on their visualization. Topics are often compared to idealize the difference between corpora. However, to capture greater semantics from different corpora, researchers have started to compare texts based on the sentiment of the topics related to the text. Comparing the words carrying the most weightage, we can get an idea about the important topics for corpus. There are multiple existing texts comparing methods present that compare topics rather than sentiments but we feel that focusing on sentiment-carrying words would better compare the two corpora. Since only sentiments can explain the real feeling of the text and not just the topic, topics without sentiments are just nouns. We aim to differentiate the corpus with a focus on sentiment, as opposed to comparing all the words appearing in the two corpora. The rationale behind this is, that the two corpora do not many have identical words for side-by-side comparison, so comparing the sentiment words gives us an idea of how the corpora are appealing to the emotions of the reader. We can argue that the entropy or the unexpectedness and divergence of topics should also be of importance and help us to identify key pivot points and the importance of certain topics in the corpus alongside relative sentiment.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2083368822,
        "newsscientist":0.1872051461,
        "technologyreview":0.2443802853,
        "venturebeat":0.2305586809,
        "wired":0.2421536311,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13771v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658952271000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11679v1",
        "predicted_newsworthiness":0.5130413474,
        "title":"Affective Behaviour Analysis Using Pretrained Model with Facial Priori",
        "summary":"Affective behaviour analysis has aroused researchers' attention due to its broad applications. However, it is labor exhaustive to obtain accurate annotations for massive face images. Thus, we propose to utilize the prior facial information via Masked Auto-Encoder (MAE) pretrained on unlabeled face images. Furthermore, we combine MAE pretrained Vision Transformer (ViT) and AffectNet pretrained CNN to perform multi-task emotion recognition. We notice that expression and action unit (AU) scores are pure and intact features for valence-arousal (VA) regression. As a result, we utilize AffectNet pretrained CNN to extract expression scores concatenating with expression and AU scores from ViT to obtain the final VA features. Moreover, we also propose a co-training framework with two parallel MAE pretrained ViT for expression recognition tasks. In order to make the two views independent, we random mask most patches during the training process. Then, JS divergence is performed to make the predictions of the two views as consistent as possible. The results on ABAW4 show that our methods are effective.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.131740986,
        "newsscientist":0.1583954677,
        "technologyreview":0.2398651421,
        "venturebeat":0.2295376908,
        "wired":0.1759585561,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11679v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658647688000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12558v1",
        "predicted_newsworthiness":0.5129932411,
        "title":"A Pilot Study on The Impact of Stereoscopic Display Type on User Interactions Within A Immersive Analytics Environment",
        "summary":"Immersive Analytics (IA) and consumer adoption of augmented reality (AR) and virtual reality (VR) head-mounted displays (HMDs) are both rapidly growing. When used in conjunction, stereoscopic IA environments can offer improved user understanding and engagement; however, it is unclear how the choice of stereoscopic display impacts user interactions within an IA environment. This paper presents a pilot study that examines the impact of stereoscopic display type on object manipulation and environmental navigation using consumer-available AR and VR displays. This work finds that the display type can impact how users manipulate virtual content, how they navigate the environment, and how able they are to answer questions about the represented data.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1654458589,
        "newsscientist":0.1850078154,
        "technologyreview":0.2669494047,
        "venturebeat":0.3573921503,
        "wired":0.2804465324,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12558v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1658787833000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.13313v1",
        "predicted_newsworthiness":0.5129501125,
        "title":"Social Live-Streaming Use & Well-being: Examining Participation, Financial Commitment, Social Capital, and Psychological Well-being on Twitch.tv",
        "summary":"This study examines how active participation, financial commitment, and passive participation in the leading social live-streaming service, Twitch.tv, relate to individuals' psychological well-being. The three dimensions of social capital-structural, relational, and cognitive-as well as parasocial relationship are explored as mediators. Cross-sectional survey data from 396 respondents was analyzed by comparing two fully saturated structural equation models. Findings indicate actively participating in a favorite streamers' Chat is positively associated with increased well-being. Structural social capital, or having more social interaction ties, positively mediates the relationship between active participation and well-being, as well as financial commitment and well-being. Greater cognitive social capital, or shared values and goals with a favorite streamer, is related to decreased well-being. Parasocial relationship does not significantly mediate the relationship between use and well-being. Our results demonstrate the importance of tangible social ties over the perceived relationships or identification with a favorite streamer.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2735497946,
        "newsscientist":0.2010433447,
        "technologyreview":0.3040207314,
        "venturebeat":0.3341923051,
        "wired":0.319646331,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13313v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.hc"
        ],
        "published":1658902209000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.13801v1",
        "predicted_newsworthiness":0.5129383694,
        "title":"Towards Sleep Scoring Generalization Through Self-Supervised Meta-Learning",
        "summary":"In this work we introduce a novel meta-learning method for sleep scoring based on self-supervised learning. Our approach aims at building models for sleep scoring that can generalize across different patients and recording facilities, but do not require a further adaptation step to the target data. Towards this goal, we build our method on top of the Model Agnostic Meta-Learning (MAML) framework by incorporating a self-supervised learning (SSL) stage, and call it S2MAML. We show that S2MAML can significantly outperform MAML. The gain in performance comes from the SSL stage, which we base on a general purpose pseudo-task that limits the overfitting to the subject-specific patterns present in the training dataset. We show that S2MAML outperforms standard supervised learning and MAML on the SC, ST, ISRUC, UCD and CAP datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1247591825,
        "newsscientist":0.177549287,
        "technologyreview":0.2306316924,
        "venturebeat":0.2204864388,
        "wired":0.1620668745,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13801v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658957244000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13814v1",
        "predicted_newsworthiness":0.5128109014,
        "title":"Data Driven Modeling Social Media Influence using Differential Equations",
        "summary":"Individuals modify their opinions towards a topic based on their social interactions. Opinion evolution models conceptualize the change of opinion as a uni-dimensional continuum, and the effect of influence is built by the group size, the network structures, or the relations among opinions within the group. However, how to model the personal opinion evolution process under the effect of the online social influence as a function remains unclear. Here, we show that the uni-dimensional continuous user opinions can be represented by compressed high-dimensional word embeddings, and its evolution can be accurately modelled by an ordinary differential equation (ODE) that reflects the social network influencer interactions. Our three major contributions are: (1) introduce a data-driven pipeline representing the personal evolution of opinions with a time kernel, (2) based on previous psychology models, we model the opinion evolution process as a function of online social influence using an ordinary differential equation, and (3) applied Our opinion evolution model to the real-time Twitter data. We perform our analysis on 87 active users with corresponding influencers on the COVID-19 topic from 2020 to 2022. The regression results demonstrate that 99% of the variation in the quantified opinions can be explained by the way we model the connected opinions from their influencers. Our research on the COVID-19 topic and for the account analysed shows that social media users primarily shift their opinion based on influencers they follow (e.g., model explains for 99% variation) and self-evolution of opinion over a long time scale is limited.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2214376348,
        "newsscientist":0.2108330636,
        "technologyreview":0.3102021036,
        "venturebeat":0.2824540012,
        "wired":0.2908621952,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13814v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1658960662000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.14798v1",
        "predicted_newsworthiness":0.5126456074,
        "title":"Personalized Promotion Decision Making Based on Direct and Enduring Effect Predictions",
        "summary":"Promotions have been trending in the e-commerce marketplace to build up customer relationships and guide customers towards the desired actions. Since incentives are effective to engage customers and customers have different preferences for different types of incentives, the demand for personalized promotion decision making is increasing over time. However, research on promotion decision making has focused specifically on purchase conversion during the promotion period (the direct effect), while generally disregarding the enduring effect in the post promotion period. To achieve a better lift return on investment (lift ROI) on the enduring effect of the promotion and improve customer retention and loyalty, we propose a framework of multiple treatment promotion decision making by modeling each customer's direct and enduring response. First, we propose a customer direct and enduring effect (CDEE) model which predicts the customer direct and enduring response. With the help of the predictions of the CDEE, we personalize incentive allocation to optimize the enduring effect while keeping the cost under the budget. To estimate the effect of decision making, we apply an unbiased evaluation approach of business metrics with randomized control trial (RCT) data. We compare our method with benchmarks using two promotions in Mercari and achieve significantly better results.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1668100658,
        "newsscientist":0.1508793091,
        "technologyreview":0.2216382102,
        "venturebeat":0.2993651441,
        "wired":0.1898735059,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14798v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.lg"
        ],
        "published":1658560437000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.01963v1",
        "predicted_newsworthiness":0.5125098253,
        "title":"Localization and Classification of Parasitic Eggs in Microscopic Images Using an EfficientDet Detector",
        "summary":"IPIs caused by protozoan and helminth parasites are among the most common infections in humans in LMICs. They are regarded as a severe public health concern, as they cause a wide array of potentially detrimental health conditions. Researchers have been developing pattern recognition techniques for the automatic identification of parasite eggs in microscopic images. Existing solutions still need improvements to reduce diagnostic errors and generate fast, efficient, and accurate results. Our paper addresses this and proposes a multi-modal learning detector to localize parasitic eggs and categorize them into 11 categories. The experiments were conducted on the novel Chula-ParasiteEgg-11 dataset that was used to train both EfficientDet model with EfficientNet-v2 backbone and EfficientNet-B7+SVM. The dataset has 11,000 microscopic training images from 11 categories. Our results show robust performance with an accuracy of 92%, and an F1 score of 93%. Additionally, the IOU distribution illustrates the high localization capability of the detector.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1315140855,
        "newsscientist":0.1829487415,
        "technologyreview":0.1923283673,
        "venturebeat":0.1772685626,
        "wired":0.132456473,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01963v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659522498000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13315v1",
        "predicted_newsworthiness":0.5123693732,
        "title":"Portrait Interpretation and a Benchmark",
        "summary":"We propose a task we name Portrait Interpretation and construct a dataset named Portrait250K for it. Current researches on portraits such as human attribute recognition and person re-identification have achieved many successes, but generally, they: 1) may lack mining the interrelationship between various tasks and the possible benefits it may bring; 2) design deep models specifically for each task, which is inefficient; 3) may be unable to cope with the needs of a unified model and comprehensive perception in actual scenes. In this paper, the proposed portrait interpretation recognizes the perception of humans from a new systematic perspective. We divide the perception of portraits into three aspects, namely Appearance, Posture, and Emotion, and design corresponding sub-tasks for each aspect. Based on the framework of multi-task learning, portrait interpretation requires a comprehensive description of static attributes and dynamic states of portraits. To invigorate research on this new task, we construct a new dataset that contains 250,000 images labeled with identity, gender, age, physique, height, expression, and posture of the whole body and arms. Our dataset is collected from 51 movies, hence covering extensive diversity. Furthermore, we focus on representation learning for portrait interpretation and propose a baseline that reflects our systematic perspective. We also propose an appropriate metric for this task. Our experimental results demonstrate that combining the tasks related to portrait interpretation can yield benefits. Code and dataset will be made public.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.12882212,
        "newsscientist":0.1549788579,
        "technologyreview":0.2496037636,
        "venturebeat":0.2169230622,
        "wired":0.1885691983,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13315v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658903109000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12571v1",
        "predicted_newsworthiness":0.5117052956,
        "title":"Innovations in Neural Data-to-text Generation",
        "summary":"The neural boom that has sparked natural language processing (NLP) research through the last decade has similarly led to significant innovations in data-to-text generation (DTG). This survey offers a consolidated view into the neural DTG paradigm with a structured examination of the approaches, benchmark datasets, and evaluation protocols. This survey draws boundaries separating DTG from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella. With this holistic view, we highlight promising avenues for DTG research that not only focus on the design of linguistically capable systems but also systems that exhibit fairness and accountability.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.130636573,
        "newsscientist":0.1627560626,
        "technologyreview":0.2845436837,
        "venturebeat":0.2824860868,
        "wired":0.2231290891,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12571v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658791308000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14017v1",
        "predicted_newsworthiness":0.51154977,
        "title":"Unsupervised Frequent Pattern Mining for CEP",
        "summary":"Complex Event Processing (CEP) is a set of methods that allow efficient knowledge extraction from massive data streams using complex and highly descriptive patterns. Numerous applications, such as online finance, healthcare monitoring and fraud detection use CEP technologies to capture critical alerts, potential threats, or vital notifications in real time. As of today, in many fields, patterns are manually defined by human experts. However, desired patterns often contain convoluted relations that are difficult for humans to detect, and human expertise is scarce in many domains. We present REDEEMER (REinforcement baseD cEp pattErn MinER), a novel reinforcement and active learning approach aimed at mining CEP patterns that allow expansion of the knowledge extracted while reducing the human effort required. This approach includes a novel policy gradient method for vast multivariate spaces and a new way to combine reinforcement and active learning for CEP rule learning while minimizing the number of labels needed for training. REDEEMER aims to enable CEP integration in domains that could not utilize it before. To the best of our knowledge, REDEEMER is the first system that suggests new CEP rules that were not observed beforehand, and is the first method aimed for increasing pattern knowledge in fields where experts do not possess sufficient information required for CEP tools. Our experiments on diverse data-sets demonstrate that REDEEMER is able to extend pattern knowledge while outperforming several state-of-the-art reinforcement learning methods for pattern mining.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1376550709,
        "newsscientist":0.1703173469,
        "technologyreview":0.2747903689,
        "venturebeat":0.3002170839,
        "wired":0.2104566648,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14017v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659007445000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12471v1",
        "predicted_newsworthiness":0.5105566879,
        "title":"Secure Service Implementation with Slice Isolation and WireGuard",
        "summary":"Network slicing enables the provision of services for different verticals over a shared infrastructure. Nevertheless, security is still one of the main challenges when sharing resources. In this paper, we study how WireGuard can provide an encrypted Virtual Private Network (VPN) tunnel as a service between network functions in 5G setting. The open source management and orchestration entity deploys and orchestrates the network functions into network services and slices. We create multiple scenarios emulating a real-life cellular network deploying VPN-as-a-Service between the different network functions to secure and isolate network slices. The performance measurements demonstrate from 0.8 Gbps to 2.5 Gbps throughput and below 1ms delay between network functions using WireGuard. The performance evaluation results are aligned with 5G key performance indicators, making WireGuard suited to provide security in slice isolation in future generations of cellular networks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1363959659,
        "newsscientist":0.124084879,
        "technologyreview":0.2212880144,
        "venturebeat":0.2846002373,
        "wired":0.2259848507,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12471v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658774893000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2208.02205v1",
        "predicted_newsworthiness":0.5105206768,
        "title":"DAHiTrA: Damage Assessment Using a Novel Hierarchical Transformer Architecture",
        "summary":"This paper presents DAHiTrA, a novel deep-learning model with hierarchical transformers to classify building damages based on satellite images in the aftermath of hurricanes. An automated building damage assessment provides critical information for decision making and resource allocation for rapid emergency response. Satellite imagery provides real-time, high-coverage information and offers opportunities to inform large-scale post-disaster building damage assessment. In addition, deep-learning methods have shown to be promising in classifying building damage. In this work, a novel transformer-based network is proposed for assessing building damage. This network leverages hierarchical spatial features of multiple resolutions and captures temporal difference in the feature domain after applying a transformer encoder on the spatial features. The proposed network achieves state-of-the-art-performance when tested on a large-scale disaster damage dataset (xBD) for building localization and damage classification, as well as on LEVIR-CD dataset for change detection tasks. In addition, we introduce a new high-resolution satellite imagery dataset, Ida-BD (related to the 2021 Hurricane Ida in Louisiana in 2021, for domain adaptation to further evaluate the capability of the model to be applied to newly damaged areas with scarce data. The domain adaptation results indicate that the proposed model can be adapted to a new event with only limited fine-tuning. Hence, the proposed model advances the current state of the art through better performance and domain adaptation. Also, Ida-BD provides a higher-resolution annotated dataset for future studies in this field.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.180115397,
        "newsscientist":0.1990865383,
        "technologyreview":0.2602415796,
        "venturebeat":0.2360261227,
        "wired":0.2186781733,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02205v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659544899000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00775v1",
        "predicted_newsworthiness":0.5104688599,
        "title":"Pavementscapes: a large-scale hierarchical image dataset for asphalt pavement damage segmentation",
        "summary":"Pavement damage segmentation has benefited enormously from deep learning. % and large-scale datasets. However, few current public datasets limit the potential exploration of deep learning in the application of pavement damage segmentation. To address this problem, this study has proposed Pavementscapes, a large-scale dataset to develop and evaluate methods for pavement damage segmentation. Pavementscapes is comprised of 4,000 images with a resolution of $1024 \\times 2048$, which have been recorded in the real-world pavement inspection projects with 15 different pavements. A total of 8,680 damage instances are manually labeled with six damage classes at the pixel level. The statistical study gives a thorough investigation and analysis of the proposed dataset. The numeral experiments propose the top-performing deep neural networks capable of segmenting pavement damages, which provides the baselines of the open challenge for pavement inspection. The experiment results also indicate the existing problems for damage segmentation using deep learning, and this study provides potential solutions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1187800705,
        "newsscientist":0.1630575986,
        "technologyreview":0.2478408055,
        "venturebeat":0.2274898048,
        "wired":0.1915409748,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00775v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658634027000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13975v1",
        "predicted_newsworthiness":0.5095939873,
        "title":"On the Effects of Different Types of Label Noise in Multi-Label Remote Sensing Image Classification",
        "summary":"The development of accurate methods for multi-label classification (MLC) of remote sensing (RS) images is one of the most important research topics in RS. To address MLC problems, the use of deep neural networks that require a high number of reliable training images annotated by multiple land-cover class labels (multi-labels) have been found popular in RS. However, collecting such annotations is time-consuming and costly. A common procedure to obtain annotations at zero labeling cost is to rely on thematic products or crowdsourced labels. As a drawback, these procedures come with the risk of label noise that can distort the learning process of the MLC algorithms. In the literature, most label noise robust methods are designed for single label classification (SLC) problems in computer vision (CV), where each image is annotated by a single label. Unlike SLC, label noise in MLC can be associated with: 1) subtractive label-noise (a land cover class label is not assigned to an image while that class is present in the image); 2) additive label-noise (a land cover class label is assigned to an image although that class is not present in the given image); and 3) mixed label-noise (a combination of both). In this paper, we investigate three different noise robust CV SLC methods and adapt them to be robust for multi-label noise scenarios in RS. During experiments we study the effects of different types of multi-label noise and evaluate the adapted methods rigorously. To this end, we also introduce a synthetic multi-label noise injection strategy that is more adequate to simulate operational scenarios compared to the uniform label noise injection strategy, in which the labels of absent and present classes are flipped at uniform probability. Further, we study the relevance of different evaluation metrics in MLC problems under noisy multi-labels.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.124487012,
        "newsscientist":0.1622368029,
        "technologyreview":0.2156710121,
        "venturebeat":0.1860865701,
        "wired":0.1642964181,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13975v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659001110000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14686v1",
        "predicted_newsworthiness":0.509586423,
        "title":"Forensic License Plate Recognition with Compression-Informed Transformers",
        "summary":"Forensic license plate recognition (FLPR) remains an open challenge in legal contexts such as criminal investigations, where unreadable license plates (LPs) need to be deciphered from highly compressed and\/or low resolution footage, e.g., from surveillance cameras. In this work, we propose a side-informed Transformer architecture that embeds knowledge on the input compression level to improve recognition under strong compression. We show the effectiveness of Transformers for license plate recognition (LPR) on a low-quality real-world dataset. We also provide a synthetic dataset that includes strongly degraded, illegible LP images and analyze the impact of knowledge embedding on it. The network outperforms existing FLPR methods and standard state-of-the art image recognition models while requiring less parameters. For the severest degraded images, we can improve recognition by up to 8.9 percent points.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1123341638,
        "newsscientist":0.1538601404,
        "technologyreview":0.2587795132,
        "venturebeat":0.2245435379,
        "wired":0.182898168,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14686v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659103104000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13343v1",
        "predicted_newsworthiness":0.5094683424,
        "title":"Towards Soft Fairness in Restless Multi-Armed Bandits",
        "summary":"Restless multi-armed bandits (RMAB) is a framework for allocating limited resources under uncertainty. It is an extremely useful model for monitoring beneficiaries and executing timely interventions to ensure maximum benefit in public health settings (e.g., ensuring patients take medicines in tuberculosis settings, ensuring pregnant mothers listen to automated calls about good pregnancy practices). Due to the limited resources, typically certain communities or regions are starved of interventions that can have follow-on effects. To avoid starvation in the executed interventions across individuals\/regions\/communities, we first provide a soft fairness constraint and then provide an approach to enforce the soft fairness constraint in RMABs. The soft fairness constraint requires that an algorithm never probabilistically favor one arm over another if the long-term cumulative reward of choosing the latter arm is higher. Our approach incorporates softmax based value iteration method in the RMAB setting to design selection algorithms that manage to satisfy the proposed fairness constraint. Our method, referred to as SoftFair, also provides theoretical performance guarantees and is asymptotically optimal. Finally, we demonstrate the utility of our approaches on simulated benchmarks and show that the soft fairness constraint can be handled without a significant sacrifice on value.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1777799978,
        "newsscientist":0.1663407018,
        "technologyreview":0.216453469,
        "venturebeat":0.2016760197,
        "wired":0.1660216299,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13343v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cy"
        ],
        "published":1658908592000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13791v1",
        "predicted_newsworthiness":0.5092620868,
        "title":"Learning to Assess Danger from Movies for Cooperative Escape Planning in Hazardous Environments",
        "summary":"There has been a plethora of work towards improving robot perception and navigation, yet their application in hazardous environments, like during a fire or an earthquake, is still at a nascent stage. We hypothesize two key challenges here: first, it is difficult to replicate such scenarios in the real world, which is necessary for training and testing purposes. Second, current systems are not fully able to take advantage of the rich multi-modal data available in such hazardous environments. To address the first challenge, we propose to harness the enormous amount of visual content available in the form of movies and TV shows, and develop a dataset that can represent hazardous environments encountered in the real world. The data is annotated with high-level danger ratings for realistic disaster images, and corresponding keywords are provided that summarize the content of the scene. In response to the second challenge, we propose a multi-modal danger estimation pipeline for collaborative human-robot escape scenarios. Our Bayesian framework improves danger estimation by fusing information from robot's camera sensor and language inputs from the human. Furthermore, we augment the estimation module with a risk-aware planner that helps in identifying safer paths out of the dangerous environment. Through extensive simulations, we exhibit the advantages of our multi-modal perception framework that gets translated into tangible benefits such as higher success rate in a collaborative human-robot mission.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1731658809,
        "newsscientist":0.2128647533,
        "technologyreview":0.2766727407,
        "venturebeat":0.2535707246,
        "wired":0.2477685787,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13791v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv",
            "cs.hc",
            "cs.lg"
        ],
        "published":1658956035000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.12544v1",
        "predicted_newsworthiness":0.5092567966,
        "title":"End-User Puppeteering of Expressive Movements",
        "summary":"The end-user programming of social robot behavior is usually limited by a predefined set of movements. We are proposing a puppeteering robotic interface that provides a more intuitive method of programming robot expressive movements. As the user manipulates the puppet of a robot, the actual robot replicates the movements, providing real-time visual feedback. Through this proposed interface, even with limited training, a novice user can design and program expressive movements efficiently. We present our preliminary user study results in this extended abstract.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1373422117,
        "newsscientist":0.2076588488,
        "technologyreview":0.2845459625,
        "venturebeat":0.2620222145,
        "wired":0.2388949757,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12544v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.hc"
        ],
        "published":1658785146000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01368v1",
        "predicted_newsworthiness":0.5090979892,
        "title":"PyABSA: Open Framework for Aspect-based Sentiment Analysis",
        "summary":"Aspect-based sentiment analysis (ABSA) has become a prevalent task in recent years. However, the absence of a unified framework in the present ABSA research makes it challenging to compare different models' performance fairly. Therefore, we created an open-source ABSA framework, namely PYABSA. Besides, previous efforts usually neglect the precursor aspect term extraction (ASC) subtask and focus on the aspect sentiment classification (ATE) subtask. Compared to previous works, PYABSA includes the features of aspect term extraction, aspect sentiment classification, and text classification, while multiple ABSA subtasks can be adapted to PYABSA owing to its modular architecture. To facilitate ABSA applications, PYABSAseamless integrates multilingual modelling, automated dataset annotation, etc., which are helpful in deploying ABSA services. In ASC and ATE, PYABSA provides up to 33 and 7 built-in models, respectively, while all the models provide quick training and instant inference. Besides, PYABSA contains 180K+ ABSA instances from 21 augmented ABSA datasets for applications and studies. PyABSA is available at https:\/\/github.com\/yangheng95\/PyABSA",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1412473523,
        "newsscientist":0.1310567671,
        "technologyreview":0.2180526106,
        "venturebeat":0.2363001881,
        "wired":0.1992751088,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01368v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659439656000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11936v1",
        "predicted_newsworthiness":0.5088753881,
        "title":"Cloud-native 5G experimental platform with over-the-air transmissions and end-to-end monitoring",
        "summary":"5G represents a revolutionary shift with respect to previous generations given its design centered on network softwarization. Within such a change of paradigm, cloud-native solutions are widely regarded as the future of vertical application development because of their enhanced flexibility and adaptability to complex and dynamic scenarios. In this context, we present an experimental framework with over-the-air transmissions that tackles two critical aspects for enhancing the lifecycle management of 5G and beyond networks: cloud-native deployments of 5G core network functions (NFs) and end-to-end monitoring. First, we deploy Open5GS and Prometheus-based monitoring as containerized network functions (CNFs) in a Kubernetes cluster spanning a multi-tier network with a multi-access edge computing (MEC) host. We then demonstrate the end-to-end monitoring system by showcasing via Grafana dashboards both infrastructure resources and radio metrics of two scenarios; one devoted to user plane function (UPF) re-selection and the other to user mobility.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1103500795,
        "newsscientist":0.131547968,
        "technologyreview":0.2077998377,
        "venturebeat":0.2951505188,
        "wired":0.221157593,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11936v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658732465000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.13131v1",
        "predicted_newsworthiness":0.5087998164,
        "title":"Semi-analytical Industrial Cooling System Model for Reinforcement Learning",
        "summary":"We present a hybrid industrial cooling system model that embeds analytical solutions within a multi-physics simulation. This model is designed for reinforcement learning (RL) applications and balances simplicity with simulation fidelity and interpretability. The model's fidelity is evaluated against real world data from a large scale cooling system. This is followed by a case study illustrating how the model can be used for RL research. For this, we develop an industrial task suite that allows specifying different problem settings and levels of complexity, and use it to evaluate the performance of different RL algorithms.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1275516826,
        "newsscientist":0.1670056027,
        "technologyreview":0.2613134546,
        "venturebeat":0.2427068561,
        "wired":0.1823463268,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13131v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.lg",
            "cs.ro"
        ],
        "published":1658859557000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.12267v1",
        "predicted_newsworthiness":0.5079661877,
        "title":"Continuous ErrP detections during multimodal human-robot interaction",
        "summary":"Human-in-the-loop approaches are of great importance for robot applications. In the presented study, we implemented a multimodal human-robot interaction (HRI) scenario, in which a simulated robot communicates with its human partner through speech and gestures. The robot announces its intention verbally and selects the appropriate action using pointing gestures. The human partner, in turn, evaluates whether the robot's verbal announcement (intention) matches the action (pointing gesture) chosen by the robot. For cases where the verbal announcement of the robot does not match the corresponding action choice of the robot, we expect error-related potentials (ErrPs) in the human electroencephalogram (EEG). These intrinsic evaluations of robot actions by humans, evident in the EEG, were recorded in real time, continuously segmented online and classified asynchronously. For feature selection, we propose an approach that allows the combinations of forward and backward sliding windows to train a classifier. We achieved an average classification performance of 91% across 9 subjects. As expected, we also observed a relatively high variability between the subjects. In the future, the proposed feature selection approach will be extended to allow for customization of feature selection. To this end, the best combinations of forward and backward sliding windows will be automatically selected to account for inter-subject variability in classification performance. In addition, we plan to use the intrinsic human error evaluation evident in the error case by the ErrP in interactive reinforcement learning to improve multimodal human-robot interaction.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.12368053,
        "newsscientist":0.2008340435,
        "technologyreview":0.2695984032,
        "venturebeat":0.242006659,
        "wired":0.2051761619,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12267v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.hc",
            "cs.lg"
        ],
        "published":1658763572000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.11754v1",
        "predicted_newsworthiness":0.5078442625,
        "title":"Virtual Reality Therapy for the Psychological Well-being of Palliative Care Patients in Hong Kong",
        "summary":"In this paper we introduce novel Virtual Reality (VR) and Augmented Reality (AR) treatments to improve the psychological well being of patients in palliative care, based on interviews with a clinical psychologist who has successfully implemented VR assisted interventions on palliative care patients in the Hong Kong hospital system. Our VR and AR assisted interventions are adaptations of traditional palliative care therapies which simultaneously facilitate patients communication with family and friends while isolated in hospital due to physical weakness and COVID-19 related restrictions. The first system we propose is a networked, metaverse platform for palliative care patients to create customized virtual environments with therapists, family and friends which function as immersive and collaborative versions of 'life review' and 'reminiscence therapy'. The second proposed system will investigate the use of Mixed Reality telepresence and haptic touch in an AR environment, which will allow palliative care patients to physically feel friends and family in a virtual space, adding to the sense of presence and immersion in that environment.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2384737461,
        "newsscientist":0.2542823617,
        "technologyreview":0.2990903702,
        "venturebeat":0.3733186473,
        "wired":0.3010346386,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11754v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1658673112000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.12114v1",
        "predicted_newsworthiness":0.5077495458,
        "title":"When Virtual Reality Meets Rate Splitting Multiple Access: A Joint Communication and Computation Approach",
        "summary":"Rate Splitting Multiple Access (RSMA) has emerged as an effective interference management scheme for applications that require high data rates. Although RSMA has shown advantages in rate enhancement and spectral efficiency, it has yet not to be ready for latency-sensitive applications such as virtual reality streaming, which is an essential building block of future 6G networks. Unlike conventional High-Definition streaming applications, streaming virtual reality applications requires not only stringent latency requirements but also the computation capability of the transmitter to quickly respond to dynamic users' demands. Thus, conventional RSMA approaches usually fail to address the challenges caused by computational demands at the transmitter, let alone the dynamic nature of the virtual reality streaming applications. To overcome the aforementioned challenges, we first formulate the virtual reality streaming problem assisted by RSMA as a joint communication and computation optimization problem. A novel multicast approach is then proposed to cluster users into different groups based on a Field-of-View metric and transmit multicast streams in a hierarchical manner. After that, we propose a deep reinforcement learning approach to obtain the solution for the optimization problem. Extensive simulations show that our framework can achieve the millisecond-latency requirement, which is much lower than other baseline schemes.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0886115882,
        "newsscientist":0.1343077829,
        "technologyreview":0.2076965095,
        "venturebeat":0.2943850229,
        "wired":0.2043308761,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12114v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658751906000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.12615v1",
        "predicted_newsworthiness":0.5076654999,
        "title":"Exploring the Design of Adaptation Protocols for Improved Generalization and Machine Learning Safety",
        "summary":"While directly fine-tuning (FT) large-scale, pretrained models on task-specific data is well-known to induce strong in-distribution task performance, recent works have demonstrated that different adaptation protocols, such as linear probing (LP) prior to FT, can improve out-of-distribution generalization. However, the design space of such adaptation protocols remains under-explored and the evaluation of such protocols has primarily focused on distribution shifts. Therefore, in this work, we evaluate common adaptation protocols across distributions shifts and machine learning safety metrics (e.g., anomaly detection, calibration, robustness to corruptions). We find that protocols induce disparate trade-offs that were not apparent from prior evaluation. Further, we demonstrate that appropriate pairing of data augmentation and protocol can substantially mitigate this trade-off. Finally, we hypothesize and empirically see that using hardness-promoting augmentations during LP and then FT with augmentations may be particularly effective for trade-off mitigation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1167557379,
        "newsscientist":0.1716535758,
        "technologyreview":0.3136871296,
        "venturebeat":0.2871525122,
        "wired":0.2083230214,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12615v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658802784000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12259v1",
        "predicted_newsworthiness":0.5074911001,
        "title":"Surrogate Modeling of Melt Pool Thermal Field using Deep Learning",
        "summary":"Powder-based additive manufacturing has transformed the manufacturing industry over the last decade. In Laser Powder Bed Fusion, a specific part is built in an iterative manner in which two-dimensional cross-sections are formed on top of each other by melting and fusing the proper areas of the powder bed. In this process, the behavior of the melt pool and its thermal field has a very important role in predicting the quality of the manufactured part and its possible defects. However, the simulation of such a complex phenomenon is usually very time-consuming and requires huge computational resources. Flow-3D is one of the software packages capable of executing such simulations using iterative numerical solvers. In this work, we create three datasets of single-trail processes using Flow-3D and use them to train a convolutional neural network capable of predicting the behavior of the three-dimensional thermal field of the melt pool solely by taking three parameters as input: laser power, laser velocity, and time step. The CNN achieves a relative Root Mean Squared Error of 2% to 3% for the temperature field and an average Intersection over Union score of 80% to 90% in predicting the melt pool area. Moreover, since time is included as one of the inputs of the model, the thermal field can be instantly obtained for any arbitrary time step without the need to iterate and compute all the steps",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0796687541,
        "newsscientist":0.1465325791,
        "technologyreview":0.2380804786,
        "venturebeat":0.214044609,
        "wired":0.1523019636,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12259v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658762836000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12817v1",
        "predicted_newsworthiness":0.5074496672,
        "title":"Bodily Behaviors in Social Interaction: Novel Annotations and State-of-the-Art Evaluation",
        "summary":"Body language is an eye-catching social signal and its automatic analysis can significantly advance artificial intelligence systems to understand and actively participate in social interactions. While computer vision has made impressive progress in low-level tasks like head and body pose estimation, the detection of more subtle behaviors such as gesturing, grooming, or fumbling is not well explored. In this paper we present BBSI, the first set of annotations of complex Bodily Behaviors embedded in continuous Social Interactions in a group setting. Based on previous work in psychology, we manually annotated 26 hours of spontaneous human behavior in the MPIIGroupInteraction dataset with 15 distinct body language classes. We present comprehensive descriptive statistics on the resulting dataset as well as results of annotation quality evaluations. For automatic detection of these behaviors, we adapt the Pyramid Dilated Attention Network (PDAN), a state-of-the-art approach for human action detection. We perform experiments using four variants of spatial-temporal features as input to PDAN: Two-Stream Inflated 3D CNN, Temporal Segment Networks, Temporal Shift Module and Swin Transformer. Results are promising and indicate a great room for improvement in this difficult task. Representing a key piece in the puzzle towards automatic understanding of social behavior, BBSI is fully available to the research community.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1507807136,
        "newsscientist":0.1888199572,
        "technologyreview":0.2399696645,
        "venturebeat":0.2240989476,
        "wired":0.2007354337,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12817v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658834640000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12576v1",
        "predicted_newsworthiness":0.5070097774,
        "title":"WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models",
        "summary":"While vision-and-language models perform well on tasks such as visual question answering, they struggle when it comes to basic human commonsense reasoning skills. In this work, we introduce WinoGAViL: an online game to collect vision-and-language associations, (e.g., werewolves to a full moon), used as a dynamic benchmark to evaluate state-of-the-art models. Inspired by the popular card game Codenames, a spymaster gives a textual cue related to several visual candidates, and another player has to identify them. Human players are rewarded for creating associations that are challenging for a rival AI model but still solvable by other human players. We use the game to collect 3.5K instances, finding that they are intuitive for humans (>90% Jaccard index) but challenging for state-of-the-art AI models, where the best model (ViLT) achieves a score of 52%, succeeding mostly where the cue is visually salient. Our analysis as well as the feedback we collect from players indicate that the collected associations require diverse reasoning skills, including general knowledge, common sense, abstraction, and more. We release the dataset, the code and the interactive game, aiming to allow future data collection that can be used to develop models with better association abilities.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1689371934,
        "newsscientist":0.2403265841,
        "technologyreview":0.3609131936,
        "venturebeat":0.3394299081,
        "wired":0.280492619,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12576v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai",
            "cs.cv",
            "cs.hc"
        ],
        "published":1658793464000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14723v1",
        "predicted_newsworthiness":0.5064149622,
        "title":"Meta Reinforcement Learning with Successor Feature Based Context",
        "summary":"Most reinforcement learning (RL) methods only focus on learning a single task from scratch and are not able to use prior knowledge to learn other tasks more effectively. Context-based meta RL techniques are recently proposed as a possible solution to tackle this. However, they are usually less efficient than conventional RL and may require many trial-and-errors during training. To address this, we propose a novel meta-RL approach that achieves competitive performance comparing to existing meta-RL algorithms, while requires significantly fewer environmental interactions. By combining context variables with the idea of decomposing reward in successor feature framework, our method does not only learn high-quality policies for multiple tasks simultaneously but also can quickly adapt to new tasks with a small amount of training. Compared with state-of-the-art meta-RL baselines, we empirically show the effectiveness and data efficiency of our method on several continuous control tasks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1005921355,
        "newsscientist":0.1394935514,
        "technologyreview":0.250343718,
        "venturebeat":0.2288813407,
        "wired":0.1644699259,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14723v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659106367000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13398v1",
        "predicted_newsworthiness":0.5063116064,
        "title":"Emergent social NPC interactions in the Social NPCs Skyrim mod and beyond",
        "summary":"This work presents an implementation of a social architecture model for authoring Non-Player Character (NPC) in open world games inspired in academic research on agentbased modeling. Believable NPC authoring is burdensome in terms of rich dialogue and responsive behaviors. We briefly present the characteristics and advantages of using a social agent architecture for this task and describe an implementation of a social agent architecture CiF-CK released as a mod Social NPCs for The Elder Scrolls V: Skyrim",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.181415313,
        "newsscientist":0.1943178989,
        "technologyreview":0.2767890024,
        "venturebeat":0.3004958508,
        "wired":0.2815830041,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13398v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.hc"
        ],
        "published":1658914223000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.12112v1",
        "predicted_newsworthiness":0.5059738604,
        "title":"Active Learning Strategies for Weakly-supervised Object Detection",
        "summary":"Object detectors trained with weak annotations are affordable alternatives to fully-supervised counterparts. However, there is still a significant performance gap between them. We propose to narrow this gap by fine-tuning a base pre-trained weakly-supervised detector with a few fully-annotated samples automatically selected from the training set using ``box-in-box'' (BiB), a novel active learning strategy designed specifically to address the well-documented failure modes of weakly-supervised detectors. Experiments on the VOC07 and COCO benchmarks show that BiB outperforms other active learning techniques and significantly improves the base weakly-supervised detector's performance with only a few fully-annotated images per class. BiB reaches 97% of the performance of fully-supervised Fast RCNN with only 10% of fully-annotated images on VOC07. On COCO, using on average 10 fully-annotated images per class, or equivalently 1% of the training set, BiB also reduces the performance gap (in AP) between the weakly-supervised detector and the fully-supervised Fast RCNN by over 70%, showing a good trade-off between performance and data efficiency. Our code is publicly available at https:\/\/github.com\/huyvvo\/BiB.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0894791468,
        "newsscientist":0.1404847196,
        "technologyreview":0.2192275854,
        "venturebeat":0.189742361,
        "wired":0.1465656323,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12112v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658751721000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11720v1",
        "predicted_newsworthiness":0.5053991418,
        "title":"Progressive Feature Learning for Realistic Cloth-Changing Gait Recognition",
        "summary":"Gait recognition is instrumental in crime prevention and social security, for it can be conducted at a long distance without the cooperation of subjects. However, existing datasets and methods cannot deal with the most challenging problem in realistic gait recognition effectively: walking in different clothes (CL). In order to tackle this problem, we propose two benchmarks: CASIA-BN-RCC and OUMVLP-RCC, to simulate the cloth-changing condition in practice. The two benchmarks can force the algorithm to realize cross-view and cross-cloth with two sub-datasets. Furthermore, we propose a new framework that can be applied with off-the-shelf backbones to improve its performance in the Realistic Cloth-Changing problem with Progressive Feature Learning. Specifically, in our framework, we design Progressive Mapping and Progressive Uncertainty to extract the cross-view features and then extract cross-cloth features on the basis. In this way, the features from the cross-view sub-dataset can first dominate the feature space and relieve the uneven distribution caused by the adverse effect from the cross-cloth sub-dataset. The experiments on our benchmarks show that our framework can effectively improve the recognition performance in CL conditions. Our codes and datasets will be released after accepted.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1114397153,
        "newsscientist":0.1505566664,
        "technologyreview":0.2079549165,
        "venturebeat":0.1920810299,
        "wired":0.170617697,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11720v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658662013000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12062v1",
        "predicted_newsworthiness":0.5053189787,
        "title":"Meta Neural Ordinary Differential Equations For Adaptive Asynchronous Control",
        "summary":"Model-based Reinforcement Learning and Control have demonstrated great potential in various sequential decision making problem domains, including in robotics settings. However, real-world robotics systems often present challenges that limit the applicability of those methods. In particular, we note two problems that jointly happen in many industrial systems: 1) Irregular\/asynchronous observations and actions and 2) Dramatic changes in environment dynamics from an episode to another (e.g. varying payload inertial properties). We propose a general framework that overcomes those difficulties by meta-learning adaptive dynamics models for continuous-time prediction and control. We evaluate the proposed approach on a simulated industrial robot. Evaluations on real robotic systems will be added in future iterations of this pre-print.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1035619786,
        "newsscientist":0.1541085167,
        "technologyreview":0.2603095715,
        "venturebeat":0.2182621635,
        "wired":0.1918280374,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12062v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ro"
        ],
        "published":1658747249000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14096v2",
        "predicted_newsworthiness":0.5052457696,
        "title":"Towards Large-Scale Small Object Detection: Survey and Benchmarks",
        "summary":"With the rise of deep convolutional neural networks, object detection has achieved prominent advances in past years. However, such prosperity could not camouflage the unsatisfactory situation of Small Object Detection (SOD), one of the notoriously challenging tasks in computer vision, owing to the poor visual appearance and noisy representation caused by the intrinsic structure of small targets. In addition, large-scale dataset for benchmarking small object detection methods remains a bottleneck. In this paper, we first conduct a thorough review of small object detection. Then, to catalyze the development of SOD, we construct two large-scale Small Object Detection dAtasets (SODA), SODA-D and SODA-A, which focus on the Driving and Aerial scenarios respectively. SODA-D includes 24704 high-quality traffic images and 277596 instances of 9 categories. For SODA-A, we harvest 2510 high-resolution aerial images and annotate 800203 instances over 9 classes. The proposed datasets, as we know, are the first-ever attempt to large-scale benchmarks with a vast collection of exhaustively annotated instances tailored for multi-category SOD. Finally, we evaluate the performance of mainstream methods on SODA. We expect the released benchmarks could facilitate the development of SOD and spawn more breakthroughs in this field. Datasets and codes will be available soon at: \\url{https:\/\/shaunyuan22.github.io\/SODA}.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1111033019,
        "newsscientist":0.1810206868,
        "technologyreview":0.2614363397,
        "venturebeat":0.2426001522,
        "wired":0.2171824317,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14096v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659016938000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01166v1",
        "predicted_newsworthiness":0.5052124475,
        "title":"Ithaca365: Dataset and Driving Perception under Repeated and Challenging Weather Conditions",
        "summary":"Advances in perception for self-driving cars have accelerated in recent years due to the availability of large-scale datasets, typically collected at specific locations and under nice weather conditions. Yet, to achieve the high safety requirement, these perceptual systems must operate robustly under a wide variety of weather conditions including snow and rain. In this paper, we present a new dataset to enable robust autonomous driving via a novel data collection process - data is repeatedly recorded along a 15 km route under diverse scene (urban, highway, rural, campus), weather (snow, rain, sun), time (day\/night), and traffic conditions (pedestrians, cyclists and cars). The dataset includes images and point clouds from cameras and LiDAR sensors, along with high-precision GPS\/INS to establish correspondence across routes. The dataset includes road and object annotations using amodal masks to capture partial occlusions and 3D bounding boxes. We demonstrate the uniqueness of this dataset by analyzing the performance of baselines in amodal segmentation of road and objects, depth estimation, and 3D object detection. The repeated routes opens new research directions in object discovery, continual learning, and anomaly detection. Link to Ithaca365: https:\/\/ithaca365.mae.cornell.edu\/",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1370173594,
        "newsscientist":0.1961252331,
        "technologyreview":0.2993014572,
        "venturebeat":0.283778556,
        "wired":0.2557012645,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01166v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659394532000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11769v1",
        "predicted_newsworthiness":0.5046702354,
        "title":"CODiT: Conformal Out-of-Distribution Detection in Time-Series Data",
        "summary":"Machine learning models are prone to making incorrect predictions on inputs that are far from the training distribution. This hinders their deployment in safety-critical applications such as autonomous vehicles and healthcare. The detection of a shift from the training distribution of individual datapoints has gained attention. A number of techniques have been proposed for such out-of-distribution (OOD) detection. But in many applications, the inputs to a machine learning model form a temporal sequence. Existing techniques for OOD detection in time-series data either do not exploit temporal relationships in the sequence or do not provide any guarantees on detection. We propose using deviation from the in-distribution temporal equivariance as the non-conformity measure in conformal anomaly detection framework for OOD detection in time-series data.Computing independent predictions from multiple conformal detectors based on the proposed measure and combining these predictions by Fisher's method leads to the proposed detector CODiT with guarantees on false detection in time-series data. We illustrate the efficacy of CODiT by achieving state-of-the-art results on computer vision datasets in autonomous driving. We also show that CODiT can be used for OOD detection in non-vision datasets by performing experiments on the physiological GAIT sensory dataset. Code, data, and trained models are available at https:\/\/github.com\/kaustubhsridhar\/time-series-OOD.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1130337519,
        "newsscientist":0.1757153079,
        "technologyreview":0.2538479104,
        "venturebeat":0.2417850349,
        "wired":0.1933009103,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11769v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658680874000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12165v1",
        "predicted_newsworthiness":0.504548075,
        "title":"dCAM: Dimension-wise Class Activation Map for Explaining Multivariate Data Series Classification",
        "summary":"Data series classification is an important and challenging problem in data science. Explaining the classification decisions by finding the discriminant parts of the input that led the algorithm to some decisions is a real need in many applications. Convolutional neural networks perform well for the data series classification task; though, the explanations provided by this type of algorithm are poor for the specific case of multivariate data series. Addressing this important limitation is a significant challenge. In this paper, we propose a novel method that solves this problem by highlighting both the temporal and dimensional discriminant information. Our contribution is two-fold: we first describe a convolutional architecture that enables the comparison of dimensions; then, we propose a method that returns dCAM, a Dimension-wise Class Activation Map specifically designed for multivariate time series (and CNN-based models). Experiments with several synthetic and real datasets demonstrate that dCAM is not only more accurate than previous approaches, but the only viable solution for discriminant feature discovery and classification explanation in multivariate time series. This paper has appeared in SIGMOD'22.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.128170612,
        "newsscientist":0.1712743861,
        "technologyreview":0.2632004913,
        "venturebeat":0.2567300006,
        "wired":0.1792194091,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12165v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658754245000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01084v1",
        "predicted_newsworthiness":0.5033644779,
        "title":"Robotic Interestingness via Human-Informed Few-Shot Object Detection",
        "summary":"Interestingness recognition is crucial for decision making in autonomous exploration for mobile robots. Previous methods proposed an unsupervised online learning approach that can adapt to environments and detect interesting scenes quickly, but lack the ability to adapt to human-informed interesting objects. To solve this problem, we introduce a human-interactive framework, AirInteraction, that can detect human-informed objects via few-shot online learning. To reduce the communication bandwidth, we first apply an online unsupervised learning algorithm on the unmanned vehicle for interestingness recognition and then only send the potential interesting scenes to a base-station for human inspection. The human operator is able to draw and provide bounding box annotations for particular interesting objects, which are sent back to the robot to detect similar objects via few-shot learning. Only using few human-labeled examples, the robot can learn novel interesting object categories during the mission and detect interesting scenes that contain the objects. We evaluate our method on various interesting scene recognition datasets. To the best of our knowledge, it is the first human-informed few-shot object detection framework for autonomous exploration.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1241544361,
        "newsscientist":0.2246804613,
        "technologyreview":0.3045650446,
        "venturebeat":0.2804564244,
        "wired":0.2623137821,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01084v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659378981000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01910v1",
        "predicted_newsworthiness":0.5030101396,
        "title":"Multimodal Generation of Novel Action Appearances for Synthetic-to-Real Recognition of Activities of Daily Living",
        "summary":"Domain shifts, such as appearance changes, are a key challenge in real-world applications of activity recognition models, which range from assistive robotics and smart homes to driver observation in intelligent vehicles. For example, while simulations are an excellent way of economical data collection, a Synthetic-to-Real domain shift leads to a > 60% drop in accuracy when recognizing activities of Daily Living (ADLs). We tackle this challenge and introduce an activity domain generation framework which creates novel ADL appearances (novel domains) from different existing activity modalities (source domains) inferred from video training data. Our framework computes human poses, heatmaps of body joints, and optical flow maps and uses them alongside the original RGB videos to learn the essence of source domains in order to generate completely new ADL domains. The model is optimized by maximizing the distance between the existing source appearances and the generated novel appearances while ensuring that the semantics of an activity is preserved through an additional classification loss. While source data multimodality is an important concept in this design, our setup does not rely on multi-sensor setups, (i.e., all source modalities are inferred from a single video only.) The newly created activity domains are then integrated in the training of the ADL classification networks, resulting in models far less susceptible to changes in data distributions. Extensive experiments on the Synthetic-to-Real benchmark Sims4Action demonstrate the potential of the domain generation paradigm for cross-domain ADL recognition, setting new state-of-the-art results. Our code is publicly available at https:\/\/github.com\/Zrrr1997\/syn2real_DG",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1192711568,
        "newsscientist":0.1749133126,
        "technologyreview":0.2478732547,
        "venturebeat":0.2339122669,
        "wired":0.2035922627,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01910v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659515313000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00344v1",
        "predicted_newsworthiness":0.5024508613,
        "title":"Towards Intercultural Affect Recognition: Audio-Visual Affect Recognition in the Wild Across Six Cultures",
        "summary":"In our multicultural world, affect-aware AI systems that support humans need the ability to perceive affect across variations in emotion expression patterns across cultures. These models must perform well in cultural contexts on which they have not been trained. A standard assumption in affective computing is that affect recognition models trained and used within the same culture (intracultural) will perform better than models trained on one culture and used on different cultures (intercultural). We test this assumption and present the first systematic study of intercultural affect recognition models using videos of real-world dyadic interactions from six cultures. We develop an attention-based feature selection approach under temporal causal discovery to identify behavioral cues that can be leveraged in intercultural affect recognition models. Across all six cultures, our findings demonstrate that intercultural affect recognition models were as effective or more effective than intracultural models. We identify and contribute useful behavioral features for intercultural affect recognition; facial features from the visual modality were more useful than the audio modality in this study's context. Our paper presents a proof-of-concept and motivation for the future development of intercultural affect recognition systems.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1849243161,
        "newsscientist":0.206333183,
        "technologyreview":0.2825213408,
        "venturebeat":0.2699810044,
        "wired":0.2324288074,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00344v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.hc",
            "cs.lg"
        ],
        "published":1659235157000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12720v1",
        "predicted_newsworthiness":0.5021041186,
        "title":"Convolutional neural networks and multi-threshold analysis for contamination detection in the apparel industry",
        "summary":"Quality control of apparel items is mandatory in modern textile industry, as consumer's awareness and expectations about the highest possible standard is constantly increasing in favor of sustainable and ethical textile products. Such a level of quality is achieved by checking the product throughout its life cycle, from raw materials to boxed stock. Checks may include color shading tests, fasteners fatigue tests, fabric weigh tests, contamination tests, etc. This work deals specifically with the automatic detection of contaminations given by small parts in the finished product such as raw material like little stones and plastic bits or materials from the construction process, like a whole needle or a clip. Identification is performed by a two-level processing of X-ray images of the items: in the first, a multi-threshold analysis recognizes the contaminations by gray level and shape attributes; the second level consists of a deep learning classifier that has been trained to distinguish between true positives and false positives. The automatic detector was successfully deployed in an actual production plant, since the results satisfy the technical specification of the process, namely a number of false negatives smaller than 3% and a number of false positives smaller than 15%.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1549100991,
        "newsscientist":0.195151778,
        "technologyreview":0.250476699,
        "venturebeat":0.2247521406,
        "wired":0.1694311781,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12720v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658823701000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13362v1",
        "predicted_newsworthiness":0.5018770957,
        "title":"Camouflaged Object Detection via Context-aware Cross-level Fusion",
        "summary":"Camouflaged object detection (COD) aims to identify the objects that conceal themselves in natural scenes. Accurate COD suffers from a number of challenges associated with low boundary contrast and the large variation of object appearances, e.g., object size and shape. To address these challenges, we propose a novel Context-aware Cross-level Fusion Network (C2F-Net), which fuses context-aware cross-level features for accurately identifying camouflaged objects. Specifically, we compute informative attention coefficients from multi-level features with our Attention-induced Cross-level Fusion Module (ACFM), which further integrates the features under the guidance of attention coefficients. We then propose a Dual-branch Global Context Module (DGCM) to refine the fused features for informative feature representations by exploiting rich global context information. Multiple ACFMs and DGCMs are integrated in a cascaded manner for generating a coarse prediction from high-level features. The coarse prediction acts as an attention map to refine the low-level features before passing them to our Camouflage Inference Module (CIM) to generate the final prediction. We perform extensive experiments on three widely used benchmark datasets and compare C2F-Net with state-of-the-art (SOTA) models. The results show that C2F-Net is an effective COD model and outperforms SOTA models remarkably. Further, an evaluation on polyp segmentation datasets demonstrates the promising potentials of our C2F-Net in COD downstream applications. Our code is publicly available at: https:\/\/github.com\/Ben57882\/C2FNet-TSCVT.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0923098315,
        "newsscientist":0.1518222044,
        "technologyreview":0.2360991238,
        "venturebeat":0.2066477458,
        "wired":0.1675864055,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13362v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658910856000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12080v1",
        "predicted_newsworthiness":0.5016476869,
        "title":"Intention-Conditioned Long-Term Human Egocentric Action Forecasting @ EGO4D Challenge 2022",
        "summary":"To anticipate how a human would act in the future, it is essential to understand the human intention since it guides the human towards a certain goal. In this paper, we propose a hierarchical architecture which assumes a sequence of human action (low-level) can be driven from the human intention (high-level). Based on this, we deal with Long-Term Action Anticipation task in egocentric videos. Our framework first extracts two level of human information over the N observed videos human actions through a Hierarchical Multi-task MLP Mixer (H3M). Then, we condition the uncertainty of the future through an Intention-Conditioned Variational Auto-Encoder (I-CVAE) that generates K stable predictions of the next Z=20 actions that the observed human might perform. By leveraging human intention as high-level information, we claim that our model is able to anticipate more time-consistent actions in the long-term, thus improving the results over baseline methods in EGO4D Challenge. This work ranked first in the EGO4D LTA Challenge by providing more plausible anticipated sequences, improving the anticipation of nouns and overall actions. The code is available at https:\/\/github.com\/Evm7\/ego4dlta-icvae.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1198288235,
        "newsscientist":0.1830629884,
        "technologyreview":0.2451412582,
        "venturebeat":0.2272465362,
        "wired":0.2052798394,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12080v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658750221000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14315v1",
        "predicted_newsworthiness":0.5012964665,
        "title":"SPot-the-Difference Self-Supervised Pre-training for Anomaly Detection and Segmentation",
        "summary":"Visual anomaly detection is commonly used in industrial quality inspection. In this paper, we present a new dataset as well as a new self-supervised learning method for ImageNet pre-training to improve anomaly detection and segmentation in 1-class and 2-class 5\/10\/high-shot training setups. We release the Visual Anomaly (VisA) Dataset consisting of 10,821 high-resolution color images (9,621 normal and 1,200 anomalous samples) covering 12 objects in 3 domains, making it the largest industrial anomaly detection dataset to date. Both image and pixel-level labels are provided. We also propose a new self-supervised framework - SPot-the-difference (SPD) - which can regularize contrastive self-supervised pre-training, such as SimSiam, MoCo and SimCLR, to be more suitable for anomaly detection tasks. Our experiments on VisA and MVTec-AD dataset show that SPD consistently improves these contrastive pre-training baselines and even the supervised pre-training. For example, SPD improves Area Under the Precision-Recall curve (AU-PR) for anomaly segmentation by 5.9% and 6.8% over SimSiam and supervised pre-training respectively in the 2-class high-shot regime. We open-source the project at http:\/\/github.com\/amazon-research\/spot-diff .",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1022035577,
        "newsscientist":0.1622593234,
        "technologyreview":0.2526241709,
        "venturebeat":0.2325645297,
        "wired":0.1684471474,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14315v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659031203000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00102v1",
        "predicted_newsworthiness":0.5011323806,
        "title":"An Open Source Interactive Visual Analytics Tool for Comparative Programming Comprehension",
        "summary":"This paper proposes an open source visual analytics tool consisting of several views and perspectives on eye movement data collected during code reading tasks when writing computer programs. Hence the focus of this work is on code and program comprehension. The source code is shown as a visual stimulus. It can be inspected in combination with overlaid scanpaths in which the saccades can be visually encoded in several forms, including straight, curved, and orthogonal lines, modifiable by interaction techniques. The tool supports interaction techniques like filter functions, aggregations, data sampling, and many more. We illustrate the usefulness of our tool by applying it to the eye movements of 216 programmers of multiple expertise levels that were collected during two code comprehension tasks. Our tool helped to analyze the difference between the strategic program comprehension of programmers based on their demographic background, time taken to complete the task, choice of programming task, and expertise.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1347708655,
        "newsscientist":0.152568272,
        "technologyreview":0.2292103362,
        "venturebeat":0.2305655818,
        "wired":0.1998808949,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00102v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ir"
        ],
        "published":1659137028000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.00848v1",
        "predicted_newsworthiness":0.5011202827,
        "title":"DeFL: Decentralized Weight Aggregation for Cross-silo Federated Learning",
        "summary":"Federated learning (FL) is an emerging promising paradigm of privacy-preserving machine learning (ML). An important type of FL is cross-silo FL, which enables a small scale of organizations to cooperatively train a shared model by keeping confidential data locally and aggregating weights on a central parameter server. However, the central server may be vulnerable to malicious attacks or software failures in practice. To address this issue, in this paper, we propose DeFL, a novel decentralized weight aggregation framework for cross-silo FL. DeFL eliminates the central server by aggregating weights on each participating node and weights of only the current training round are maintained and synchronized among all nodes. We use Multi-Krum to enable aggregating correct weights from honest nodes and use HotStuff to ensure the consistency of the training round number and weights among all nodes. Besides, we theoretically analyze the Byzantine fault tolerance, convergence, and complexity of DeFL. We conduct extensive experiments over two widely-adopted public datasets, i.e. CIFAR-10 and Sentiment140, to evaluate the performance of DeFL. Results show that DeFL defends against common threat models with minimal accuracy loss, and achieves up to 100x reduction in storage overhead and up to 12x reduction in network overhead, compared to state-of-the-art decentralized FL approaches.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1477998749,
        "newsscientist":0.1710535235,
        "technologyreview":0.2936651762,
        "venturebeat":0.3019206577,
        "wired":0.2492941872,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00848v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659361009000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01996v1",
        "predicted_newsworthiness":0.5010574692,
        "title":"Adaptive Domain Generalization via Online Disagreement Minimization",
        "summary":"Deep neural networks suffer from significant performance deterioration when there exists distribution shift between deployment and training. Domain Generalization (DG) aims to safely transfer a model to unseen target domains by only relying on a set of source domains. Although various DG approaches have been proposed, a recent study named DomainBed, reveals that most of them do not beat the simple Empirical Risk Minimization (ERM). To this end, we propose a general framework that is orthogonal to existing DG algorithms and could improve their performance consistently. Unlike previous DG works that stake on a static source model to be hopefully a universal one, our proposed AdaODM adaptively modifies the source model at test time for different target domains. Specifically, we create multiple domain-specific classifiers upon a shared domain-generic feature extractor. The feature extractor and classifiers are trained in an adversarial way, where the feature extractor embeds the input samples into a domain-invariant space, and the multiple classifiers capture the distinct decision boundaries that each of them relates to a specific source domain. During testing, distribution differences between target and source domains could be effectively measured by leveraging prediction disagreement among source classifiers. By fine-tuning source models to minimize the disagreement at test time, target domain features are well aligned to the invariant feature space. We verify AdaODM on two popular DG methods, namely ERM and CORAL, and four DG benchmarks, namely VLCS, PACS, OfficeHome, and TerraIncognita. The results show AdaODM stably improves the generalization capacity on unseen domains and achieves state-of-the-art performance.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1028014484,
        "newsscientist":0.1469453542,
        "technologyreview":0.2998480435,
        "venturebeat":0.2755838522,
        "wired":0.1874162419,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01996v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659527471000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14083v1",
        "predicted_newsworthiness":0.500585364,
        "title":"Weakly-Supervised Camouflaged Object Detection with Scribble Annotations",
        "summary":"Existing camouflaged object detection (COD) methods rely heavily on large-scale datasets with pixel-wise annotations. However, due to the ambiguous boundary, it is very time-consuming and labor-intensive to annotate camouflage objects pixel-wisely (which takes ~ 60 minutes per image). In this paper, we propose the first weakly-supervised camouflaged object detection (COD) method, using scribble annotations as supervision. To achieve this, we first construct a scribble-based camouflaged object dataset with 4,040 images and corresponding scribble annotations. It is worth noting that annotating the scribbles used in our dataset takes only ~ 10 seconds per image, which is 360 times faster than per-pixel annotations. However, the network directly using scribble annotations for supervision will fail to localize the boundary of camouflaged objects and tend to have inconsistent predictions since scribble annotations only describe the primary structure of objects without details. To tackle this problem, we propose a novel consistency loss composed of two parts: a reliable cross-view loss to attain reliable consistency over different images, and a soft inside-view loss to maintain consistency inside a single prediction map. Besides, we observe that humans use semantic information to segment regions near boundaries of camouflaged objects. Therefore, we design a feature-guided loss, which includes visual features directly extracted from images and semantically significant features captured by models. Moreover, we propose a novel network that detects camouflaged objects by scribble learning on structural information and semantic relations. Experimental results show that our model outperforms relevant state-of-the-art methods on three COD benchmarks with an average improvement of 11.0% on MAE, 3.2% on S-measure, 2.5% on E-measure and 4.4% on weighted F-measure.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0992731509,
        "newsscientist":0.1550351597,
        "technologyreview":0.2313495334,
        "venturebeat":0.1943752524,
        "wired":0.1628568754,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14083v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659015607000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00788v1",
        "predicted_newsworthiness":0.5004448056,
        "title":"A Hybrid CNN-LSTM model for Video Deepfake Detection by Leveraging Optical Flow Features",
        "summary":"Deepfakes are the synthesized digital media in order to create ultra-realistic fake videos to trick the spectator. Deep generative algorithms, such as, Generative Adversarial Networks(GAN) are widely used to accomplish such tasks. This approach synthesizes pseudo-realistic contents that are very difficult to distinguish by traditional detection methods. In most cases, Convolutional Neural Network(CNN) based discriminators are being used for detecting such synthesized media. However, it emphasise primarily on the spatial attributes of individual video frames, thereby fail to learn the temporal information from their inter-frame relations. In this paper, we leveraged an optical flow based feature extraction approach to extract the temporal features, which are then fed to a hybrid model for classification. This hybrid model is based on the combination of CNN and recurrent neural network (RNN) architectures. The hybrid model provides effective performance on open source data-sets such as, DFDC, FF++ and Celeb-DF. This proposed method shows an accuracy of 66.26%, 91.21% and 79.49% in DFDC, FF++, and Celeb-DF respectively with a very reduced No of sample size of approx 100 samples(frames). This promises early detection of fake contents compared to existing modalities.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1236057586,
        "newsscientist":0.1766247627,
        "technologyreview":0.2492137337,
        "venturebeat":0.2297704111,
        "wired":0.2094972608,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00788v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659001089000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14403v1",
        "predicted_newsworthiness":0.5004033732,
        "title":"Interactive Evaluation of Dialog Track at DSTC9",
        "summary":"The ultimate goal of dialog research is to develop systems that can be effectively used in interactive settings by real users. To this end, we introduced the Interactive Evaluation of Dialog Track at the 9th Dialog System Technology Challenge. This track consisted of two sub-tasks. The first sub-task involved building knowledge-grounded response generation models. The second sub-task aimed to extend dialog models beyond static datasets by assessing them in an interactive setting with real users. Our track challenges participants to develop strong response generation models and explore strategies that extend them to back-and-forth interactions with real users. The progression from static corpora to interactive evaluation introduces unique challenges and facilitates a more thorough assessment of open-domain dialog systems. This paper provides an overview of the track, including the methodology and results. Furthermore, it provides insights into how to best evaluate open-domain dialog models",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1322595085,
        "newsscientist":0.1533361829,
        "technologyreview":0.2613064436,
        "venturebeat":0.2871194555,
        "wired":0.2515716392,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14403v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659048844000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.13664v1",
        "predicted_newsworthiness":0.5003637881,
        "title":"Generic Approach to Visualization of Time Series Data",
        "summary":"Time series is a collection of data instances that are ordered according to a time stamp. Stock prices, temperature, etc are examples of time series data in real life. Time series data are used for forecasting sales, predicting trends. Visualization is the process of visually representing data or the relationship between features of a data either in a two-dimensional plot or a three-dimensional plot. Visualizing the time series data constitutes an important part of the process for working with a time series dataset. Visualizing the data not only helps in the modelling process but it can also be used to identify trends and features that cause those trends. In this work, we take a real-life time series dataset and analyse how the target feature relates to other features of the dataset through visualization. From the work that has been carried out, we present an effective method of visualization for time series data which will be much useful for machine learning modelling with such datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1495452562,
        "newsscientist":0.1853017017,
        "technologyreview":0.2477189694,
        "venturebeat":0.2501728975,
        "wired":0.192022073,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13664v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1658732903000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.14704v1",
        "predicted_newsworthiness":0.5001643732,
        "title":"Understanding the Relation of User and News Representations in Content-Based Neural News Recommendation",
        "summary":"A number of models for neural content-based news recommendation have been proposed. However, there is limited understanding of the relative importances of the three main components of such systems (news encoder, user encoder, and scoring function) and the trade-offs involved. In this paper, we assess the hypothesis that the most widely used means of matching user and candidate news representations is not expressive enough. We allow our system to model more complex relations between the two by assessing more expressive scoring functions. Across a wide range of baseline and established systems this results in consistent improvements of around 6 points in AUC. Our results also indicate a trade-off between the complexity of news encoder and scoring function: A fairly simple baseline model scores well above 68% AUC on the MIND dataset and comes within 2 points of the published state-of-the-art, while requiring a fraction of the computational costs.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.147293423,
        "newsscientist":0.1525243891,
        "technologyreview":0.2568997959,
        "venturebeat":0.2563295572,
        "wired":0.2422920993,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14704v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.cl"
        ],
        "published":1659104665000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.00392v1",
        "predicted_newsworthiness":0.499909632,
        "title":"PVBM: A Python Vasculature Biomarker Toolbox Based On Retinal Blood Vessel Segmentation",
        "summary":"Introduction: Blood vessels can be non-invasively visualized from a digital fundus image (DFI). Several studies have shown an association between cardiovascular risk and vascular features obtained from DFI. Recent advances in computer vision and image segmentation enable automatising DFI blood vessel segmentation. There is a need for a resource that can automatically compute digital vasculature biomarkers (VBM) from these segmented DFI. Methods: In this paper, we introduce a Python Vasculature BioMarker toolbox, denoted PVBM. A total of 11 VBMs were implemented. In particular, we introduce new algorithmic methods to estimate tortuosity and branching angles. Using PVBM, and as a proof of usability, we analyze geometric vascular differences between glaucomatous patients and healthy controls. Results: We built a fully automated vasculature biomarker toolbox based on DFI segmentations and provided a proof of usability to characterize the vascular changes in glaucoma. For arterioles and venules, all biomarkers were significant and lower in glaucoma patients compared to healthy controls except for tortuosity, venular singularity length and venular branching angles. Conclusion: We have automated the computation of 11 VBMs from retinal blood vessel segmentation. The PVBM toolbox is made open source under a GNU GPL 3 license and is available on physiozoo.com (following publication).",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0971728386,
        "newsscientist":0.1497532749,
        "technologyreview":0.169734384,
        "venturebeat":0.1509594914,
        "wired":0.1174164329,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00392v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659255779000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.09858v2",
        "predicted_newsworthiness":0.4996923465,
        "title":"UniHPF : Universal Healthcare Predictive Framework with Zero Domain Knowledge",
        "summary":"Despite the abundance of Electronic Healthcare Records (EHR), its heterogeneity restricts the utilization of medical data in building predictive models. To address this challenge, we propose Universal Healthcare Predictive Framework (UniHPF), which requires no medical domain knowledge and minimal pre-processing for multiple prediction tasks. Experimental results demonstrate that UniHPF is capable of building large-scale EHR models that can process any form of medical data from distinct EHR systems. Our framework significantly outperforms baseline models in multi-source learning tasks, including transfer and pooled learning, while also showing comparable results when trained on a single medical dataset. To empirically demonstrate the efficacy of our work, we conducted extensive experiments using various datasets, model structures, and tasks. We believe that our findings can provide helpful insights for further research on the multi-source learning of EHRs.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1531349821,
        "newsscientist":0.1796452413,
        "technologyreview":0.2816087974,
        "venturebeat":0.2918297854,
        "wired":0.1990018978,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.09858v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658321186000,
        "published_hr":"Jul 20, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11931v1",
        "predicted_newsworthiness":0.4989305867,
        "title":"Hybrid Classifiers for Spatio-temporal Real-time Abnormal Behaviors Detection, Tracking, and Recognition in Massive Hajj Crowds",
        "summary":"Individual abnormal behaviors vary depending on crowd sizes, contexts, and scenes. Challenges such as partial occlusions, blurring, large-number abnormal behavior, and camera viewing occur in large-scale crowds when detecting, tracking, and recognizing individuals with abnormal behaviors. In this paper, our contribution is twofold. First, we introduce an annotated and labeled large-scale crowd abnormal behaviors Hajj dataset (HAJJv2). Second, we propose two methods of hybrid Convolutional Neural Networks (CNNs) and Random Forests (RFs) to detect and recognize Spatio-temporal abnormal behaviors in small and large-scales crowd videos. In small-scale crowd videos, a ResNet-50 pre-trained CNN model is fine-tuned to verify whether every frame is normal or abnormal in the spatial domain. If anomalous behaviors are observed, a motion-based individuals detection method based on the magnitudes and orientations of Horn-Schunck optical flow is used to locate and track individuals with abnormal behaviors. A Kalman filter is employed in large-scale crowd videos to predict and track the detected individuals in the subsequent frames. Then, means, variances, and standard deviations statistical features are computed and fed to the RF to classify individuals with abnormal behaviors in the temporal domain. In large-scale crowds, we fine-tune the ResNet-50 model using YOLOv2 object detection technique to detect individuals with abnormal behaviors in the spatial domain.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1847372474,
        "newsscientist":0.1897585951,
        "technologyreview":0.2476686434,
        "venturebeat":0.2142228041,
        "wired":0.2214983333,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11931v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658731975000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01174v1",
        "predicted_newsworthiness":0.4983928172,
        "title":"TextWorldExpress: Simulating Text Games at One Million Steps Per Second",
        "summary":"Text-based games offer a challenging test bed to evaluate virtual agents at language understanding, multi-step problem-solving, and common-sense reasoning. However, speed is a major limitation of current text-based games, capping at 300 steps per second, mainly due to the use of legacy tooling. In this work we present TextWorldExpress, a high-performance implementation of three common text game benchmarks that increases simulation throughput by approximately three orders of magnitude, reaching over one million steps per second on common desktop hardware. This significantly reduces experiment runtime, enabling billion-step-scale experiments in about one day.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1413612245,
        "newsscientist":0.2146461895,
        "technologyreview":0.3210595218,
        "venturebeat":0.3228197229,
        "wired":0.2786533303,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01174v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1659397428000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.13427v1",
        "predicted_newsworthiness":0.4980978874,
        "title":"An Object Deformation-Agnostic Framework for Human-Robot Collaborative Transportation",
        "summary":"In this study, an adaptive object deformability-agnostic human-robot collaborative transportation framework is presented. The proposed framework enables to combine the haptic information transferred through the object with the human kinematic information obtained from a motion capture system to generate reactive whole-body motions on a mobile collaborative robot. Furthermore, it allows rotating the objects in an intuitive and accurate way during co-transportation based on an algorithm that detects the human rotation intention using the torso and hand movements. First, we validate the framework with the two extremities of the object deformability range (i.e, purely rigid aluminum rod and highly deformable rope) by utilizing a mobile manipulator which consists of an Omni-directional mobile base and a collaborative robotic arm. Next, its performance is compared with an admittance controller during a co-carry task of a partially deformable object in a 12-subjects user study. Quantitative and qualitative results of this experiment show that the proposed framework can effectively handle the transportation of objects regardless of their deformability and provides intuitive assistance to human partners. Finally, we have demonstrated the potential of our framework in a different scenario, where the human and the robot co-transport a manikin using a deformable sheet.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0987483016,
        "newsscientist":0.1557677302,
        "technologyreview":0.2133253706,
        "venturebeat":0.186929625,
        "wired":0.182072454,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13427v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658916524000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13443v1",
        "predicted_newsworthiness":0.4976314994,
        "title":"Lecture Notes on Neural Information Retrieval",
        "summary":"These lecture notes focus on the recent advancements in neural information retrieval, with particular emphasis on the systems and models exploiting transformer networks. These networks, originally proposed by Google in 2017, have seen a large success in many natural language processing and information retrieval tasks. While there are many fantastic textbook on information retrieval and natural language processing as well as specialised books for a more advanced audience, these lecture notes target people aiming at developing a basic understanding of the main information retrieval techniques and approaches based on deep learning. These notes have been prepared for a IR graduate course of the MSc program in Artificial Intelligence and Data Engineering at the University of Pisa, Italy.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1298452994,
        "newsscientist":0.1547376784,
        "technologyreview":0.2700509959,
        "venturebeat":0.2654530562,
        "wired":0.1932424572,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13443v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1658918607000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.13591v1",
        "predicted_newsworthiness":0.4969689047,
        "title":"RobotIO: A Python Library for Robot Manipulation Experiments",
        "summary":"Setting up robot environments to quickly test newly developed algorithms is still a difficult and time consuming process. This presents a significant hurdle to researchers interested in performing real-world robotic experiments. RobotIO is a python library designed to solve this problem. It focuses on providing common, simple, and well structured python interfaces for robots, grippers, and cameras, etc. These are provided with implementations of these interfaces for common hardware. This enables code using RobotIO to be portable across different robot setups. In terms of architecture, RobotIO is designed to be compatible with OpenAI gym environments, as well as ROS; examples of both of these are provided. The library comes together with a number of helpful tools, such as camera calibration scripts and episode recording functionality that further support algorithm development.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.099984398,
        "newsscientist":0.1971491869,
        "technologyreview":0.2784982249,
        "venturebeat":0.2329766592,
        "wired":0.2237763842,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13591v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658936773000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.10524v2",
        "predicted_newsworthiness":0.4969090706,
        "title":"NusaCrowd: A Call for Open and Reproducible NLP Research in Indonesian Languages",
        "summary":"At the center of the underlying issues that halt Indonesian natural language processing (NLP) research advancement, we find data scarcity. Resources in Indonesian languages, especially the local ones, are extremely scarce and underrepresented. Many Indonesian researchers do not publish their dataset. Furthermore, the few public datasets that we have are scattered across different platforms, thus makes performing reproducible and data-centric research in Indonesian NLP even more arduous. Rising to this challenge, we initiate the first Indonesian NLP crowdsourcing effort, NusaCrowd. NusaCrowd strives to provide the largest datasheets aggregation with standardized data loading for NLP tasks in all Indonesian languages. By enabling open and centralized access to Indonesian NLP resources, we hope NusaCrowd can tackle the data scarcity problem hindering NLP progress in Indonesia and bring NLP practitioners to move towards collaboration.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1815287907,
        "newsscientist":0.1679946181,
        "technologyreview":0.2526145598,
        "venturebeat":0.2511655606,
        "wired":0.2173252593,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.10524v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1658415942000,
        "published_hr":"Jul 21, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.01455v1",
        "predicted_newsworthiness":0.4964278161,
        "title":"CathSim: An Open-source Simulator for Autonomous Cannulation",
        "summary":"Autonomous robots in endovascular operations have the potential to navigate circulatory systems safely and reliably while decreasing the susceptibility to human errors. However, there are numerous challenges involved with the process of training such robots such as long training duration due to sample inefficiency of machine learning algorithms and safety issues arising from the interaction between the catheter and the endovascular phantom. Physics simulators have been used in the context of endovascular procedures, but they are typically employed for staff training and generally do not conform to the autonomous cannulation goal. Furthermore, most current simulators are closed-source which hinders the collaborative development of safe and reliable autonomous systems. In this work, we introduce CathSim, an open-source simulation environment that accelerates the development of machine learning algorithms for autonomous endovascular navigation. We first simulate the high-fidelity catheter and aorta with the state-of-the-art endovascular robot. We then provide the capability of real-time force sensing between the catheter and the aorta in the simulation environment. We validate our simulator by conducting two different catheterisation tasks within two primary arteries using two popular reinforcement learning algorithms, Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC). The experimental results show that using our open-source simulator, we can successfully train the reinforcement learning agents to perform different autonomous cannulation tasks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1173389844,
        "newsscientist":0.1926267384,
        "technologyreview":0.2875655913,
        "venturebeat":0.2423226474,
        "wired":0.2014762078,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01455v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659448024000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00116v1",
        "predicted_newsworthiness":0.4961751325,
        "title":"Adaptive Feature Fusion for Cooperative Perception using LiDAR Point Clouds",
        "summary":"Cooperative perception allows a Connected Autonomous Vehicle (CAV) to interact with the other CAVs in the vicinity to enhance perception of surrounding objects to increase safety and reliability. It can compensate for the limitations of the conventional vehicular perception such as blind spots, low resolution, and weather effects. An effective feature fusion model for the intermediate fusion methods of cooperative perception can improve feature selection and information aggregation to further enhance the perception accuracy. We propose adaptive feature fusion models with trainable feature selection modules. One of our proposed models Spatial-wise Adaptive feature Fusion (S-AdaFusion) outperforms all other state-of-the-art models on the two subsets of OPV2V dataset: default CARLA towns for vehicle detection and the Culver City for domain adaptation. In addition, previous studies have only tested cooperative perception for vehicle detection. A pedestrian, however, is much more likely to be seriously injured in a traffic accident. We evaluate the performance of cooperative perception for both vehicle and pedestrian detection using the CODD dataset. Our architecture achieves higher Average Precision (AP) than other existing models for both vehicle and pedestrian detection on the CODD dataset. The experiments demonstrate that cooperative perception also can improve the pedestrian detection accuracy compared to the conventional perception process.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1005004439,
        "newsscientist":0.1601641913,
        "technologyreview":0.26390327,
        "venturebeat":0.2512328295,
        "wired":0.2156063266,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00116v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659145985000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12939v1",
        "predicted_newsworthiness":0.4960779573,
        "title":"Semantic Segmentation for Autonomous Driving: Model Evaluation, Dataset Generation, Perspective Comparison, and Real-Time Capability",
        "summary":"Environmental perception is an important aspect within the field of autonomous vehicles that provides crucial information about the driving domain, including but not limited to identifying clear driving areas and surrounding obstacles. Semantic segmentation is a widely used perception method for self-driving cars that associates each pixel of an image with a predefined class. In this context, several segmentation models are evaluated regarding accuracy and efficiency. Experimental results on the generated dataset confirm that the segmentation model FasterSeg is fast enough to be used in realtime on lowpower computational (embedded) devices in self-driving cars. A simple method is also introduced to generate synthetic training data for the model. Moreover, the accuracy of the first-person perspective and the bird's eye view perspective are compared. For a $320 \\times 256$ input in the first-person perspective, FasterSeg achieves $65.44\\,\\%$ mean Intersection over Union (mIoU), and for a $320 \\times 256$ input from the bird's eye view perspective, FasterSeg achieves $64.08\\,\\%$ mIoU. Both perspectives achieve a frame rate of $247.11$ Frames per Second (FPS) on the NVIDIA Jetson AGX Xavier. Lastly, the frame rate and the accuracy with respect to the arithmetic 16-bit Floating Point (FP16) and 32-bit Floating Point (FP32) of both perspectives are measured and compared on the target hardware.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0942907799,
        "newsscientist":0.1538603108,
        "technologyreview":0.2656240964,
        "venturebeat":0.2564263074,
        "wired":0.2224295049,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12939v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.ro"
        ],
        "published":1658846744000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01358v1",
        "predicted_newsworthiness":0.4955288817,
        "title":"What can we Learn by Predicting Accuracy?",
        "summary":"This paper seeks to answer the following question: \"What can we learn by predicting accuracy?\" Indeed, classification is one of the most popular task in machine learning and many loss functions have been developed to maximize this non-differentiable objective. Unlike past work on loss function design, which was mostly guided by intuition and theory before being validated by experimentation, here we propose to approach this problem in the opposite way : we seek to extract knowledge from experiments. This data-driven approach is similar to that used in physics to discover general laws from data. We used a symbolic regression method to automatically find a mathematical expression that is highly correlated with the accuracy of a linear classifier. The formula discovered on more than 260 datasets has a Pearson correlation of 0.96 and a r2 of 0.93. More interestingly, this formula is highly explainable and confirms insights from various previous papers on loss design. We hope this work will open new perspectives in the search for new heuristics leading to a deeper understanding of machine learning theory.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1346908023,
        "newsscientist":0.1959371119,
        "technologyreview":0.3355642077,
        "venturebeat":0.3033229937,
        "wired":0.216615031,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01358v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1659437897000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01294v1",
        "predicted_newsworthiness":0.4953390168,
        "title":"Understanding the classes better with class-specific and rule-specific feature selection, and redundancy control in a fuzzy rule based framework",
        "summary":"Recently, several studies have claimed that using class-specific feature subsets provides certain advantages over using a single feature subset for representing the data for a classification problem. Unlike traditional feature selection methods, the class-specific feature selection methods select an optimal feature subset for each class. Typically class-specific feature selection (CSFS) methods use one-versus-all split of the data set that leads to issues such as class imbalance, decision aggregation, and high computational overhead. We propose a class-specific feature selection method embedded in a fuzzy rule-based classifier, which is free from the drawbacks associated with most existing class-specific methods. Additionally, our method can be adapted to control the level of redundancy in the class-specific feature subsets by adding a suitable regularizer to the learning objective. Our method results in class-specific rules involving class-specific subsets. We also propose an extension where different rules of a particular class are defined by different feature subsets to model different substructures within the class. The effectiveness of the proposed method has been validated through experiments on three synthetic data sets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0828796416,
        "newsscientist":0.1245256686,
        "technologyreview":0.2045326674,
        "venturebeat":0.2022724031,
        "wired":0.1308747632,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01294v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659426334000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13117v1",
        "predicted_newsworthiness":0.4950657463,
        "title":"Users and Contemporary SERPs: A (Re-)Investigation Examining User Interactions and Experiences",
        "summary":"The Search Engine Results Page (SERP) has evolved significantly over the last two decades, moving away from the simple ten blue links paradigm to considerably more complex presentations that contain results from multiple verticals and granularities of textual information. Prior works have investigated how user interactions on the SERP are influenced by the presence or absence of heterogeneous content (e.g., images, videos, or news content), the layout of the SERP (list vs. grid layout), and task complexity. In this paper, we reproduce the user studies conducted in prior works-specifically those of Arguello et al. [4] and Siu and Chaparro [29]-to explore to what extent the findings from research conducted five to ten years ago still hold today as the average web user has become accustomed to SERPs with ever-increasing presentational complexity. To this end, we designed and ran a user study with four different SERP interfaces: (i) a heterogeneous grid; (ii) a heterogeneous list; (iii) a simple grid; and (iv) a simple list. We collected the interactions of 41 study participants over 12 search tasks for our analyses. We observed that SERP types and task complexity affect user interactions with search results. We also find evidence to support most (6 out of 8) observations from [4 , 29] indicating that user interactions with different interfaces and to solve tasks of different complexity have remained mostly similar over time.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1475346606,
        "newsscientist":0.1567216726,
        "technologyreview":0.2311968423,
        "venturebeat":0.2513298375,
        "wired":0.2278569851,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13117v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.hc"
        ],
        "published":1658858802000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.07765v2",
        "predicted_newsworthiness":0.4947922512,
        "title":"FairFuse: Interactive Visual Support for Fair Consensus Ranking",
        "summary":"Fair consensus building combines the preferences of multiple rankers into a single consensus ranking, while ensuring any group defined by a protected attribute (such as race or gender) is not disadvantaged compared to other groups. Manually generating a fair consensus ranking is time-consuming and impractical -- even for a fairly small number of candidates. While algorithmic approaches for auditing and generating fair consensus rankings have been developed, these have not been operationalized in interactive systems. To bridge this gap, we introduce FairFuse, a visualization system for generating, analyzing, and auditing fair consensus rankings. We construct a data model which includes base rankings entered by rankers, augmented with measures of group fairness, and algorithms for generating consensus rankings with varying degrees of fairness. We design novel visualizations that encode these measures in a parallel-coordinates style rank visualization, with interactions for generating and exploring fair consensus rankings. We describe use cases in which FairFuse supports a decision-maker in ranking scenarios in which fairness is important, and discuss emerging challenges for future efforts supporting fairness-oriented rank analysis. Code and demo videos available at https:\/\/osf.io\/hd639\/.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2059955648,
        "newsscientist":0.1667413644,
        "technologyreview":0.2479251938,
        "venturebeat":0.2508967713,
        "wired":0.2359444632,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.07765v2",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1657921073000,
        "published_hr":"Jul 15, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.00651v1",
        "predicted_newsworthiness":0.4945634229,
        "title":"De-biased Representation Learning for Fairness with Unreliable Labels",
        "summary":"Removing bias while keeping all task-relevant information is challenging for fair representation learning methods since they would yield random or degenerate representations w.r.t. labels when the sensitive attributes correlate with labels. Existing works proposed to inject the label information into the learning procedure to overcome such issues. However, the assumption that the observed labels are clean is not always met. In fact, label bias is acknowledged as the primary source inducing discrimination. In other words, the fair pre-processing methods ignore the discrimination encoded in the labels either during the learning procedure or the evaluation stage. This contradiction puts a question mark on the fairness of the learned representations. To circumvent this issue, we explore the following question: \\emph{Can we learn fair representations predictable to latent ideal fair labels given only access to unreliable labels?} In this work, we propose a \\textbf{D}e-\\textbf{B}iased \\textbf{R}epresentation Learning for \\textbf{F}airness (DBRF) framework which disentangles the sensitive information from non-sensitive attributes whilst keeping the learned representations predictable to ideal fair labels rather than observed biased ones. We formulate the de-biased learning framework through information-theoretic concepts such as mutual information and information bottleneck. The core concept is that DBRF advocates not to use unreliable labels for supervision when sensitive information benefits the prediction of unreliable labels. Experiment results over both synthetic and real-world data demonstrate that DBRF effectively learns de-biased representations towards ideal labels.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1563574774,
        "newsscientist":0.1641936167,
        "technologyreview":0.2739733842,
        "venturebeat":0.2320884806,
        "wired":0.183926962,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00651v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cy"
        ],
        "published":1659338200000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12202v1",
        "predicted_newsworthiness":0.4940462087,
        "title":"Video object tracking based on YOLOv7 and DeepSORT",
        "summary":"Multiple object tracking (MOT) is an important technology in the field of computer vision, which is widely used in automatic driving, intelligent monitoring, behavior recognition and other directions. Among the current popular MOT methods based on deep learning, Detection Based Tracking (DBT) is the most widely used in industry, and the performance of them depend on their object detection network. At present, the DBT algorithm with good performance and the most widely used is YOLOv5-DeepSORT. Inspired by YOLOv5-DeepSORT, with the proposal of YOLOv7 network, which performs better in object detection, we apply YOLOv7 as the object detection part to the DeepSORT, and propose YOLOv7-DeepSORT. After experimental evaluation, compared with the previous YOLOv5-DeepSORT, YOLOv7-DeepSORT performances better in tracking accuracy.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0678347733,
        "newsscientist":0.1386323558,
        "technologyreview":0.2421816833,
        "venturebeat":0.2272749482,
        "wired":0.1665759789,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12202v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658756614000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01704v1",
        "predicted_newsworthiness":0.4939891873,
        "title":"Binary Classification with Positive Labeling Sources",
        "summary":"To create a large amount of training labels for machine learning models effectively and efficiently, researchers have turned to Weak Supervision (WS), which uses programmatic labeling sources rather than manual annotation. Existing works of WS for binary classification typically assume the presence of labeling sources that are able to assign both positive and negative labels to data in roughly balanced proportions. However, for many tasks of interest where there is a minority positive class, negative examples could be too diverse for developers to generate indicative labeling sources. Thus, in this work, we study the application of WS on binary classification tasks with positive labeling sources only. We propose WEAPO, a simple yet competitive WS method for producing training labels without negative labeling sources. On 10 benchmark datasets, we show WEAPO achieves the highest averaged performance in terms of both the quality of synthesized labels and the performance of the final classifier supervised with these labels. We incorporated the implementation of \\method into WRENCH, an existing benchmarking platform.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1082765424,
        "newsscientist":0.148870736,
        "technologyreview":0.2592208183,
        "venturebeat":0.2575352126,
        "wired":0.1829671676,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01704v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659468728000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13699v1",
        "predicted_newsworthiness":0.4939604191,
        "title":"Modelling non-reinforced preferences using selective attention",
        "summary":"How can artificial agents learn non-reinforced preferences to continuously adapt their behaviour to a changing environment? We decompose this question into two challenges: ($i$) encoding diverse memories and ($ii$) selectively attending to these for preference formation. Our proposed \\emph{no}n-\\emph{re}inforced preference learning mechanism using selective attention, \\textsc{Nore}, addresses both by leveraging the agent's world model to collect a diverse set of experiences which are interleaved with imagined roll-outs to encode memories. These memories are selectively attended to, using attention and gating blocks, to update agent's preferences. We validate \\textsc{Nore} in a modified OpenAI Gym FrozenLake environment (without any external signal) with and without volatility under a fixed model of the environment -- and compare its behaviour to \\textsc{Pepper}, a Hebbian preference learning mechanism. We demonstrate that \\textsc{Nore} provides a straightforward framework to induce exploratory preferences in the absence of external signals.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1317257508,
        "newsscientist":0.1936606073,
        "technologyreview":0.2582675891,
        "venturebeat":0.2188737383,
        "wired":0.1888456704,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13699v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658786492000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14381v1",
        "predicted_newsworthiness":0.4938325932,
        "title":"Pro-tuning: Unified Prompt Tuning for Vision Tasks",
        "summary":"In computer vision, fine-tuning is the de-facto approach to leverage pre-trained vision models to perform downstream tasks. However, deploying it in practice is quite challenging, due to adopting parameter inefficient global update and heavily relying on high-quality downstream data. Recently, prompt-based learning, which adds a task-relevant prompt to adapt the downstream tasks to pre-trained models, has drastically boosted the performance of many natural language downstream tasks. In this work, we extend this notable transfer ability benefited from prompt into vision models as an alternative to fine-tuning. To this end, we propose parameter-efficient Prompt tuning (Pro-tuning) to adapt frozen vision models to various downstream vision tasks. The key to Pro-tuning is prompt-based tuning, i.e., learning task-specific vision prompts for downstream input images with the pre-trained model frozen. By only training a few additional parameters, it can work on diverse CNN-based and Transformer-based architectures. Extensive experiments evidence that Pro-tuning outperforms fine-tuning in a broad range of vision tasks and scenarios, including image classification (generic objects, class imbalance, image corruption, adversarial robustness, and out-of-distribution generalization), and dense prediction tasks such as object detection and semantic segmentation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0985059376,
        "newsscientist":0.1331125239,
        "technologyreview":0.25209335,
        "venturebeat":0.2346835498,
        "wired":0.16845975,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14381v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659042571000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01376v1",
        "predicted_newsworthiness":0.4937579824,
        "title":"Active entailment encoding for explanation tree construction using parsimonious generation of hard negatives",
        "summary":"Entailment trees have been proposed to simulate the human reasoning process of explanation generation in the context of open--domain textual question answering. However, in practice, manually constructing these explanation trees proves a laborious process that requires active human involvement. Given the complexity of capturing the line of reasoning from question to the answer or from claim to premises, the issue arises of how to assist the user in efficiently constructing multi--level entailment trees given a large set of available facts. In this paper, we frame the construction of entailment trees as a sequence of active premise selection steps, i.e., for each intermediate node in an explanation tree, the expert needs to annotate positive and negative examples of premise facts from a large candidate list. We then iteratively fine--tune pre--trained Transformer models with the resulting positive and tightly controlled negative samples and aim to balance the encoding of semantic relationships and explanatory entailment relationships. Experimental evaluation confirms the measurable efficiency gains of the proposed active fine--tuning method in facilitating entailment trees construction: up to 20\\% improvement in explanatory premise selection when compared against several alternatives.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.12113512,
        "newsscientist":0.1434241935,
        "technologyreview":0.2297204827,
        "venturebeat":0.2154629844,
        "wired":0.1652505271,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01376v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1659440899000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00639v1",
        "predicted_newsworthiness":0.4937206302,
        "title":"Dress Well via Fashion Cognitive Learning",
        "summary":"Fashion compatibility models enable online retailers to easily obtain a large number of outfit compositions with good quality. However, effective fashion recommendation demands precise service for each customer with a deeper cognition of fashion. In this paper, we conduct the first study on fashion cognitive learning, which is fashion recommendations conditioned on personal physical information. To this end, we propose a Fashion Cognitive Network (FCN) to learn the relationships among visual-semantic embedding of outfit composition and appearance features of individuals. FCN contains two submodules, namely outfit encoder and Multi-label Graph Neural Network (ML-GCN). The outfit encoder uses a convolutional layer to encode an outfit into an outfit embedding. The latter module learns label classifiers via stacked GCN. We conducted extensive experiments on the newly collected O4U dataset, and the results provide strong qualitative and quantitative evidence that our framework outperforms alternative methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1283507941,
        "newsscientist":0.1537003023,
        "technologyreview":0.2328803583,
        "venturebeat":0.2394918988,
        "wired":0.2033280268,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00639v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659336757000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01040v1",
        "predicted_newsworthiness":0.4932297517,
        "title":"CircuitNet: An Open-Source Dataset for Machine Learning Applications in Electronic Design Automation (EDA)",
        "summary":"The electronic design automation (EDA) community has been actively exploring machine learning for very-large-scale-integrated computer aided design (VLSI CAD). Many studies have explored learning based techniques for cross-stage prediction tasks in the design flow to achieve faster design convergence. Although building machine learning (ML) models usually requires a large amount of data, most studies can only generate small internal datasets for validation due to the lack of large public datasets. In this essay, we present the first open-source dataset for machine learning tasks in VLSI CAD called CircuitNet. The dataset consists of more than 10K samples extracted from versatile runs of commercial design tools based on 6 open-source RISC-V designs.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0848221472,
        "newsscientist":0.1488841808,
        "technologyreview":0.2935144218,
        "venturebeat":0.28269631,
        "wired":0.2127992205,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01040v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659318568000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01529v1",
        "predicted_newsworthiness":0.4926134448,
        "title":"CIPCaD-Bench: Continuous Industrial Process datasets for benchmarking Causal Discovery methods",
        "summary":"Causal relationships are commonly examined in manufacturing processes to support faults investigations, perform interventions, and make strategic decisions. Industry 4.0 has made available an increasing amount of data that enable data-driven Causal Discovery (CD). Considering the growing number of recently proposed CD methods, it is necessary to introduce strict benchmarking procedures on publicly available datasets since they represent the foundation for a fair comparison and validation of different methods. This work introduces two novel public datasets for CD in continuous manufacturing processes. The first dataset employs the well-known Tennessee Eastman simulator for fault detection and process control. The second dataset is extracted from an ultra-processed food manufacturing plant, and it includes a description of the plant, as well as multiple ground truths. These datasets are used to propose a benchmarking procedure based on different metrics and evaluated on a wide selection of CD algorithms. This work allows testing CD methods in realistic conditions enabling the selection of the most suitable method for specific target applications. The datasets are available at the following link: https:\/\/github.com\/giovanniMen",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.120217088,
        "newsscientist":0.1461866027,
        "technologyreview":0.2299324723,
        "venturebeat":0.2354346618,
        "wired":0.164823745,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01529v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659454210000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12552v1",
        "predicted_newsworthiness":0.4925948353,
        "title":"Peduncle Gripping and Cutting Force for Strawberry Harvesting Robotic End-effector Design",
        "summary":"Robotic harvesting of strawberries has gained much interest in the recent past. Although there are many innovations, they haven't yet reached a level that is comparable to an expert human picker. The end effector unit plays a major role in defining the efficiency of such a robotic harvesting system. Even though there are reports on various end effectors for strawberry harvesting, but there they lack a picture of certain parameters that the researchers can rely upon to develop new end effectors. These parameters include the limit of gripping force that can be applied on the peduncle for effective gripping, the force required to cut the strawberry peduncle, etc. These estimations would be helpful in the design cycle of the end effectors that target to grip and cut the strawberry peduncle during the harvesting action. This paper studies the estimation and analysis of these parameters experimentally. It has been estimated that the peduncle gripping force can be limited to 10 N. This enables an end effector to grip a strawberry of mass up to 50 grams with a manipulation acceleration of 50 m\/s$^2$ without squeezing the peduncle. The study on peduncle cutting force reveals that a force of 15 N is sufficient to cut a strawberry peduncle using a blade with a wedge angle of 16.6 degrees at a 30-degree orientation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1028593737,
        "newsscientist":0.1638044467,
        "technologyreview":0.1715705153,
        "venturebeat":0.1176720125,
        "wired":0.1345153133,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12552v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658787286000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00467v1",
        "predicted_newsworthiness":0.491628762,
        "title":"COCOA: Cross Modality Contrastive Learning for Sensor Data",
        "summary":"Self-Supervised Learning (SSL) is a new paradigm for learning discriminative representations without labelled data and has reached comparable or even state-of-the-art results in comparison to supervised counterparts. Contrastive Learning (CL) is one of the most well-known approaches in SSL that attempts to learn general, informative representations of data. CL methods have been mostly developed for applications in computer vision and natural language processing where only a single sensor modality is used. A majority of pervasive computing applications, however, exploit data from a range of different sensor modalities. While existing CL methods are limited to learning from one or two data sources, we propose COCOA (Cross mOdality COntrastive leArning), a self-supervised model that employs a novel objective function to learn quality representations from multisensor data by computing the cross-correlation between different data modalities and minimizing the similarity between irrelevant instances. We evaluate the effectiveness of COCOA against eight recently introduced state-of-the-art self-supervised models, and two supervised baselines across five public datasets. We show that COCOA achieves superior classification performance to all other approaches. Also, COCOA is far more label-efficient than the other baselines including the fully supervised model using only one-tenth of available labelled data.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1083091979,
        "newsscientist":0.1545189565,
        "technologyreview":0.2521945932,
        "venturebeat":0.252868227,
        "wired":0.2154393484,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00467v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659285373000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11727v1",
        "predicted_newsworthiness":0.4915732928,
        "title":"Can we achieve robustness from data alone?",
        "summary":"Adversarial training and its variants have come to be the prevailing methods to achieve adversarially robust classification using neural networks. However, its increased computational cost together with the significant gap between standard and robust performance hinder progress and beg the question of whether we can do better. In this work, we take a step back and ask: Can models achieve robustness via standard training on a suitably optimized set? To this end, we devise a meta-learning method for robust classification, that optimizes the dataset prior to its deployment in a principled way, and aims to effectively remove the non-robust parts of the data. We cast our optimization method as a multi-step PGD procedure on kernel regression, with a class of kernels that describe infinitely wide neural nets (Neural Tangent Kernels - NTKs). Experiments on MNIST and CIFAR-10 demonstrate that the datasets we produce enjoy very high robustness against PGD attacks, when deployed in both kernel regression classifiers and neural networks. However, this robustness is somewhat fallacious, as alternative attacks manage to fool the models, which we find to be the case for previous similar works in the literature as well. We discuss potential reasons for this and outline further avenues of research.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1308058446,
        "newsscientist":0.1844816529,
        "technologyreview":0.3110313605,
        "venturebeat":0.2633555844,
        "wired":0.2256126582,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11727v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1658664888000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01956v1",
        "predicted_newsworthiness":0.491503405,
        "title":"Augmentation Learning for Semi-Supervised Classification",
        "summary":"Recently, a number of new Semi-Supervised Learning methods have emerged. As the accuracy for ImageNet and similar datasets increased over time, the performance on tasks beyond the classification of natural images is yet to be explored. Most Semi-Supervised Learning methods rely on a carefully manually designed data augmentation pipeline that is not transferable for learning on images of other domains. In this work, we propose a Semi-Supervised Learning method that automatically selects the most effective data augmentation policy for a particular dataset. We build upon the Fixmatch method and extend it with meta-learning of augmentations. The augmentation is learned in additional training before the classification training and makes use of bi-level optimization, to optimize the augmentation policy and maximize accuracy. We evaluate our approach on two domain-specific datasets, containing satellite images and hand-drawn sketches, and obtain state-of-the-art results. We further investigate in an ablation the different parameters relevant for learning augmentation policies and show how policy learning can be used to adapt augmentations to datasets beyond ImageNet.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1066058673,
        "newsscientist":0.1531939932,
        "technologyreview":0.2754201256,
        "venturebeat":0.2412231544,
        "wired":0.1809332833,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01956v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659521211000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11724v1",
        "predicted_newsworthiness":0.491465565,
        "title":"Adaptive Decision Making at the Intersection for Autonomous Vehicles Based on Skill Discovery",
        "summary":"In urban environments, the complex and uncertain intersection scenarios are challenging for autonomous driving. To ensure safety, it is crucial to develop an adaptive decision making system that can handle the interaction with other vehicles. Manually designed model-based methods are reliable in common scenarios. But in uncertain environments, they are not reliable, so learning-based methods are proposed, especially reinforcement learning (RL) methods. However, current RL methods need retraining when the scenarios change. In other words, current RL methods cannot reuse accumulated knowledge. They forget learned knowledge when new scenarios are given. To solve this problem, we propose a hierarchical framework that can autonomously accumulate and reuse knowledge. The proposed method combines the idea of motion primitives (MPs) with hierarchical reinforcement learning (HRL). It decomposes complex problems into multiple basic subtasks to reduce the difficulty. The proposed method and other baseline methods are tested in a challenging intersection scenario based on the CARLA simulator. The intersection scenario contains three different subtasks that can reflect the complexity and uncertainty of real traffic flow. After offline learning and testing, the proposed method is proved to have the best performance among all methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1082623294,
        "newsscientist":0.1392940166,
        "technologyreview":0.2903687887,
        "venturebeat":0.2466564497,
        "wired":0.2064527194,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11724v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai"
        ],
        "published":1658663805000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01897v1",
        "predicted_newsworthiness":0.4912771562,
        "title":"Combined CNN Transformer Encoder for Enhanced Fine-grained Human Action Recognition",
        "summary":"Fine-grained action recognition is a challenging task in computer vision. As fine-grained datasets have small inter-class variations in spatial and temporal space, fine-grained action recognition model requires good temporal reasoning and discrimination of attribute action semantics. Leveraging on CNN's ability in capturing high level spatial-temporal feature representations and Transformer's modeling efficiency in capturing latent semantics and global dependencies, we investigate two frameworks that combine CNN vision backbone and Transformer Encoder to enhance fine-grained action recognition: 1) a vision-based encoder to learn latent temporal semantics, and 2) a multi-modal video-text cross encoder to exploit additional text input and learn cross association between visual and text semantics. Our experimental results show that both our Transformer encoder frameworks effectively learn latent temporal semantics and cross-modality association, with improved recognition performance over CNN vision model. We achieve new state-of-the-art performance on the FineGym benchmark dataset for both proposed architectures.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0822674475,
        "newsscientist":0.105374508,
        "technologyreview":0.1738414677,
        "venturebeat":0.1627526414,
        "wired":0.1390969989,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01897v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659513715000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00210v1",
        "predicted_newsworthiness":0.4911984645,
        "title":"Multiple Categories Of Visual Smoke Detection Database",
        "summary":"Smoke detection has become a significant task in associated industries due to the close relationship between the petrochemical industry's smoke emission and its safety production and environmental damage. There are several production situations in the real industrial production environment, including complete combustion of exhaust gas, inadequate combustion of exhaust gas, direct emission of exhaust gas, etc. We discovered that the datasets used in previous research work can only determine whether smoke is present or not, not its type. That is, the dataset's category does not map to the real-world production situations, which are not conducive to the precise regulation of the production system. As a result, we created a multi-categories smoke detection database that includes a total of 70196 images. We further employed multiple models to conduct the experiment on the proposed database, the results show that the performance of the current algorithms needs to be improved and demonstrate the effectiveness of the proposed database.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1273756164,
        "newsscientist":0.1819550767,
        "technologyreview":0.2237925773,
        "venturebeat":0.2027662542,
        "wired":0.1517129606,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00210v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659186846000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13048v1",
        "predicted_newsworthiness":0.4911197341,
        "title":"Domain Adaptation under Open Set Label Shift",
        "summary":"We introduce the problem of domain adaptation under Open Set Label Shift (OSLS) where the label distribution can change arbitrarily and a new class may arrive during deployment, but the class-conditional distributions p(x|y) are domain-invariant. OSLS subsumes domain adaptation under label shift and Positive-Unlabeled (PU) learning. The learner's goals here are two-fold: (a) estimate the target label distribution, including the novel class; and (b) learn a target classifier. First, we establish necessary and sufficient conditions for identifying these quantities. Second, motivated by advances in label shift and PU learning, we propose practical methods for both tasks that leverage black-box predictors. Unlike typical Open Set Domain Adaptation (OSDA) problems, which tend to be ill-posed and amenable only to heuristics, OSLS offers a well-posed problem amenable to more principled machinery. Experiments across numerous semi-synthetic benchmarks on vision, language, and medical datasets demonstrate that our methods consistently outperform OSDA baselines, achieving 10--25% improvements in target domain accuracy. Finally, we analyze the proposed methods, establishing finite-sample convergence to the true label marginal and convergence to optimal classifier for linear models in a Gaussian setup. Code is available at https:\/\/github.com\/acmi-lab\/Open-Set-Label-Shift.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0937838061,
        "newsscientist":0.1363349846,
        "technologyreview":0.2315004278,
        "venturebeat":0.2231282643,
        "wired":0.156180635,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13048v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658855388000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00902v1",
        "predicted_newsworthiness":0.4905401344,
        "title":"Retrieval of surgical phase transitions using reinforcement learning",
        "summary":"In minimally invasive surgery, surgical workflow segmentation from video analysis is a well studied topic. The conventional approach defines it as a multi-class classification problem, where individual video frames are attributed a surgical phase label. We introduce a novel reinforcement learning formulation for offline phase transition retrieval. Instead of attempting to classify every video frame, we identify the timestamp of each phase transition. By construction, our model does not produce spurious and noisy phase transitions, but contiguous phase blocks. We investigate two different configurations of this model. The first does not require processing all frames in a video (only <60% and <20% of frames in 2 different applications), while producing results slightly under the state-of-the-art accuracy. The second configuration processes all video frames, and outperforms the state-of-the art at a comparable computational cost. We compare our method against the recent top-performing frame-based approaches TeCNO and Trans-SVNet on the public dataset Cholec80 and also on an in-house dataset of laparoscopic sacrocolpopexy. We perform both a frame-based (accuracy, precision, recall and F1-score) and an event-based (event ratio) evaluation of our algorithms.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1043760959,
        "newsscientist":0.1575681247,
        "technologyreview":0.2274119466,
        "venturebeat":0.2024144857,
        "wired":0.1572747782,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00902v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659364995000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11635v2",
        "predicted_newsworthiness":0.4888739046,
        "title":"Spatial-temporal Analysis for Automated Concrete Workability Estimation",
        "summary":"Concrete workability measure is mostly determined based on subjective assessment of a certified assessor with visual inspections. The potential human error in measuring the workability and the resulting unnecessary adjustments for the workability is a major challenge faced by the construction industry, leading to significant costs, material waste and delay. In this paper, we try to apply computer vision techniques to observe the concrete mixing process and estimate the workability. Specifically, we collected the video data and then built three different deep neural networks for spatial-temporal regression. The pilot study demonstrates a practical application with computer vision techniques to estimate the concrete workability during the mixing process.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1292810066,
        "newsscientist":0.1672853554,
        "technologyreview":0.2447028198,
        "venturebeat":0.2149431558,
        "wired":0.1913834439,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11635v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658627597000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00297v1",
        "predicted_newsworthiness":0.488005255,
        "title":"Privacy-Preserving Edge Caching: A Probabilistic Approach",
        "summary":"Edge caching (EC) decreases the average access delay of the end-users through caching popular content at the edge network, however, it increases the leakage probability of valuable information such as users preferences. Most of the existing privacy-preserving approaches focus on adding layers of encryption, which confronts the network with more challenges such as energy and computation limitations. We employ a chunk-based joint probabilistic caching (JPC) approach to mislead an adversary eavesdropping on the communication inside an EC and maximizing the adversary's error in estimating the requested file and requesting cache. In JPC, we optimize the probability of each cache placement to minimize the communication cost while guaranteeing the desired privacy and then, formulate the optimization problem as a linear programming (LP) problem. Since JPC inherits the curse of dimensionality, we also propose scalable JPC (SPC), which reduces the number of feasible cache placements by dividing files into non-overlapping subsets. We also compare the JPC and SPC approaches against an existing probabilistic method, referred to as disjoint probabilistic caching (DPC) and random dummy-based approach (RDA). Results obtained through extensive numerical evaluations confirm the validity of the analytical approach, the superiority of JPC and SPC over DPC and RDA.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.119725915,
        "newsscientist":0.1412251197,
        "technologyreview":0.2149404116,
        "venturebeat":0.2104625305,
        "wired":0.2061564452,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00297v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1659210208000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.12819v1",
        "predicted_newsworthiness":0.4879448231,
        "title":"S-Prompts Learning with Pre-trained Transformers: An Occam's Razor for Domain Incremental Learning",
        "summary":"State-of-the-art deep neural networks are still struggling to address the catastrophic forgetting problem in continual learning. In this paper, we propose one simple paradigm (named as S-Prompting) and two concrete approaches to highly reduce the forgetting degree in one of the most typical continual learning scenarios, i.e., domain increment learning (DIL). The key idea of the paradigm is to learn prompts independently across domains with pre-trained transformers, avoiding the use of exemplars that commonly appear in conventional methods. This results in a win-win game where the prompting can achieve the best for each domain. The independent prompting across domains only requests one single cross-entropy loss for training and one simple K-NN operation as a domain identifier for inference. The learning paradigm derives an image prompt learning approach and a brand-new language-image prompt learning approach. Owning an excellent scalability (0.03% parameter increase per domain), the best of our approaches achieves a remarkable relative improvement (an average of about 30%) over the best of the state-of-the-art exemplar-free methods for three standard DIL tasks, and even surpasses the best of them relatively by about 6% in average when they use exemplars.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0999660324,
        "newsscientist":0.1382385684,
        "technologyreview":0.2433000223,
        "venturebeat":0.2367939813,
        "wired":0.1614330509,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12819v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658835047000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12380v1",
        "predicted_newsworthiness":0.4870393412,
        "title":"Task-Relevant Failure Detection for Trajectory Predictors in Autonomous Vehicles",
        "summary":"In modern autonomy stacks, prediction modules are paramount to planning motions in the presence of other mobile agents. However, failures in prediction modules can mislead the downstream planner into making unsafe decisions. Indeed, the high uncertainty inherent to the task of trajectory forecasting ensures that such mispredictions occur frequently. Motivated by the need to improve safety of autonomous vehicles without compromising on their performance, we develop a probabilistic run-time monitor that detects when a \"harmful\" prediction failure occurs, i.e., a task-relevant failure detector. We achieve this by propagating trajectory prediction errors to the planning cost to reason about their impact on the AV. Furthermore, our detector comes equipped with performance measures on the false-positive and the false-negative rate and allows for data-free calibration. In our experiments we compared our detector with various others and found that our detector has the highest area under the receiver operator characteristic curve.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1346700794,
        "newsscientist":0.186729826,
        "technologyreview":0.3109641958,
        "venturebeat":0.2890500067,
        "wired":0.259919047,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12380v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658771365000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.11934v2",
        "predicted_newsworthiness":0.486382071,
        "title":"Optimal Boxes: Boosting End-to-End Scene Text Recognition by Adjusting Annotated Bounding Boxes via Reinforcement Learning",
        "summary":"Text detection and recognition are essential components of a modern OCR system. Most OCR approaches attempt to obtain accurate bounding boxes of text at the detection stage, which is used as the input of the text recognition stage. We observe that when using tight text bounding boxes as input, a text recognizer frequently fails to achieve optimal performance due to the inconsistency between bounding boxes and deep representations of text recognition. In this paper, we propose Box Adjuster, a reinforcement learning-based method for adjusting the shape of each text bounding box to make it more compatible with text recognition models. Additionally, when dealing with cross-domain problems such as synthetic-to-real, the proposed method significantly reduces mismatches in domain distribution between the source and target domains. Experiments demonstrate that the performance of end-to-end text recognition systems can be improved when using the adjusted bounding boxes as the ground truths for training. Specifically, on several benchmark datasets for scene text understanding, the proposed method outperforms state-of-the-art text spotters by an average of 2.0% F-Score on end-to-end text recognition tasks and 4.6% F-Score on domain adaptation tasks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1054115434,
        "newsscientist":0.1349569408,
        "technologyreview":0.2447600117,
        "venturebeat":0.2391201301,
        "wired":0.1667077832,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11934v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658732325000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00791v1",
        "predicted_newsworthiness":0.4862683801,
        "title":"Partial Connection Based on Channel Attention for Differentiable Neural Architecture Search",
        "summary":"Differentiable neural architecture search (DARTS), as a gradient-guided search method, greatly reduces the cost of computation and speeds up the search. In DARTS, the architecture parameters are introduced to the candidate operations, but the parameters of some weight-equipped operations may not be trained well in the initial stage, which causes unfair competition between candidate operations. The weight-free operations appear in large numbers which results in the phenomenon of performance crash. Besides, a lot of memory will be occupied during training supernet which causes the memory utilization to be low. In this paper, a partial channel connection based on channel attention for differentiable neural architecture search (ADARTS) is proposed. Some channels with higher weights are selected through the attention mechanism and sent into the operation space while the other channels are directly contacted with the processed channels. Selecting a few channels with higher attention weights can better transmit important feature information into the search space and greatly improve search efficiency and memory utilization. The instability of network structure caused by random selection can also be avoided. The experimental results show that ADARTS achieved 2.46% and 17.06% classification error rates on CIFAR-10 and CIFAR-100, respectively. ADARTS can effectively solve the problem that too many skip connections appear in the search process and obtain network structures with better performance.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0716664446,
        "newsscientist":0.1308533635,
        "technologyreview":0.2382588335,
        "venturebeat":0.205610085,
        "wired":0.1480712464,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00791v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659355555000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00823v1",
        "predicted_newsworthiness":0.4859410292,
        "title":"Designing Programming Exercises from Board Games",
        "summary":"This paper introduces a collection of board games specifically chosen to serve as a basis for programming exercises. We examine the attractiveness of board games in this context as well as features that make a particular game a good exercise. The collection is annotated across several dimensions to assist choosing a game suitable for the target topic and student level. We discuss possible changes into exercise tasks to make them more challenging and introduce new topics. The work relies on established topics taxonomy and board games resources which makes extending the current collection easy.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1614409192,
        "newsscientist":0.178100734,
        "technologyreview":0.2262554204,
        "venturebeat":0.2533260699,
        "wired":0.243305601,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00823v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.cy"
        ],
        "published":1659063666000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.01510v1",
        "predicted_newsworthiness":0.4858156811,
        "title":"s-LIME: Reconciling Locality and Fidelity in Linear Explanations",
        "summary":"The benefit of locality is one of the major premises of LIME, one of the most prominent methods to explain black-box machine learning models. This emphasis relies on the postulate that the more locally we look at the vicinity of an instance, the simpler the black-box model becomes, and the more accurately we can mimic it with a linear surrogate. As logical as this seems, our findings suggest that, with the current design of LIME, the surrogate model may degenerate when the explanation is too local, namely, when the bandwidth parameter $\\sigma$ tends to zero. Based on this observation, the contribution of this paper is twofold. Firstly, we study the impact of both the bandwidth and the training vicinity on the fidelity and semantics of LIME explanations. Secondly, and based on our findings, we propose \\slime, an extension of LIME that reconciles fidelity and locality.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1192703081,
        "newsscientist":0.1783718546,
        "technologyreview":0.292880812,
        "venturebeat":0.2646319151,
        "wired":0.2052739312,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01510v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659452108000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01404v1",
        "predicted_newsworthiness":0.485580442,
        "title":"PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics",
        "summary":"Promotions are commonly used by e-commerce merchants to boost sales. The efficacy of different promotion strategies can help sellers adapt their offering to customer demand in order to survive and thrive. Current approaches to designing promotion strategies are either based on econometrics, which may not scale to large amounts of sales data, or are spontaneous and provide little explanation of sales volume. Moreover, accurately measuring the effects of promotion designs and making bootstrappable adjustments accordingly remains a challenge due to the incompleteness and complexity of the information describing promotion strategies and their market environments. We present PromotionLens, a visual analytics system for exploring, comparing, and modeling the impact of various promotion strategies. Our approach combines representative multivariant time-series forecasting models and well-designed visualizations to demonstrate and explain the impact of sales and promotional factors, and to support \"what-if\" analysis of promotions. Two case studies, expert feedback, and a qualitative user study demonstrate the efficacy of PromotionLens.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1863882405,
        "newsscientist":0.1708350953,
        "technologyreview":0.2465173381,
        "venturebeat":0.3314291984,
        "wired":0.2479529194,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01404v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1659327204000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.13332v1",
        "predicted_newsworthiness":0.4854836318,
        "title":"RealTime QA: What's the Answer Right Now?",
        "summary":"We introduce RealTime QA, a dynamic question answering (QA) platform that announces questions and evaluates systems on a regular basis (weekly in this version). RealTime QA inquires about the current world, and QA systems need to answer questions about novel events or information. It therefore challenges static, conventional assumptions in open domain QA datasets and pursues, instantaneous applications. We build strong baseline models upon large pretrained language models, including GPT-3 and T5. Our benchmark is an ongoing effort, and this preliminary report presents real-time evaluation results over the past month. Our experimental results show that GPT-3 can often properly update its generation results, based on newly-retrieved documents, highlighting the importance of up-to-date information retrieval. Nonetheless, we find that GPT-3 tends to return outdated answers when retrieved documents do not provide sufficient information to find an answer. This suggests an important avenue for future research: can an open domain QA system identify such unanswerable cases and communicate with the user or even the retrieval module to modify the retrieval results? We hope that RealTime QA will spur progress in instantaneous applications of question answering and beyond.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1170064377,
        "newsscientist":0.1564192668,
        "technologyreview":0.2626344459,
        "venturebeat":0.2893080368,
        "wired":0.2388926883,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13332v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658906761000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.08817v2",
        "predicted_newsworthiness":0.4853742076,
        "title":"Research Trends and Applications of Data Augmentation Algorithms",
        "summary":"In the Machine Learning research community, there is a consensus regarding the relationship between model complexity and the required amount of data and computation power. In real world applications, these computational requirements are not always available, motivating research on regularization methods. In addition, current and past research have shown that simpler classification algorithms can reach state-of-the-art performance on computer vision tasks given a robust method to artificially augment the training dataset. Because of this, data augmentation techniques became a popular research topic in recent years. However, existing data augmentation methods are generally less transferable than other regularization methods. In this paper we identify the main areas of application of data augmentation algorithms, the types of algorithms used, significant research trends, their progression over time and research gaps in data augmentation literature. To do this, the related literature was collected through the Scopus database. Its analysis was done following network science, text mining and exploratory analysis approaches. We expect readers to understand the potential of data augmentation, as well as identify future research directions and open questions within data augmentation research.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1388447948,
        "newsscientist":0.19981711,
        "technologyreview":0.3408159881,
        "venturebeat":0.32707388,
        "wired":0.228731104,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.08817v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658144312000,
        "published_hr":"Jul 18, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01371v1",
        "predicted_newsworthiness":0.4847070754,
        "title":"Multi-Module G2P Converter for Persian Focusing on Relations between Words",
        "summary":"In this paper, we investigate the application of end-to-end and multi-module frameworks for G2P conversion for the Persian language. The results demonstrate that our proposed multi-module G2P system outperforms our end-to-end systems in terms of accuracy and speed. The system consists of a pronunciation dictionary as our look-up table, along with separate models to handle homographs, OOVs and ezafe in Persian created using GRU and Transformer architectures. The system is sequence-level rather than word-level, which allows it to effectively capture the unwritten relations between words (cross-word information) necessary for homograph disambiguation and ezafe recognition without the need for any pre-processing. After evaluation, our system achieved a 94.48% word-level accuracy, outperforming the previous G2P systems for Persian.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0833713718,
        "newsscientist":0.0968330459,
        "technologyreview":0.1561780977,
        "venturebeat":0.1824593938,
        "wired":0.1445090239,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01371v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659440028000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14760v1",
        "predicted_newsworthiness":0.484658745,
        "title":"SimCURL: Simple Contrastive User Representation Learning from Command Sequences",
        "summary":"User modeling is crucial to understanding user behavior and essential for improving user experience and personalized recommendations. When users interact with software, vast amounts of command sequences are generated through logging and analytics systems. These command sequences contain clues to the users' goals and intents. However, these data modalities are highly unstructured and unlabeled, making it difficult for standard predictive systems to learn from. We propose SimCURL, a simple yet effective contrastive self-supervised deep learning framework that learns user representation from unlabeled command sequences. Our method introduces a user-session network architecture, as well as session dropout as a novel way of data augmentation. We train and evaluate our method on a real-world command sequence dataset of more than half a billion commands. Our method shows significant improvement over existing methods when the learned representation is transferred to downstream tasks such as experience and expertise classification.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1163443921,
        "newsscientist":0.1771752063,
        "technologyreview":0.2963156341,
        "venturebeat":0.3154166695,
        "wired":0.2386355874,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14760v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1659110763000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.11005v2",
        "predicted_newsworthiness":0.4839443678,
        "title":"Revisiting Parameter Reuse to Overcome Catastrophic Forgetting in Neural Networks",
        "summary":"Neural networks tend to forget previously learned knowledge when continuously learning on datasets with varying distributions, a phenomenon known as catastrophic forgetting. More significant distribution shifts among datasets lead to more forgetting. Recently, parameter-isolation-based approaches have shown great potential in overcoming forgetting with significant distribution shifts. However, they suffer from poor generalization as they fix the neural path for each dataset during training and require dataset labels during inference. In addition, they do not support backward knowledge transfer as they prioritize past data over future ones. In this paper, we propose a new adaptive learning method, named AdaptCL, that fully reuses and grows on learned parameters to overcome catastrophic forgetting and allows the positive backward transfer without requiring dataset labels. Our proposed technique adaptively grows on the same neural path by allowing optimal reuse of frozen parameters. Besides, it uses parameter-level data-driven pruning to assign equal priority to the data. We conduct extensive experiments on MNIST Variants, DomainNet, and Food Freshness Detection datasets under different intensities of distribution shifts without requiring dataset labels. Results demonstrate that our proposed method is superior to alternative baselines in minimizing forgetting and enabling positive backward knowledge transfer.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1113388741,
        "newsscientist":0.1573712331,
        "technologyreview":0.2479541285,
        "venturebeat":0.212908538,
        "wired":0.1494572738,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11005v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658486886000,
        "published_hr":"Jul 22, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14460v1",
        "predicted_newsworthiness":0.4839261372,
        "title":"RCA: Ride Comfort-Aware Visual Navigation via Self-Supervised Learning",
        "summary":"Under shared autonomy, wheelchair users expect vehicles to provide safe and comfortable rides while following users high-level navigation plans. To find such a path, vehicles negotiate with different terrains and assess their traversal difficulty. Most prior works model surroundings either through geometric representations or semantic classifications, which do not reflect perceived motion intensity and ride comfort in downstream navigation tasks. We propose to model ride comfort explicitly in traversability analysis using proprioceptive sensing. We develop a self-supervised learning framework to predict traversability costmap from first-person-view images by leveraging vehicle states as training signals. Our approach estimates how the vehicle would feel if traversing over based on terrain appearances. We then show our navigation system provides human-preferred ride comfort through robot experiments together with a human evaluation study.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1320019865,
        "newsscientist":0.1922895394,
        "technologyreview":0.274064963,
        "venturebeat":0.2607522144,
        "wired":0.2411979291,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14460v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai"
        ],
        "published":1659065921000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00880v1",
        "predicted_newsworthiness":0.4839142021,
        "title":"Physics-informed Machine Learning of Parameterized Fundamental Diagrams",
        "summary":"Fundamental diagrams describe the relationship between speed, flow, and density for some roadway (or set of roadway) configuration(s). These diagrams typically do not reflect, however, information on how speed-flow relationships change as a function of exogenous variables such as curb configuration, weather or other exogenous, contextual information. In this paper we present a machine learning methodology that respects known engineering constraints and physical laws of roadway flux - those that are captured in fundamental diagrams - and show how this can be used to introduce contextual information into the generation of these diagrams. The modeling task is formulated as a probe vehicle trajectory reconstruction problem with Neural Ordinary Differential Equations (Neural ODEs). With the presented methodology, we extend the fundamental diagram to non-idealized roadway segments with potentially obstructed traffic data. For simulated data, we generalize this relationship by introducing contextual information at the learning stage, i.e. vehicle composition, driver behavior, curb zoning configuration, etc, and show how the speed-flow relationship changes as a function of these exogenous factors independent of roadway design.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.129007718,
        "newsscientist":0.1684913216,
        "technologyreview":0.2536712592,
        "venturebeat":0.2263510247,
        "wired":0.2252364465,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00880v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659363186000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11565v1",
        "predicted_newsworthiness":0.483826818,
        "title":"Context based lemmatizer for Polish language",
        "summary":"Lemmatization is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. In computational linguistics, lemmatisation is the algorithmic process of determining the lemma of a word based on its intended meaning. Unlike stemming, lemmatisation depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence. As a result, developing efficient lemmatisation algorithm is the complex task. In recent years it can be observed that deep learning models used for this task outperform other methods including machine learning algorithms. In this paper the polish lemmatizer based on Google T5 model is presented. The training was run with different context lengths. The model achieves the best results for polish language lemmatisation process.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0996107664,
        "newsscientist":0.1186528885,
        "technologyreview":0.2227380045,
        "venturebeat":0.2309693768,
        "wired":0.1428439594,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11565v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1658599336000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00629v1",
        "predicted_newsworthiness":0.4833889688,
        "title":"XOOD: Extreme Value Based Out-Of-Distribution Detection For Image Classification",
        "summary":"Detecting out-of-distribution (OOD) data at inference time is crucial for many applications of machine learning. We present XOOD: a novel extreme value-based OOD detection framework for image classification that consists of two algorithms. The first, XOOD-M, is completely unsupervised, while the second XOOD-L is self-supervised. Both algorithms rely on the signals captured by the extreme values of the data in the activation layers of the neural network in order to distinguish between in-distribution and OOD instances. We show experimentally that both XOOD-M and XOOD-L outperform state-of-the-art OOD detection methods on many benchmark data sets in both efficiency and accuracy, reducing false-positive rate (FPR95) by 50%, while improving the inferencing time by an order of magnitude.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0829376433,
        "newsscientist":0.1384846769,
        "technologyreview":0.2116231167,
        "venturebeat":0.1983258177,
        "wired":0.1475990131,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00629v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cv"
        ],
        "published":1659334953000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12321v1",
        "predicted_newsworthiness":0.4831640809,
        "title":"RSG-Net: Towards Rich Sematic Relationship Prediction for Intelligent Vehicle in Complex Environments",
        "summary":"Behavioral and semantic relationships play a vital role on intelligent self-driving vehicles and ADAS systems. Different from other research focused on trajectory, position, and bounding boxes, relationship data provides a human understandable description of the object's behavior, and it could describe an object's past and future status in an amazingly brief way. Therefore it is a fundamental method for tasks such as risk detection, environment understanding, and decision making. In this paper, we propose RSG-Net (Road Scene Graph Net): a graph convolutional network designed to predict potential semantic relationships from object proposals, and produces a graph-structured result, called \"Road Scene Graph\". The experimental results indicate that this network, trained on Road Scene Graph dataset, could efficiently predict potential semantic relationships among objects around the ego-vehicle.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1056413812,
        "newsscientist":0.1538383962,
        "technologyreview":0.2760273631,
        "venturebeat":0.2625621203,
        "wired":0.21286012,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12321v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1657975217000,
        "published_hr":"Jul 16, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01424v1",
        "predicted_newsworthiness":0.4830947891,
        "title":"Connection Reduction Is All You Need",
        "summary":"Convolutional Neural Networks (CNN) increase depth by stacking convolutional layers, and deeper network models perform better in image recognition. Empirical research shows that simply stacking convolutional layers does not make the network train better, and skip connection (residual learning) can improve network model performance. For the image classification task, models with global densely connected architectures perform well in large datasets like ImageNet, but are not suitable for small datasets such as CIFAR-10 and SVHN. Different from dense connections, we propose two new algorithms to connect layers. Baseline is a densely connected network, and the networks connected by the two new algorithms are named ShortNet1 and ShortNet2 respectively. The experimental results of image classification on CIFAR-10 and SVHN show that ShortNet1 has a 5% lower test error rate and 25% faster inference time than Baseline. ShortNet2 speeds up inference time by 40% with less loss in test accuracy.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0949705932,
        "newsscientist":0.143079063,
        "technologyreview":0.2681964137,
        "venturebeat":0.2401932612,
        "wired":0.1614828878,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01424v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659445235000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00647v1",
        "predicted_newsworthiness":0.4829582398,
        "title":"An Evidential Neural Network Model for Regression Based on Random Fuzzy Numbers",
        "summary":"We introduce a distance-based neural network model for regression, in which prediction uncertainty is quantified by a belief function on the real line. The model interprets the distances of the input vector to prototypes as pieces of evidence represented by Gaussian random fuzzy numbers (GRFN's) and combined by the generalized product intersection rule, an operator that extends Dempster's rule to random fuzzy sets. The network output is a GRFN that can be summarized by three numbers characterizing the most plausible predicted value, variability around this value, and epistemic uncertainty. Experiments with real datasets demonstrate the very good performance of the method as compared to state-of-the-art evidential and statistical learning algorithms. \\keywords{Evidence theory, Dempster-Shafer theory, belief functions, machine learning, random fuzzy sets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1342241457,
        "newsscientist":0.1813146934,
        "technologyreview":0.2801447069,
        "venturebeat":0.259305927,
        "wired":0.1935709691,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00647v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659338011000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14571v2",
        "predicted_newsworthiness":0.4829139163,
        "title":"Prompting for Multi-Modal Tracking",
        "summary":"Multi-modal tracking gains attention due to its ability to be more accurate and robust in complex scenarios compared to traditional RGB-based tracking. Its key lies in how to fuse multi-modal data and reduce the gap between modalities. However, multi-modal tracking still severely suffers from data deficiency, thus resulting in the insufficient learning of fusion modules. Instead of building such a fusion module, in this paper, we provide a new perspective on multi-modal tracking by attaching importance to the multi-modal visual prompts. We design a novel multi-modal prompt tracker (ProTrack), which can transfer the multi-modal inputs to a single modality by the prompt paradigm. By best employing the tracking ability of pre-trained RGB trackers learning at scale, our ProTrack can achieve high-performance multi-modal tracking by only altering the inputs, even without any extra training on multi-modal data. Extensive experiments on 5 benchmark datasets demonstrate the effectiveness of the proposed ProTrack.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0869881557,
        "newsscientist":0.144517074,
        "technologyreview":0.2286593918,
        "venturebeat":0.2367945554,
        "wired":0.1901848602,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14571v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659087302000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00214v1",
        "predicted_newsworthiness":0.4826930899,
        "title":"Towards Privacy-Preserving, Real-Time and Lossless Feature Matching",
        "summary":"Most visual retrieval applications store feature vectors for downstream matching tasks. These vectors, from where user information can be spied out, will cause privacy leakage if not carefully protected. To mitigate privacy risks, current works primarily utilize non-invertible transformations or fully cryptographic algorithms. However, transformation-based methods usually fail to achieve satisfying matching performances while cryptosystems suffer from heavy computational overheads. In addition, secure levels of current methods should be improved to confront potential adversary attacks. To address these issues, this paper proposes a plug-in module called SecureVector that protects features by random permutations, 4L-DEC converting and existing homomorphic encryption techniques. For the first time, SecureVector achieves real-time and lossless feature matching among sanitized features, along with much higher security levels than current state-of-the-arts. Extensive experiments on face recognition, person re-identification, image retrieval, and privacy analyses demonstrate the effectiveness of our method. Given limited public projects in this field, codes of our method and implemented baselines are made open-source in https:\/\/github.com\/IrvingMeng\/SecureVector.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1309113753,
        "newsscientist":0.1734443823,
        "technologyreview":0.2624218258,
        "venturebeat":0.2470594961,
        "wired":0.2296650724,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00214v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659187559000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13890v1",
        "predicted_newsworthiness":0.4824064146,
        "title":"Why Accuracy Is Not Enough: The Need for Consistency in Object Detection",
        "summary":"Object detectors are vital to many modern computer vision applications. However, even state-of-the-art object detectors are not perfect. On two images that look similar to human eyes, the same detector can make different predictions because of small image distortions like camera sensor noise and lighting changes. This problem is called inconsistency. Existing accuracy metrics do not properly account for inconsistency, and similar work in this area only targets improvements on artificial image distortions. Therefore, we propose a method to use non-artificial video frames to measure object detection consistency over time, across frames. Using this method, we show that the consistency of modern object detectors ranges from 83.2% to 97.1% on different video datasets from the Multiple Object Tracking Challenge. We conclude by showing that applying image distortion corrections like .WEBP Image Compression and Unsharp Masking can improve consistency by as much as 5.1%, with no loss in accuracy.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1069398575,
        "newsscientist":0.1739673001,
        "technologyreview":0.2532774907,
        "venturebeat":0.2294432607,
        "wired":0.19890026,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13890v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658987478000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00630v1",
        "predicted_newsworthiness":0.4823707111,
        "title":"Identifying Influential Brokers on Social Media from Social Network Structure",
        "summary":"Identifying influencers in a given social network has become an important research problem for various applications, including accelerating the spread of information in viral marketing and preventing the spread of fake news and rumors. The literature contains a rich body of studies on identifying influential source spreaders who can spread their own messages to many other nodes. In contrast, the identification of influential brokers who can spread other nodes' messages to many nodes has not been fully explored. Theoretical and empirical studies suggest that involvement of both influential source spreaders and brokers is a key to facilitating large-scale information diffusion cascades. Therefore, this paper explores ways to identify influential brokers from a given social network. By using three social media datasets, we investigate the characteristics of influential brokers by comparing them with influential source spreaders and central nodes obtained from centrality measures. Our results show that (i) most of the influential source spreaders are not influential brokers (and vice versa) and (ii) the overlap between central nodes and influential brokers is small (less than 15%) in Twitter datasets. We also tackle the problem of identifying influential brokers from centrality measures and node embeddings, and we examine the effectiveness of social network features in the broker identification task. Our results show that (iii) although a single centrality measure cannot characterize influential brokers well, prediction models using node embedding features achieve F$_1$ scores of 0.35--0.68, suggesting the effectiveness of social network features for identifying influential brokers.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1942220085,
        "newsscientist":0.1706066035,
        "technologyreview":0.3061913982,
        "venturebeat":0.2790993942,
        "wired":0.2941000907,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00630v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.cy"
        ],
        "published":1659335244000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.14651v1",
        "predicted_newsworthiness":0.4818936855,
        "title":"Real-time Continuous Uncertainty Annotation (RCUA) for Spatial Navigation Studies",
        "summary":"Navigation uncertainty is crucial for understanding the wayfinding behaviors while no method has been developed to effectively measured uncertainty in real-world scenarios. We developed the Real-time Continous Uncertainty Annotation (RCUA) to continuously measure perceived uncertainty by asking users to push the joystick in during the wayfinding process. We tested its test-retest reliability and validated RCUA based on 40 participants using the known group and known treatment. We also compared it with a discrete self-report scale and continuous postexperiment video annotation (CUA). The result demonstrated that most participants were able to output four distinct levels of uncertainty, though high variability and errors were observed. Both known group and known treatment proved good validity of the measure and RCUA was moderately correlated with self-report uncertainty. Self-report surveys showed that participants can continuously push the joystick and conduct wayfinding tasks at the same time. A comparison between RCUA and CUA showed that RCUA had a higher granularity but participants tended to overreport uncertainty using RCUA.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1696581904,
        "newsscientist":0.205559818,
        "technologyreview":0.2467923939,
        "venturebeat":0.2708985951,
        "wired":0.2503514087,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14651v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1658934528000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.01582v1",
        "predicted_newsworthiness":0.4801619729,
        "title":"ViP3D: End-to-end Visual Trajectory Prediction via 3D Agent Queries",
        "summary":"Existing autonomous driving pipelines separate the perception module from the prediction module. The two modules communicate via hand-picked features such as agent boxes and trajectories as interfaces. Due to this separation, the prediction module only receives partial information from the perception module. Even worse, errors from the perception modules can propagate and accumulate, adversely affecting the prediction results. In this work, we propose ViP3D, a visual trajectory prediction pipeline that leverages the rich information from raw videos to predict future trajectories of agents in a scene. ViP3D employs sparse agent queries throughout the pipeline, making it fully differentiable and interpretable. Furthermore, we propose an evaluation metric for this novel end-to-end visual trajectory prediction task. Extensive experimental results on the nuScenes dataset show the strong performance of ViP3D over traditional pipelines and previous end-to-end models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.108691948,
        "newsscientist":0.1712867982,
        "technologyreview":0.2769213922,
        "venturebeat":0.2697710472,
        "wired":0.2224592768,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01582v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ro"
        ],
        "published":1659458308000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00238v1",
        "predicted_newsworthiness":0.4801007465,
        "title":"Improving Fine-tuning of Self-supervised Models with Contrastive Initialization",
        "summary":"Self-supervised learning (SSL) has achieved remarkable performance in pretraining the models that can be further used in downstream tasks via fine-tuning. However, these self-supervised models may not capture meaningful semantic information since the images belonging to the same class are always regarded as negative pairs in the contrastive loss. Consequently, the images of the same class are often located far away from each other in learned feature space, which would inevitably hamper the fine-tuning process. To address this issue, we seek to provide a better initialization for the self-supervised models by enhancing the semantic information. To this end, we propose a Contrastive Initialization (COIN) method that breaks the standard fine-tuning pipeline by introducing an extra initialization stage before fine-tuning. Extensive experiments show that, with the enriched semantics, our COIN significantly outperforms existing methods without introducing extra training cost and sets new state-of-the-arts on multiple downstream tasks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0957959564,
        "newsscientist":0.1496256008,
        "technologyreview":0.2648697931,
        "venturebeat":0.2412286063,
        "wired":0.1631003768,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00238v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659192357000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12926v1",
        "predicted_newsworthiness":0.4800383746,
        "title":"A Guide to Image and Video based Small Object Detection using Deep Learning : Case Study of Maritime Surveillance",
        "summary":"Small object detection (SOD) in optical images and videos is a challenging problem that even state-of-the-art generic object detection methods fail to accurately localize and identify such objects. Typically, small objects appear in real-world due to large camera-object distance. Because small objects occupy only a small area in the input image (e.g., less than 10%), the information extracted from such a small area is not always rich enough to support decision making. Multidisciplinary strategies are being developed by researchers working at the interface of deep learning and computer vision to enhance the performance of SOD deep learning based methods. In this paper, we provide a comprehensive review of over 160 research papers published between 2017 and 2022 in order to survey this growing subject. This paper summarizes the existing literature and provide a taxonomy that illustrates the broad picture of current research. We investigate how to improve the performance of small object detection in maritime environments, where increasing performance is critical. By establishing a connection between generic and maritime SOD research, future directions have been identified. In addition, the popular datasets that have been used for SOD for generic and maritime applications are discussed, and also well-known evaluation metrics for the state-of-the-art methods on some of the datasets are provided.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1130905766,
        "newsscientist":0.1833305463,
        "technologyreview":0.2327504111,
        "venturebeat":0.2175497764,
        "wired":0.1888283887,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12926v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658845718000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00278v1",
        "predicted_newsworthiness":0.4798691327,
        "title":"Robust Contact State Estimation in Humanoid Walking Gaits",
        "summary":"In this article, we propose a deep learning framework that provides a unified approach to the problem of leg contact detection in humanoid robot walking gaits. Our formulation accomplishes to accurately and robustly estimate the contact state probability for each leg (i.e., stable or slip\/no contact). The proposed framework employs solely proprioceptive sensing and although it relies on simulated ground-truth contact data for the classification process, we demonstrate that it generalizes across varying friction surfaces and different legged robotic platforms and, at the same time, is readily transferred from simulation to practice. The framework is quantitatively and qualitatively assessed in simulation via the use of ground-truth contact data and is contrasted against state of-the-art methods with an ATLAS, a NAO, and a TALOS humanoid robot. Furthermore, its efficacy is demonstrated in base estimation with a real TALOS humanoid. To reinforce further research endeavors, our implementation is offered as an open-source ROS\/Python package, coined Legged Contact Detection (LCD).",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0895707092,
        "newsscientist":0.163114034,
        "technologyreview":0.2184872225,
        "venturebeat":0.1951128912,
        "wired":0.1799622434,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00278v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai",
            "cs.lg"
        ],
        "published":1659201587000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00671v1",
        "predicted_newsworthiness":0.4798266515,
        "title":"RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports",
        "summary":"Experts in racket sports like tennis and badminton use tactical analysis to gain insight into competitors' playing styles. Many data-driven methods apply pattern mining to racket sports data -- which is often recorded as multivariate event sequences -- to uncover sports tactics. However, tactics obtained in this way are often inconsistent with those deduced by experts through their domain knowledge, which can be confusing to those experts. This work introduces RASIPAM, a RAcket-Sports Interactive PAttern Mining system, which allows experts to incorporate their knowledge into data mining algorithms to discover meaningful tactics interactively. RASIPAM consists of a constraint-based pattern mining algorithm that responds to the analysis demands of experts: Experts provide suggestions for finding tactics in intuitive written language, and these suggestions are translated into constraints to run the algorithm. RASIPAM further introduces a tailored visual interface that allows experts to compare the new tactics with the original ones and decide whether to apply a given adjustment. This interactive workflow iteratively progresses until experts are satisfied with all tactics. We conduct a quantitative experiment to show that our algorithm supports real-time interaction. Two case studies in tennis and in badminton respectively, each involving two domain experts, are conducted to show the effectiveness and usefulness of the system.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1501767251,
        "newsscientist":0.1713630427,
        "technologyreview":0.2470054335,
        "venturebeat":0.2547712136,
        "wired":0.1980172096,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00671v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1659341054000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.12691v1",
        "predicted_newsworthiness":0.4790122961,
        "title":"CENet: Toward Concise and Efficient LiDAR Semantic Segmentation for Autonomous Driving",
        "summary":"Accurate and fast scene understanding is one of the challenging task for autonomous driving, which requires to take full advantage of LiDAR point clouds for semantic segmentation. In this paper, we present a \\textbf{concise} and \\textbf{efficient} image-based semantic segmentation network, named \\textbf{CENet}. In order to improve the descriptive power of learned features and reduce the computational as well as time complexity, our CENet integrates the convolution with larger kernel size instead of MLP, carefully-selected activation functions, and multiple auxiliary segmentation heads with corresponding loss functions into architecture. Quantitative and qualitative experiments conducted on publicly available benchmarks, SemanticKITTI and SemanticPOSS, demonstrate that our pipeline achieves much better mIoU and inference performance compared with state-of-the-art models. The code will be available at https:\/\/github.com\/huixiancheng\/CENet.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1006106344,
        "newsscientist":0.1411182449,
        "technologyreview":0.2378734695,
        "venturebeat":0.2266088248,
        "wired":0.1818938347,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12691v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658820139000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11493v1",
        "predicted_newsworthiness":0.4784651071,
        "title":"Active Pointly-Supervised Instance Segmentation",
        "summary":"The requirement of expensive annotations is a major burden for training a well-performed instance segmentation model. In this paper, we present an economic active learning setting, named active pointly-supervised instance segmentation (APIS), which starts with box-level annotations and iteratively samples a point within the box and asks if it falls on the object. The key of APIS is to find the most desirable points to maximize the segmentation accuracy with limited annotation budgets. We formulate this setting and propose several uncertainty-based sampling strategies. The model developed with these strategies yields consistent performance gain on the challenging MS-COCO dataset, compared against other learning strategies. The results suggest that APIS, integrating the advantages of active learning and point-based supervision, is an effective learning paradigm for label-efficient instance segmentation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0650907915,
        "newsscientist":0.1244284923,
        "technologyreview":0.1701232829,
        "venturebeat":0.1621550973,
        "wired":0.1230745096,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11493v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658575524000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00322v1",
        "predicted_newsworthiness":0.4780493159,
        "title":"PrePARE: Predictive Proprioception for Agile Failure Event Detection in Robotic Exploration of Extreme Terrains",
        "summary":"Legged robots can traverse a wide variety of terrains, some of which may be challenging for wheeled robots, such as stairs or highly uneven surfaces. However, quadruped robots face stability challenges on slippery surfaces. This can be resolved by adjusting the robot's locomotion by switching to more conservative and stable locomotion modes, such as crawl mode (where three feet are in contact with the ground always) or amble mode (where one foot touches down at a time) to prevent potential falls. To tackle these challenges, we propose an approach to learn a model from past robot experience for predictive detection of potential failures. Accordingly, we trigger gait switching merely based on proprioceptive sensory information. To learn this predictive model, we propose a semi-supervised process for detecting and annotating ground truth slip events in two stages: We first detect abnormal occurrences in the time series sequences of the gait data using an unsupervised anomaly detector, and then, the anomalies are verified with expert human knowledge in a replay simulation to assert the event of a slip. These annotated slip events are then used as ground truth examples to train an ensemble decision learner for predicting slip probabilities across terrains for traversability. We analyze our model on data recorded by a legged robot on multiple sites with slippery terrain. We demonstrate that a potential slip event can be predicted up to 720 ms ahead of a potential fall with an average precision greater than 0.95 and an average F-score of 0.82. Finally, we validate our approach in real-time by deploying it on a legged robot and switching its gait mode based on slip event detection.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1145020297,
        "newsscientist":0.200885771,
        "technologyreview":0.25072773,
        "venturebeat":0.2197685728,
        "wired":0.2115673884,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00322v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659224251000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13211v1",
        "predicted_newsworthiness":0.4778568219,
        "title":"A Survey of Intent Classification and Slot-Filling Datasets for Task-Oriented Dialog",
        "summary":"Interest in dialog systems has grown substantially in the past decade. By extension, so too has interest in developing and improving intent classification and slot-filling models, which are two components that are commonly used in task-oriented dialog systems. Moreover, good evaluation benchmarks are important in helping to compare and analyze systems that incorporate such models. Unfortunately, much of the literature in the field is limited to analysis of relatively few benchmark datasets. In an effort to promote more robust analyses of task-oriented dialog systems, we have conducted a survey of publicly available datasets for the tasks of intent classification and slot-filling. We catalog the important characteristics of each dataset, and offer discussion on the applicability, strengths, and weaknesses of each. Our goal is that this survey aids in increasing the accessibility of these datasets, which we hope will enable their use in future evaluations of intent classification and slot-filling models for task-oriented dialog systems.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1167249695,
        "newsscientist":0.1387647299,
        "technologyreview":0.2500560872,
        "venturebeat":0.2782914511,
        "wired":0.2157794085,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13211v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658877603000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.13757v1",
        "predicted_newsworthiness":0.4777872957,
        "title":"The Leaf Clinical Trials Corpus: a new resource for query generation from clinical trial eligibility criteria",
        "summary":"Identifying cohorts of patients based on eligibility criteria such as medical conditions, procedures, and medication use is critical to recruitment for clinical trials. Such criteria are often most naturally described in free-text, using language familiar to clinicians and researchers. In order to identify potential participants at scale, these criteria must first be translated into queries on clinical databases, which can be labor-intensive and error-prone. Natural language processing (NLP) methods offer a potential means of such conversion into database queries automatically. However they must first be trained and evaluated using corpora which capture clinical trials criteria in sufficient detail. In this paper, we introduce the Leaf Clinical Trials (LCT) corpus, a human-annotated corpus of over 1,000 clinical trial eligibility criteria descriptions using highly granular structured labels capturing a range of biomedical phenomena. We provide details of our schema, annotation process, corpus quality, and statistics. Additionally, we present baseline information extraction results on this corpus as benchmarks for future work.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1772075544,
        "newsscientist":0.1913948866,
        "technologyreview":0.2471263987,
        "venturebeat":0.2304582178,
        "wired":0.1673651381,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13757v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658949744000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00617v1",
        "predicted_newsworthiness":0.4776676377,
        "title":"Improving Fine-Grained Visual Recognition in Low Data Regimes via Self-Boosting Attention Mechanism",
        "summary":"The challenge of fine-grained visual recognition often lies in discovering the key discriminative regions. While such regions can be automatically identified from a large-scale labeled dataset, a similar method might become less effective when only a few annotations are available. In low data regimes, a network often struggles to choose the correct regions for recognition and tends to overfit spurious correlated patterns from the training data. To tackle this issue, this paper proposes the self-boosting attention mechanism, a novel method for regularizing the network to focus on the key regions shared across samples and classes. Specifically, the proposed method first generates an attention map for each training image, highlighting the discriminative part for identifying the ground-truth object category. Then the generated attention maps are used as pseudo-annotations. The network is enforced to fit them as an auxiliary task. We call this approach the self-boosting attention mechanism (SAM). We also develop a variant by using SAM to create multiple attention maps to pool convolutional maps in a style of bilinear pooling, dubbed SAM-Bilinear. Through extensive experimental studies, we show that both methods can significantly improve fine-grained visual recognition performance on low data regimes and can be incorporated into existing network architectures. The source code is publicly available at: https:\/\/github.com\/GANPerf\/SAM",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0946763966,
        "newsscientist":0.1468863233,
        "technologyreview":0.2490954903,
        "venturebeat":0.2128249162,
        "wired":0.1652076723,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00617v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659332187000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02049v1",
        "predicted_newsworthiness":0.4776091941,
        "title":"AutoLaparo: A New Dataset of Integrated Multi-tasks for Image-guided Surgical Automation in Laparoscopic Hysterectomy",
        "summary":"Computer-assisted minimally invasive surgery has great potential in benefiting modern operating theatres. The video data streamed from the endoscope provides rich information to support context-awareness for next-generation intelligent surgical systems. To achieve accurate perception and automatic manipulation during the procedure, learning based technique is a promising way, which enables advanced image analysis and scene understanding in recent years. However, learning such models highly relies on large-scale, high-quality, and multi-task labelled data. This is currently a bottleneck for the topic, as available public dataset is still extremely limited in the field of CAI. In this paper, we present and release the first integrated dataset (named AutoLaparo) with multiple image-based perception tasks to facilitate learning-based automation in hysterectomy surgery. Our AutoLaparo dataset is developed based on full-length videos of entire hysterectomy procedures. Specifically, three different yet highly correlated tasks are formulated in the dataset, including surgical workflow recognition, laparoscope motion prediction, and instrument and key anatomy segmentation. In addition, we provide experimental results with state-of-the-art models as reference benchmarks for further model developments and evaluations on this dataset. The dataset is available at https:\/\/autolaparo.github.io.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1055414595,
        "newsscientist":0.181482512,
        "technologyreview":0.2746870214,
        "venturebeat":0.2442248735,
        "wired":0.1975091107,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02049v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659532643000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00802v1",
        "predicted_newsworthiness":0.4775749982,
        "title":"Underwater autonomous mapping and characterization of marine debris in urban water bodies",
        "summary":"Marine debris originating from human activity has been accumulating in underwater environments such as oceans, lakes, and rivers for decades. The extent, type, and amount of waste is hard to assess as the exact mechanisms for spread are not understood, yielding unknown consequences for the marine environment and human health. Methods for detecting and mapping marine debris is therefore vital in order to gain insight into pollution dynamics, which in turn can be used to effectively plan and execute physical removal. Using an autonomous underwater vehicle (AUV), equipped with an underwater hyperspectral imager (UHI) and stereo-camera, marine debris was autonomously detected, mapped and quantified in the sheltered bay Store Lungegaardsvann in Bergen, Norway.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1929244816,
        "newsscientist":0.2416365825,
        "technologyreview":0.2051435774,
        "venturebeat":0.1645792803,
        "wired":0.193229788,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00802v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659357398000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13341v1",
        "predicted_newsworthiness":0.4775292435,
        "title":"Towards Clear Expectations for Uncertainty Estimation",
        "summary":"If Uncertainty Quantification (UQ) is crucial to achieve trustworthy Machine Learning (ML), most UQ methods suffer from disparate and inconsistent evaluation protocols. We claim this inconsistency results from the unclear requirements the community expects from UQ. This opinion paper offers a new perspective by specifying those requirements through five downstream tasks where we expect uncertainty scores to have substantial predictive power. We design these downstream tasks carefully to reflect real-life usage of ML models. On an example benchmark of 7 classification datasets, we did not observe statistical superiority of state-of-the-art intrinsic UQ methods against simple baselines. We believe that our findings question the very rationale of why we quantify uncertainty and call for a standardized protocol for UQ evaluation based on metrics proven to be relevant for the ML practitioner.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1457694857,
        "newsscientist":0.1850165142,
        "technologyreview":0.3082120934,
        "venturebeat":0.3014087263,
        "wired":0.2218694909,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13341v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658908257000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01916v1",
        "predicted_newsworthiness":0.4769157329,
        "title":"N-RPN: Hard Example Learning for Region Proposal Networks",
        "summary":"The region proposal task is to generate a set of candidate regions that contain an object. In this task, it is most important to propose as many candidates of ground-truth as possible in a fixed number of proposals. In a typical image, however, there are too few hard negative examples compared to the vast number of easy negatives, so region proposal networks struggle to train on hard negatives. Because of this problem, networks tend to propose hard negatives as candidates, while failing to propose ground-truth candidates, which leads to poor performance. In this paper, we propose a Negative Region Proposal Network(nRPN) to improve Region Proposal Network(RPN). The nRPN learns from the RPN's false positives and provide hard negative examples to the RPN. Our proposed nRPN leads to a reduction in false positives and better RPN performance. An RPN trained with an nRPN achieves performance improvements on the PASCAL VOC 2007 dataset.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0779476324,
        "newsscientist":0.1324496005,
        "technologyreview":0.1992369115,
        "venturebeat":0.1721466211,
        "wired":0.1315075428,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01916v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659516513000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12376v1",
        "predicted_newsworthiness":0.4765731962,
        "title":"Fine-Tuning BERT for Automatic ADME Semantic Labeling in FDA Drug Labeling to Enhance Product-Specific Guidance Assessment",
        "summary":"Product-specific guidances (PSGs) recommended by the United States Food and Drug Administration (FDA) are instrumental to promote and guide generic drug product development. To assess a PSG, the FDA assessor needs to take extensive time and effort to manually retrieve supportive drug information of absorption, distribution, metabolism, and excretion (ADME) from the reference listed drug labeling. In this work, we leveraged the state-of-the-art pre-trained language models to automatically label the ADME paragraphs in the pharmacokinetics section from the FDA-approved drug labeling to facilitate PSG assessment. We applied a transfer learning approach by fine-tuning the pre-trained Bidirectional Encoder Representations from Transformers (BERT) model to develop a novel application of ADME semantic labeling, which can automatically retrieve ADME paragraphs from drug labeling instead of manual work. We demonstrated that fine-tuning the pre-trained BERT model can outperform the conventional machine learning techniques, achieving up to 11.6% absolute F1 improvement. To our knowledge, we were the first to successfully apply BERT to solve the ADME semantic labeling task. We further assessed the relative contribution of pre-training and fine-tuning to the overall performance of the BERT model in the ADME semantic labeling task using a series of analysis methods such as attention similarity and layer-based ablations. Our analysis revealed that the information learned via fine-tuning is focused on task-specific knowledge in the top layers of the BERT, whereas the benefit from the pre-trained BERT model is from the bottom layers.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1477341581,
        "newsscientist":0.1857614402,
        "technologyreview":0.2484351994,
        "venturebeat":0.2448266548,
        "wired":0.1673124734,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12376v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai",
            "cs.lg"
        ],
        "published":1658771016000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14722v1",
        "predicted_newsworthiness":0.4764965928,
        "title":"Automatic Reward Design via Learning Motivation-Consistent Intrinsic Rewards",
        "summary":"Reward design is a critical part of the application of reinforcement learning, the performance of which strongly depends on how well the reward signal frames the goal of the designer and how well the signal assesses progress in reaching that goal. In many cases, the extrinsic rewards provided by the environment (e.g., win or loss of a game) are very sparse and make it difficult to train agents directly. Researchers usually assist the learning of agents by adding some auxiliary rewards in practice. However, designing auxiliary rewards is often turned to a trial-and-error search for reward settings that produces acceptable results. In this paper, we propose to automatically generate goal-consistent intrinsic rewards for the agent to learn, by maximizing which the expected accumulative extrinsic rewards can be maximized. To this end, we introduce the concept of motivation which captures the underlying goal of maximizing certain rewards and propose the motivation based reward design method. The basic idea is to shape the intrinsic rewards by minimizing the distance between the intrinsic and extrinsic motivations. We conduct extensive experiments and show that our method performs better than the state-of-the-art methods in handling problems of delayed reward, exploration, and credit assignment.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1230532245,
        "newsscientist":0.1715647618,
        "technologyreview":0.2496238419,
        "venturebeat":0.2361163868,
        "wired":0.1829443136,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14722v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659106322000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01773v1",
        "predicted_newsworthiness":0.4764285862,
        "title":"On CAD Informed Adaptive Robotic Assembly",
        "summary":"We introduce a robotic assembly system that streamlines the design-to-make workflow for going from a CAD model of a product assembly to a fully programmed and adaptive assembly process. Our system captures (in the CAD tool) the intent of the assembly process for a specific robotic workcell and generates a recipe of task-level instructions. By integrating visual sensing with deep-learned perception models, the robots infer the necessary actions to assemble the design from the generated recipe. The perception models are trained directly from simulation, allowing the system to identify various parts based on CAD information. We demonstrate the system with a workcell of two robots to assemble interlocking 3D part designs. We first build and tune the assembly process in simulation, verifying the generated recipe. Finally, the real robotic workcell assembles the design using the same behavior.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0765273283,
        "newsscientist":0.160139117,
        "technologyreview":0.2759889798,
        "venturebeat":0.2401424087,
        "wired":0.2063667208,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01773v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659479051000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13353v1",
        "predicted_newsworthiness":0.4763875071,
        "title":"One-Trimap Video Matting",
        "summary":"Recent studies made great progress in video matting by extending the success of trimap-based image matting to the video domain. In this paper, we push this task toward a more practical setting and propose One-Trimap Video Matting network (OTVM) that performs video matting robustly using only one user-annotated trimap. A key of OTVM is the joint modeling of trimap propagation and alpha prediction. Starting from baseline trimap propagation and alpha prediction networks, our OTVM combines the two networks with an alpha-trimap refinement module to facilitate information flow. We also present an end-to-end training strategy to take full advantage of the joint model. Our joint modeling greatly improves the temporal stability of trimap propagation compared to the previous decoupled methods. We evaluate our model on two latest video matting benchmarks, Deep Video Matting and VideoMatting108, and outperform state-of-the-art by significant margins (MSE improvements of 56.4% and 56.7%, respectively). The source code and model are available online: https:\/\/github.com\/Hongje\/OTVM.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0671505467,
        "newsscientist":0.0932497082,
        "technologyreview":0.1656097743,
        "venturebeat":0.1599871641,
        "wired":0.1327517513,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13353v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658909981000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12327v1",
        "predicted_newsworthiness":0.4762111585,
        "title":"Technical Report: Assisting Backdoor Federated Learning with Whole Population Knowledge Alignment",
        "summary":"Due to the distributed nature of Federated Learning (FL), researchers have uncovered that FL is vulnerable to backdoor attacks, which aim at injecting a sub-task into the FL without corrupting the performance of the main task. Single-shot backdoor attack achieves high accuracy on both the main task and backdoor sub-task when injected at the FL model convergence. However, the early-injected single-shot backdoor attack is ineffective because: (1) the maximum backdoor effectiveness is not reached at injection because of the dilution effect from normal local updates; (2) the backdoor effect decreases quickly as the backdoor will be overwritten by the newcoming normal local updates. In this paper, we strengthen the early-injected single-shot backdoor attack utilizing FL model information leakage. We show that the FL convergence can be expedited if the client trains on a dataset that mimics the distribution and gradients of the whole population. Based on this observation, we proposed a two-phase backdoor attack, which includes a preliminary phase for the subsequent backdoor attack. In the preliminary phase, the attacker-controlled client first launches a whole population distribution inference attack and then trains on a locally crafted dataset that is aligned with both the gradient and inferred distribution. Benefiting from the preliminary phase, the later injected backdoor achieves better effectiveness as the backdoor effect will be less likely to be diluted by the normal model updates. Extensive experiments are conducted on MNIST dataset under various data heterogeneity settings to evaluate the effectiveness of the proposed backdoor attack. Results show that the proposed backdoor outperforms existing backdoor attacks in both success rate and longevity, even when defense mechanisms are in place.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1235282369,
        "newsscientist":0.1762479071,
        "technologyreview":0.3074635436,
        "venturebeat":0.2750529374,
        "wired":0.2088559397,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12327v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.lg"
        ],
        "published":1658767111000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2208.00519v1",
        "predicted_newsworthiness":0.4759729337,
        "title":"Assessing The Performance of YOLOv5 Algorithm for Detecting Volunteer Cotton Plants in Corn Fields at Three Different Growth Stages",
        "summary":"The boll weevil (Anthonomus grandis L.) is a serious pest that primarily feeds on cotton plants. In places like Lower Rio Grande Valley of Texas, due to sub-tropical climatic conditions, cotton plants can grow year-round and therefore the left-over seeds from the previous season during harvest can continue to grow in the middle of rotation crops like corn (Zea mays L.) and sorghum (Sorghum bicolor L.). These feral or volunteer cotton (VC) plants when reach the pinhead squaring phase (5-6 leaf stage) can act as hosts for the boll weevil pest. The Texas Boll Weevil Eradication Program (TBWEP) employs people to locate and eliminate VC plants growing by the side of roads or fields with rotation crops but the ones growing in the middle of fields remain undetected. In this paper, we demonstrate the application of computer vision (CV) algorithm based on You Only Look Once version 5 (YOLOv5) for detecting VC plants growing in the middle of corn fields at three different growth stages (V3, V6, and VT) using unmanned aircraft systems (UAS) remote sensing imagery. All the four variants of YOLOv5 (s, m, l, and x) were used and their performances were compared based on classification accuracy, mean average precision (mAP), and F1-score. It was found that YOLOv5s could detect VC plants with a maximum classification accuracy of 98% and mAP of 96.3 % at the V6 stage of corn while YOLOv5s and YOLOv5m resulted in the lowest classification accuracy of 85% and YOLOv5m and YOLOv5l had the least mAP of 86.5% at the VT stage on images of size 416 x 416 pixels. The developed CV algorithm has the potential to effectively detect and locate VC plants growing in the middle of corn fields as well as expedite the management aspects of TBWEP.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.146307951,
        "newsscientist":0.2012286793,
        "technologyreview":0.2133000681,
        "venturebeat":0.1788043664,
        "wired":0.1765097159,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00519v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659301420000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14140v1",
        "predicted_newsworthiness":0.4752705672,
        "title":"Playing a 2D Game Indefinitely using NEAT and Reinforcement Learning",
        "summary":"For over a decade now, robotics and the use of artificial agents have become a common thing.Testing the performance of new path finding or search space optimization algorithms has also become a challenge as they require simulation or an environment to test them.The creation of artificial environments with artificial agents is one of the methods employed to test such algorithms.Games have also become an environment to test them.The performance of the algorithms can be compared by using artificial agents that will behave according to the algorithm in the environment they are put in.The performance parameters can be, how quickly the agent is able to differentiate between rewarding actions and hostile actions.This can be tested by placing the agent in an environment with different types of hurdles and the goal of the agent is to reach the farthest by taking decisions on actions that will lead to avoiding all the obstacles.The environment chosen is a game called \"Flappy Bird\".The goal of the game is to make the bird fly through a set of pipes of random heights.The bird must go in between these pipes and must not hit the top, the bottom, or the pipes themselves.The actions that the bird can take are either to flap its wings or drop down with gravity.The algorithms that are enforced on the artificial agents are NeuroEvolution of Augmenting Topologies (NEAT) and Reinforcement Learning.The NEAT algorithm takes an \"N\" initial population of artificial agents.They follow genetic algorithms by considering an objective function, crossover, mutation, and augmenting topologies.Reinforcement learning, on the other hand, remembers the state, the action taken at that state, and the reward received for the action taken using a single agent and a Deep Q-learning Network.The performance of the NEAT algorithm improves as the initial population of the artificial agents is increased.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1479927587,
        "newsscientist":0.2416220844,
        "technologyreview":0.3350522129,
        "venturebeat":0.2909525026,
        "wired":0.2641180294,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14140v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659020486000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00596v1",
        "predicted_newsworthiness":0.4747801657,
        "title":"A System for Imitation Learning of Contact-Rich Bimanual Manipulation Policies",
        "summary":"In this paper, we discuss a framework for teaching bimanual manipulation tasks by imitation. To this end, we present a system and algorithms for learning compliant and contact-rich robot behavior from human demonstrations. The presented system combines insights from admittance control and machine learning to extract control policies that can (a) recover from and adapt to a variety of disturbances in time and space, while also (b) effectively leveraging physical contact with the environment. We demonstrate the effectiveness of our approach using a real-world insertion task involving multiple simultaneous contacts between a manipulated object and insertion pegs. We also investigate efficient means of collecting training data for such bimanual settings. To this end, we conduct a human-subject study and analyze the effort and mental demand as reported by the users. Our experiments show that, while harder to provide, the additional force\/torque information available in teleoperated demonstrations is crucial for phase estimation and task success. Ultimately, force\/torque data substantially improves manipulation robustness, resulting in a 90% success rate in a multipoint insertion task. Code and videos can be found at https:\/\/bimanualmanipulation.com\/",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.093502627,
        "newsscientist":0.1739386072,
        "technologyreview":0.2429412664,
        "venturebeat":0.1956010603,
        "wired":0.1905075479,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00596v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.hc"
        ],
        "published":1659325913000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00376v1",
        "predicted_newsworthiness":0.4745962271,
        "title":"Using Chatbots to Teach Languages",
        "summary":"This paper reports on progress towards building an online language learning tool to provide learners with conversational experience by using dialog systems as conversation practice partners. Our system can adapt to users' language proficiency on the fly. We also provide automatic grammar error feedback to help users learn from their mistakes. According to our first adopters, our system is entertaining and useful. Furthermore, we will provide the learning technology community a large-scale conversation dataset on language learning and grammar correction. Our next step is to make our system more adaptive to user profile information by using reinforcement learning algorithms.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1272822034,
        "newsscientist":0.1547555617,
        "technologyreview":0.2866228661,
        "venturebeat":0.2842276629,
        "wired":0.2160664603,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00376v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1659250895000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00478v1",
        "predicted_newsworthiness":0.4742131621,
        "title":"Robot Policy Learning from Demonstration Using Advantage Weighting and Early Termination",
        "summary":"Learning robotic tasks in the real world is still highly challenging and effective practical solutions remain to be found. Traditional methods used in this area are imitation learning and reinforcement learning, but they both have limitations when applied to real robots. Combining reinforcement learning with pre-collected demonstrations is a promising approach that can help in learning control policies to solve robotic tasks. In this paper, we propose an algorithm that uses novel techniques to leverage offline expert data using offline and online training to obtain faster convergence and improved performance. The proposed algorithm (AWET) weights the critic losses with a novel agent advantage weight to improve over the expert data. In addition, AWET makes use of an automatic early termination technique to stop and discard policy rollouts that are not similar to expert trajectories -- to prevent drifting far from the expert data. In an ablation study, AWET showed improved and promising performance when compared to state-of-the-art baselines on four standard robotic tasks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1010498607,
        "newsscientist":0.1514793706,
        "technologyreview":0.2772962719,
        "venturebeat":0.235155203,
        "wired":0.1855973677,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00478v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ro"
        ],
        "published":1659289462000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13329v1",
        "predicted_newsworthiness":0.4738968918,
        "title":"Gaia: Graph Neural Network with Temporal Shift aware Attention for Gross Merchandise Value Forecast in E-commerce",
        "summary":"E-commerce has gone a long way in empowering merchants through the internet. In order to store the goods efficiently and arrange the marketing resource properly, it is important for them to make the accurate gross merchandise value (GMV) prediction. However, it's nontrivial to make accurate prediction with the deficiency of digitized data. In this article, we present a solution to better forecast GMV inside Alipay app. Thanks to graph neural networks (GNN) which has great ability to correlate different entities to enrich information, we propose Gaia, a graph neural network (GNN) model with temporal shift aware attention. Gaia leverages the relevant e-seller' sales information and learn neighbor correlation based on temporal dependencies. By testing on Alipay's real dataset and comparing with other baselines, Gaia has shown the best performance. And Gaia is deployed in the simulated online environment, which also achieves great improvement compared with baselines.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1299381844,
        "newsscientist":0.1614332332,
        "technologyreview":0.2837066973,
        "venturebeat":0.3283630608,
        "wired":0.2186448524,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13329v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658906634000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00783v1",
        "predicted_newsworthiness":0.4737679072,
        "title":"Location retrieval using visible landmarks based qualitative place signatures",
        "summary":"Location retrieval based on visual information is to retrieve the location of an agent (e.g. human, robot) or the area they see by comparing the observations with a certain form of representation of the environment. Existing methods generally require precise measurement and storage of the observed environment features, which may not always be robust due to the change of season, viewpoint, occlusion, etc. They are also challenging to scale up and may not be applicable for humans due to the lack of measuring\/imaging devices. Considering that humans often use less precise but easily produced qualitative spatial language and high-level semantic landmarks when describing an environment, a qualitative location retrieval method is proposed in this work by describing locations\/places using qualitative place signatures (QPS), defined as the perceived spatial relations between ordered pairs of co-visible landmarks from viewers' perspective. After dividing the space into place cells each with individual signatures attached, a coarse-to-fine location retrieval method is proposed to efficiently identify the possible location(s) of viewers based on their qualitative observations. The usability and effectiveness of the proposed method were evaluated using openly available landmark datasets, together with simulated observations by considering the possible perception error.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1461665363,
        "newsscientist":0.1796202378,
        "technologyreview":0.2314862901,
        "venturebeat":0.2268272796,
        "wired":0.2156742334,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00783v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658843869000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00457v1",
        "predicted_newsworthiness":0.4734079811,
        "title":"INSightR-Net: Interpretable Neural Network for Regression using Similarity-based Comparisons to Prototypical Examples",
        "summary":"Convolutional neural networks (CNNs) have shown exceptional performance for a range of medical imaging tasks. However, conventional CNNs are not able to explain their reasoning process, therefore limiting their adoption in clinical practice. In this work, we propose an inherently interpretable CNN for regression using similarity-based comparisons (INSightR-Net) and demonstrate our methods on the task of diabetic retinopathy grading. A prototype layer incorporated into the architecture enables visualization of the areas in the image that are most similar to learned prototypes. The final prediction is then intuitively modeled as a mean of prototype labels, weighted by the similarities. We achieved competitive prediction performance with our INSightR-Net compared to a ResNet baseline, showing that it is not necessary to compromise performance for interpretability. Furthermore, we quantified the quality of our explanations using sparsity and diversity, two concepts considered important for a good explanation, and demonstrated the effect of several parameters on the latent space embeddings.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1230154033,
        "newsscientist":0.1828897517,
        "technologyreview":0.3016626147,
        "venturebeat":0.2634319158,
        "wired":0.1888885879,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00457v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659282975000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.09934v2",
        "predicted_newsworthiness":0.4731104995,
        "title":"DeepIPC: Deeply Integrated Perception and Control for Mobile Robot in Real Environments",
        "summary":"We propose DeepIPC, an end-to-end multi-task model that handles both perception and control tasks in driving a mobile robot autonomously. The model consists of two main parts, perception and controller modules. The perception module takes RGB image and depth map to perform semantic segmentation and bird's eye view (BEV) semantic mapping along with providing their encoded features. Meanwhile, the controller module processes these features with the measurement of GNSS locations and angular speed to estimate waypoints that come with latent features. Then, two different agents are used to translate waypoints and latent features into a set of navigational controls to drive the robot. The model is evaluated by predicting driving records and performing automated driving under various conditions in the real environment. Based on the experimental results, DeepIPC achieves the best drivability and multi-task performance even with fewer parameters compared to the other models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.07823664,
        "newsscientist":0.1711707022,
        "technologyreview":0.2999380885,
        "venturebeat":0.2658962278,
        "wired":0.2397066717,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.09934v2",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai",
            "cs.cv"
        ],
        "published":1658326835000,
        "published_hr":"Jul 20, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01340v1",
        "predicted_newsworthiness":0.4728095894,
        "title":"Parameterizing Kterm Hashing",
        "summary":"Kterm Hashing provides an innovative approach to novelty detection on massive data streams. Previous research focused on maximizing the efficiency of Kterm Hashing and succeeded in scaling First Story Detection to Twitter-size data stream without sacrificing detection accuracy. In this paper, we focus on improving the effectiveness of Kterm Hashing. Traditionally, all kterms are considered as equally important when calculating a document's degree of novelty with respect to the past. We believe that certain kterms are more important than others and hypothesize that uniform kterm weights are sub-optimal for determining novelty in data streams. To validate our hypothesis, we parameterize Kterm Hashing by assigning weights to kterms based on their characteristics. Our experiments apply Kterm Hashing in a First Story Detection setting and reveal that parameterized Kterm Hashing can surpass state-of-the-art detection accuracy and significantly outperform the uniformly weighted approach.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.132337773,
        "newsscientist":0.1612088941,
        "technologyreview":0.2171913508,
        "venturebeat":0.2309442007,
        "wired":0.2192084335,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01340v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1659434462000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.12876v1",
        "predicted_newsworthiness":0.4723897334,
        "title":"Repeated Environment Inference for Invariant Learning",
        "summary":"We study the problem of invariant learning when the environment labels are unknown. We focus on the invariant representation notion when the Bayes optimal conditional label distribution is the same across different environments. Previous work conducts Environment Inference (EI) by maximizing the penalty term from Invariant Risk Minimization (IRM) framework. The EI step uses a reference model which focuses on spurious correlations to efficiently reach a good environment partition. However, it is not clear how to find such a reference model. In this work, we propose to repeat the EI process and retrain an ERM model on the \\textit{majority} environment inferred by the previous EI step. Under mild assumptions, we find that this iterative process helps learn a representation capturing the spurious correlation better than the single step. This results in better Environment Inference and better Invariant Learning. We show that this method outperforms baselines on both synthetic and real-world datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0959680954,
        "newsscientist":0.1339714458,
        "technologyreview":0.2178070919,
        "venturebeat":0.1957171832,
        "wired":0.1321021839,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12876v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658840842000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14138v1",
        "predicted_newsworthiness":0.4722917296,
        "title":"Towards Robust Ad Hoc Teamwork Agents By Creating Diverse Training Teammates",
        "summary":"Ad hoc teamwork (AHT) is the problem of creating an agent that must collaborate with previously unseen teammates without prior coordination. Many existing AHT methods can be categorised as type-based methods, which require a set of predefined teammates for training. Designing teammate types for training is a challenging issue that determines the generalisation performance of agents when dealing with teammate types unseen during training. In this work, we propose a method to discover diverse teammate types based on maximising best response diversity metrics. We show that our proposed approach yields teammate types that require a wider range of best responses from the learner during collaboration, which potentially improves the robustness of a learner's performance in AHT compared to alternative methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1167694399,
        "newsscientist":0.1742325389,
        "technologyreview":0.3022478384,
        "venturebeat":0.2833721578,
        "wired":0.2041648895,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14138v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659020312000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01780v1",
        "predicted_newsworthiness":0.4722518537,
        "title":"Striking a Balance: Reader Takeaways and Preferences when Integrating Text and Charts",
        "summary":"While visualizations are an effective way to represent insights about information, they rarely stand alone. When designing a visualization, text is often added to provide additional context and guidance for the reader. However, there is little experimental evidence to guide designers as to what is the right amount of text to show within a chart, what its qualitative properties should be, and where it should be placed. Prior work also shows variation in personal preferences for charts versus textual representations. In this paper, we explore several research questions about the relative value of textual components of visualizations. 302 participants ranked univariate line charts containing varying amounts of text, ranging from no text (except for the axes) to a written paragraph with no visuals. Participants also described what information they could take away from line charts containing text with varying semantic content. We find that heavily annotated charts were not penalized. In fact, participants preferred the charts with the largest number of textual annotations over charts with fewer annotations or text alone. We also find effects of semantic content. For instance, the text that describes statistical or relational components of a chart leads to more takeaways referring to statistics or relational comparisons than text describing elemental or encoded components. Finally, we find different effects for the semantic levels based on the placement of the text on the chart; some kinds of information are best placed in the title, while others should be placed closer to the data. We compile these results into four chart design guidelines and discuss future implications for the combination of text and charts.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1988727568,
        "newsscientist":0.1835139004,
        "technologyreview":0.2212928327,
        "venturebeat":0.2156097059,
        "wired":0.2137303957,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01780v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1659482025000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.13306v1",
        "predicted_newsworthiness":0.4721416936,
        "title":"Object-ABN: Learning to Generate Sharp Attention Maps for Action Recognition",
        "summary":"In this paper we propose an extension of the Attention Branch Network (ABN) by using instance segmentation for generating sharper attention maps for action recognition. Methods for visual explanation such as Grad-CAM usually generate blurry maps which are not intuitive for humans to understand, particularly in recognizing actions of people in videos. Our proposed method, Object-ABN, tackles this issue by introducing a new mask loss that makes the generated attention maps close to the instance segmentation result. Further the PC loss and multiple attention maps are introduced to enhance the sharpness of the maps and improve the performance of classification. Experimental results with UCF101 and SSv2 shows that the generated maps by the proposed method are much clearer qualitatively and quantitatively than those of the original ABN.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0926149744,
        "newsscientist":0.1397404784,
        "technologyreview":0.2133655456,
        "venturebeat":0.1843877838,
        "wired":0.1559314226,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13306v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658899858000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12392v1",
        "predicted_newsworthiness":0.4720135424,
        "title":"Self-Distilled Vision Transformer for Domain Generalization",
        "summary":"In recent past, several domain generalization (DG) methods have been proposed, showing encouraging performance, however, almost all of them build on convolutional neural networks (CNNs). There is little to no progress on studying the DG performance of vision transformers (ViTs), which are challenging the supremacy of CNNs on standard benchmarks, often built on i.i.d assumption. This renders the real-world deployment of ViTs doubtful. In this paper, we attempt to explore ViTs towards addressing the DG problem. Similar to CNNs, ViTs also struggle in out-of-distribution scenarios and the main culprit is overfitting to source domains. Inspired by the modular architecture of ViTs, we propose a simple DG approach for ViTs, coined as self-distillation for ViTs. It reduces the overfitting to source domains by easing the learning of input-output mapping problem through curating non-zero entropy supervisory signals for intermediate transformer blocks. Further, it does not introduce any new parameters and can be seamlessly plugged into the modular composition of different ViTs. We empirically demonstrate notable performance gains with different DG baselines and various ViT backbones in five challenging datasets. Moreover, we report favorable performance against recent state-of-the-art DG methods. Our code along with pre-trained models are publicly available at: https:\/\/github.com\/maryam089\/SDViT",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0930949948,
        "newsscientist":0.1472342372,
        "technologyreview":0.2750222574,
        "venturebeat":0.2497664704,
        "wired":0.1857880153,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12392v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1658771825000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13866v1",
        "predicted_newsworthiness":0.4719682711,
        "title":"MKANet: A Lightweight Network with Sobel Boundary Loss for Efficient Land-cover Classification of Satellite Remote Sensing Imagery",
        "summary":"Land cover classification is a multi-class segmentation task to classify each pixel into a certain natural or man-made category of the earth surface, such as water, soil, natural vegetation, crops, and human infrastructure. Limited by hardware computational resources and memory capacity, most existing studies preprocessed original remote sensing images by down sampling or cropping them into small patches less than 512*512 pixels before sending them to a deep neural network. However, down sampling images incurs spatial detail loss, renders small segments hard to discriminate, and reverses the spatial resolution progress obtained by decades of years of efforts. Cropping images into small patches causes a loss of long-range context information, and restoring the predicted results to their original size brings extra latency. In response to the above weaknesses, we present an efficient lightweight semantic segmentation network termed MKANet. Aimed at the characteristics of top view high-resolution remote sensing imagery, MKANet utilizes sharing kernels to simultaneously and equally handle ground segments of inconsistent scales, and also employs parallel and shallow architecture to boost inference speed and friendly support image patches more than 10X larger. To enhance boundary and small segments discrimination, we also propose a method that captures category impurity areas, exploits boundary information and exerts an extra penalty on boundaries and small segment misjudgment. Both visual interpretations and quantitative metrics of extensive experiments demonstrate that MKANet acquires state-of-the-art accuracy on two land-cover classification datasets and infers 2X faster than other competitive lightweight networks. All these merits highlight the potential of MKANet in practical applications.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1249132719,
        "newsscientist":0.1545659672,
        "technologyreview":0.2067908494,
        "venturebeat":0.1810665729,
        "wired":0.1565430016,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13866v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658978948000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01341v1",
        "predicted_newsworthiness":0.4718538378,
        "title":"Bias in (Non)-Contextual Clinical Word Embeddings",
        "summary":"Clinical word embeddings are extensively used in various Bio-NLP problems as a state-of-the-art feature vector representation. Although they are quite successful at the semantic representation of words, due to the dataset - which potentially carries statistical and societal bias - on which they are trained, they might exhibit gender stereotypes. This study analyses gender bias of clinical embeddings on three medical categories: mental disorders, sexually transmitted diseases, and personality traits. To this extent, we analyze two different pre-trained embeddings namely (contextualized) clinical-BERT and (non-contextualized) BioWordVec. We show that both embeddings are biased towards sensitive gender groups but BioWordVec exhibits a higher bias than clinical-BERT for all three categories. Moreover, our analyses show that clinical embeddings carry a high degree of bias for some medical terms and diseases which is conflicting with medical literature. Having such an ill-founded relationship might cause harm in downstream applications that use clinical embeddings.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2205597532,
        "newsscientist":0.2106547359,
        "technologyreview":0.2703463246,
        "venturebeat":0.2372603024,
        "wired":0.2255224667,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01341v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.cy"
        ],
        "published":1659434541000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00725v1",
        "predicted_newsworthiness":0.4717202549,
        "title":"Fashion Recommendation Based on Style and Social Events",
        "summary":"Fashion recommendation is often declined as the task of finding complementary items given a query garment or retrieving outfits that are suitable for a given user. In this work we address the problem by adding an additional semantic layer based on the style of the proposed dressing. We model style according to two important aspects: the mood and the emotion concealed behind color combination patterns and the appropriateness of the retrieved garments for a given type of social event. To address the former we rely on Shigenobu Kobayashi's color image scale, which associated emotional patterns and moods to color triples. The latter instead is analyzed by extracting garments from images of social events. Overall, we integrate in a state of the art garment recommendation framework a style classifier and an event classifier in order to condition recommendation on a given query.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1563761285,
        "newsscientist":0.1654615028,
        "technologyreview":0.2198608138,
        "venturebeat":0.2230731142,
        "wired":0.2238884109,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00725v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ir"
        ],
        "published":1659348894000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12984v1",
        "predicted_newsworthiness":0.4714941369,
        "title":"Explaining Deep Neural Networks for Point Clouds using Gradient-based Visualisations",
        "summary":"Explaining decisions made by deep neural networks is a rapidly advancing research topic. In recent years, several approaches have attempted to provide visual explanations of decisions made by neural networks designed for structured 2D image input data. In this paper, we propose a novel approach to generate coarse visual explanations of networks designed to classify unstructured 3D data, namely point clouds. Our method uses gradients flowing back to the final feature map layers and maps these values as contributions of the corresponding points in the input point cloud. Due to dimensionality disagreement and lack of spatial consistency between input points and final feature maps, our approach combines gradients with points dropping to compute explanations of different parts of the point cloud iteratively. The generality of our approach is tested on various point cloud classification networks, including 'single object' networks PointNet, PointNet++, DGCNN, and a 'scene' network VoteNet. Our method generates symmetric explanation maps that highlight important regions and provide insight into the decision-making process of network architectures. We perform an exhaustive evaluation of trust and interpretability of our explanation method against comparative approaches using quantitative, quantitative and human studies. All our code is implemented in PyTorch and will be made publicly available.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1480880077,
        "newsscientist":0.1901787496,
        "technologyreview":0.2848351987,
        "venturebeat":0.2631062116,
        "wired":0.2215793271,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12984v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658850128000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13821v1",
        "predicted_newsworthiness":0.4714248376,
        "title":"Multi-Objective Provisioning of Network Slices using Deep Reinforcement Learning",
        "summary":"Network Slicing (NS) is crucial for efficiently enabling divergent network applications in next generation networks. Nonetheless, the complex Quality of Service (QoS) requirements and diverse heterogeneity in network services entails high computational time for Network Slice Provisioning (NSP) optimization. The legacy optimization methods are challenging to meet the low latency and high reliability of network applications. To this end, we model the real-time NSP as an Online Network Slice Provisioning (ONSP) problem. Specifically, we formulate the ONSP problem as an online Multi-Objective Integer Programming Optimization (MOIPO) problem. Then, we approximate the solution of the MOIPO problem by applying the Proximal Policy Optimization (PPO) method to the traffic demand prediction. Our simulation results show the effectiveness of the proposed method compared to the state-of-the-art MOIPO solvers with a lower SLA violation rate and network operation cost.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0747964844,
        "newsscientist":0.0882514348,
        "technologyreview":0.1751672383,
        "venturebeat":0.2054354198,
        "wired":0.1378274819,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13821v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni",
            "cs.lg"
        ],
        "published":1658963062000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.13137v1",
        "predicted_newsworthiness":0.4713401115,
        "title":"Bayesian Evidential Learning for Few-Shot Classification",
        "summary":"Few-Shot Classification(FSC) aims to generalize from base classes to novel classes given very limited labeled samples, which is an important step on the path toward human-like machine learning. State-of-the-art solutions involve learning to find a good metric and representation space to compute the distance between samples. Despite the promising accuracy performance, how to model uncertainty for metric-based FSC methods effectively is still a challenge. To model uncertainty, We place a distribution over class probability based on the theory of evidence. As a result, uncertainty modeling and metric learning can be decoupled. To reduce the uncertainty of classification, we propose a Bayesian evidence fusion theorem. Given observed samples, the network learns to get posterior distribution parameters given the prior parameters produced by the pre-trained network. Detailed gradient analysis shows that our method provides a smooth optimization target and can capture the uncertainty. The proposed method is agnostic to metric learning strategies and can be implemented as a plug-and-play module. We integrate our method into several newest FSC methods and demonstrate the improved accuracy and uncertainty quantification on standard FSC benchmarks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0869432894,
        "newsscientist":0.1464239459,
        "technologyreview":0.2420464208,
        "venturebeat":0.2224465256,
        "wired":0.1605232858,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13137v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658203080000,
        "published_hr":"Jul 18, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00493v1",
        "predicted_newsworthiness":0.4711749161,
        "title":"Scrutinizing Shipment Records To Thwart Illegal Timber Trade",
        "summary":"Timber and forest products made from wood, like furniture, are valuable commodities, and like the global trade of many highly-valued natural resources, face challenges of corruption, fraud, and illegal harvesting. These grey and black market activities in the wood and forest products sector are not limited to the countries where the wood was harvested, but extend throughout the global supply chain and have been tied to illicit financial flows, like trade-based money laundering, document fraud, species mislabeling, and other illegal activities. The task of finding such fraudulent activities using trade data, in the absence of ground truth, can be modelled as an unsupervised anomaly detection problem. However existing approaches suffer from certain shortcomings in their applicability towards large scale trade data. Trade data is heterogeneous, with both categorical and numerical attributes in a tabular format. The overall challenge lies in the complexity, volume and velocity of data, with large number of entities and lack of ground truth labels. To mitigate these, we propose a novel unsupervised anomaly detection -- Contrastive Learning based Heterogeneous Anomaly Detection (CHAD) that is generally applicable for large-scale heterogeneous tabular data. We demonstrate our model CHAD performs favorably against multiple comparable baselines for public benchmark datasets, and outperforms them in the case of trade data. More importantly we demonstrate our approach reduces assumptions and efforts required hyperparameter tuning, which is a key challenging aspect in an unsupervised training paradigm. Specifically, our overarching objective pertains to detecting suspicious timber shipments and patterns using Bill of Lading trade record data. Detecting anomalous transactions in shipment records can enable further investigation by government agencies and supply chain constituents.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1708884054,
        "newsscientist":0.1667100485,
        "technologyreview":0.262824051,
        "venturebeat":0.2533256933,
        "wired":0.2141334576,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00493v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659293692000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14443v1",
        "predicted_newsworthiness":0.4711158022,
        "title":"A Survey of Learning on Small Data",
        "summary":"Learning on big data brings success for artificial intelligence (AI), but the annotation and training costs are expensive. In future, learning on small data is one of the ultimate purposes of AI, which requires machines to recognize objectives and scenarios relying on small data as humans. A series of machine learning models is going on this way such as active learning, few-shot learning, deep clustering. However, there are few theoretical guarantees for their generalization performance. Moreover, most of their settings are passive, that is, the label distribution is explicitly controlled by one specified sampling scenario. This survey follows the agnostic active sampling under a PAC (Probably Approximately Correct) framework to analyze the generalization error and label complexity of learning on small data using a supervised and unsupervised fashion. With these theoretical analyses, we categorize the small data learning models from two geometric perspectives: the Euclidean and non-Euclidean (hyperbolic) mean representation, where their optimization solutions are also presented and discussed. Later, some potential learning scenarios that may benefit from small data learning are then summarized, and their potential learning scenarios are also analyzed. Finally, some challenging applications such as computer vision, natural language processing that may benefit from learning on small data are also surveyed.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0967480042,
        "newsscientist":0.1579924866,
        "technologyreview":0.2827507212,
        "venturebeat":0.268183582,
        "wired":0.1747288134,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14443v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659062059000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14627v1",
        "predicted_newsworthiness":0.4708775378,
        "title":"\"Do you follow me?\": A Survey of Recent Approaches in Dialogue State Tracking",
        "summary":"While communicating with a user, a task-oriented dialogue system has to track the user's needs at each turn according to the conversation history. This process called dialogue state tracking (DST) is crucial because it directly informs the downstream dialogue policy. DST has received a lot of interest in recent years with the text-to-text paradigm emerging as the favored approach. In this review paper, we first present the task and its associated datasets. Then, considering a large number of recent publications, we identify highlights and advances of research in 2021-2022. Although neural approaches have enabled significant progress, we argue that some critical aspects of dialogue systems such as generalizability are still underexplored. To motivate future studies, we propose several research avenues.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1127102297,
        "newsscientist":0.1379391856,
        "technologyreview":0.2443872373,
        "venturebeat":0.2541057963,
        "wired":0.1991288955,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14627v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659095602000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.01951v1",
        "predicted_newsworthiness":0.4706576035,
        "title":"Exploration with Model Uncertainty at Extreme Scale in Real-Time Bidding",
        "summary":"In this work, we present a scalable and efficient system for exploring the supply landscape in real-time bidding. The system directs exploration based on the predictive uncertainty of models used for click-through rate prediction and works in a high-throughput, low-latency environment. Through online A\/B testing, we demonstrate that exploration with model uncertainty has a positive impact on model performance and business KPIs.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1008293484,
        "newsscientist":0.114586459,
        "technologyreview":0.2025071615,
        "venturebeat":0.2696244877,
        "wired":0.1746364553,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01951v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ir"
        ],
        "published":1659520751000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00293v1",
        "predicted_newsworthiness":0.4705750517,
        "title":"Global Attention-based Encoder-Decoder LSTM Model for Temperature Prediction of Permanent Magnet Synchronous Motors",
        "summary":"Temperature monitoring is critical for electrical motors to determine if device protection measures should be executed. However, the complexity of the internal structure of Permanent Magnet Synchronous Motors (PMSM) makes the direct temperature measurement of the internal components difficult. This work pragmatically develops three deep learning models to estimate the PMSMs' internal temperature based on readily measurable external quantities. The proposed supervised learning models exploit Long Short-Term Memory (LSTM) modules, bidirectional LSTM, and attention mechanism to form encoder-decoder structures to predict simultaneously the temperatures of the stator winding, tooth, yoke, and permanent magnet. Experiments were conducted in an exhaustive manner on a benchmark dataset to verify the proposed models' performances. The comparative analysis shows that the proposed global attention-based encoder-decoder (EnDec) model provides a competitive overall performance of 1.72 Mean Squared Error (MSE) and 5.34 Mean Absolute Error (MAE).",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0970752163,
        "newsscientist":0.1393131341,
        "technologyreview":0.2120128905,
        "venturebeat":0.2005513803,
        "wired":0.1476147025,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00293v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659207597000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11539v1",
        "predicted_newsworthiness":0.4705376775,
        "title":"HPS-Det: Dynamic Sample Assignment with Hyper-Parameter Search for Object Detection",
        "summary":"Sample assignment plays a prominent part in modern object detection approaches. However, most existing methods rely on manual design to assign positive \/ negative samples, which do not explicitly establish the relationships between sample assignment and object detection performance. In this work, we propose a novel dynamic sample assignment scheme based on hyper-parameter search. We first define the number of positive samples assigned to each ground truth as the hyper-parameters and employ a surrogate optimization algorithm to derive the optimal choices. Then, we design a dynamic sample assignment procedure to dynamically select the optimal number of positives at each training iteration. Experiments demonstrate that the resulting HPS-Det brings improved performance over different object detection baselines. Moreover, We analyze the hyper-parameter reusability when transferring between different datasets and between different backbones for object detection, which exhibits the superiority and versatility of our method.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0845547279,
        "newsscientist":0.1393543785,
        "technologyreview":0.2159820379,
        "venturebeat":0.2025009768,
        "wired":0.1639370735,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11539v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658589237000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14367v1",
        "predicted_newsworthiness":0.4704906174,
        "title":"A Recommender System for Equitable Public Art Curation and Installation",
        "summary":"The placement of art in public spaces can have a significant impact on who feels a sense of belonging. In cities, public art communicates whose interests and culture are being favored. In this paper, we propose a graph matching approach with local constraints to build a curatorial tool for selecting public art in a way that supports inclusive spaces. We develop a cost matrix by drawing on Schelling's model of segregation. Using the cost matrix as an input, the optimization problem is solved via projected gradient descent to obtain a soft assignment matrix. We discuss regularization terms to set curatorial constraints. Our optimization program allocates artwork to public spaces and walls in a way that de-prioritizes \"in-group\" preferences, by satisfying minimum representation and exposure criteria. We draw on existing literature to develop a fairness metric for our algorithmic output. Using Tufts University as a testbed, we assess the effectiveness of our approach and discuss its potential pitfalls from both a curatorial and equity standpoint.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2138239691,
        "newsscientist":0.170284994,
        "technologyreview":0.2411489685,
        "venturebeat":0.2303327123,
        "wired":0.2419532518,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14367v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.lg"
        ],
        "published":1659038381000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.00335v1",
        "predicted_newsworthiness":0.4701901314,
        "title":"Functional Rule Extraction Method for Artificial Neural Networks",
        "summary":"The idea I propose in this paper is a method that is based on comprehensive functions for directed and undirected rule extraction from artificial neural network operations. Firstly, I defined comprehensive functions, then constructed a comprehensive multilayer network (denoted as \ud835\udeee). Each activation function of \ud835\udeee is parametrized to a comprehensive function. Following \ud835\udeee construction, I extracted rules from the network by observing that the network output depends on probabilities of composite functions that are comprehensive functions. This functional rule extraction method applies to the perceptron and multilayer neural network. For any \ud835\udeee model that is trained to predict some outcome given some event, that model behaviour can be expressed \u2013 using the functional rule extraction method \u2013 as a formal rule or informal rule obeyed by the network to predict that outcome. As example, figure 1 consist of a comprehensive physics function that is parameter for one of the network hidden activation functions. Using the functional rule extraction method, I deduced that the comprehensive multilayer network prediction depends on probability of that physics function and probabilities of other composite comprehensive functions in \ud835\udeee. Additionally, functional rule extraction method can aid in applied settings for generation of equations of learned phenomena. This generation can be achieved by first training an \ud835\udeee model toward predicting outcome of a phenomenon, then extracting the rules and assuming that probability values of the network comprehensive functions are constants. Finally, to simplify the generated equation, comprehensive functions with probability \ud835\udc5d = 0 can be omitted.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0964076179,
        "newsscientist":0.1675408642,
        "technologyreview":0.2480217174,
        "venturebeat":0.2036924723,
        "wired":0.1602156831,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00335v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659232360000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12101v1",
        "predicted_newsworthiness":0.4701762398,
        "title":"Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?",
        "summary":"The use of Deep Learning and Computer Vision in the Cultural Heritage domain is becoming highly relevant in the last few years with lots of applications about audio smart guides, interactive museums and augmented reality. All these technologies require lots of data to work effectively and be useful for the user. In the context of artworks, such data is annotated by experts in an expensive and time consuming process. In particular, for each artwork, an image of the artwork and a description sheet have to be collected in order to perform common tasks like Visual Question Answering. In this paper we propose a method for Visual Question Answering that allows to generate at runtime a description sheet that can be used for answering both visual and contextual questions about the artwork, avoiding completely the image and the annotation process. For this purpose, we investigate on the use of GPT-3 for generating descriptions for artworks analyzing the quality of generated descriptions through captioning metrics. Finally we evaluate the performance for Visual Question Answering and captioning tasks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1299203372,
        "newsscientist":0.175850285,
        "technologyreview":0.273757185,
        "venturebeat":0.2699666575,
        "wired":0.224639503,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12101v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.cl"
        ],
        "published":1658751166000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12074v1",
        "predicted_newsworthiness":0.4699033358,
        "title":"A Piecewise Monotonic Gait Phase Estimation Model for Controlling a Powered Transfemoral Prosthesis in Various Locomotion Modes",
        "summary":"Gait phase-based control is a trending research topic for walking-aid robots, especially robotic lower-limb prostheses. Gait phase estimation is a challenge for gait phase-based control. Previous researches used the integration or the differential of the human's thigh angle to estimate the gait phase, but accumulative measurement errors and noises can affect the estimation results. In this paper, a more robust gait phase estimation method is proposed using a unified form of piecewise monotonic gait phase-thigh angle models for various locomotion modes. The gait phase is estimated from only the thigh angle, which is a stable variable and avoids phase drifting. A Kalman filter-based smoother is designed to further suppress the mutations of the estimated gait phase. Based on the proposed gait phase estimation method, a gait phase-based joint angle tracking controller is designed for a transfemoral prosthesis. The proposed gait estimation method, the gait phase smoother, and the controller are evaluated through offline analysis on walking data in various locomotion modes. And the real-time performance of the gait phase-based controller is validated in an experiment on the transfemoral prosthesis.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0576926334,
        "newsscientist":0.1264031264,
        "technologyreview":0.121900929,
        "venturebeat":0.1083433678,
        "wired":0.1077478033,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12074v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658749632000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00374v1",
        "predicted_newsworthiness":0.4697120202,
        "title":"Neuro-Symbolic Learning: Principles and Applications in Ophthalmology",
        "summary":"Neural networks have been rapidly expanding in recent years, with novel strategies and applications. However, challenges such as interpretability, explainability, robustness, safety, trust, and sensibility remain unsolved in neural network technologies, despite the fact that they will unavoidably be addressed for critical applications. Attempts have been made to overcome the challenges in neural network computing by representing and embedding domain knowledge in terms of symbolic representations. Thus, the neuro-symbolic learning (NeSyL) notion emerged, which incorporates aspects of symbolic representation and bringing common sense into neural networks (NeSyL). In domains where interpretability, reasoning, and explainability are crucial, such as video and image captioning, question-answering and reasoning, health informatics, and genomics, NeSyL has shown promising outcomes. This review presents a comprehensive survey on the state-of-the-art NeSyL approaches, their principles, advances in machine and deep learning algorithms, applications such as opthalmology, and most importantly, future perspectives of this emerging field.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1369726684,
        "newsscientist":0.2132992908,
        "technologyreview":0.3305771358,
        "venturebeat":0.2950986723,
        "wired":0.2147986999,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00374v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1659250099000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12764v1",
        "predicted_newsworthiness":0.4696471382,
        "title":"Clustering Object-Centric Event Logs",
        "summary":"Process mining provides various algorithms to analyze process executions based on event data. Process discovery, the most prominent category of process mining techniques, aims to discover process models from event logs, however, it leads to spaghetti models when working with real-life data. Therefore, several clustering techniques have been proposed on top of traditional event logs (i.e., event logs with a single case notion) to reduce the complexity of process models and discover homogeneous subsets of cases. Nevertheless, in real-life processes, particularly in the context of Business-to-Business (B2B) processes, multiple objects are involved in a process. Recently, Object-Centric Event Logs (OCELs) have been introduced to capture the information of such processes, and several process discovery techniques have been developed on top of OCELs. Yet, the output of the proposed discovery techniques on real OCELs leads to more informative but also more complex models. In this paper, we propose a clustering-based approach to cluster similar objects in OCELs to simplify the obtained process models. Using a case study of a real B2B process, we demonstrate that our approach reduces the complexity of the process models and generates coherent subsets of objects which help the end-users gain insights into the process.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0831681824,
        "newsscientist":0.0938588853,
        "technologyreview":0.1628764902,
        "venturebeat":0.2135237401,
        "wired":0.1197208202,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12764v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1658826999000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.13678v1",
        "predicted_newsworthiness":0.4694412616,
        "title":"Multi-layer Representation Learning for Robust OOD Image Classification",
        "summary":"Convolutional Neural Networks have become the norm in image classification. Nevertheless, their difficulty to maintain high accuracy across datasets has become apparent in the past few years. In order to utilize such models in real-world scenarios and applications, they must be able to provide trustworthy predictions on unseen data. In this paper, we argue that extracting features from a CNN's intermediate layers can assist in the model's final prediction. Specifically, we adapt the Hypercolumns method to a ResNet-18 and find a significant increase in the model's accuracy, when evaluating on the NICO dataset.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1200247669,
        "newsscientist":0.1644812644,
        "technologyreview":0.2859348875,
        "venturebeat":0.2590689081,
        "wired":0.206079282,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13678v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658943966000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14386v1",
        "predicted_newsworthiness":0.4693598098,
        "title":"Efficient Model Finetuning for Text Classification via Data Filtering",
        "summary":"As model finetuning is central to the modern NLP, we set to maximize its efficiency. Motivated by training examples are often redundant, we design an algorithm that filters the examples in a streaming fashion. Our key techniques are two: (1) automatically determine a training loss threshold for skipping the backward propagation; and (2) maintain a meta predictor for further skipping the forward propagation. Incarnated as a three-stage process, on a diverse set of benchmarks our algorithm reduces the required training examples by up to 5$\\times$ while only seeing minor degradation on average. Our method is effective even for as few as one training epoch, where each training example is encountered once. It is simple to implement and is compatible with the existing model finetuning optimizations such as layer freezing.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1035067412,
        "newsscientist":0.1204256602,
        "technologyreview":0.2200418897,
        "venturebeat":0.2209941847,
        "wired":0.1722569271,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14386v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659044611000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14373v1",
        "predicted_newsworthiness":0.4692940439,
        "title":"Eye Gaze Estimation Model Analysis",
        "summary":"We explore techniques for eye gaze estimation using machine learning. Eye gaze estimation is a common problem for various behavior analysis and human-computer interfaces. The purpose of this work is to discuss various model types for eye gaze estimation and present the results from predicting gaze direction using eye landmarks in unconstrained settings. In unconstrained real-world settings, feature-based and model-based methods are outperformed by recent appearance-based methods due to factors like illumination changes and other visual artifacts. We discuss a learning-based method for eye region landmark localization trained exclusively on synthetic data. We discuss how to use detected landmarks as input to iterative model-fitting and lightweight learning-based gaze estimation methods and how to use the model for person-independent and personalized gaze estimations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0948945076,
        "newsscientist":0.1511533644,
        "technologyreview":0.2257537389,
        "venturebeat":0.2201249188,
        "wired":0.1818108549,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14373v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.cl"
        ],
        "published":1659040803000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12393v1",
        "predicted_newsworthiness":0.4692754127,
        "title":"CelebV-HQ: A Large-Scale Video Facial Attributes Dataset",
        "summary":"Large-scale datasets have played indispensable roles in the recent success of face generation\/editing and significantly facilitated the advances of emerging research fields. However, the academic community still lacks a video dataset with diverse facial attribute annotations, which is crucial for the research on face-related videos. In this work, we propose a large-scale, high-quality, and diverse video dataset with rich facial attribute annotations, named the High-Quality Celebrity Video Dataset (CelebV-HQ). CelebV-HQ contains 35,666 video clips with the resolution of 512x512 at least, involving 15,653 identities. All clips are labeled manually with 83 facial attributes, covering appearance, action, and emotion. We conduct a comprehensive analysis in terms of age, ethnicity, brightness stability, motion smoothness, head pose diversity, and data quality to demonstrate the diversity and temporal coherence of CelebV-HQ. Besides, its versatility and potential are validated on two representative tasks, i.e., unconditional video generation and video facial attribute editing. Furthermore, we envision the future potential of CelebV-HQ, as well as the new opportunities and challenges it would bring to related research directions. Data, code, and models are publicly available. Project page: https:\/\/celebv-hq.github.io.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1344968569,
        "newsscientist":0.168042585,
        "technologyreview":0.2555773065,
        "venturebeat":0.2400704422,
        "wired":0.2251925136,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12393v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658771827000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13340v1",
        "predicted_newsworthiness":0.469140269,
        "title":"PointFix: Learning to Fix Domain Bias for Robust Online Stereo Adaptation",
        "summary":"Online stereo adaptation tackles the domain shift problem, caused by different environments between synthetic (training) and real (test) datasets, to promptly adapt stereo models in dynamic real-world applications such as autonomous driving. However, previous methods often fail to counteract particular regions related to dynamic objects with more severe environmental changes. To mitigate this issue, we propose to incorporate an auxiliary point-selective network into a meta-learning framework, called PointFix, to provide a robust initialization of stereo models for online stereo adaptation. In a nutshell, our auxiliary network learns to fix local variants intensively by effectively back-propagating local information through the meta-gradient for the robust initialization of the baseline model. This network is model-agnostic, so can be used in any kind of architectures in a plug-and-play manner. We conduct extensive experiments to verify the effectiveness of our method under three adaptation settings such as short-, mid-, and long-term sequences. Experimental results show that the proper initialization of the base stereo model by the auxiliary network enables our learning paradigm to achieve state-of-the-art performance at inference.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0921010741,
        "newsscientist":0.1323516699,
        "technologyreview":0.237844668,
        "venturebeat":0.2190362381,
        "wired":0.1894400723,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13340v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658908109000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01787v1",
        "predicted_newsworthiness":0.4690116732,
        "title":"Present and Future of SLAM in Extreme Underground Environments",
        "summary":"This paper reports on the state of the art in underground SLAM by discussing different SLAM strategies and results across six teams that participated in the three-year-long SubT competition. In particular, the paper has four main goals. First, we review the algorithms, architectures, and systems adopted by the teams; particular emphasis is put on lidar-centric SLAM solutions (the go-to approach for virtually all teams in the competition), heterogeneous multi-robot operation (including both aerial and ground robots), and real-world underground operation (from the presence of obscurants to the need to handle tight computational constraints). We do not shy away from discussing the dirty details behind the different SubT SLAM systems, which are often omitted from technical papers. Second, we discuss the maturity of the field by highlighting what is possible with the current SLAM systems and what we believe is within reach with some good systems engineering. Third, we outline what we believe are fundamental open problems, that are likely to require further research to break through. Finally, we provide a list of open-source SLAM implementations and datasets that have been produced during the SubT challenge and related efforts, and constitute a useful resource for researchers and practitioners.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1202529515,
        "newsscientist":0.1834489284,
        "technologyreview":0.2191019351,
        "venturebeat":0.1969675089,
        "wired":0.2189603902,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01787v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659484488000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.12503v2",
        "predicted_newsworthiness":0.4687733769,
        "title":"Benchmark time series data sets for PyTorch -- the torchtime package",
        "summary":"The development of models for Electronic Health Record data is an area of active research featuring a small number of public benchmark data sets. Researchers typically write custom data processing code but this hinders reproducibility and can introduce errors. The Python package torchtime provides reproducible implementations of commonly used PhysioNet and UEA & UCR time series classification repository data sets for PyTorch. Features are provided for working with irregularly sampled and partially observed time series of unequal length. It aims to simplify access to PhysioNet data and enable fair comparisons of models in this exciting area of research.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1819701509,
        "newsscientist":0.2073685034,
        "technologyreview":0.2426090266,
        "venturebeat":0.2483688964,
        "wired":0.1893504646,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12503v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658779596000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00068v1",
        "predicted_newsworthiness":0.4687173205,
        "title":"IMUNet: Efficient Regression Architecture for IMU Navigation and Positioning",
        "summary":"Data-driven based method for navigation and positioning has absorbed attention in recent years and it outperforms all its competitor methods in terms of accuracy and efficiency. This paper introduces a new architecture called IMUNet which is accurate and efficient for position estimation on edge device implementation receiving a sequence of raw IMU measurements. The architecture has been compared with one dimension version of the state-of-the-art CNN networks that have been introduced recently for edge device implementation in terms of accuracy and efficiency. Moreover, a new method for collecting a dataset using IMU sensors on cell phones and Google ARCore API has been proposed and a publicly available dataset has been recorded. A comprehensive evaluation using four different datasets as well as the proposed dataset and real device implementation has been done to prove the performance of the architecture. All the code in both Pytorch and Tensorflow framework as well as the Android application code have been shared to improve further research.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1176714903,
        "newsscientist":0.1716334452,
        "technologyreview":0.2703079971,
        "venturebeat":0.2857912923,
        "wired":0.2596953135,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00068v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659127321000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.12141v1",
        "predicted_newsworthiness":0.4686148089,
        "title":"Live in the Moment: Learning Dynamics Model Adapted to Evolving Policy",
        "summary":"Model-based reinforcement learning (RL) achieves higher sample efficiency in practice than model-free RL by learning a dynamics model to generate samples for policy learning. Previous works learn a \"global\" dynamics model to fit the state-action visitation distribution for all historical policies. However, in this paper, we find that learning a global dynamics model does not necessarily benefit model prediction for the current policy since the policy in use is constantly evolving. The evolving policy during training will cause state-action visitation distribution shifts. We theoretically analyze how the distribution of historical policies affects the model learning and model rollouts. We then propose a novel model-based RL method, named \\textit{Policy-adaptation Model-based Actor-Critic (PMAC)}, which learns a policy-adapted dynamics model based on a policy-adaptation mechanism. This mechanism dynamically adjusts the historical policy mixture distribution to ensure the learned model can continually adapt to the state-action visitation distribution of the evolving policy. Experiments on a range of continuous control environments in MuJoCo show that PMAC achieves state-of-the-art asymptotic performance and almost two times higher sample efficiency than prior model-based methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1229645711,
        "newsscientist":0.1318698907,
        "technologyreview":0.2375300155,
        "venturebeat":0.2131911817,
        "wired":0.1683384442,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12141v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658753158000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11901v1",
        "predicted_newsworthiness":0.468389058,
        "title":"A Closed-Loop Perception, Decision-Making and Reasoning Mechanism for Human-Like Navigation",
        "summary":"Reliable navigation systems have a wide range of applications in robotics and autonomous driving. Current approaches employ an open-loop process that converts sensor inputs directly into actions. However, these open-loop schemes are challenging to handle complex and dynamic real-world scenarios due to their poor generalization. Imitating human navigation, we add a reasoning process to convert actions back to internal latent states, forming a two-stage closed loop of perception, decision-making, and reasoning. Firstly, VAE-Enhanced Demonstration Learning endows the model with the understanding of basic navigation rules. Then, two dual processes in RL-Enhanced Interaction Learning generate reward feedback for each other and collectively enhance obstacle avoidance capability. The reasoning model can substantially promote generalization and robustness, and facilitate the deployment of the algorithm to real-world robots without elaborate transfers. Experiments show our method is more adaptable to novel scenarios compared with state-of-the-art approaches.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1045477997,
        "newsscientist":0.1794781166,
        "technologyreview":0.2913657548,
        "venturebeat":0.2411394687,
        "wired":0.2077837247,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11901v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658723774000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.14502v1",
        "predicted_newsworthiness":0.4683680131,
        "title":"Language Models Can Teach Themselves to Program Better",
        "summary":"This work shows how one can use large-scale language models (LMs) to synthesize programming problems with verified solutions, in the form of programming puzzles, which can then in turn be used to fine-tune those same models, improving their performance. This work builds on two recent developments. First, LMs have achieved breakthroughs in non-trivial reasoning and algorithm implementation, generating code that can solve some intermediate-level competitive programming problems. However, training code LMs involves curated sets of natural-language problem descriptions and source-code tests and solutions, which are limited in size. Second, a new format of programming challenge called a programming puzzle was introduced, which does not require a natural language description and is directly specified by a source-code test. In this work we show how generating synthetic programming puzzles and solutions, verified for correctness by a Python interpreter, can be used to improve performance in solving test puzzles from P3, a public benchmark set of Python Programming Puzzles. Additionally, we release a dataset of 1 million puzzles and solutions generated by the Codex model, which we show can improve smaller models through fine-tuning.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1000944044,
        "newsscientist":0.1584867729,
        "technologyreview":0.2797172961,
        "venturebeat":0.2580891704,
        "wired":0.1930724317,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14502v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659077008000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02150v1",
        "predicted_newsworthiness":0.4680999577,
        "title":"Empirical Study of Overfitting in Deep FNN Prediction Models for Breast Cancer Metastasis",
        "summary":"Overfitting is defined as the fact that the current model fits a specific data set perfectly, resulting in weakened generalization, and ultimately may affect the accuracy in predicting future data. In this research we used an EHR dataset concerning breast cancer metastasis to study overfitting of deep feedforward Neural Networks (FNNs) prediction models. We included 11 hyperparameters of the deep FNNs models and took an empirical approach to study how each of these hyperparameters was affecting both the prediction performance and overfitting when given a large range of values. We also studied how some of the interesting pairs of hyperparameters were interacting to influence the model performance and overfitting. The 11 hyperparameters we studied include activate function; weight initializer, number of hidden layers, learning rate, momentum, decay, dropout rate, batch size, epochs, L1, and L2. Our results show that most of the single hyperparameters are either negatively or positively corrected with model prediction performance and overfitting. In particular, we found that overfitting overall tends to negatively correlate with learning rate, decay, batch sides, and L2, but tends to positively correlate with momentum, epochs, and L1. According to our results, learning rate, decay, and batch size may have a more significant impact on both overfitting and prediction performance than most of the other hyperparameters, including L1, L2, and dropout rate, which were designed for minimizing overfitting. We also find some interesting interacting pairs of hyperparameters such as learning rate and momentum, learning rate and decay, and batch size and epochs. Keywords: Deep learning, overfitting, prediction, grid search, feedforward neural networks, breast cancer metastasis.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1366810651,
        "newsscientist":0.1816776742,
        "technologyreview":0.281650317,
        "venturebeat":0.2436807674,
        "wired":0.1797400827,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02150v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659540972000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00064v1",
        "predicted_newsworthiness":0.4675969489,
        "title":"Thutmose Tagger: Single-pass neural model for Inverse Text Normalization",
        "summary":"Inverse text normalization (ITN) is an essential post-processing step in automatic speech recognition (ASR). It converts numbers, dates, abbreviations, and other semiotic classes from the spoken form generated by ASR to their written forms. One can consider ITN as a Machine Translation task and use neural sequence-to-sequence models to solve it. Unfortunately, such neural models are prone to hallucinations that could lead to unacceptable errors. To mitigate this issue, we propose a single-pass token classifier model that regards ITN as a tagging task. The model assigns a replacement fragment to every input token or marks it for deletion or copying without changes. We present a dataset preparation method based on the granular alignment of ITN examples. The proposed model is less prone to hallucination errors. The model is trained on the Google Text Normalization dataset and achieves state-of-the-art sentence accuracy on both English and Russian test sets. One-to-one correspondence between tags and input words improves the interpretability of the model's predictions, simplifies debugging, and allows for post-processing corrections. The model is simpler than sequence-to-sequence models and easier to optimize in production settings. The model and the code to prepare the dataset is published as part of NeMo project.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1083774087,
        "newsscientist":0.1341947036,
        "technologyreview":0.2351651464,
        "venturebeat":0.243172939,
        "wired":0.1912203297,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00064v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1659127142000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14769v1",
        "predicted_newsworthiness":0.4675709995,
        "title":"Image Quality Assessment: Integrating Model-Centric and Data-Centric Approaches",
        "summary":"Learning-based image quality assessment (IQA) has made remarkable progress in the past decade, but nearly all consider the two key components - model and data - in relative isolation. Specifically, model-centric IQA focuses on developing \"better\" objective quality methods on fixed and extensively reused datasets, with a great danger of overfitting. Data-centric IQA involves conducting psychophysical experiments to construct \"better\" human-annotated datasets, which unfortunately ignores current IQA models during dataset creation. In this paper, we first design a series of experiments to probe computationally that such isolation of model and data impedes further progress of IQA. We then describe a computational framework that integrates model-centric and data-centric IQA. As a specific example, we design computational modules to quantify the sampling-worthiness of candidate images based on blind IQA (BIQA) model predictions and deep content-aware features. Experimental results show that the proposed sampling-worthiness module successfully spots diverse failures of the examined BIQA models, which are indeed worthy samples to be included in next-generation datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1212831432,
        "newsscientist":0.1787662828,
        "technologyreview":0.2823817095,
        "venturebeat":0.2446022837,
        "wired":0.1901791698,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14769v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659111837000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12547v1",
        "predicted_newsworthiness":0.4673861429,
        "title":"An Empirical Deep Dive into Deep Learning's Driving Dynamics",
        "summary":"We present an empirical dataset surveying the deep learning phenomenon on fully-connected networks, encompassing the training and test performance of numerous network topologies, sweeping across multiple learning tasks, depths, numbers of free parameters, learning rates, batch sizes, and regularization penalties. The dataset probes 178 thousand hyperparameter settings with an average of 20 repetitions each, totaling 3.5 million training runs and 20 performance metrics for each of the 13.1 billion training epochs observed. Accumulating this 671 GB dataset utilized 5,448 CPU core-years, 17.8 GPU-years, and 111.2 node-years. Additionally, we provide a preliminary analysis revealing patterns which persist across learning tasks and topologies. We aim to inspire work empirically studying modern machine learning techniques as a catalyst for the theoretical discoveries needed to progress the field beyond energy-intensive and heuristic practices.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1229677257,
        "newsscientist":0.1908419357,
        "technologyreview":0.3530742392,
        "venturebeat":0.3326370673,
        "wired":0.2574210019,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12547v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658785532000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01899v1",
        "predicted_newsworthiness":0.4671425849,
        "title":"Understanding Adversarial Imitation Learning in Small Sample Regime: A Stage-coupled Analysis",
        "summary":"Imitation learning learns a policy from expert trajectories. While the expert data is believed to be crucial for imitation quality, it was found that a kind of imitation learning approach, adversarial imitation learning (AIL), can have exceptional performance. With as little as only one expert trajectory, AIL can match the expert performance even in a long horizon, on tasks such as locomotion control. There are two mysterious points in this phenomenon. First, why can AIL perform well with only a few expert trajectories? Second, why does AIL maintain good performance despite the length of the planning horizon? In this paper, we theoretically explore these two questions. For a total-variation-distance-based AIL (called TV-AIL), our analysis shows a horizon-free imitation gap $\\mathcal O(\\{\\min\\{1, \\sqrt{|\\mathcal S|\/N} \\})$ on a class of instances abstracted from locomotion control tasks. Here $|\\mathcal S|$ is the state space size for a tabular Markov decision process, and $N$ is the number of expert trajectories. We emphasize two important features of our bound. First, this bound is meaningful in both small and large sample regimes. Second, this bound suggests that the imitation gap of TV-AIL is at most 1 regardless of the planning horizon. Therefore, this bound can explain the empirical observation. Technically, we leverage the structure of multi-stage policy optimization in TV-AIL and present a new stage-coupled analysis via dynamic programming",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1122762413,
        "newsscientist":0.1682314439,
        "technologyreview":0.2924524383,
        "venturebeat":0.2336545363,
        "wired":0.1967306477,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01899v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659513813000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00553v1",
        "predicted_newsworthiness":0.4667939406,
        "title":"Search for or Navigate to? Dual Adaptive Thinking for Object Navigation",
        "summary":"\"Search for\" or \"Navigate to\"? When finding an object, the two choices always come up in our subconscious mind. Before seeing the target, we search for the target based on experience. After seeing the target, we remember the target location and navigate to. However, recently methods in object navigation field almost only consider using object association to enhance \"search for\" phase while neglect the importance of \"navigate to\" phase. Therefore, this paper proposes the dual adaptive thinking (DAT) method to flexibly adjust the different thinking strategies at different navigation stages. Dual thinking includes search thinking with the object association ability and navigation thinking with the target location ability. To make the navigation thinking more effective, we design the target-oriented memory graph (TOMG) to store historical target information and the target-aware multi-scale aggregator (TAMSA) to encode the relative target position. We assess our methods on the AI2-Thor dataset. Compared with the state-of-the-art (SOTA) method, our method reports 10.8%, 21.5% and 15.7% increase in success rate (SR), success weighted by path length (SPL) and success weighted by navigation efficiency (SNE), respectively.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.110953581,
        "newsscientist":0.1924817311,
        "technologyreview":0.2883367019,
        "venturebeat":0.2544515117,
        "wired":0.2066357793,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00553v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.ro"
        ],
        "published":1659315865000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2208.01232v1",
        "predicted_newsworthiness":0.4665471228,
        "title":"DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement Learning",
        "summary":"Analytical dashboards are popular in business intelligence to facilitate insight discovery with multiple charts. However, creating an effective dashboard is highly demanding, which requires users to have adequate data analysis background and be familiar with professional tools, such as Power BI. To create a dashboard, users have to configure charts by selecting data columns and exploring different chart combinations to optimize the communication of insights, which is trial-and-error. Recent research has started to use deep learning methods for dashboard generation to lower the burden of visualization creation. However, such efforts are greatly hindered by the lack of large-scale and high-quality datasets of dashboards. In this work, we propose using deep reinforcement learning to generate analytical dashboards that can use well-established visualization knowledge and the estimation capacity of reinforcement learning. Specifically, we use visualization knowledge to construct a training environment and rewards for agents to explore and imitate human exploration behavior with a well-designed agent network. The usefulness of the deep reinforcement learning model is demonstrated through ablation studies and user studies. In conclusion, our work opens up new opportunities to develop effective ML-based visualization recommenders without beforehand training datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.139088832,
        "newsscientist":0.188486988,
        "technologyreview":0.3394052907,
        "venturebeat":0.35769314,
        "wired":0.2481551628,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01232v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1659412082000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.00755v1",
        "predicted_newsworthiness":0.4661453186,
        "title":"Off-Policy Correction for Actor-Critic Algorithms in Deep Reinforcement Learning",
        "summary":"Compared to on-policy policy gradient techniques, off-policy model-free deep reinforcement learning (RL) approaches that use previously gathered data can improve sampling efficiency. However, off-policy learning becomes challenging when the discrepancy between the distributions of the policy of interest and the policies that collected the data increases. Although the well-studied importance sampling and off-policy policy gradient techniques were proposed to compensate for this discrepancy, they usually require a collection of long trajectories that increases the computational complexity and induce additional problems such as vanishing or exploding gradients. Moreover, their generalization to continuous action domains is strictly limited as they require action probabilities, which is unsuitable for deterministic policies. To overcome these limitations, we introduce an alternative off-policy correction algorithm for continuous action spaces, Actor-Critic Off-Policy Correction (AC-Off-POC), to mitigate the potential drawbacks introduced by the previously collected data. Through a novel discrepancy measure computed by the agent's most recent action decisions on the states of the randomly sampled batch of transitions, the approach does not require actual or estimated action probabilities for any policy and offers an adequate one-step importance sampling. Theoretical results show that the introduced approach can achieve a contraction mapping with a fixed unique point, which allows a \"safe\" off-policy learning. Our empirical results suggest that AC-Off-POC consistently improves the state-of-the-art and attains higher returns in fewer steps than the competing methods by efficiently scheduling the learning rate in Q-learning and policy optimization.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1073473378,
        "newsscientist":0.1223126908,
        "technologyreview":0.2367884271,
        "venturebeat":0.2053442069,
        "wired":0.1509746835,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00755v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659353592000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11951v1",
        "predicted_newsworthiness":0.4661219175,
        "title":"Deep Forest with Hashing Screening and Window Screening",
        "summary":"As a novel deep learning model, gcForest has been widely used in various applications. However, the current multi-grained scanning of gcForest produces many redundant feature vectors, and this increases the time cost of the model. To screen out redundant feature vectors, we introduce a hashing screening mechanism for multi-grained scanning and propose a model called HW-Forest which adopts two strategies, hashing screening and window screening. HW-Forest employs perceptual hashing algorithm to calculate the similarity between feature vectors in hashing screening strategy, which is used to remove the redundant feature vectors produced by multi-grained scanning and can significantly decrease the time cost and memory consumption. Furthermore, we adopt a self-adaptive instance screening strategy to improve the performance of our approach, called window screening, which can achieve higher accuracy without hyperparameter tuning on different datasets. Our experimental results show that HW-Forest has higher accuracy than other models, and the time cost is also reduced.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0720019633,
        "newsscientist":0.1221914753,
        "technologyreview":0.2225655312,
        "venturebeat":0.2130822555,
        "wired":0.136857252,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11951v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658734795000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13287v1",
        "predicted_newsworthiness":0.4659280972,
        "title":"Detecting Concept Drift in the Presence of Sparsity -- A Case Study of Automated Change Risk Assessment System",
        "summary":"Missing values, widely called as \\textit{sparsity} in literature, is a common characteristic of many real-world datasets. Many imputation methods have been proposed to address this problem of data incompleteness or sparsity. However, the accuracy of a data imputation method for a given feature or a set of features in a dataset is highly dependent on the distribution of the feature values and its correlation with other features. Another problem that plagues industry deployments of machine learning (ML) solutions is concept drift detection, which becomes more challenging in the presence of missing values. Although data imputation and concept drift detection have been studied extensively, little work has attempted a combined study of the two phenomena, i.e., concept drift detection in the presence of sparsity. In this work, we carry out a systematic study of the following: (i) different patterns of missing values, (ii) various statistical and ML based data imputation methods for different kinds of sparsity, (iii) several concept drift detection methods, (iv) practical analysis of the various drift detection metrics, (v) selecting the best concept drift detector given a dataset with missing values based on the different metrics. We first analyze it on synthetic data and publicly available datasets, and finally extend the findings to our deployed solution of automated change risk assessment system. One of the major findings from our empirical study is the absence of supremacy of any one concept drift detection method across all the relevant metrics. Therefore, we adopt a majority voting based ensemble of concept drift detectors for abrupt and gradual concept drifts. Our experiments show optimal or near optimal performance can be achieved for this ensemble method across all the metrics.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1432243635,
        "newsscientist":0.1689078008,
        "technologyreview":0.2310052027,
        "venturebeat":0.2500369241,
        "wired":0.1776012192,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13287v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658896069000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14279v1",
        "predicted_newsworthiness":0.4654651105,
        "title":"The One Where They Reconstructed 3D Humans and Environments in TV Shows",
        "summary":"TV shows depict a wide variety of human behaviors and have been studied extensively for their potential to be a rich source of data for many applications. However, the majority of the existing work focuses on 2D recognition tasks. In this paper, we make the observation that there is a certain persistence in TV shows, i.e., repetition of the environments and the humans, which makes possible the 3D reconstruction of this content. Building on this insight, we propose an automatic approach that operates on an entire season of a TV show and aggregates information in 3D; we build a 3D model of the environment, compute camera information, static 3D scene structure and body scale information. Then, we demonstrate how this information acts as rich 3D context that can guide and improve the recovery of 3D human pose and position in these environments. Moreover, we show that reasoning about humans and their environment in 3D enables a broad range of downstream applications: re-identification, gaze estimation, cinematography and image editing. We apply our approach on environments from seven iconic TV shows and perform an extensive evaluation of the proposed system.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1134735784,
        "newsscientist":0.1642798041,
        "technologyreview":0.2139203558,
        "venturebeat":0.214185394,
        "wired":0.2149462448,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14279v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659031050000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12318v1",
        "predicted_newsworthiness":0.4654147968,
        "title":"Action Quality Assessment using Transformers",
        "summary":"Action quality assessment (AQA) is an active research problem in video-based applications that is a challenging task due to the score variance per frame. Existing methods address this problem via convolutional-based approaches but suffer from its limitation of effectively capturing long-range dependencies. With the recent advancements in Transformers, we show that they are a suitable alternative to the conventional convolutional-based architectures. Specifically, can transformer-based models solve the task of AQA by effectively capturing long-range dependencies, parallelizing computation, and providing a wider receptive field for diving videos? To demonstrate the effectiveness of our proposed architectures, we conducted comprehensive experiments and achieved a competitive Spearman correlation score of 0.9317. Additionally, we explore the hyperparameters effect on the model's performance and pave a new path for exploiting Transformers in AQA.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.107540264,
        "newsscientist":0.158860427,
        "technologyreview":0.2256690355,
        "venturebeat":0.2122371427,
        "wired":0.1826512695,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12318v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658336413000,
        "published_hr":"Jul 20, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00444v1",
        "predicted_newsworthiness":0.4651957418,
        "title":"BYOLMed3D: Self-Supervised Representation Learning of Medical Videos using Gradient Accumulation Assisted 3D BYOL Framework",
        "summary":"Applications on Medical Image Analysis suffer from acute shortage of large volume of data properly annotated by medical experts. Supervised Learning algorithms require a large volumes of balanced data to learn robust representations. Often supervised learning algorithms require various techniques to deal with imbalanced data. Self-supervised learning algorithms on the other hand are robust to imbalance in the data and are capable of learning robust representations. In this work, we train a 3D BYOL self-supervised model using gradient accumulation technique to deal with the large number of samples in a batch generally required in a self-supervised algorithm. To the best of our knowledge, this work is one of the first of its kind in this domain. We compare the results obtained through our experiments in the downstream task of ACL Tear Injury detection with the contemporary self-supervised pre-training methods and also with ResNet3D-18 initialized with the Kinetics-400 pre-trained weights. From the downstream task experiments, it is evident that the proposed framework outperforms the existing baselines.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0971226907,
        "newsscientist":0.1407131061,
        "technologyreview":0.2322508261,
        "venturebeat":0.2099491565,
        "wired":0.1501407329,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00444v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659278886000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01022v1",
        "predicted_newsworthiness":0.4649010337,
        "title":"A performance contextualization approach to validating camera models for robot simulation",
        "summary":"The focus of this contribution is on camera simulation as it comes into play in simulating autonomous robots for their virtual prototyping. We propose a camera model validation methodology based on the performance of a perception algorithm and the context in which the performance is measured. This approach is different than traditional validation of synthetic images, which is often done at a pixel or feature level, and tends to require matching pairs of synthetic and real images. Due to the high cost and constraints of acquiring paired images, the proposed approach is based on datasets that are not necessarily paired. Within a real and a simulated dataset, A and B, respectively, we find subsets Ac and Bc of similar content and judge, statistically, the perception algorithm's response to these similar subsets. This validation approach obtains a statistical measure of performance similarity, as well as a measure of similarity between the content of A and B. The methodology is demonstrated using images generated with Chrono::Sensor and a scaled autonomous vehicle, using an object detector as the perception algorithm. The results demonstrate the ability to quantify (i) differences between simulated and real data; (ii) the propensity of training methods to mitigate the sim-to-real gap; and (iii) the context overlap between two datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0869072836,
        "newsscientist":0.1752074886,
        "technologyreview":0.2708770582,
        "venturebeat":0.2470023423,
        "wired":0.2123308177,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01022v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659376464000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.14807v1",
        "predicted_newsworthiness":0.464679228,
        "title":"PageNet: Towards End-to-End Weakly Supervised Page-Level Handwritten Chinese Text Recognition",
        "summary":"Handwritten Chinese text recognition (HCTR) has been an active research topic for decades. However, most previous studies solely focus on the recognition of cropped text line images, ignoring the error caused by text line detection in real-world applications. Although some approaches aimed at page-level text recognition have been proposed in recent years, they either are limited to simple layouts or require very detailed annotations including expensive line-level and even character-level bounding boxes. To this end, we propose PageNet for end-to-end weakly supervised page-level HCTR. PageNet detects and recognizes characters and predicts the reading order between them, which is more robust and flexible when dealing with complex layouts including multi-directional and curved text lines. Utilizing the proposed weakly supervised learning framework, PageNet requires only transcripts to be annotated for real data; however, it can still output detection and recognition results at both the character and line levels, avoiding the labor and cost of labeling bounding boxes of characters and text lines. Extensive experiments conducted on five datasets demonstrate the superiority of PageNet over existing weakly supervised and fully supervised page-level methods. These experimental results may spark further research beyond the realms of existing methods based on connectionist temporal classification or attention. The source code is available at https:\/\/github.com\/shannanyinxiang\/PageNet.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0902529195,
        "newsscientist":0.1266210769,
        "technologyreview":0.2047761244,
        "venturebeat":0.1885529988,
        "wired":0.1474891854,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14807v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659116865000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00657v1",
        "predicted_newsworthiness":0.4645201401,
        "title":"SiamixFormer: A Siamese Transformer Network For Building Detection And Change Detection From Bi-Temporal Remote Sensing Images",
        "summary":"Building detection and change detection using remote sensing images can help urban and rescue planning. Moreover, they can be used for building damage assessment after natural disasters. Currently, most of the existing models for building detection use only one image (pre-disaster image) to detect buildings. This is based on the idea that post-disaster images reduce the model's performance because of presence of destroyed buildings. In this paper, we propose a siamese model, called SiamixFormer, which uses pre- and post-disaster images as input. Our model has two encoders and has a hierarchical transformer architecture. The output of each stage in both encoders is given to a temporal transformer for feature fusion in a way that query is generated from pre-disaster images and (key, value) is generated from post-disaster images. To this end, temporal features are also considered in feature fusion. Another advantage of using temporal transformers in feature fusion is that they can better maintain large receptive fields generated by transformer encoders compared with CNNs. Finally, the output of the temporal transformer is given to a simple MLP decoder at each stage. The SiamixFormer model is evaluated on xBD, and WHU datasets, for building detection and on LEVIR-CD and CDD datasets for change detection and could outperform the state-of-the-art.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1422971519,
        "newsscientist":0.1683419318,
        "technologyreview":0.2161210073,
        "venturebeat":0.1924579187,
        "wired":0.1943754592,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00657v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659339345000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00625v1",
        "predicted_newsworthiness":0.4638128138,
        "title":"RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics",
        "summary":"Restructuring the regional industrial structure (RIS) has the potential to halt economic recession and achieve revitalization. Understanding the current status and dynamics of RIS will greatly assist in studying and evaluating the current industrial structure. Previous studies have focused on qualitative and quantitative research to rationalize RIS from a macroscopic perspective. Although recent studies have traced information at the industrial enterprise level to complement existing research from a micro perspective, the ambiguity of the underlying variables contributing to the industrial sector and its composition, the dynamic nature, and the large number of multivariant features of RIS records have obscured a deep and fine-grained understanding of RIS. To this end, we propose an interactive visualization system, RISeer, which is based on interpretable machine learning models and enhanced visualizations designed to identify the evolutionary patterns of the RIS and facilitate inter-regional inspection and comparison. Two case studies confirm the effectiveness of our approach, and feedback from experts indicates that RISeer helps them to gain a fine-grained understanding of the dynamics and evolution of the RIS.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2210096745,
        "newsscientist":0.182156412,
        "technologyreview":0.2739600344,
        "venturebeat":0.2846252239,
        "wired":0.2289689243,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00625v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1659333236000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.01029v1",
        "predicted_newsworthiness":0.463789302,
        "title":"On the Limitations of Sociodemographic Adaptation with Transformers",
        "summary":"Sociodemographic factors (e.g., gender or age) shape our language. Previous work showed that incorporating specific sociodemographic factors can consistently improve performance for various NLP tasks in traditional NLP models. We investigate whether these previous findings still hold with state-of-the-art pretrained Transformers. We use three common specialization methods proven effective for incorporating external knowledge into pretrained Transformers (e.g., domain-specific or geographic knowledge). We adapt the language representations for the sociodemographic dimensions of gender and age, using continuous language modeling and dynamic multi-task learning for adaptation, where we couple language modeling with the prediction of a sociodemographic class. Our results when employing a multilingual model show substantial performance gains across four languages (English, German, French, and Danish). These findings are in line with the results of previous work and hold promise for successful sociodemographic specialization. However, controlling for confounding factors like domain and language shows that, while sociodemographic adaptation does improve downstream performance, the gains do not always solely stem from sociodemographic knowledge. Our results indicate that sociodemographic specialization, while very important, is still an unresolved problem in NLP.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1519382247,
        "newsscientist":0.1362594913,
        "technologyreview":0.223447239,
        "venturebeat":0.2131424626,
        "wired":0.1799565732,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01029v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659376682000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.01841v1",
        "predicted_newsworthiness":0.4637240004,
        "title":"Robust Learning of Deep Time Series Anomaly Detection Models with Contaminated Training Data",
        "summary":"Time series anomaly detection (TSAD) is an important data mining task with numerous applications in the IoT era. In recent years, a large number of deep neural network-based methods have been proposed, demonstrating significantly better performance than conventional methods on addressing challenging TSAD problems in a variety of areas. Nevertheless, these deep TSAD methods typically rely on a clean training dataset that is not polluted by anomalies to learn the \"normal profile\" of the underlying dynamics. This requirement is nontrivial since a clean dataset can hardly be provided in practice. Moreover, without the awareness of their robustness, blindly applying deep TSAD methods with potentially contaminated training data can possibly incur significant performance degradation in the detection phase. In this work, to tackle this important challenge, we firstly investigate the robustness of commonly used deep TSAD methods with contaminated training data which provides a guideline for applying these methods when the provided training data are not guaranteed to be anomaly-free. Furthermore, we propose a model-agnostic method which can effectively improve the robustness of learning mainstream deep TSAD models with potentially contaminated data. Experiment results show that our method can consistently prevent or mitigate performance degradation of mainstream deep TSAD models on widely used benchmark datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1238175598,
        "newsscientist":0.1766286494,
        "technologyreview":0.2736194647,
        "venturebeat":0.268494319,
        "wired":0.2015923898,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01841v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659502328000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02012v1",
        "predicted_newsworthiness":0.4635462995,
        "title":"Character Generation through Self-Supervised Vectorization",
        "summary":"The prevalent approach in self-supervised image generation is to operate on pixel level representations. While this approach can produce high quality images, it cannot benefit from the simplicity and innate quality of vectorization. Here we present a drawing agent that operates on stroke-level representation of images. At each time step, the agent first assesses the current canvas and decides whether to stop or keep drawing. When a 'draw' decision is made, the agent outputs a program indicating the stroke to be drawn. As a result, it produces a final raster image by drawing the strokes on a canvas, using a minimal number of strokes and dynamically deciding when to stop. We train our agent through reinforcement learning on MNIST and Omniglot datasets for unconditional generation and parsing (reconstruction) tasks. We utilize our parsing agent for exemplar generation and type conditioned concept generation in Omniglot challenge without any further training. We present successful results on all three generation tasks and the parsing task. Crucially, we do not need any stroke-level or vector supervision; we only use raster images for training.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1127831674,
        "newsscientist":0.1676803991,
        "technologyreview":0.266252194,
        "venturebeat":0.2278467819,
        "wired":0.1888237847,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02012v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659529915000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14200v1",
        "predicted_newsworthiness":0.4634564583,
        "title":"CrAM: A Compression-Aware Minimizer",
        "summary":"We examine the question of whether SGD-based optimization of deep neural networks (DNNs) can be adapted to produce models which are both highly-accurate and easily-compressible. We propose a new compression-aware minimizer dubbed CrAM, which modifies the SGD training iteration in a principled way, in order to produce models whose local loss behavior is stable under compression operations such as weight pruning or quantization. Experimental results on standard image classification tasks show that CrAM produces dense models that can be more accurate than standard SGD-type baselines, but which are surprisingly stable under weight pruning: for instance, for ResNet50 on ImageNet, CrAM-trained models can lose up to 70% of their weights in one shot with only minor accuracy loss.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0845188998,
        "newsscientist":0.1336646662,
        "technologyreview":0.2461469719,
        "venturebeat":0.2247441453,
        "wired":0.1527350771,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14200v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659024808000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13770v1",
        "predicted_newsworthiness":0.4634322527,
        "title":"Calibrate: Interactive Analysis of Probabilistic Model Output",
        "summary":"Analyzing classification model performance is a crucial task for machine learning practitioners. While practitioners often use count-based metrics derived from confusion matrices, like accuracy, many applications, such as weather prediction, sports betting, or patient risk prediction, rely on a classifier's predicted probabilities rather than predicted labels. In these instances, practitioners are concerned with producing a calibrated model, that is, one which outputs probabilities that reflect those of the true distribution. Model calibration is often analyzed visually, through static reliability diagrams, however, the traditional calibration visualization may suffer from a variety of drawbacks due to the strong aggregations it necessitates. Furthermore, count-based approaches are unable to sufficiently analyze model calibration. We present Calibrate, an interactive reliability diagram that addresses the aforementioned issues. Calibrate constructs a reliability diagram that is resistant to drawbacks in traditional approaches, and allows for interactive subgroup analysis and instance-level inspection. We demonstrate the utility of Calibrate through use cases on both real-world and synthetic data. We further validate Calibrate by presenting the results of a think-aloud experiment with data scientists who routinely analyze model calibration.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1688325422,
        "newsscientist":0.2013553703,
        "technologyreview":0.2832228821,
        "venturebeat":0.2753977125,
        "wired":0.2106201599,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13770v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.lg"
        ],
        "published":1658952087000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.00859v1",
        "predicted_newsworthiness":0.4632414188,
        "title":"Learning from flowsheets: A generative transformer model for autocompletion of flowsheets",
        "summary":"We propose a novel method enabling autocompletion of chemical flowsheets. This idea is inspired by the autocompletion of text. We represent flowsheets as strings using the text-based SFILES 2.0 notation and learn the grammatical structure of the SFILES 2.0 language and common patterns in flowsheets using a transformer-based language model. We pre-train our model on synthetically generated flowsheets to learn the flowsheet language grammar. Then, we fine-tune our model in a transfer learning step on real flowsheet topologies. Finally, we use the trained model for causal language modeling to autocomplete flowsheets. Eventually, the proposed method can provide chemical engineers with recommendations during interactive flowsheet synthesis. The results demonstrate a high potential of this approach for future AI-assisted process synthesis.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.111706054,
        "newsscientist":0.1668984674,
        "technologyreview":0.2521600108,
        "venturebeat":0.2378229361,
        "wired":0.1764479251,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00859v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cl"
        ],
        "published":1659361438000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00316v1",
        "predicted_newsworthiness":0.4631362213,
        "title":"On Interactive Explanations as Non-Monotonic Reasoning",
        "summary":"Recent work shows issues of consistency with explanations, with methods generating local explanations that seem reasonable instance-wise, but that are inconsistent across instances. This suggests not only that instance-wise explanations can be unreliable, but mainly that, when interacting with a system via multiple inputs, a user may actually lose confidence in the system. To better analyse this issue, in this work we treat explanations as objects that can be subject to reasoning and present a formal model of the interactive scenario between user and system, via sequences of inputs, outputs, and explanations. We argue that explanations can be thought of as committing to some model behaviour (even if only prima facie), suggesting a form of entailment, which, we argue, should be thought of as non-monotonic. This allows: 1) to solve some considered inconsistencies in explanation, such as via a specificity relation; 2) to consider properties from the non-monotonic reasoning literature and discuss their desirability, gaining more insight on the interactive explanation scenario.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1284867014,
        "newsscientist":0.1637009871,
        "technologyreview":0.2406804217,
        "venturebeat":0.2104639974,
        "wired":0.1899113575,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00316v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1659218915000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.11971v1",
        "predicted_newsworthiness":0.4627233733,
        "title":"Jigsaw-ViT: Learning Jigsaw Puzzles in Vision Transformer",
        "summary":"The success of Vision Transformer (ViT) in various computer vision tasks has promoted the ever-increasing prevalence of this convolution-free network. The fact that ViT works on image patches makes it potentially relevant to the problem of jigsaw puzzle solving, which is a classical self-supervised task aiming at reordering shuffled sequential image patches back to their natural form. Despite its simplicity, solving jigsaw puzzle has been demonstrated to be helpful for diverse tasks using Convolutional Neural Networks (CNNs), such as self-supervised feature representation learning, domain generalization, and fine-grained classification. In this paper, we explore solving jigsaw puzzle as a self-supervised auxiliary loss in ViT for image classification, named Jigsaw-ViT. We show two modifications that can make Jigsaw-ViT superior to standard ViT: discarding positional embeddings and masking patches randomly. Yet simple, we find that Jigsaw-ViT is able to improve both in generalization and robustness over the standard ViT, which is usually rather a trade-off. Experimentally, we show that adding the jigsaw puzzle branch provides better generalization than ViT on large-scale image classification on ImageNet. Moreover, the auxiliary task also improves robustness to noisy labels on Animal-10N, Food-101N, and Clothing1M as well as adversarial examples. Our implementation is available at https:\/\/yingyichen-cyy.github.io\/Jigsaw-ViT\/.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1141787221,
        "newsscientist":0.1827519124,
        "technologyreview":0.2953409488,
        "venturebeat":0.2415287785,
        "wired":0.1925302346,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11971v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658737098000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13378v1",
        "predicted_newsworthiness":0.462669288,
        "title":"Identifying Hard Noise in Long-Tailed Sample Distribution",
        "summary":"Conventional de-noising methods rely on the assumption that all samples are independent and identically distributed, so the resultant classifier, though disturbed by noise, can still easily identify the noises as the outliers of training distribution. However, the assumption is unrealistic in large-scale data that is inevitably long-tailed. Such imbalanced training data makes a classifier less discriminative for the tail classes, whose previously \"easy\" noises are now turned into \"hard\" ones -- they are almost as outliers as the clean tail samples. We introduce this new challenge as Noisy Long-Tailed Classification (NLT). Not surprisingly, we find that most de-noising methods fail to identify the hard noises, resulting in significant performance drop on the three proposed NLT benchmarks: ImageNet-NLT, Animal10-NLT, and Food101-NLT. To this end, we design an iterative noisy learning framework called Hard-to-Easy (H2E). Our bootstrapping philosophy is to first learn a classifier as noise identifier invariant to the class and context distributional changes, reducing \"hard\" noises to \"easy\" ones, whose removal further improves the invariance. Experimental results show that our H2E outperforms state-of-the-art de-noising methods and their ablations on long-tailed settings while maintaining a stable performance on the conventional balanced settings. Datasets and codes are available at https:\/\/github.com\/yxymessi\/H2E-Framework",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1123291913,
        "newsscientist":0.1612907165,
        "technologyreview":0.2480026044,
        "venturebeat":0.2242630151,
        "wired":0.1740754903,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13378v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658912583000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12201v1",
        "predicted_newsworthiness":0.4622039999,
        "title":"Calibrated One-class Classification for Unsupervised Time Series Anomaly Detection",
        "summary":"Unsupervised time series anomaly detection is instrumental in monitoring and alarming potential faults of target systems in various domains. Current state-of-the-art time series anomaly detectors mainly focus on devising advanced neural network structures and new reconstruction\/prediction learning objectives to learn data normality (normal patterns and behaviors) as accurately as possible. However, these one-class learning methods can be deceived by unknown anomalies in the training data (i.e., anomaly contamination). Further, their normality learning also lacks knowledge about the anomalies of interest. Consequently, they often learn a biased, inaccurate normality boundary. This paper proposes a novel one-class learning approach, named calibrated one-class classification, to tackle this problem. Our one-class classifier is calibrated in two ways: (1) by adaptively penalizing uncertain predictions, which helps eliminate the impact of anomaly contamination while accentuating the predictions that the one-class model is confident in, and (2) by discriminating the normal samples from native anomaly examples that are generated to simulate genuine time series abnormal behaviors on the basis of original data. These two calibrations result in contamination-tolerant, anomaly-informed one-class learning, yielding a significantly improved normality modeling. Extensive experiments on six real-world datasets show that our model substantially outperforms twelve state-of-the-art competitors and obtains 6% - 31% F1 score improvement. The source code is available at \\url{https:\/\/github.com\/xuhongzuo\/couta}.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.119283916,
        "newsscientist":0.1680819975,
        "technologyreview":0.2456695053,
        "venturebeat":0.2421839593,
        "wired":0.1874708006,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12201v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658756593000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14484v1",
        "predicted_newsworthiness":0.462003921,
        "title":"Adaptive Gradient Methods at the Edge of Stability",
        "summary":"Very little is known about the training dynamics of adaptive gradient methods like Adam in deep learning. In this paper, we shed light on the behavior of these algorithms in the full-batch and sufficiently large batch settings. Specifically, we empirically demonstrate that during full-batch training, the maximum eigenvalue of the preconditioned Hessian typically equilibrates at a certain numerical value -- the stability threshold of a gradient descent algorithm. For Adam with step size $\\eta$ and $\\beta_1 = 0.9$, this stability threshold is $38\/\\eta$. Similar effects occur during minibatch training, especially as the batch size grows. Yet, even though adaptive methods train at the ``Adaptive Edge of Stability'' (AEoS), their behavior in this regime differs in a significant way from that of non-adaptive methods at the EoS. Whereas non-adaptive algorithms at the EoS are blocked from entering high-curvature regions of the loss landscape, adaptive gradient methods at the AEoS can keep advancing into high-curvature regions, while adapting the preconditioner to compensate. Our findings can serve as a foundation for the community's future understanding of adaptive gradient methods in deep learning.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0804107895,
        "newsscientist":0.1242316901,
        "technologyreview":0.2303782186,
        "venturebeat":0.2102649723,
        "wired":0.1469722691,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14484v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659072227000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11504v1",
        "predicted_newsworthiness":0.4615566599,
        "title":"Intelligent 3D Network Protocol for Multimedia Data Classification using Deep Learning",
        "summary":"In videos, the human's actions are of three-dimensional (3D) signals. These videos investigate the spatiotemporal knowledge of human behavior. The promising ability is investigated using 3D convolution neural networks (CNNs). The 3D CNNs have not yet achieved high output for their well-established two-dimensional (2D) equivalents in still photographs. Board 3D Convolutional Memory and Spatiotemporal fusion face training difficulty preventing 3D CNN from accomplishing remarkable evaluation. In this paper, we implement Hybrid Deep Learning Architecture that combines STIP and 3D CNN features to enhance the performance of 3D videos effectively. After implementation, the more detailed and deeper charting for training in each circle of space-time fusion. The training model further enhances the results after handling complicated evaluations of models. The video classification model is used in this implemented model. Intelligent 3D Network Protocol for Multimedia Data Classification using Deep Learning is introduced to further understand spacetime association in human endeavors. In the implementation of the result, the well-known dataset, i.e., UCF101 to, evaluates the performance of the proposed hybrid technique. The results beat the proposed hybrid technique that substantially beats the initial 3D CNNs. The results are compared with state-of-the-art frameworks from literature for action recognition on UCF101 with an accuracy of 95%.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0976371112,
        "newsscientist":0.1599041363,
        "technologyreview":0.2393476883,
        "venturebeat":0.2275140981,
        "wired":0.1814946946,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11504v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658579092000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01889v1",
        "predicted_newsworthiness":0.461541219,
        "title":"Multi-Scale User Behavior Network for Entire Space Multi-Task Learning",
        "summary":"Modelling the user's multiple behaviors is an essential part of modern e-commerce, whose widely adopted application is to jointly optimize click-through rate (CTR) and conversion rate (CVR) predictions. Most of existing methods overlook the effect of two key characteristics of the user's behaviors: for each item list, (i) contextual dependence refers to that the user's behaviors on any item are not purely determinated by the item itself but also are influenced by the user's previous behaviors (e.g., clicks, purchases) on other items in the same sequence; (ii) multiple time scales means that users are likely to click frequently but purchase periodically. To this end, we develop a new multi-scale user behavior network named Hierarchical rEcurrent Ranking On the Entire Space (HEROES) which incorporates the contextual information to estimate the user multiple behaviors in a multi-scale fashion. Concretely, we introduce a hierarchical framework, where the lower layer models the user's engagement behaviors while the upper layer estimates the user's satisfaction behaviors. The proposed architecture can automatically learn a suitable time scale for each layer to capture the dynamic user's behavioral patterns. Besides the architecture, we also introduce the Hawkes process to form a novel recurrent unit which can not only encode the items' features in the context but also formulate the excitation or discouragement from the user's previous behaviors. We further show that HEROES can be extended to build unbiased ranking systems through combinations with the survival analysis technique. Extensive experiments over three large-scale industrial datasets demonstrate the superiority of our model compared with the state-of-the-art methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1188172689,
        "newsscientist":0.1483332776,
        "technologyreview":0.2338967934,
        "venturebeat":0.2655333078,
        "wired":0.2014041679,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01889v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1659512287000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.13929v1",
        "predicted_newsworthiness":0.4615393341,
        "title":"MLRIP: Pre-training a military language representation model with informative factual knowledge and professional knowledge base",
        "summary":"Incorporating prior knowledge into pre-trained language models has proven to be effective for knowledge-driven NLP tasks, such as entity typing and relation extraction. Current pre-training procedures usually inject external knowledge into models by using knowledge masking, knowledge fusion and knowledge replacement. However, factual information contained in the input sentences have not been fully mined, and the external knowledge for injecting have not been strictly checked. As a result, the context information cannot be fully exploited and extra noise will be introduced or the amount of knowledge injected is limited. To address these issues, we propose MLRIP, which modifies the knowledge masking strategies proposed by ERNIE-Baidu, and introduce a two-stage entity replacement strategy. Extensive experiments with comprehensive analyses illustrate the superiority of MLRIP over BERT-based models in military knowledge-driven NLP tasks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1283082068,
        "newsscientist":0.1428797726,
        "technologyreview":0.2590944195,
        "venturebeat":0.2551238524,
        "wired":0.2086183214,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13929v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658993970000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11581v1",
        "predicted_newsworthiness":0.4611750231,
        "title":"Self-Supervised Learning of Echocardiogram Videos Enables Data-Efficient Clinical Diagnosis",
        "summary":"Given the difficulty of obtaining high-quality labels for medical image recognition tasks, there is a need for deep learning techniques that can be adequately fine-tuned on small labeled data sets. Recent advances in self-supervised learning techniques have shown that such an in-domain representation learning approach can provide a strong initialization for supervised fine-tuning, proving much more data-efficient than standard transfer learning from a supervised pretraining task. However, these applications are not adapted to applications to medical diagnostics captured in a video format. With this progress in mind, we developed a self-supervised learning approach catered to echocardiogram videos with the goal of learning strong representations for downstream fine-tuning on the task of diagnosing aortic stenosis (AS), a common and dangerous disease of the aortic valve. When fine-tuned on 1% of the training data, our best self-supervised learning model achieves 0.818 AUC (95% CI: 0.794, 0.840), while the standard transfer learning approach reaches 0.644 AUC (95% CI: 0.610, 0.677). We also find that our self-supervised model attends more closely to the aortic valve when predicting severe AS as demonstrated by saliency map visualizations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1031198835,
        "newsscientist":0.1424549506,
        "technologyreview":0.243313708,
        "venturebeat":0.2250451551,
        "wired":0.1539961653,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11581v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658603846000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11898v1",
        "predicted_newsworthiness":0.4610019558,
        "title":"Domain Adaptive Person Search",
        "summary":"Person search is a challenging task which aims to achieve joint pedestrian detection and person re-identification (ReID). Previous works have made significant advances under fully and weakly supervised settings. However, existing methods ignore the generalization ability of the person search models. In this paper, we take a further step and present Domain Adaptive Person Search (DAPS), which aims to generalize the model from a labeled source domain to the unlabeled target domain. Two major challenges arises under this new setting: one is how to simultaneously solve the domain misalignment issue for both detection and Re-ID tasks, and the other is how to train the ReID subtask without reliable detection results on the target domain. To address these challenges, we propose a strong baseline framework with two dedicated designs. 1) We design a domain alignment module including image-level and task-sensitive instance-level alignments, to minimize the domain discrepancy. 2) We take full advantage of the unlabeled data with a dynamic clustering strategy, and employ pseudo bounding boxes to support ReID and detection training on the target domain. With the above designs, our framework achieves 34.7% in mAP and 80.6% in top-1 on PRW dataset, surpassing the direct transferring baseline by a large margin. Surprisingly, the performance of our unsupervised DAPS model even surpasses some of the fully and weakly supervised methods. The code is available at https:\/\/github.com\/caposerenity\/DAPS.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0765147987,
        "newsscientist":0.1136540517,
        "technologyreview":0.1801443211,
        "venturebeat":0.1567273677,
        "wired":0.1285631664,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11898v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658721759000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14042v1",
        "predicted_newsworthiness":0.4608873495,
        "title":"Robust Self-Tuning Data Association for Geo-Referencing Using Lane Markings",
        "summary":"Localization in aerial imagery-based maps offers many advantages, such as global consistency, geo-referenced maps, and the availability of publicly accessible data. However, the landmarks that can be observed from both aerial imagery and on-board sensors is limited. This leads to ambiguities or aliasing during the data association. Building upon a highly informative representation (that allows efficient data association), this paper presents a complete pipeline for resolving these ambiguities. Its core is a robust self-tuning data association that adapts the search area depending on the entropy of the measurements. Additionally, to smooth the final result, we adjust the information matrix for the associated data as a function of the relative transform produced by the data association process. We evaluate our method on real data from urban and rural scenarios around the city of Karlsruhe in Germany. We compare state-of-the-art outlier mitigation methods with our self-tuning approach, demonstrating a considerable improvement, especially for outer-urban scenarios.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1269791357,
        "newsscientist":0.1515405644,
        "technologyreview":0.2248272484,
        "venturebeat":0.2044567863,
        "wired":0.2027172699,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14042v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv"
        ],
        "published":1659011379000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.12696v1",
        "predicted_newsworthiness":0.4606820841,
        "title":"Advanced Conditional Variational Autoencoders (A-CVAE): Towards interpreting open-domain conversation generation via disentangling latent feature representation",
        "summary":"Currently end-to-end deep learning based open-domain dialogue systems remain black box models, making it easy to generate irrelevant contents with data-driven models. Specifically, latent variables are highly entangled with different semantics in the latent space due to the lack of priori knowledge to guide the training. To address this problem, this paper proposes to harness the generative model with a priori knowledge through a cognitive approach involving mesoscopic scale feature disentanglement. Particularly, the model integrates the macro-level guided-category knowledge and micro-level open-domain dialogue data for the training, leveraging the priori knowledge into the latent space, which enables the model to disentangle the latent variables within the mesoscopic scale. Besides, we propose a new metric for open-domain dialogues, which can objectively evaluate the interpretability of the latent space distribution. Finally, we validate our model on different datasets and experimentally demonstrate that our model is able to generate higher quality and more interpretable dialogues than other models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1164556377,
        "newsscientist":0.1381114859,
        "technologyreview":0.2320783122,
        "venturebeat":0.2266151254,
        "wired":0.1863396508,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12696v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1658821176000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00118v1",
        "predicted_newsworthiness":0.4606255165,
        "title":"Research on Stable Obstacle Avoidance Control Strategy for Tracked Intelligent Transportation Vehicles in Non-structural Environment Based on Deep Learning",
        "summary":"Existing intelligent driving technology often has a problem in balancing smooth driving and fast obstacle avoidance, especially when the vehicle is in a non-structural environment, and is prone to instability in emergency situations. Therefore, this study proposed an autonomous obstacle avoidance control strategy that can effectively guarantee vehicle stability based on Attention-long short-term memory (Attention-LSTM) deep learning model with the idea of humanoid driving. First, we designed the autonomous obstacle avoidance control rules to guarantee the safety of unmanned vehicles. Second, we improved the autonomous obstacle avoidance control strategy combined with the stability analysis of special vehicles. Third, we constructed a deep learning obstacle avoidance control through experiments, and the average relative error of this system was 15%. Finally, the stability and accuracy of this control strategy were verified numerically and experimentally. The method proposed in this study can ensure that the unmanned vehicle can successfully avoid the obstacles while driving smoothly.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0898303079,
        "newsscientist":0.1449178082,
        "technologyreview":0.2522692265,
        "venturebeat":0.2143779448,
        "wired":0.1955530612,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00118v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659146405000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.14452v1",
        "predicted_newsworthiness":0.4601332902,
        "title":"Deep Learning-based Occluded Person Re-identification: A Survey",
        "summary":"Occluded person re-identification (Re-ID) aims at addressing the occlusion problem when retrieving the person of interest across multiple cameras. With the promotion of deep learning technology and the increasing demand for intelligent video surveillance, the frequent occlusion in real-world applications has made occluded person Re-ID draw considerable interest from researchers. A large number of occluded person Re-ID methods have been proposed while there are few surveys that focus on occlusion. To fill this gap and help boost future research, this paper provides a systematic survey of occluded person Re-ID. Through an in-depth analysis of the occlusion in person Re-ID, most existing methods are found to only consider part of the problems brought by occlusion. Therefore, we review occlusion-related person Re-ID methods from the perspective of issues and solutions. We summarize four issues caused by occlusion in person Re-ID, i.e., position misalignment, scale misalignment, noisy information, and missing information. The occlusion-related methods addressing different issues are then categorized and introduced accordingly. After that, we summarize and compare the performance of recent occluded person Re-ID methods on four popular datasets: Partial-ReID, Partial-iLIDS, Occluded-ReID, and Occluded-DukeMTMC. Finally, we provide insights on promising future research directions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1042439123,
        "newsscientist":0.1399985144,
        "technologyreview":0.2185142764,
        "venturebeat":0.1931250356,
        "wired":0.1545038971,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14452v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659064218000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01997v1",
        "predicted_newsworthiness":0.4601303405,
        "title":"Convolutional Fine-Grained Classification with Self-Supervised Target Relation Regularization",
        "summary":"Fine-grained visual classification can be addressed by deep representation learning under supervision of manually pre-defined targets (e.g., one-hot or the Hadamard codes). Such target coding schemes are less flexible to model inter-class correlation and are sensitive to sparse and imbalanced data distribution as well. In light of this, this paper introduces a novel target coding scheme -- dynamic target relation graphs (DTRG), which, as an auxiliary feature regularization, is a self-generated structural output to be mapped from input images. Specifically, online computation of class-level feature centers is designed to generate cross-category distance in the representation space, which can thus be depicted by a dynamic graph in a non-parametric manner. Explicitly minimizing intra-class feature variations anchored on those class-level centers can encourage learning of discriminative features. Moreover, owing to exploiting inter-class dependency, the proposed target graphs can alleviate data sparsity and imbalanceness in representation learning. Inspired by recent success of the mixup style data augmentation, this paper introduces randomness into soft construction of dynamic target relation graphs to further explore relation diversity of target classes. Experimental results can demonstrate the effectiveness of our method on a number of diverse benchmarks of multiple visual classification tasks, especially achieving the state-of-the-art performance on popular fine-grained object benchmarks and superior robustness against sparse and imbalanced data. Source codes are made publicly available at https:\/\/github.com\/AkonLau\/DTRG.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0924956764,
        "newsscientist":0.136709723,
        "technologyreview":0.2363269645,
        "venturebeat":0.2034593918,
        "wired":0.1640191854,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01997v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659527513000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11810v1",
        "predicted_newsworthiness":0.4598675047,
        "title":"VizWiz-FewShot: Locating Objects in Images Taken by People With Visual Impairments",
        "summary":"We introduce a few-shot localization dataset originating from photographers who authentically were trying to learn about the visual content in the images they took. It includes nearly 10,000 segmentations of 100 categories in over 4,500 images that were taken by people with visual impairments. Compared to existing few-shot object detection and instance segmentation datasets, our dataset is the first to locate holes in objects (e.g., found in 12.3\\% of our segmentations), it shows objects that occupy a much larger range of sizes relative to the images, and text is over five times more common in our objects (e.g., found in 22.4\\% of our segmentations). Analysis of three modern few-shot localization algorithms demonstrates that they generalize poorly to our new dataset. The algorithms commonly struggle to locate objects with holes, very small and very large objects, and objects lacking text. To encourage a larger community to work on these unsolved challenges, we publicly share our annotated few-shot dataset at https:\/\/vizwiz.org .",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1296939146,
        "newsscientist":0.1774646238,
        "technologyreview":0.2475304392,
        "venturebeat":0.2276798121,
        "wired":0.220055754,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11810v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658695491000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02126v1",
        "predicted_newsworthiness":0.4598014048,
        "title":"Noise tolerance of learning to rank under class-conditional label noise",
        "summary":"Often, the data used to train ranking models is subject to label noise. For example, in web-search, labels created from clickstream data are noisy due to issues such as insufficient information in item descriptions on the SERP, query reformulation by the user, and erratic or unexpected user behavior. In practice, it is difficult to handle label noise without making strong assumptions about the label generation process. As a result, practitioners typically train their learning-to-rank (LtR) models directly on this noisy data without additional consideration of the label noise. Surprisingly, we often see strong performance from LtR models trained in this way. In this work, we describe a class of noise-tolerant LtR losses for which empirical risk minimization is a consistent procedure, even in the context of class-conditional label noise. We also develop noise-tolerant analogs of commonly used loss functions. The practical implications of our theoretical findings are further supported by experimental results.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.089278288,
        "newsscientist":0.128620577,
        "technologyreview":0.217224735,
        "venturebeat":0.2298847613,
        "wired":0.1649457277,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02126v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.lg"
        ],
        "published":1659539088000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.02011v1",
        "predicted_newsworthiness":0.4592131919,
        "title":"Equivariant Disentangled Transformation for Domain Generalization under Combination Shift",
        "summary":"Machine learning systems may encounter unexpected problems when the data distribution changes in the deployment environment. A major reason is that certain combinations of domains and labels are not observed during training but appear in the test environment. Although various invariance-based algorithms can be applied, we find that the performance gain is often marginal. To formally analyze this issue, we provide a unique algebraic formulation of the combination shift problem based on the concepts of homomorphism, equivariance, and a refined definition of disentanglement. The algebraic requirements naturally derive a simple yet effective method, referred to as equivariant disentangled transformation (EDT), which augments the data based on the algebraic structures of labels and makes the transformation satisfy the equivariance and disentanglement requirements. Experimental results demonstrate that invariance may be insufficient, and it is important to exploit the equivariance structure in the combination shift problem.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0768428469,
        "newsscientist":0.1342607098,
        "technologyreview":0.2203475835,
        "venturebeat":0.1917185061,
        "wired":0.1374645968,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02011v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659529891000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13506v1",
        "predicted_newsworthiness":0.4592047821,
        "title":"Satellite Image Based Cross-view Localization for Autonomous Vehicle",
        "summary":"Existing spatial localization techniques for autonomous vehicles mostly use a pre-built 3D-HD map, often constructed using a survey-grade 3D mapping vehicle, which is not only expensive but also laborious. This paper shows that by using an off-the-shelf high-definition satellite image as a ready-to-use map, we are able to achieve cross-view vehicle localization up to a satisfactory accuracy, providing a cheaper and more practical way for localization. Although the idea of using satellite images for cross-view localization is not new, previous methods almost exclusively treat the task as image retrieval, namely matching a vehicle-captured ground-view image with the satellite image. This paper presents a novel cross-view localization method, which departs from the common wisdom of image retrieval. Specifically, our method develops (1) a Geometric-align Feature Extractor (GaFE) that leverages measured 3D points to bridge the geometric gap between ground view and overhead view, (2) a Pose Aware Branch (PAB) adopting a triplet loss to encourage pose-aware feature extracting, and (3) a Recursive Pose Refine Branch (RPRB) using the Levenberg-Marquardt (LM) algorithm to align the initial pose towards the true vehicle pose iteratively. Our method is validated on KITTI and Ford Multi-AV Seasonal datasets as ground view and Google Maps as the satellite view. The results demonstrate the superiority of our method in cross-view localization with spatial and angular errors within 1 meter and $2^\\circ$, respectively. The code will be made publicly available.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.093867723,
        "newsscientist":0.1405159019,
        "technologyreview":0.2208480025,
        "venturebeat":0.2077966059,
        "wired":0.2035307318,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13506v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658927799000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11524v1",
        "predicted_newsworthiness":0.4591719654,
        "title":"Audio-driven Neural Gesture Reenactment with Video Motion Graphs",
        "summary":"Human speech is often accompanied by body gestures including arm and hand gestures. We present a method that reenacts a high-quality video with gestures matching a target speech audio. The key idea of our method is to split and re-assemble clips from a reference video through a novel video motion graph encoding valid transitions between clips. To seamlessly connect different clips in the reenactment, we propose a pose-aware video blending network which synthesizes video frames around the stitched frames between two clips. Moreover, we developed an audio-based gesture searching algorithm to find the optimal order of the reenacted frames. Our system generates reenactments that are consistent with both the audio rhythms and the speech content. We evaluate our synthesized video quality quantitatively, qualitatively, and with user studies, demonstrating that our method produces videos of much higher quality and consistency with the target audio compared to previous work and baselines.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1094339996,
        "newsscientist":0.1547731853,
        "technologyreview":0.2134933996,
        "venturebeat":0.2251558496,
        "wired":0.2100360382,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11524v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658584977000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01562v2",
        "predicted_newsworthiness":0.4590960511,
        "title":"An Online Sparse Streaming Feature Selection Algorithm",
        "summary":"Online streaming feature selection (OSFS), which conducts feature selection in an online manner, plays an important role in dealing with high-dimensional data. In many real applications such as intelligent healthcare platform, streaming feature always has some missing data, which raises a crucial challenge in conducting OSFS, i.e., how to establish the uncertain relationship between sparse streaming features and labels. Unfortunately, existing OSFS algorithms never consider such uncertain relationship. To fill this gap, we in this paper propose an online sparse streaming feature selection with uncertainty (OS2FSU) algorithm. OS2FSU consists of two main parts: 1) latent factor analysis is utilized to pre-estimate the missing data in sparse streaming features before con-ducting feature selection, and 2) fuzzy logic and neighborhood rough set are employed to alleviate the uncertainty between estimated streaming features and labels during conducting feature selection. In the experiments, OS2FSU is compared with five state-of-the-art OSFS algorithms on six real datasets. The results demonstrate that OS2FSU outperforms its competitors when missing data are encountered in OSFS.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.102375593,
        "newsscientist":0.1307366859,
        "technologyreview":0.2054862056,
        "venturebeat":0.2197409485,
        "wired":0.1514008296,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01562v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659456502000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01030v1",
        "predicted_newsworthiness":0.4590503464,
        "title":"SMART: Sentences as Basic Units for Text Evaluation",
        "summary":"Widely used evaluation metrics for text generation either do not work well with longer texts or fail to evaluate all aspects of text quality. In this paper, we introduce a new metric called SMART to mitigate such limitations. Specifically, We treat sentences as basic units of matching instead of tokens, and use a sentence matching function to soft-match candidate and reference sentences. Candidate sentences are also compared to sentences in the source documents to allow grounding (e.g., factuality) evaluation. Our results show that system-level correlations of our proposed metric with a model-based matching function outperforms all competing metrics on the SummEval summarization meta-evaluation dataset, while the same metric with a string-based matching function is competitive with current model-based metrics. The latter does not use any neural model, which is useful during model development phases where resources can be limited and fast evaluation is required. Finally, we also conducted extensive analyses showing that our proposed metrics work well with longer summaries and are less biased towards specific models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1109730429,
        "newsscientist":0.1256607066,
        "technologyreview":0.2084180431,
        "venturebeat":0.2146296872,
        "wired":0.1664913061,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01030v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659376685000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11439v1",
        "predicted_newsworthiness":0.4589435676,
        "title":"A Taxonomy of Recurrent Learning Rules",
        "summary":"Backpropagation through time (BPTT) is the de facto standard for training recurrent neural networks (RNNs), but it is non-causal and non-local. Real-time recurrent learning is a causal alternative, but it is highly inefficient. Recently, e-prop was proposed as a causal, local, and efficient practical alternative to these algorithms, providing an approximation of the exact gradient by radically pruning the recurrent dependencies carried over time. Here, we derive RTRL from BPTT using a detailed notation bringing intuition and clarification to how they are connected. Furthermore, we frame e-prop within in the picture, formalising what it approximates. Finally, we derive a family of algorithms of which e-prop is a special case.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1071500465,
        "newsscientist":0.160868255,
        "technologyreview":0.248671062,
        "venturebeat":0.2280064477,
        "wired":0.1782490653,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11439v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658559822000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00638v1",
        "predicted_newsworthiness":0.4585874587,
        "title":"Composable Text Control Operations in Latent Space with Ordinary Differential Equations",
        "summary":"Real-world text applications often involve composing a wide range of text control operations, such as editing the text w.r.t. an attribute, manipulating keywords and structure, and generating new text of desired properties. Prior work typically learns\/finetunes a language model (LM) to perform individual or specific subsets of operations. Recent research has studied combining operations in a plug-and-play manner, often with costly search or optimization in the complex sequence space. This paper proposes a new efficient approach for composable text operations in the compact latent space of text. The low-dimensionality and differentiability of the text latent vector allow us to develop an efficient sampler based on ordinary differential equations (ODEs) given arbitrary plug-in operators (e.g., attribute classifiers). By connecting pretrained LMs (e.g., GPT2) to the latent space through efficient adaption, we then decode the sampled vectors into desired text sequences. The flexible approach permits diverse control operators (sentiment, tense, formality, keywords, etc.) acquired using any relevant data from different domains. Experiments show that composing those operators within our approach manages to generate or edit high-quality text, substantially improving over previous methods in terms of generation quality and efficiency.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1175838631,
        "newsscientist":0.1370512327,
        "technologyreview":0.2318521344,
        "venturebeat":0.2292407791,
        "wired":0.1873214109,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00638v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1659336705000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.01545v1",
        "predicted_newsworthiness":0.4585301366,
        "title":"The Curse of Low Task Diversity: On the Failure of Transfer Learning to Outperform MAML and Their Empirical Equivalence",
        "summary":"Recently, it has been observed that a transfer learning solution might be all we need to solve many few-shot learning benchmarks -- thus raising important questions about when and how meta-learning algorithms should be deployed. In this paper, we seek to clarify these questions by 1. proposing a novel metric -- the diversity coefficient -- to measure the diversity of tasks in a few-shot learning benchmark and 2. by comparing Model-Agnostic Meta-Learning (MAML) and transfer learning under fair conditions (same architecture, same optimizer, and all models trained to convergence). Using the diversity coefficient, we show that the popular MiniImageNet and CIFAR-FS few-shot learning benchmarks have low diversity. This novel insight contextualizes claims that transfer learning solutions are better than meta-learned solutions in the regime of low diversity under a fair comparison. Specifically, we empirically find that a low diversity coefficient correlates with a high similarity between transfer learning and MAML learned solutions in terms of accuracy at meta-test time and classification layer similarity (using feature based distance metrics like SVCCA, PWCCA, CKA, and OPD). To further support our claim, we find this meta-test accuracy holds even as the model size changes. Therefore, we conclude that in the low diversity regime, MAML and transfer learning have equivalent meta-test performance when both are compared fairly. We also hope our work inspires more thoughtful constructions and quantitative evaluations of meta-learning benchmarks in the future.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0967489013,
        "newsscientist":0.1387232308,
        "technologyreview":0.2556089121,
        "venturebeat":0.2392190988,
        "wired":0.161542076,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01545v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659455351000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02225v1",
        "predicted_newsworthiness":0.4585022599,
        "title":"Sequence Model Imitation Learning with Unobserved Contexts",
        "summary":"We consider imitation learning problems where the expert has access to a per-episode context that is hidden from the learner, both in the demonstrations and at test-time. While the learner might not be able to accurately reproduce expert behavior early on in an episode, by considering the entire history of states and actions, they might be able to eventually identify the context and act as the expert would. We prove that on-policy imitation learning algorithms (with or without access to a queryable expert) are better equipped to handle these sorts of asymptotically realizable problems than off-policy methods and are able to avoid the latching behavior (naive repetition of past actions) that plagues the latter. We conduct experiments in a toy bandit domain that show that there exist sharp phase transitions of whether off-policy approaches are able to match expert performance asymptotically, in contrast to the uniformly good performance of on-policy approaches. We demonstrate that on several continuous control tasks, on-policy approaches are able to use history to identify the context while off-policy approaches actually perform worse when given access to history.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0975556479,
        "newsscientist":0.1697349534,
        "technologyreview":0.2645389233,
        "venturebeat":0.2218040821,
        "wired":0.1781781072,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02225v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659547664000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14656v1",
        "predicted_newsworthiness":0.4583942635,
        "title":"Multimodal SuperCon: Classifier for Drivers of Deforestation in Indonesia",
        "summary":"Deforestation is one of the contributing factors to climate change. Climate change has a serious impact on human life, and it occurs due to emission of greenhouse gases, such as carbon dioxide, to the atmosphere. It is important to know the causes of deforestation for mitigation efforts, but there is a lack of data-driven research studies to predict these deforestation drivers. In this work, we propose a contrastive learning architecture, called Multimodal SuperCon, for classifying drivers of deforestation in Indonesia using satellite images obtained from Landsat 8. Multimodal SuperCon is an architecture which combines contrastive learning and multimodal fusion to handle the available deforestation dataset. Our proposed model outperforms previous work on driver classification, giving a 7% improvement in accuracy in comparison to a state-of-the-art rotation equivariant model for the same task.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1989880116,
        "newsscientist":0.2348166977,
        "technologyreview":0.2825553135,
        "venturebeat":0.2418201483,
        "wired":0.2179495859,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14656v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659099811000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13147v1",
        "predicted_newsworthiness":0.4580930857,
        "title":"FP4: Line-rate Greybox Fuzz Testing for P4 Switches",
        "summary":"Compared to fixed-function switches, the flexibility of programmable switches comes at a cost, as programmer mistakes frequently result in subtle bugs in the network data plane. In this paper, we present the design and implementation of FP4, a fuzz-testing framework for P4 switches that achieves high expressiveness, coverage, and scalability. FP4 directly tests running switches by generating semi-random input packets and observing their resulting execution in the data plane. To achieve high coverage and scalability, at runtime, FP4 leverages P4 itself with another \"tester\" switch that generates and mutates billions of test packets per second entirely in the dataplane. Because testing some program branches requires navigating complex semantic input requirements, FP4 additionally leverages the programmability of P4 by instrumenting the tested program to pass coverage information back to the tester through the packet header. We present case studies showing that FP4 can validate both safety and stateful properties, improves efficiency over existing random packet generation baselines, and reaches 100% coverage in under a minute on a wide range of examples.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0862939617,
        "newsscientist":0.1113466924,
        "technologyreview":0.1568226209,
        "venturebeat":0.1783698312,
        "wired":0.1555351649,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13147v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658861990000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2208.00394v1",
        "predicted_newsworthiness":0.4579181659,
        "title":"STrajNet: Occupancy Flow Prediction via Multi-modal Swin Transformer",
        "summary":"Making an accurate prediction of occupancy and flow is essential to enable better safety and interaction for autonomous vehicles under complex traffic scenarios. This work proposes STrajNet: a multi-modal Swin Transformerbased framework for effective scene occupancy and flow predictions. We employ Swin Transformer to encode the image and interaction-aware motion representations and propose a cross-attention module to inject motion awareness into grid cells across different time steps. Flow and occupancy predictions are then decoded through temporalsharing Pyramid decoders. The proposed method shows competitive prediction accuracy and other evaluation metrics in the Waymo Open Dataset benchmark.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1104024484,
        "newsscientist":0.1624952334,
        "technologyreview":0.2552109034,
        "venturebeat":0.2570302164,
        "wired":0.2286526051,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00394v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659256615000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12049v1",
        "predicted_newsworthiness":0.4578890546,
        "title":"Few-Shot Object Detection by Knowledge Distillation Using Bag-of-Visual-Words Representations",
        "summary":"While fine-tuning based methods for few-shot object detection have achieved remarkable progress, a crucial challenge that has not been addressed well is the potential class-specific overfitting on base classes and sample-specific overfitting on novel classes. In this work we design a novel knowledge distillation framework to guide the learning of the object detector and thereby restrain the overfitting in both the pre-training stage on base classes and fine-tuning stage on novel classes. To be specific, we first present a novel Position-Aware Bag-of-Visual-Words model for learning a representative bag of visual words (BoVW) from a limited size of image set, which is used to encode general images based on the similarities between the learned visual words and an image. Then we perform knowledge distillation based on the fact that an image should have consistent BoVW representations in two different feature spaces. To this end, we pre-learn a feature space independently from the object detection, and encode images using BoVW in this space. The obtained BoVW representation for an image can be considered as distilled knowledge to guide the learning of object detector: the extracted features by the object detector for the same image are expected to derive the consistent BoVW representations with the distilled knowledge. Extensive experiments validate the effectiveness of our method and demonstrate the superiority over other state-of-the-art methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0792652664,
        "newsscientist":0.1228326105,
        "technologyreview":0.1937604319,
        "venturebeat":0.168570724,
        "wired":0.1392227474,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12049v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658745640000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00818v1",
        "predicted_newsworthiness":0.4577457686,
        "title":"Learning to Grasp on the Moon from 3D Octree Observations with Deep Reinforcement Learning",
        "summary":"Extraterrestrial rovers with a general-purpose robotic arm have many potential applications in lunar and planetary exploration. Introducing autonomy into such systems is desirable for increasing the time that rovers can spend gathering scientific data and collecting samples. This work investigates the applicability of deep reinforcement learning for vision-based robotic grasping of objects on the Moon. A novel simulation environment with procedurally-generated datasets is created to train agents under challenging conditions in unstructured scenes with uneven terrain and harsh illumination. A model-free off-policy actor-critic algorithm is then employed for end-to-end learning of a policy that directly maps compact octree observations to continuous actions in Cartesian space. Experimental evaluation indicates that 3D data representations enable more effective learning of manipulation skills when compared to traditionally used image-based observations. Domain randomization improves the generalization of learned policies to novel scenes with previously unseen objects and different illumination conditions. To this end, we demonstrate zero-shot sim-to-real transfer by evaluating trained agents on a real robot in a Moon-analogue facility.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1010754191,
        "newsscientist":0.2022766465,
        "technologyreview":0.2737340309,
        "venturebeat":0.2196767211,
        "wired":0.2058114654,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00818v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai",
            "cs.lg"
        ],
        "published":1659358743000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13370v1",
        "predicted_newsworthiness":0.4576351436,
        "title":"MagGlove: A Haptic Glove with Movable Magnetic Force for Manipulation Learning",
        "summary":"Recently, haptic gloves have been extensively explored for various practical applications, such as manipulation learning. Previous glove devices have different force-driven systems, such as shape memory alloys, servo motors and pneumatic actuators; however, these proposed devices may have difficulty in fast finger movement, easy reproduction, and safety issues. In this study, we propose MagGlove, a novel haptic glove with a movable magnet mechanism that has a linear motor, to solve these issues. The proposed MagGlove device is a compact system on the back of the wearer's hand with high responsiveness, ease of use, and good safety. The proposed device is adaptive with the modification of the magnitude of the current flowing through the coil. Based on our evaluation study, it is verified that the proposed device can achieve finger motion in the given tasks. Therefore, MagGlove can provide flexible support tailored to the wearers' learning levels in manipulation learning tasks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1040811779,
        "newsscientist":0.1927519499,
        "technologyreview":0.2234955803,
        "venturebeat":0.1973840176,
        "wired":0.1886449536,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13370v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ro"
        ],
        "published":1658912075000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.13083v1",
        "predicted_newsworthiness":0.4576165602,
        "title":"Task Agnostic and Post-hoc Unseen Distribution Detection",
        "summary":"Despite the recent advances in out-of-distribution(OOD) detection, anomaly detection, and uncertainty estimation tasks, there do not exist a task-agnostic and post-hoc approach. To address this limitation, we design a novel clustering-based ensembling method, called Task Agnostic and Post-hoc Unseen Distribution Detection (TAPUDD) that utilizes the features extracted from the model trained on a specific task. Explicitly, it comprises of TAP-Mahalanobis, which clusters the training datasets' features and determines the minimum Mahalanobis distance of the test sample from all clusters. Further, we propose the Ensembling module that aggregates the computation of iterative TAP-Mahalanobis for a different number of clusters to provide reliable and efficient cluster computation. Through extensive experiments on synthetic and real-world datasets, we observe that our approach can detect unseen samples effectively across diverse tasks and performs better or on-par with the existing baselines. To this end, we eliminate the necessity of determining the optimal value of the number of clusters and demonstrate that our method is more viable for large-scale classification tasks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1017484178,
        "newsscientist":0.1632961006,
        "technologyreview":0.2434863961,
        "venturebeat":0.2405273711,
        "wired":0.1790214749,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13083v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cv"
        ],
        "published":1658858115000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12859v1",
        "predicted_newsworthiness":0.4573609941,
        "title":"Visually explaining 3D-CNN predictions for video classification with an adaptive occlusion sensitivity analysis",
        "summary":"This paper proposes a method for visually explaining the decision-making process of 3D convolutional neural networks (CNN) with a temporal extension of occlusion sensitivity analysis. The key idea here is to occlude a specific volume of data by a 3D mask in an input 3D temporal-spatial data space and then measure the change degree in the output score. The occluded volume data that produces a larger change degree is regarded as a more critical element for classification. However, while the occlusion sensitivity analysis is commonly used to analyze single image classification, it is not so straightforward to apply this idea to video classification as a simple fixed cuboid cannot deal with the motions. To this end, we adapt the shape of a 3D occlusion mask to complicated motions of target objects. Our flexible mask adaptation is performed by considering the temporal continuity and spatial co-occurrence of the optical flows extracted from the input video data. We further propose to approximate our method by using the first-order partial derivative of the score with respect to an input image to reduce its computational cost. We demonstrate the effectiveness of our method through various and extensive comparisons with the conventional methods in terms of the deletion\/insertion metric and the pointing metric on the UCF-101. The code is available at: https:\/\/github.com\/uchiyama33\/AOSA.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1232358201,
        "newsscientist":0.1752209997,
        "technologyreview":0.2437789113,
        "venturebeat":0.227375614,
        "wired":0.1973371971,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12859v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658839371000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00787v1",
        "predicted_newsworthiness":0.4570015362,
        "title":"On the robustness of self-supervised representations for multi-view object classification",
        "summary":"It is known that representations from self-supervised pre-training can perform on par, and often better, on various downstream tasks than representations from fully-supervised pre-training. This has been shown in a host of settings such as generic object classification and detection, semantic segmentation, and image retrieval. However, some issues have recently come to the fore that demonstrate some of the failure modes of self-supervised representations, such as performance on non-ImageNet-like data, or complex scenes. In this paper, we show that self-supervised representations based on the instance discrimination objective lead to better representations of objects that are more robust to changes in the viewpoint and perspective of the object. We perform experiments of modern self-supervised methods against multiple supervised baselines to demonstrate this, including approximating object viewpoint variation through homographies, and real-world tests based on several multi-view datasets. We find that self-supervised representations are more robust to object viewpoint and appear to encode more pertinent information about objects that facilitate the recognition of objects from novel views.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0854385046,
        "newsscientist":0.1427422201,
        "technologyreview":0.225560127,
        "venturebeat":0.1980500071,
        "wired":0.1535014307,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00787v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658942695000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12203v1",
        "predicted_newsworthiness":0.4569903454,
        "title":"Improving Adversarial Robustness via Mutual Information Estimation",
        "summary":"Deep neural networks (DNNs) are found to be vulnerable to adversarial noise. They are typically misled by adversarial samples to make wrong predictions. To alleviate this negative effect, in this paper, we investigate the dependence between outputs of the target model and input adversarial samples from the perspective of information theory, and propose an adversarial defense method. Specifically, we first measure the dependence by estimating the mutual information (MI) between outputs and the natural patterns of inputs (called natural MI) and MI between outputs and the adversarial patterns of inputs (called adversarial MI), respectively. We find that adversarial samples usually have larger adversarial MI and smaller natural MI compared with those w.r.t. natural samples. Motivated by this observation, we propose to enhance the adversarial robustness by maximizing the natural MI and minimizing the adversarial MI during the training process. In this way, the target model is expected to pay more attention to the natural pattern that contains objective semantics. Empirical evaluations demonstrate that our method could effectively improve the adversarial accuracy against multiple attacks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1327362669,
        "newsscientist":0.1902592675,
        "technologyreview":0.3206905298,
        "venturebeat":0.2669460465,
        "wired":0.2252369619,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12203v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658756711000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02034v1",
        "predicted_newsworthiness":0.4569523193,
        "title":"SSformer: A Lightweight Transformer for Semantic Segmentation",
        "summary":"It is well believed that Transformer performs better in semantic segmentation compared to convolutional neural networks. Nevertheless, the original Vision Transformer may lack of inductive biases of local neighborhoods and possess a high time complexity. Recently, Swin Transformer sets a new record in various vision tasks by using hierarchical architecture and shifted windows while being more efficient. However, as Swin Transformer is specifically designed for image classification, it may achieve suboptimal performance on dense prediction-based segmentation task. Further, simply combing Swin Transformer with existing methods would lead to the boost of model size and parameters for the final segmentation model. In this paper, we rethink the Swin Transformer for semantic segmentation, and design a lightweight yet effective transformer model, called SSformer. In this model, considering the inherent hierarchical design of Swin Transformer, we propose a decoder to aggregate information from different layers, thus obtaining both local and global attentions. Experimental results show the proposed SSformer yields comparable mIoU performance with state-of-the-art models, while maintaining a smaller model size and lower compute.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0789804173,
        "newsscientist":0.1218016452,
        "technologyreview":0.22098169,
        "venturebeat":0.2161283316,
        "wired":0.1592437213,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02034v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659531420000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12106v1",
        "predicted_newsworthiness":0.4560773219,
        "title":"Black-box Few-shot Knowledge Distillation",
        "summary":"Knowledge distillation (KD) is an efficient approach to transfer the knowledge from a large \"teacher\" network to a smaller \"student\" network. Traditional KD methods require lots of labeled training samples and a white-box teacher (parameters are accessible) to train a good student. However, these resources are not always available in real-world applications. The distillation process often happens at an external party side where we do not have access to much data, and the teacher does not disclose its parameters due to security and privacy concerns. To overcome these challenges, we propose a black-box few-shot KD method to train the student with few unlabeled training samples and a black-box teacher. Our main idea is to expand the training set by generating a diverse set of out-of-distribution synthetic images using MixUp and a conditional variational auto-encoder. These synthetic images along with their labels obtained from the teacher are used to train the student. We conduct extensive experiments to show that our method significantly outperforms recent SOTA few\/zero-shot KD methods on image classification tasks. The code and models are available at: https:\/\/github.com\/nphdang\/FS-BBT",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1005017236,
        "newsscientist":0.1465866646,
        "technologyreview":0.247618877,
        "venturebeat":0.2156067505,
        "wired":0.1604357551,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12106v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1658751413000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14508v1",
        "predicted_newsworthiness":0.4555738294,
        "title":"Transfer Learning for Segmentation Problems: Choose the Right Encoder and Skip the Decoder",
        "summary":"It is common practice to reuse models initially trained on different data to increase downstream task performance. Especially in the computer vision domain, ImageNet-pretrained weights have been successfully used for various tasks. In this work, we investigate the impact of transfer learning for segmentation problems, being pixel-wise classification problems that can be tackled with encoder-decoder architectures. We find that transfer learning the decoder does not help downstream segmentation tasks, while transfer learning the encoder is truly beneficial. We demonstrate that pretrained weights for a decoder may yield faster convergence, but they do not improve the overall model performance as one can obtain equivalent results with randomly initialized decoders. However, we show that it is more effective to reuse encoder weights trained on a segmentation or reconstruction task than reusing encoder weights trained on classification tasks. This finding implicates that using ImageNet-pretrained encoders for downstream segmentation problems is suboptimal. We also propose a contrastive self-supervised approach with multiple self-reconstruction tasks, which provides encoders that are suitable for transfer learning in segmentation problems in the absence of segmentation labels.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0720359583,
        "newsscientist":0.1036334942,
        "technologyreview":0.1969360328,
        "venturebeat":0.1690405083,
        "wired":0.116980101,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14508v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659078125000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12883v1",
        "predicted_newsworthiness":0.4554304985,
        "title":"Enhancing Collaborative Filtering Recommender with Prompt-Based Sentiment Analysis",
        "summary":"Collaborative Filtering(CF) recommender is a crucial application in the online market and ecommerce. However, CF recommender has been proven to suffer from persistent problems related to sparsity of the user rating that will further lead to a cold-start issue. Existing methods address the data sparsity issue by applying token-level sentiment analysis that translate text review into sentiment scores as a complement of the user rating. In this paper, we attempt to optimize the sentiment analysis with advanced NLP models including BERT and RoBERTa, and experiment on whether the CF recommender has been further enhanced. We build the recommenders on the Amazon US Reviews dataset, and tune the pretrained BERT and RoBERTa with the traditional fine-tuned paradigm as well as the new prompt-based learning paradigm. Experimental result shows that the recommender enhanced with the sentiment ratings predicted by the fine-tuned RoBERTa has the best performance, and achieved 30.7% overall gain by comparing MAP, NDCG and precision at K to the baseline recommender. Prompt-based learning paradigm, although superior to traditional fine-tune paradigm in pure sentiment analysis, fail to further improve the CF recommender.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1042906455,
        "newsscientist":0.1152562768,
        "technologyreview":0.2203594152,
        "venturebeat":0.2603706518,
        "wired":0.1929236566,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12883v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.cl",
            "cs.lg"
        ],
        "published":1658264671000,
        "published_hr":"Jul 19, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.11812v1",
        "predicted_newsworthiness":0.4553441327,
        "title":"Federated Graph Machine Learning: A Survey of Concepts, Techniques, and Applications",
        "summary":"Graph machine learning has gained great attention in both academia and industry recently. Most of the graph machine learning models, such as Graph Neural Networks (GNNs), are trained over massive graph data. However, in many real-world scenarios, such as hospitalization prediction in healthcare systems, the graph data is usually stored at multiple data owners and cannot be directly accessed by any other parties due to privacy concerns and regulation restrictions. Federated Graph Machine Learning (FGML) is a promising solution to tackle this challenge by training graph machine learning models in a federated manner. In this survey, we conduct a comprehensive review of the literature in FGML. Specifically, we first provide a new taxonomy to divide the existing problems in FGML into two settings, namely, \\emph{FL with structured data} and \\emph{structured FL}. Then, we review the mainstream techniques in each setting and elaborate on how they address the challenges under FGML. In addition, we summarize the real-world applications of FGML from different domains and introduce open graph datasets and platforms adopted in FGML. Finally, we present several limitations in the existing studies with promising research directions in this field.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1286194236,
        "newsscientist":0.1590772384,
        "technologyreview":0.2781569697,
        "venturebeat":0.2820646114,
        "wired":0.2062051407,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11812v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658695583000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02063v1",
        "predicted_newsworthiness":0.455277714,
        "title":"Evaluation and comparison of eight popular Lidar and Visual SLAM algorithms",
        "summary":"In this paper, we evaluate eight popular and open-source 3D Lidar and visual SLAM (Simultaneous Localization and Mapping) algorithms, namely LOAM, Lego LOAM, LIO SAM, HDL Graph, ORB SLAM3, Basalt VIO, and SVO2. We have devised experiments both indoor and outdoor to investigate the effect of the following items: i) effect of mounting positions of the sensors, ii) effect of terrain type and vibration, iii) effect of motion (variation in linear and angular speed). We compare their performance in terms of relative and absolute pose error. We also provide comparison on their required computational resources. We thoroughly analyse and discuss the results and identify the best performing system for the environment cases with our multi-camera and multi-Lidar indoor and outdoor datasets. We hope our findings help one to choose a sensor and the corresponding SLAM algorithm combination suiting their needs, based on their target environment.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0964493343,
        "newsscientist":0.1465210631,
        "technologyreview":0.1911235845,
        "venturebeat":0.1890207294,
        "wired":0.1738099484,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02063v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv"
        ],
        "published":1659533469000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.14225v1",
        "predicted_newsworthiness":0.4548453326,
        "title":"Electricity Price Forecasting Model based on Gated Recurrent Units",
        "summary":"The participation of consumers and producers in demand response programs has increased in smart grids, which reduces investment and operation costs of power systems. Also, with the advent of renewable energy sources, the electricity market is becoming more complex and unpredictable. To effectively implement demand response programs, forecasting the future price of electricity is very crucial for producers in the electricity market. Electricity prices are very volatile and change under the influence of various factors such as temperature, wind speed, rainfall, intensity of commercial and daily activities, etc. Therefore, considering the influencing factors as dependent variables can increase the accuracy of the forecast. In this paper, a model for electricity price forecasting is presented based on Gated Recurrent Units. The electrical load consumption is considered as an input variable in this model. Noise in electricity price seriously reduces the efficiency and effectiveness of analysis. Therefore, an adaptive noise reducer is integrated into the model for noise reduction. The SAEs are then used to extract features from the de-noised electricity price. Finally, the de-noised features are fed into the GRU to train predictor. Results on real dataset shows that the proposed methodology can perform effectively in prediction of electricity price.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1517690727,
        "newsscientist":0.156935779,
        "technologyreview":0.222708639,
        "venturebeat":0.2181571825,
        "wired":0.1581629762,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14225v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1659026943000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11742v1",
        "predicted_newsworthiness":0.4548132835,
        "title":"From Multi-label Learning to Cross-Domain Transfer: A Model-Agnostic Approach",
        "summary":"In multi-label learning, a particular case of multi-task learning where a single data point is associated with multiple target labels, it was widely assumed in the literature that, to obtain best accuracy, the dependence among the labels should be explicitly modeled. This premise led to a proliferation of methods offering techniques to learn and predict labels together, for example where the prediction for one label influences predictions for other labels. Even though it is now acknowledged that in many contexts a model of dependence is not required for optimal performance, such models continue to outperform independent models in some of those very contexts, suggesting alternative explanations for their performance beyond label dependence, which the literature is only recently beginning to unravel. Leveraging and extending recent discoveries, we turn the original premise of multi-label learning on its head, and approach the problem of joint-modeling specifically under the absence of any measurable dependence among task labels; for example, when task labels come from separate problem domains. We shift insights from this study towards building an approach for transfer learning that challenges the long-held assumption that transferability of tasks comes from measurements of similarity between the source and target domains or models. This allows us to design and test a method for transfer learning, which is model driven rather than purely data driven, and furthermore it is black box and model-agnostic (any base model class can be considered). We show that essentially we can create task-dependence based on source-model capacity. The results we obtain have important implications and provide clear directions for future work, both in the areas of multi-label and transfer learning.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.085497873,
        "newsscientist":0.1200079286,
        "technologyreview":0.2103912429,
        "venturebeat":0.1985106454,
        "wired":0.1331132908,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11742v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658669845000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13460v2",
        "predicted_newsworthiness":0.4547883946,
        "title":"Adaptive sampling for scanning pixel cameras",
        "summary":"A scanning pixel camera is a novel low-cost, low-power sensor that is not diffraction limited. It produces data as a sequence of samples extracted from various parts of the scene during the course of a scan. It can provide very detailed images at the expense of samplerates and slow image acquisition time. This paper proposes a new algorithm which allows the sensor to adapt the samplerate over the course of this sequence. This makes it possible to overcome some of these limitations by minimising the bandwidth and time required to image and transmit a scene, while maintaining image quality. We examine applications to image classification and semantic segmentation and are able to achieve similar results compared to a fully sampled input, while using 80% fewer samples",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0852126688,
        "newsscientist":0.1413028023,
        "technologyreview":0.1982890473,
        "venturebeat":0.1897258385,
        "wired":0.1650917775,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13460v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658920907000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13916v1",
        "predicted_newsworthiness":0.4537855485,
        "title":"A Novel Data Augmentation Technique for Out-of-Distribution Sample Detection using Compounded Corruptions",
        "summary":"Modern deep neural network models are known to erroneously classify out-of-distribution (OOD) test data into one of the in-distribution (ID) training classes with high confidence. This can have disastrous consequences for safety-critical applications. A popular mitigation strategy is to train a separate classifier that can detect such OOD samples at the test time. In most practical settings OOD examples are not known at the train time, and hence a key question is: how to augment the ID data with synthetic OOD samples for training such an OOD detector? In this paper, we propose a novel Compounded Corruption technique for the OOD data augmentation termed CnC. One of the major advantages of CnC is that it does not require any hold-out data apart from the training set. Further, unlike current state-of-the-art (SOTA) techniques, CnC does not require backpropagation or ensembling at the test time, making our method much faster at inference. Our extensive comparison with 20 methods from the major conferences in last 4 years show that a model trained using CnC based data augmentation, significantly outperforms SOTA, both in terms of OOD detection accuracy as well as inference time. We include a detailed post-hoc analysis to investigate the reasons for the success of our method and identify higher relative entropy and diversity of CnC samples as probable causes. We also provide theoretical insights via a piece-wise decomposition analysis on a two-dimensional dataset to reveal (visually and quantitatively) that our approach leads to a tighter boundary around ID classes, leading to better detection of OOD samples. Source code link: https:\/\/github.com\/cnc-ood",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1053619457,
        "newsscientist":0.1625058796,
        "technologyreview":0.2626749686,
        "venturebeat":0.2494224822,
        "wired":0.1912788121,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13916v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1658992631000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11793v1",
        "predicted_newsworthiness":0.4535600593,
        "title":"Reconstructing degree distribution and triangle counts from edge-sampled graphs",
        "summary":"Often, due to prohibitively large size or to limits to data collecting APIs, it is not possible to work with a complete network dataset and sampling is required. A type of sampling which is consistent with Twitter API restrictions is uniform edge sampling. In this paper, we propose a methodology for the recovery of two fundamental network properties from an edge-sampled network: the degree distribution and the triangle count (we estimate the totals for the network and the counts associated with each edge). We use a Bayesian approach and show a range of methods for constructing a prior which does not require assumptions about the original network. Our approach is tested on two synthetic and two real datasets with diverse degree and triangle count distributions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1530907615,
        "newsscientist":0.1600540277,
        "technologyreview":0.2299167627,
        "venturebeat":0.217567361,
        "wired":0.2081068488,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11793v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1658689336000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.14801v1",
        "predicted_newsworthiness":0.4535155236,
        "title":"Recognition of Handwritten Chinese Text by Segmentation: A Segment-annotation-free Approach",
        "summary":"Online and offline handwritten Chinese text recognition (HTCR) has been studied for decades. Early methods adopted oversegmentation-based strategies but suffered from low speed, insufficient accuracy, and high cost of character segmentation annotations. Recently, segmentation-free methods based on connectionist temporal classification (CTC) and attention mechanism, have dominated the field of HCTR. However, people actually read text character by character, especially for ideograms such as Chinese. This raises the question: are segmentation-free strategies really the best solution to HCTR? To explore this issue, we propose a new segmentation-based method for recognizing handwritten Chinese text that is implemented using a simple yet efficient fully convolutional network. A novel weakly supervised learning method is proposed to enable the network to be trained using only transcript annotations; thus, the expensive character segmentation annotations required by previous segmentation-based methods can be avoided. Owing to the lack of context modeling in fully convolutional networks, we propose a contextual regularization method to integrate contextual information into the network during the training stage, which can further improve the recognition performance. Extensive experiments conducted on four widely used benchmarks, namely CASIA-HWDB, CASIA-OLHWDB, ICDAR2013, and SCUT-HCCDoc, show that our method significantly surpasses existing methods on both online and offline HCTR, and exhibits a considerably higher inference speed than CTC\/attention-based approaches.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0977423658,
        "newsscientist":0.1291466208,
        "technologyreview":0.2163482221,
        "venturebeat":0.2000978333,
        "wired":0.1474604381,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14801v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659115843000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12317v1",
        "predicted_newsworthiness":0.4533874789,
        "title":"ALTO: A Large-Scale Dataset for UAV Visual Place Recognition and Localization",
        "summary":"We present the ALTO dataset, a vision-focused dataset for the development and benchmarking of Visual Place Recognition and Localization methods for Unmanned Aerial Vehicles. The dataset is composed of two long (approximately 150km and 260km) trajectories flown by a helicopter over Ohio and Pennsylvania, and it includes high precision GPS-INS ground truth location data, high precision accelerometer readings, laser altimeter readings, and RGB downward facing camera imagery. In addition, we provide reference imagery over the flight paths, which makes this dataset suitable for VPR benchmarking and other tasks common in Localization, such as image registration and visual odometry. To the author's knowledge, this is the largest real-world aerial-vehicle dataset of this kind. Our dataset is available at https:\/\/github.com\/MetaSLAM\/ALTO.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1062810925,
        "newsscientist":0.1707119688,
        "technologyreview":0.2466956831,
        "venturebeat":0.2477638356,
        "wired":0.2465657281,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12317v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ro"
        ],
        "published":1658265224000,
        "published_hr":"Jul 19, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13891v1",
        "predicted_newsworthiness":0.4533628005,
        "title":"Quantifying Safety of Learning-based Self-Driving Control Using Almost-Barrier Functions",
        "summary":"Path-tracking control of self-driving vehicles can benefit from deep learning for tackling longstanding challenges such as nonlinearity and uncertainty. However, deep neural controllers lack safety guarantees, restricting their practical use. We propose a new approach of learning almost-barrier functions, which approximately characterizes the forward invariant set for the system under neural controllers, to quantitatively analyze the safety of deep neural controllers for path-tracking. We design sampling-based learning procedures for constructing candidate neural barrier functions, and certification procedures that utilize robustness analysis for neural networks to identify regions where the barrier conditions are fully satisfied. We use an adversarial training loop between learning and certification to optimize the almost-barrier functions. The learned barrier can also be used to construct online safety monitors through reachability analysis. We demonstrate effectiveness of our methods in quantifying safety of neural controllers in various simulation environments, ranging from simple kinematic models to the TORCS simulator with high-fidelity vehicle dynamics simulation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1208462163,
        "newsscientist":0.1761114571,
        "technologyreview":0.3137227424,
        "venturebeat":0.2756753335,
        "wired":0.2335225157,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13891v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658987626000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.12104v1",
        "predicted_newsworthiness":0.4532548004,
        "title":"W2N:Switching From Weak Supervision to Noisy Supervision for Object Detection",
        "summary":"Weakly-supervised object detection (WSOD) aims to train an object detector only requiring the image-level annotations. Recently, some works have managed to select the accurate boxes generated from a well-trained WSOD network to supervise a semi-supervised detection framework for better performance. However, these approaches simply divide the training set into labeled and unlabeled sets according to the image-level criteria, such that sufficient mislabeled or wrongly localized box predictions are chosen as pseudo ground-truths, resulting in a sub-optimal solution of detection performance. To overcome this issue, we propose a novel WSOD framework with a new paradigm that switches from weak supervision to noisy supervision (W2N). Generally, with given pseudo ground-truths generated from the well-trained WSOD network, we propose a two-module iterative training algorithm to refine pseudo labels and supervise better object detector progressively. In the localization adaptation module, we propose a regularization loss to reduce the proportion of discriminative parts in original pseudo ground-truths, obtaining better pseudo ground-truths for further training. In the semi-supervised module, we propose a two tasks instance-level split method to select high-quality labels for training a semi-supervised detector. Experimental results on different benchmarks verify the effectiveness of W2N, and our W2N outperforms all existing pure WSOD methods and transfer learning methods. Our code is publicly available at https:\/\/github.com\/1170300714\/w2n_wsod.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0773014539,
        "newsscientist":0.1274870743,
        "technologyreview":0.1999437835,
        "venturebeat":0.1633878092,
        "wired":0.1320975532,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12104v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658751228000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11469v1",
        "predicted_newsworthiness":0.4532069297,
        "title":"Progressive Scene Text Erasing with Self-Supervision",
        "summary":"Scene text erasing seeks to erase text contents from scene images and current state-of-the-art text erasing models are trained on large-scale synthetic data. Although data synthetic engines can provide vast amounts of annotated training samples, there are differences between synthetic and real-world data. In this paper, we employ self-supervision for feature representation on unlabeled real-world scene text images. A novel pretext task is designed to keep consistent among text stroke masks of image variants. We design the Progressive Erasing Network in order to remove residual texts. The scene text is erased progressively by leveraging the intermediate generated results which provide the foundation for subsequent higher quality results. Experiments show that our method significantly improves the generalization of the text erasing task and achieves state-of-the-art performance on public benchmarks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1084140027,
        "newsscientist":0.138868621,
        "technologyreview":0.2261669225,
        "venturebeat":0.2069914201,
        "wired":0.1693273527,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11469v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658567113000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12185v1",
        "predicted_newsworthiness":0.4530144956,
        "title":"Post-processing Networks: Method for Optimizing Pipeline Task-oriented Dialogue Systems using Reinforcement Learning",
        "summary":"Many studies have proposed methods for optimizing the dialogue performance of an entire pipeline task-oriented dialogue system by jointly training modules in the system using reinforcement learning. However, these methods are limited in that they can only be applied to modules implemented using trainable neural-based methods. To solve this problem, we propose a method for optimizing a pipeline system composed of modules implemented with arbitrary methods for dialogue performance. With our method, neural-based components called post-processing networks (PPNs) are installed inside such a system to post-process the output of each module. All PPNs are updated to improve the overall dialogue performance of the system by using reinforcement learning, not necessitating each module to be differentiable. Through dialogue simulation and human evaluation on the MultiWOZ dataset, we show that our method can improve the dialogue performance of pipeline systems consisting of various modules.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1034368883,
        "newsscientist":0.1335542784,
        "technologyreview":0.2678646816,
        "venturebeat":0.2612321997,
        "wired":0.1831586013,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12185v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1658755360000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11541v1",
        "predicted_newsworthiness":0.4529094244,
        "title":"FastATDC: Fast Anomalous Trajectory Detection and Classification",
        "summary":"Automated detection of anomalous trajectories is an important problem with considerable applications in intelligent transportation systems. Many existing studies have focused on distinguishing anomalous trajectories from normal trajectories, ignoring the large differences between anomalous trajectories. A recent study has made great progress in identifying abnormal trajectory patterns and proposed a two-stage algorithm for anomalous trajectory detection and classification (ATDC). This algorithm has excellent performance but suffers from a few limitations, such as high time complexity and poor interpretation. Here, we present a careful theoretical and empirical analysis of the ATDC algorithm, showing that the calculation of anomaly scores in both stages can be simplified, and that the second stage of the algorithm is much more important than the first stage. Hence, we develop a FastATDC algorithm that introduces a random sampling strategy in both stages. Experimental results show that FastATDC is 10 to 20 times faster than ATDC on real datasets. Moreover, FastATDC outperforms the baseline algorithms and is comparable to the ATDC algorithm.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1192115654,
        "newsscientist":0.166148817,
        "technologyreview":0.2262172222,
        "venturebeat":0.2187445002,
        "wired":0.1927733449,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11541v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658590353000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14444v1",
        "predicted_newsworthiness":0.4528524673,
        "title":"Code Comment Inconsistency Detection with BERT and Longformer",
        "summary":"Comments, or natural language descriptions of source code, are standard practice among software developers. By communicating important aspects of the code such as functionality and usage, comments help with software project maintenance. However, when the code is modified without an accompanying correction to the comment, an inconsistency between the comment and code can arise, which opens up the possibility for developer confusion and bugs. In this paper, we propose two models based on BERT (Devlin et al., 2019) and Longformer (Beltagy et al., 2020) to detect such inconsistencies in a natural language inference (NLI) context. Through an evaluation on a previously established corpus of comment-method pairs both during and after code changes, we demonstrate that our models outperform multiple baselines and yield comparable results to the state-of-the-art models that exclude linguistic and lexical features. We further discuss ideas for future research in using pretrained language models for both inconsistency detection and automatic comment updating.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.097422443,
        "newsscientist":0.1198752876,
        "technologyreview":0.1816020994,
        "venturebeat":0.1912064394,
        "wired":0.1432387243,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14444v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659062631000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.01814v1",
        "predicted_newsworthiness":0.4527218743,
        "title":"Benchmarking zero-shot and few-shot approaches for tokenization, tagging, and dependency parsing of Tagalog text",
        "summary":"The grammatical analysis of texts in any human language typically involves a number of basic processing tasks, such as tokenization, morphological tagging, and dependency parsing. State-of-the-art systems can achieve high accuracy on these tasks for languages with large datasets, but yield poor results for languages such as Tagalog which have little to no annotated data. To address this issue for the Tagalog language, we investigate the use of auxiliary data sources for creating task-specific models in the absence of annotated Tagalog data. We also explore the use of word embeddings and data augmentation to improve performance when only a small amount of annotated Tagalog data is available. We show that these zero-shot and few-shot approaches yield substantial improvements on grammatical analysis of both in-domain and out-of-domain Tagalog text compared to state-of-the-art supervised baselines.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1077412912,
        "newsscientist":0.1092644919,
        "technologyreview":0.1844513166,
        "venturebeat":0.188325666,
        "wired":0.1435056109,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01814v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659493210000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.12980v1",
        "predicted_newsworthiness":0.4525618333,
        "title":"Efficient One Pass Self-distillation with Zipf's Label Smoothing",
        "summary":"Self-distillation exploits non-uniform soft supervision from itself during training and improves performance without any runtime cost. However, the overhead during training is often overlooked, and yet reducing time and memory overhead during training is increasingly important in the giant models' era. This paper proposes an efficient self-distillation method named Zipf's Label Smoothing (Zipf's LS), which uses the on-the-fly prediction of a network to generate soft supervision that conforms to Zipf distribution without using any contrastive samples or auxiliary parameters. Our idea comes from an empirical observation that when the network is duly trained the output values of a network's final softmax layer, after sorting by the magnitude and averaged across samples, should follow a distribution reminiscent to Zipf's Law in the word frequency statistics of natural languages. By enforcing this property on the sample level and throughout the whole training period, we find that the prediction accuracy can be greatly improved. Using ResNet50 on the INAT21 fine-grained classification dataset, our technique achieves +3.61% accuracy gain compared to the vanilla baseline, and 0.88% more gain against the previous label smoothing or self-distillation strategies. The implementation is publicly available at https:\/\/github.com\/megvii-research\/zipfls.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0817462166,
        "newsscientist":0.1160133667,
        "technologyreview":0.227430386,
        "venturebeat":0.2181535991,
        "wired":0.1433748403,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12980v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658850016000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01871v1",
        "predicted_newsworthiness":0.4524228119,
        "title":"A Deep Learning Approach to Detect Lean Blowout in Combustion Systems",
        "summary":"Lean combustion is environment friendly with low NOx emissions and also provides better fuel efficiency in a combustion system. However, approaching towards lean combustion can make engines more susceptible to lean blowout. Lean blowout (LBO) is an undesirable phenomenon that can cause sudden flame extinction leading to sudden loss of power. During the design stage, it is quite challenging for the scientists to accurately determine the optimal operating limits to avoid sudden LBO occurrence. Therefore, it is crucial to develop accurate and computationally tractable frameworks for online LBO detection in low NOx emission engines. To the best of our knowledge, for the first time, we propose a deep learning approach to detect lean blowout in combustion systems. In this work, we utilize a laboratory-scale combustor to collect data for different protocols. We start far from LBO for each protocol and gradually move towards the LBO regime, capturing a quasi-static time series dataset at each condition. Using one of the protocols in our dataset as the reference protocol and with conditions annotated by domain experts, we find a transition state metric for our trained deep learning model to detect LBO in the other test protocols. We find that our proposed approach is more accurate and computationally faster than other baseline models to detect the transitions to LBO. Therefore, we recommend this method for real-time performance monitoring in lean combustion engines.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1196668315,
        "newsscientist":0.1695969473,
        "technologyreview":0.2401609301,
        "venturebeat":0.2345059817,
        "wired":0.1947280902,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01871v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659509310000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00404v1",
        "predicted_newsworthiness":0.4523969005,
        "title":"Intelligent decision-making method of TBM operating parameters based on multiple constraints and objective optimization",
        "summary":"The decision-making of TBM operating parameters has an important guiding significance for TBM safe and efficient construction, and it has been one of the research hotpots in the field of TBM tunneling. For this purpose, this paper introduces rock-breaking rules into machine learning method, and a rock-machine mapping dual-driven by physical-rule and data-mining is established with high accuracy. This dual-driven mappings are subsequently used as objective function and constraints to build a decision-making method for TBM operating parameters. By searching the revolution per minute and penetration corresponding to the extremum of the objective function subject to the constraints, the optimal operating parameters can be obtained. This method is verified in the field of the Second Water Source Channel of Hangzhou, China, resulting in the average penetration rate increased by 11.3%, and the total cost decreased by 10.0%, which proves the practicability and effectiveness of the developed decision-making model.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1129794988,
        "newsscientist":0.1408225201,
        "technologyreview":0.1773153086,
        "venturebeat":0.1429152629,
        "wired":0.1406504437,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00404v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659259410000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01779v1",
        "predicted_newsworthiness":0.4522323764,
        "title":"Mates2Motion: Learning How Mechanical CAD Assemblies Work",
        "summary":"We describe our work on inferring the degrees of freedom between mated parts in mechanical assemblies using deep learning on CAD representations. We train our model using a large dataset of real-world mechanical assemblies consisting of CAD parts and mates joining them together. We present methods for re-defining these mates to make them better reflect the motion of the assembly, as well as narrowing down the possible axes of motion. We also conduct a user study to create a motion-annotated test set with more reliable labels.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1020132519,
        "newsscientist":0.1822712441,
        "technologyreview":0.2719524821,
        "venturebeat":0.2362406227,
        "wired":0.2114702982,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01779v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659481957000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01607v1",
        "predicted_newsworthiness":0.4520697995,
        "title":"Enabling scalable clinical interpretation of ML-based phenotypes using real world data",
        "summary":"The availability of large and deep electronic healthcare records (EHR) datasets has the potential to enable a better understanding of real-world patient journeys, and to identify novel subgroups of patients. ML-based aggregation of EHR data is mostly tool-driven, i.e., building on available or newly developed methods. However, these methods, their input requirements, and, importantly, resulting output are frequently difficult to interpret, especially without in-depth data science or statistical training. This endangers the final step of analysis where an actionable and clinically meaningful interpretation is needed.This study investigates approaches to perform patient stratification analysis at scale using large EHR datasets and multiple clustering methods for clinical research. We have developed several tools to facilitate the clinical evaluation and interpretation of unsupervised patient stratification results, namely pattern screening, meta clustering, surrogate modeling, and curation. These tools can be used at different stages within the analysis. As compared to a standard analysis approach, we demonstrate the ability to condense results and optimize analysis time. In the case of meta clustering, we demonstrate that the number of patient clusters can be reduced from 72 to 3 in one example. In another stratification result, by using surrogate models, we could quickly identify that heart failure patients were stratified if blood sodium measurements were available. As this is a routine measurement performed for all patients with heart failure, this indicated a data bias. By using further cohort and feature curation, these patients and other irrelevant features could be removed to increase the clinical meaningfulness. These examples show the effectiveness of the proposed methods and we hope to encourage further research in this field.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1791606979,
        "newsscientist":0.1933381158,
        "technologyreview":0.2559779519,
        "venturebeat":0.2391082497,
        "wired":0.1601043544,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01607v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ir"
        ],
        "published":1659461463000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13297v1",
        "predicted_newsworthiness":0.4519133788,
        "title":"GPS-GLASS: Learning Nighttime Semantic Segmentation Using Daytime Video and GPS data",
        "summary":"Semantic segmentation for autonomous driving should be robust against various in-the-wild environments. Nighttime semantic segmentation is especially challenging due to a lack of annotated nighttime images and a large domain gap from daytime images with sufficient annotation. In this paper, we propose a novel GPS-based training framework for nighttime semantic segmentation. Given GPS-aligned pairs of daytime and nighttime images, we perform cross-domain correspondence matching to obtain pixel-level pseudo supervision. Moreover, we conduct flow estimation between daytime video frames and apply GPS-based scaling to acquire another pixel-level pseudo supervision. Using these pseudo supervisions with a confidence map, we train a nighttime semantic segmentation network without any annotation from nighttime images. Experimental results demonstrate the effectiveness of the proposed method on several nighttime semantic segmentation datasets. Our source code is available at https:\/\/github.com\/jimmy9704\/GPS-GLASS.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0898590406,
        "newsscientist":0.1395385869,
        "technologyreview":0.1993177927,
        "venturebeat":0.1780803555,
        "wired":0.1795062696,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13297v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658898304000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11805v1",
        "predicted_newsworthiness":0.4516534088,
        "title":"Weakly-Supervised Temporal Action Detection for Fine-Grained Videos with Hierarchical Atomic Actions",
        "summary":"Action understanding has evolved into the era of fine granularity, as most human behaviors in real life have only minor differences. To detect these fine-grained actions accurately in a label-efficient way, we tackle the problem of weakly-supervised fine-grained temporal action detection in videos for the first time. Without the careful design to capture subtle differences between fine-grained actions, previous weakly-supervised models for general action detection cannot perform well in the fine-grained setting. We propose to model actions as the combinations of reusable atomic actions which are automatically discovered from data through self-supervised clustering, in order to capture the commonality and individuality of fine-grained actions. The learnt atomic actions, represented by visual concepts, are further mapped to fine and coarse action labels leveraging the semantic label hierarchy. Our approach constructs a visual representation hierarchy of four levels: clip level, atomic action level, fine action class level and coarse action class level, with supervision at each level. Extensive experiments on two large-scale fine-grained video datasets, FineAction and FineGym, show the benefit of our proposed weakly-supervised model for fine-grained action detection, and it achieves state-of-the-art results.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0812100566,
        "newsscientist":0.1181032374,
        "technologyreview":0.1593163483,
        "venturebeat":0.1331883857,
        "wired":0.1283661094,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11805v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658694744000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11694v1",
        "predicted_newsworthiness":0.4515413574,
        "title":"Proving Common Mechanisms Shared by Twelve Methods of Boosting Adversarial Transferability",
        "summary":"Although many methods have been proposed to enhance the transferability of adversarial perturbations, these methods are designed in a heuristic manner, and the essential mechanism for improving adversarial transferability is still unclear. This paper summarizes the common mechanism shared by twelve previous transferability-boosting methods in a unified view, i.e., these methods all reduce game-theoretic interactions between regional adversarial perturbations. To this end, we focus on the attacking utility of all interactions between regional adversarial perturbations, and we first discover and prove the negative correlation between the adversarial transferability and the attacking utility of interactions. Based on this discovery, we theoretically prove and empirically verify that twelve previous transferability-boosting methods all reduce interactions between regional adversarial perturbations. More crucially, we consider the reduction of interactions as the essential reason for the enhancement of adversarial transferability. Furthermore, we design the interaction loss to directly penalize interactions between regional adversarial perturbations during attacking. Experimental results show that the interaction loss significantly improves the transferability of adversarial perturbations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1350351585,
        "newsscientist":0.1593247743,
        "technologyreview":0.2578468239,
        "venturebeat":0.2066017453,
        "wired":0.2004407726,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11694v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cv"
        ],
        "published":1658651772000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12252v1",
        "predicted_newsworthiness":0.4515095466,
        "title":"Accessing and Interpreting OPC UA Event Traces based on Semantic Process Descriptions",
        "summary":"The analysis of event data from production systems is the basis for many applications associated with Industry 4.0. However, heterogeneous and disjoint data is common in this domain. As a consequence, contextual information of an event might be incomplete or improperly interpreted which results in suboptimal analysis results. This paper proposes an approach to access a production systems' event data based on the event data's context (such as the product type, process type or process parameters). The approach extracts filtered event logs from a database system by combining: 1) a semantic model of a production system's hierarchical structure, 2) a formalized process description and 3) an OPC UA information model. As a proof of concept we demonstrate our approach using a sample server based on OPC UA for Machinery Companion Specifications.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0992743149,
        "newsscientist":0.0992755718,
        "technologyreview":0.1656959274,
        "venturebeat":0.2051855583,
        "wired":0.121444789,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12252v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1658762024000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2208.02210v1",
        "predicted_newsworthiness":0.4510874532,
        "title":"Free-HeadGAN: Neural Talking Head Synthesis with Explicit Gaze Control",
        "summary":"We present Free-HeadGAN, a person-generic neural talking head synthesis system. We show that modeling faces with sparse 3D facial landmarks are sufficient for achieving state-of-the-art generative performance, without relying on strong statistical priors of the face, such as 3D Morphable Models. Apart from 3D pose and facial expressions, our method is capable of fully transferring the eye gaze, from a driving actor to a source identity. Our complete pipeline consists of three components: a canonical 3D key-point estimator that regresses 3D pose and expression-related deformations, a gaze estimation network and a generator that is built upon the architecture of HeadGAN. We further experiment with an extension of our generator to accommodate few-shot learning using an attention mechanism, in case more than one source images are available. Compared to the latest models for reenactment and motion transfer, our system achieves higher photo-realism combined with superior identity preservation, while offering explicit gaze control.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0940956958,
        "newsscientist":0.1392840751,
        "technologyreview":0.2246716954,
        "venturebeat":0.2324880499,
        "wired":0.1977174062,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02210v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659545168000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01066v1",
        "predicted_newsworthiness":0.45097057,
        "title":"What Can Transformers Learn In-Context? A Case Study of Simple Function Classes",
        "summary":"In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn \"most\" functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes -- namely sparse linear functions, two-layer neural networks, and decision trees -- with performance that matches or exceeds task-specific learning algorithms. Our code and models are available at https:\/\/github.com\/dtsip\/in-context-learning .",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0823906264,
        "newsscientist":0.1388721236,
        "technologyreview":0.2727896542,
        "venturebeat":0.272044016,
        "wired":0.1784846446,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01066v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1659376900000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.12955v1",
        "predicted_newsworthiness":0.4508740271,
        "title":"Contextual Text Block Detection towards Scene Text Understanding",
        "summary":"Most existing scene text detectors focus on detecting characters or words that only capture partial text messages due to missing contextual information. For a better understanding of text in scenes, it is more desired to detect contextual text blocks (CTBs) which consist of one or multiple integral text units (e.g., characters, words, or phrases) in natural reading order and transmit certain complete text messages. This paper presents contextual text detection, a new setup that detects CTBs for better understanding of texts in scenes. We formulate the new setup by a dual detection task which first detects integral text units and then groups them into a CTB. To this end, we design a novel scene text clustering technique that treats integral text units as tokens and groups them (belonging to the same CTB) into an ordered token sequence. In addition, we create two datasets SCUT-CTW-Context and ReCTS-Context to facilitate future research, where each CTB is well annotated by an ordered sequence of integral text units. Further, we introduce three metrics that measure contextual text detection in local accuracy, continuity, and global accuracy. Extensive experiments show that our method accurately detects CTBs which effectively facilitates downstream tasks such as text classification and translation. The project is available at https:\/\/sg-vilab.github.io\/publication\/xue2022contextual\/.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.108981075,
        "newsscientist":0.1293774465,
        "technologyreview":0.1978891316,
        "venturebeat":0.2083535368,
        "wired":0.1678863872,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12955v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658847565000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00351v1",
        "predicted_newsworthiness":0.4506715676,
        "title":"Chinese grammatical error correction based on knowledge distillation",
        "summary":"In view of the poor robustness of existing Chinese grammatical error correction models on attack test sets and large model parameters, this paper uses the method of knowledge distillation to compress model parameters and improve the anti-attack ability of the model. In terms of data, the attack test set is constructed by integrating the disturbance into the standard evaluation data set, and the model robustness is evaluated by the attack test set. The experimental results show that the distilled small model can ensure the performance and improve the training speed under the condition of reducing the number of model parameters, and achieve the optimal effect on the attack test set, and the robustness is significantly improved.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1014132273,
        "newsscientist":0.1244159312,
        "technologyreview":0.202497268,
        "venturebeat":0.1718413684,
        "wired":0.1505635612,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00351v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1659237389000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00156v1",
        "predicted_newsworthiness":0.450671057,
        "title":"Reinforcement learning with experience replay and adaptation of action dispersion",
        "summary":"Effective reinforcement learning requires a proper balance of exploration and exploitation defined by the dispersion of action distribution. However, this balance depends on the task, the current stage of the learning process, and the current environment state. Existing methods that designate the action distribution dispersion require problem-dependent hyperparameters. In this paper, we propose to automatically designate the action distribution dispersion using the following principle: This distribution should have sufficient dispersion to enable the evaluation of future policies. To that end, the dispersion should be tuned to assure a sufficiently high probability (densities) of the actions in the replay buffer and the modes of the distributions that generated them, yet this dispersion should not be higher. This way, a policy can be effectively evaluated based on the actions in the buffer, but exploratory randomness in actions decreases when this policy converges. The above principle is verified here on challenging benchmarks Ant, HalfCheetah, Hopper, and Walker2D, with good results. Our method makes the action standard deviations converge to values similar to those resulting from trial-and-error optimization.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0841233325,
        "newsscientist":0.1313678938,
        "technologyreview":0.2141973457,
        "venturebeat":0.1972144236,
        "wired":0.1481797962,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00156v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659167045000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01743v1",
        "predicted_newsworthiness":0.4505895853,
        "title":"Sniffer deployment in urban area for human trajectory reconstruction and contact tracing",
        "summary":"To study the propagation of information from individual to individual, we need mobility datasets. Existing datasets are not satisfactory because they are too small, inaccurate or target a homogeneous subset of population. To draw valid conclusions, we need sufficiently large and heterogeneous datasets. Thus we aim for a passive non-intrusive data collection method, based on sniffers that are to be deployed at some well-chosen street intersections. To this end, we need optimization techniques for efficient placement of sniffers. We introduce a heuristic, based on graph theory notions like the vertex cover problem along with graph centrality measures.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1514087658,
        "newsscientist":0.1719927785,
        "technologyreview":0.2303671537,
        "venturebeat":0.198122376,
        "wired":0.1921128479,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01743v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1659103581000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.11652v1",
        "predicted_newsworthiness":0.4503403221,
        "title":"Counterfactual Reasoning for Out-of-distribution Multimodal Sentiment Analysis",
        "summary":"Existing studies on multimodal sentiment analysis heavily rely on textual modality and unavoidably induce the spurious correlations between textual words and sentiment labels. This greatly hinders the model generalization ability. To address this problem, we define the task of out-of-distribution (OOD) multimodal sentiment analysis. This task aims to estimate and mitigate the bad effect of textual modality for strong OOD generalization. To this end, we embrace causal inference, which inspects the causal relationships via a causal graph. From the graph, we find that the spurious correlations are attributed to the direct effect of textual modality on the model prediction while the indirect one is more reliable by considering multimodal semantics. Inspired by this, we devise a model-agnostic counterfactual framework for multimodal sentiment analysis, which captures the direct effect of textual modality via an extra text model and estimates the indirect one by a multimodal model. During the inference, we first estimate the direct effect by the counterfactual inference, and then subtract it from the total effect of all modalities to obtain the indirect effect for reliable prediction. Extensive experiments show the superior effectiveness and generalization ability of our proposed framework.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1506059644,
        "newsscientist":0.1501679671,
        "technologyreview":0.2311030998,
        "venturebeat":0.205554394,
        "wired":0.2021141873,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11652v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1658635060000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.01036v1",
        "predicted_newsworthiness":0.4502451063,
        "title":"Face-to-Face Contrastive Learning for Social Intelligence Question-Answering",
        "summary":"Creating artificial social intelligence - algorithms that can understand the nuances of multi-person interactions - is an exciting and emerging challenge in processing facial expressions and gestures from multimodal videos. Recent multimodal methods have set the state of the art on many tasks, but have difficulty modeling the complex face-to-face conversational dynamics across speaking turns in social interaction, particularly in a self-supervised setup. In this paper, we propose Face-to-Face Contrastive Learning (F2F-CL), a graph neural network designed to model social interactions using factorization nodes to contextualize the multimodal face-to-face interaction along the boundaries of the speaking turn. With the F2F-CL model, we propose to perform contrastive learning between the factorization nodes of different speaking turns within the same video. We experimentally evaluated the challenging Social-IQ dataset and show state-of-the-art results.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1360716031,
        "newsscientist":0.1733589661,
        "technologyreview":0.2724606726,
        "venturebeat":0.2646154855,
        "wired":0.2200022515,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01036v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cv"
        ],
        "published":1659127184000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11523v1",
        "predicted_newsworthiness":0.4500699898,
        "title":"Unstructured Road Segmentation using Hypercolumn based Random Forests of Local experts",
        "summary":"Monocular vision based road detection methods are mostly based on machine learning methods, relying on classification and feature extraction accuracy, and suffer from appearance, illumination and weather changes. Traditional methods introduce the predictions into conditional random fields or markov random fields models to improve the intermediate predictions based on structure. These methods are optimization based and therefore resource heavy and slow, making it unsuitable for real time applications. We propose a method to detect and segment roads with a random forest classifier of local experts with superpixel based machine-learned features. The random forest takes in machine learnt descriptors from a pre-trained convolutional neural network - VGG-16. The features are also pooled into their respective superpixels, allowing for local structure to be continuous. We compare our algorithm against Nueral Network based methods and Traditional approaches (based on Hand-crafted features), on both Structured Road (CamVid and Kitti) and Unstructured Road Datasets. Finally, we introduce a Road Scene Dataset with 1000 annotated images, and verify that our algorithm works well in non-urban and rural road scenarios.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0941139325,
        "newsscientist":0.142669493,
        "technologyreview":0.2442818272,
        "venturebeat":0.2082494901,
        "wired":0.1786164805,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11523v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658584597000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12757v1",
        "predicted_newsworthiness":0.4500441135,
        "title":"Controllable User Dialogue Act Augmentation for Dialogue State Tracking",
        "summary":"Prior work has demonstrated that data augmentation is useful for improving dialogue state tracking. However, there are many types of user utterances, while the prior method only considered the simplest one for augmentation, raising the concern about poor generalization capability. In order to better cover diverse dialogue acts and control the generation quality, this paper proposes controllable user dialogue act augmentation (CUDA-DST) to augment user utterances with diverse behaviors. With the augmented data, different state trackers gain improvement and show better robustness, achieving the state-of-the-art performance on MultiWOZ 2.1",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1116697379,
        "newsscientist":0.1366729108,
        "technologyreview":0.2438124016,
        "venturebeat":0.2737824794,
        "wired":0.2247935089,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12757v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658826288000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00808v1",
        "predicted_newsworthiness":0.4499410631,
        "title":"A Maintenance Planning Framework using Online and Offline Deep Reinforcement Learning",
        "summary":"Cost-effective asset management is an area of interest across several industries. Specifically, this paper develops a deep reinforcement learning (DRL) solution to automatically determine an optimal rehabilitation policy for continuously deteriorating water pipes. We approach the problem of rehabilitation planning in an online and offline DRL setting. In online DRL, the agent interacts with a simulated environment of multiple pipes with distinct length, material, and failure rate characteristics. We train the agent using deep Q-learning (DQN) to learn an optimal policy with minimal average costs and reduced failure probability. In offline learning, the agent uses static data, e.g., DQN replay data, to learn an optimal policy via a conservative Q-learning algorithm without further interactions with the environment. We demonstrate that DRL-based policies improve over standard preventive, corrective, and greedy planning alternatives. Additionally, learning from the fixed DQN replay dataset surpasses the online DQN setting. The results warrant that the existing deterioration profiles of water pipes consisting of large and diverse states and action trajectories provide a valuable avenue to learn rehabilitation policies in the offline setting without needing a simulator.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1437627676,
        "newsscientist":0.1450200406,
        "technologyreview":0.2395977719,
        "venturebeat":0.2211378359,
        "wired":0.1559594896,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00808v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659357666000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12566v1",
        "predicted_newsworthiness":0.4495982454,
        "title":"Eliciting Multimodal Gesture+Speech Interactions in a Multi-Object Augmented Reality Environment",
        "summary":"As augmented reality technology and hardware become more mature and affordable, researchers have been exploring more intuitive and discoverable interaction techniques for immersive environments. In this paper, we investigate multimodal interaction for 3D object manipulation in a multi-object virtual environment. To identify the user-defined gestures, we conducted an elicitation study involving 24 participants for 22 referents with an augmented reality headset. It yielded 528 proposals and generated a winning gesture set with 25 gestures after binning and ranking all gesture proposals. We found that for the same task, the same gesture was preferred for both one and two object manipulation, although both hands were used in the two object scenario. We presented the gestures and speech results, and the differences compared to similar studies in a single object virtual environment. The study also explored the association between speech expressions and gesture stroke during object manipulation, which could improve the recognizer efficiency in augmented reality headsets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1196053496,
        "newsscientist":0.1656110489,
        "technologyreview":0.2453180121,
        "venturebeat":0.3037156408,
        "wired":0.2512627526,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12566v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1658789500000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.13311v1",
        "predicted_newsworthiness":0.4495509953,
        "title":"JDRec: Practical Actor-Critic Framework for Online Combinatorial Recommender System",
        "summary":"A combinatorial recommender (CR) system feeds a list of items to a user at a time in the result page, in which the user behavior is affected by both contextual information and items. The CR is formulated as a combinatorial optimization problem with the objective of maximizing the recommendation reward of the whole list. Despite its importance, it is still a challenge to build a practical CR system, due to the efficiency, dynamics, personalization requirement in online environment. In particular, we tear the problem into two sub-problems, list generation and list evaluation. Novel and practical model architectures are designed for these sub-problems aiming at jointly optimizing effectiveness and efficiency. In order to adapt to online case, a bootstrap algorithm forming an actor-critic reinforcement framework is given to explore better recommendation mode in long-term user interaction. Offline and online experiment results demonstrate the efficacy of proposed JDRec framework. JDRec has been applied in online JD recommendation, improving click through rate by 2.6% and synthetical value for the platform by 5.03%. We will publish the large-scale dataset used in this study to contribute to the research community.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1008209243,
        "newsscientist":0.1258349426,
        "technologyreview":0.221389693,
        "venturebeat":0.249125432,
        "wired":0.201200505,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13311v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.ai",
            "cs.lg"
        ],
        "published":1658900832000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.01284v1",
        "predicted_newsworthiness":0.4495485484,
        "title":"In-Hand Pose Estimation and Pin Inspection for Insertion of Through-Hole Components",
        "summary":"The insertion of through-hole components is a difficult task. As the tolerances of the holes are very small, minor errors in the insertion will result in failures. These failures can damage components and will require manual intervention for recovery. Errors can occur both from imprecise object grasps and bent pins. Therefore, it is important that a system can accurately determine the object's position and reject components with bent pins. By utilizing the constraints inherent in the object grasp a method using template matching is able to obtain very precise pose estimates. Methods for pin-checking are also implemented, compared, and a successful method is shown. The set-up is performed automatically, with two novel contributions. A deep learning segmentation of the pins is performed and the inspection pose is found by simulation. From the inspection pose and the segmented pins, the templates for pose estimation and pin check are then generated. To train the deep learning method a dataset of segmented through-hole components is created. The network shows a 97.3 % accuracy on the test set. The pin-segmentation network is also tested on the insertion CAD models and successfully segment the pins. The complete system is tested on three different objects, and experiments show that the system is able to insert all objects successfully. Both by correcting in-hand grasp errors and rejecting objects with bent pins.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.06873424,
        "newsscientist":0.1468418394,
        "technologyreview":0.2133588022,
        "venturebeat":0.1890308945,
        "wired":0.1589918052,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01284v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659424404000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00090v1",
        "predicted_newsworthiness":0.4494869939,
        "title":"Explicit Occlusion Reasoning for Multi-person 3D Human Pose Estimation",
        "summary":"Occlusion poses a great threat to monocular multi-person 3D human pose estimation due to large variability in terms of the shape, appearance, and position of occluders. While existing methods try to handle occlusion with pose priors\/constraints, data augmentation, or implicit reasoning, they still fail to generalize to unseen poses or occlusion cases and may make large mistakes when multiple people are present. Inspired by the remarkable ability of humans to infer occluded joints from visible cues, we develop a method to explicitly model this process that significantly improves bottom-up multi-person human pose estimation with or without occlusions. First, we split the task into two subtasks: visible keypoints detection and occluded keypoints reasoning, and propose a Deeply Supervised Encoder Distillation (DSED) network to solve the second one. To train our model, we propose a Skeleton-guided human Shape Fitting (SSF) approach to generate pseudo occlusion labels on the existing datasets, enabling explicit occlusion reasoning. Experiments show that explicitly learning from occlusions improves human pose estimation. In addition, exploiting feature-level information of visible joints allows us to reason about occluded joints more accurately. Our method outperforms both the state-of-the-art top-down and bottom-up methods on several benchmarks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0717652672,
        "newsscientist":0.1122027947,
        "technologyreview":0.1509944637,
        "venturebeat":0.1426024623,
        "wired":0.1202741313,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00090v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659132770000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00319v1",
        "predicted_newsworthiness":0.4492821639,
        "title":"Robust Planning for Multi-stage Forceful Manipulation",
        "summary":"Multi-step forceful manipulation tasks, such as opening a push-and-twist childproof bottle, require a robot to make various planning choices that are substantially impacted by the requirement to exert force during the task. The robot must reason over discrete and continuous choices relating to the sequence of actions, such as whether to pick up an object, and the parameters of each of those actions, such how to grasp the object. To enable planning and executing forceful manipulation, we augment an existing task and motion planner with constraints that explicitly consider torque and frictional limits, captured through the proposed forceful kinematic chain constraint. In three domains, opening a childproof bottle, twisting a nut and cutting a vegetable, we demonstrate how the system selects from among a combinatorial set of strategies.We also show how cost-sensitive planning can be used to find strategies and parameters that are robust to uncertainty in the physical parameters.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0947065435,
        "newsscientist":0.135888438,
        "technologyreview":0.1990734246,
        "venturebeat":0.1602011368,
        "wired":0.1615692876,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00319v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659219655000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13948v1",
        "predicted_newsworthiness":0.4491680988,
        "title":"An Interpretability Evaluation Benchmark for Pre-trained Language Models",
        "summary":"While pre-trained language models (LMs) have brought great improvements in many NLP tasks, there is increasing attention to explore capabilities of LMs and interpret their predictions. However, existing works usually focus only on a certain capability with some downstream tasks. There is a lack of datasets for directly evaluating the masked word prediction performance and the interpretability of pre-trained LMs. To fill in the gap, we propose a novel evaluation benchmark providing with both English and Chinese annotated data. It tests LMs abilities in multiple dimensions, i.e., grammar, semantics, knowledge, reasoning and computation. In addition, it provides carefully annotated token-level rationales that satisfy sufficiency and compactness. It contains perturbed instances for each original instance, so as to use the rationale consistency under perturbations as the metric for faithfulness, a perspective of interpretability. We conduct experiments on several widely-used pre-trained LMs. The results show that they perform very poorly on the dimensions of knowledge and computation. And their plausibility in all dimensions is far from satisfactory, especially when the rationale is short. In addition, the pre-trained LMs we evaluated are not robust on syntax-aware data. We will release this evaluation benchmark at \\url{http:\/\/xyz}, and hope it can facilitate the research progress of pre-trained LMs.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.114799303,
        "newsscientist":0.1319567083,
        "technologyreview":0.2442545355,
        "venturebeat":0.2406022652,
        "wired":0.1676615002,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13948v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658996889000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00128v1",
        "predicted_newsworthiness":0.4485543384,
        "title":"Dynamically Retrieving Knowledge via Query Generation for informative dialogue response",
        "summary":"Knowledge-driven dialogue generation has recently made remarkable breakthroughs. Compared with general dialogue systems, superior knowledge-driven dialogue systems can generate more informative and knowledgeable responses with pre-provided knowledge. However, in practical applications, the dialogue system cannot be provided with corresponding knowledge in advance. In order to solve the problem, we design a knowledge-driven dialogue system named DRKQG (\\emph{Dynamically Retrieving Knowledge via Query Generation for informative dialogue response}). Specifically, the system can be divided into two modules: query generation module and dialogue generation module. First, a time-aware mechanism is utilized to capture context information and a query can be generated for retrieving knowledge. Then, we integrate copy Mechanism and Transformers, which allows the response generation module produces responses derived from the context and retrieved knowledge. Experimental results at LIC2022, Language and Intelligence Technology Competition, show that our module outperforms the baseline model by a large margin on automatic evaluation metrics, while human evaluation by Baidu Linguistics team shows that our system achieves impressive results in Factually Correct and Knowledgeable.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1050027318,
        "newsscientist":0.1352475343,
        "technologyreview":0.2475982805,
        "venturebeat":0.2702518995,
        "wired":0.2072071463,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00128v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659150343000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.13018v1",
        "predicted_newsworthiness":0.4481942697,
        "title":"Is Attention Interpretation? A Quantitative Assessment On Sets",
        "summary":"The debate around the interpretability of attention mechanisms is centered on whether attention scores can be used as a proxy for the relative amounts of signal carried by sub-components of data. We propose to study the interpretability of attention in the context of set machine learning, where each data point is composed of an unordered collection of instances with a global label. For classical multiple-instance-learning problems and simple extensions, there is a well-defined \"importance\" ground truth that can be leveraged to cast interpretation as a binary classification problem, which we can quantitatively evaluate. By building synthetic datasets over several data modalities, we perform a systematic assessment of attention-based interpretations. We find that attention distributions are indeed often reflective of the relative importance of individual instances, but that silent failures happen where a model will have high classification performance but attention patterns that do not align with expectations. Based on these observations, we propose to use ensembling to minimize the risk of misleading attention-based explanations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.121870895,
        "newsscientist":0.1744112802,
        "technologyreview":0.2679368766,
        "venturebeat":0.2459746367,
        "wired":0.1734765225,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13018v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658852738000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11514v1",
        "predicted_newsworthiness":0.4477747569,
        "title":"Semantic Abstraction: Open-World 3D Scene Understanding from 2D Vision-Language Models",
        "summary":"We study open-world 3D scene understanding, a family of tasks that require agents to reason about their 3D environment with an open-set vocabulary and out-of-domain visual inputs - a critical skill for robots to operate in the unstructured 3D world. Towards this end, we propose Semantic Abstraction (SemAbs), a framework that equips 2D Vision-Language Models (VLMs) with new 3D spatial capabilities, while maintaining their zero-shot robustness. We achieve this abstraction using relevancy maps extracted from CLIP, and learn 3D spatial and geometric reasoning skills on top of those abstractions in a semantic-agnostic manner. We demonstrate the usefulness of SemAbs on two open-world 3D scene understanding tasks: 1) completing partially observed objects and 2) localizing hidden objects from language descriptions. Experiments show that SemAbs can generalize to novel vocabulary, materials\/lighting, classes, and domains (i.e., real-world scans) from training on limited 3D synthetic data. Code and data will be available at https:\/\/semantic-abstraction.cs.columbia.edu\/",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0917898264,
        "newsscientist":0.1480348685,
        "technologyreview":0.225870359,
        "venturebeat":0.2138883078,
        "wired":0.1806819304,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11514v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ro"
        ],
        "published":1658581825000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00446v1",
        "predicted_newsworthiness":0.4475887549,
        "title":"Out-of-Distribution Detection with Semantic Mismatch under Masking",
        "summary":"This paper proposes a novel out-of-distribution (OOD) detection framework named MoodCat for image classifiers. MoodCat masks a random portion of the input image and uses a generative model to synthesize the masked image to a new image conditioned on the classification result. It then calculates the semantic difference between the original image and the synthesized one for OOD detection. Compared to existing solutions, MoodCat naturally learns the semantic information of the in-distribution data with the proposed mask and conditional synthesis strategy, which is critical to identifying OODs. Experimental results demonstrate that MoodCat outperforms state-of-the-art OOD detection solutions by a large margin.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0880917165,
        "newsscientist":0.1457455208,
        "technologyreview":0.2213757635,
        "venturebeat":0.2064256247,
        "wired":0.1640778504,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00446v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659279502000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14513v1",
        "predicted_newsworthiness":0.4475098444,
        "title":"Uncertainty-Driven Action Quality Assessment",
        "summary":"Automatic action quality assessment (AQA) has attracted more interests due to its wide applications. However, existing AQA methods usually employ the multi-branch models to generate multiple scores, which is not flexible for dealing with a variable number of judges. In this paper, we propose a novel Uncertainty-Driven AQA (UD-AQA) model to generate multiple predictions only using one single branch. Specifically, we design a CVAE (Conditional Variational Auto-Encoder) based module to encode the uncertainty, where multiple scores can be produced by sampling from the learned latent space multiple times. Moreover, we output the estimation of uncertainty and utilize the predicted uncertainty to re-weight AQA regression loss, which can reduce the contributions of uncertain samples for training. We further design an uncertainty-guided training strategy to dynamically adjust the learning order of the samples from low uncertainty to high uncertainty. The experiments show that our proposed method achieves new state-of-the-art results on the Olympic events MTL-AQA and surgical skill JIGSAWS datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1202926029,
        "newsscientist":0.133799771,
        "technologyreview":0.2180956011,
        "venturebeat":0.1932900233,
        "wired":0.1603726352,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14513v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659079275000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00906v1",
        "predicted_newsworthiness":0.4474383527,
        "title":"Understanding Adversarial Robustness of Vision Transformers via Cauchy Problem",
        "summary":"Recent research on the robustness of deep learning has shown that Vision Transformers (ViTs) surpass the Convolutional Neural Networks (CNNs) under some perturbations, e.g., natural corruption, adversarial attacks, etc. Some papers argue that the superior robustness of ViT comes from the segmentation of its input images; others say that the Multi-head Self-Attention (MSA) is the key to preserving the robustness. In this paper, we aim to introduce a principled and unified theoretical framework to investigate such an argument on ViT's robustness. We first theoretically prove that, unlike Transformers in Natural Language Processing, ViTs are Lipschitz continuous. Then we theoretically analyze the adversarial robustness of ViTs from the perspective of the Cauchy Problem, via which we can quantify how the robustness propagates through layers. We demonstrate that the first and last layers are the critical factors to affect the robustness of ViTs. Furthermore, based on our theory, we empirically show that unlike the claims from existing research, MSA only contributes to the adversarial robustness of ViTs under weak adversarial attacks, e.g., FGSM, and surprisingly, MSA actually comprises the model's adversarial robustness under stronger attacks, e.g., PGD attacks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1324966809,
        "newsscientist":0.1731192075,
        "technologyreview":0.3006390359,
        "venturebeat":0.2440302428,
        "wired":0.2134341961,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00906v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659365429000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14378v1",
        "predicted_newsworthiness":0.4473305199,
        "title":"Latent Properties of Lifelong Learning Systems",
        "summary":"Creating artificial intelligence (AI) systems capable of demonstrating lifelong learning is a fundamental challenge, and many approaches and metrics have been proposed to analyze algorithmic properties. However, for existing lifelong learning metrics, algorithmic contributions are confounded by task and scenario structure. To mitigate this issue, we introduce an algorithm-agnostic explainable surrogate-modeling approach to estimate latent properties of lifelong learning algorithms. We validate the approach for estimating these properties via experiments on synthetic data. To validate the structure of the surrogate model, we analyze real performance data from a collection of popular lifelong learning approaches and baselines adapted for lifelong classification and lifelong reinforcement learning.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1163834534,
        "newsscientist":0.1857057499,
        "technologyreview":0.3275401073,
        "venturebeat":0.3019708072,
        "wired":0.2052219408,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14378v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659041893000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00932v1",
        "predicted_newsworthiness":0.447185818,
        "title":"Masader Plus: A New Interface for Exploring +500 Arabic NLP Datasets",
        "summary":"Masader (Alyafeai et al., 2021) created a metadata structure to be used for cataloguing Arabic NLP datasets. However, developing an easy way to explore such a catalogue is a challenging task. In order to give the optimal experience for users and researchers exploring the catalogue, several design and user experience challenges must be resolved. Furthermore, user interactions with the website may provide an easy approach to improve the catalogue. In this paper, we introduce Masader Plus, a web interface for users to browse Masader. We demonstrate data exploration, filtration, and a simple API that allows users to examine datasets from the backend. Masader Plus can be explored using this link https:\/\/arbml.github.io\/masader. A video recording explaining the interface can be found here https:\/\/www.youtube.com\/watch?v=SEtdlSeqchk.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1836011636,
        "newsscientist":0.166905451,
        "technologyreview":0.2603728544,
        "venturebeat":0.2768588063,
        "wired":0.2329464348,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00932v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659367916000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.12833v1",
        "predicted_newsworthiness":0.4468757515,
        "title":"Multimodal-GuideNet: Gaze-Probe Bidirectional Guidance in Obstetric Ultrasound Scanning",
        "summary":"Eye trackers can provide visual guidance to sonographers during ultrasound (US) scanning. Such guidance is potentially valuable for less experienced operators to improve their scanning skills on how to manipulate the probe to achieve the desired plane. In this paper, a multimodal guidance approach (Multimodal-GuideNet) is proposed to capture the stepwise dependency between a real-world US video signal, synchronized gaze, and probe motion within a unified framework. To understand the causal relationship between gaze movement and probe motion, our model exploits multitask learning to jointly learn two related tasks: predicting gaze movements and probe signals that an experienced sonographer would perform in routine obstetric scanning. The two tasks are associated by a modality-aware spatial graph to detect the co-occurrence among the multi-modality inputs and share useful cross-modal information. Instead of a deterministic scanning path, Multimodal-GuideNet allows for scanning diversity by estimating the probability distribution of real scans. Experiments performed with three typical obstetric scanning examinations show that the new approach outperforms single-task learning for both probe motion guidance and gaze movement prediction. Multimodal-GuideNet also provides a visual guidance signal with an error rate of less than 10 pixels for a 224x288 US image.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1154130222,
        "newsscientist":0.184280324,
        "technologyreview":0.2472543387,
        "venturebeat":0.2279829863,
        "wired":0.1929839273,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12833v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658836670000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02192v1",
        "predicted_newsworthiness":0.4465129358,
        "title":"RealPatch: A Statistical Matching Framework for Model Patching with Real Samples",
        "summary":"Machine learning classifiers are typically trained to minimise the average error across a dataset. Unfortunately, in practice, this process often exploits spurious correlations caused by subgroup imbalance within the training data, resulting in high average performance but highly variable performance across subgroups. Recent work to address this problem proposes model patching with CAMEL. This previous approach uses generative adversarial networks to perform intra-class inter-subgroup data augmentations, requiring (a) the training of a number of computationally expensive models and (b) sufficient quality of model's synthetic outputs for the given domain. In this work, we propose RealPatch, a framework for simpler, faster, and more data-efficient data augmentation based on statistical matching. Our framework performs model patching by augmenting a dataset with real samples, mitigating the need to train generative models for the target task. We demonstrate the effectiveness of RealPatch on three benchmark datasets, CelebA, Waterbirds and a subset of iWildCam, showing improvements in worst-case subgroup performance and in subgroup performance gap in binary classification. Furthermore, we conduct experiments with the imSitu dataset with 211 classes, a setting where generative model-based patching such as CAMEL is impractical. We show that RealPatch can successfully eliminate dataset leakage while reducing model leakage and maintaining high utility. The code for RealPatch can be found at https:\/\/github.com\/wearepal\/RealPatch.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1309011556,
        "newsscientist":0.1786494573,
        "technologyreview":0.2979322508,
        "venturebeat":0.2696549336,
        "wired":0.1915826978,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02192v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659543750000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12648v1",
        "predicted_newsworthiness":0.4460037614,
        "title":"Efficient and Accurate Skeleton-Based Two-Person Interaction Recognition Using Inter- and Intra-body Graphs",
        "summary":"Skeleton-based two-person interaction recognition has been gaining increasing attention as advancements are made in pose estimation and graph convolutional networks. Although the accuracy has been gradually improving, the increasing computational complexity makes it more impractical for a real-world environment. There is still room for accuracy improvement as the conventional methods do not fully represent the relationship between inter-body joints. In this paper, we propose a lightweight model for accurately recognizing two-person interactions. In addition to the architecture, which incorporates middle fusion, we introduce a factorized convolution technique to reduce the weight parameters of the model. We also introduce a network stream that accounts for relative distance changes between inter-body joints to improve accuracy. Experiments using two large-scale datasets, NTU RGB+D 60 and 120, show that our method simultaneously achieved the highest accuracy and relatively low computational complexity compared with the conventional methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0969488254,
        "newsscientist":0.1456634096,
        "technologyreview":0.1996067595,
        "venturebeat":0.1856745703,
        "wired":0.1538476084,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12648v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658809720000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11643v1",
        "predicted_newsworthiness":0.4457580558,
        "title":"Robust Scene Inference under Noise-Blur Dual Corruptions",
        "summary":"Scene inference under low-light is a challenging problem due to severe noise in the captured images. One way to reduce noise is to use longer exposure during the capture. However, in the presence of motion (scene or camera motion), longer exposures lead to motion blur, resulting in loss of image information. This creates a trade-off between these two kinds of image degradations: motion blur (due to long exposure) vs. noise (due to short exposure), also referred as a dual image corruption pair in this paper. With the rise of cameras capable of capturing multiple exposures of the same scene simultaneously, it is possible to overcome this trade-off. Our key observation is that although the amount and nature of degradation varies for these different image captures, the semantic content remains the same across all images. To this end, we propose a method to leverage these multi exposure captures for robust inference under low-light and motion. Our method builds on a feature consistency loss to encourage similar results from these individual captures, and uses the ensemble of their final predictions for robust visual recognition. We demonstrate the effectiveness of our approach on simulated images as well as real captures with multiple exposures, and across the tasks of object detection and image classification.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0942537057,
        "newsscientist":0.131759771,
        "technologyreview":0.2173427024,
        "venturebeat":0.1740518746,
        "wired":0.1607406278,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11643v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658631120000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14391v1",
        "predicted_newsworthiness":0.4452196987,
        "title":"Distributed Stochastic Bandit Learning with Context Distributions",
        "summary":"We study the problem of distributed stochastic multi-arm contextual bandit with unknown contexts, in which M agents work collaboratively to choose optimal actions under the coordination of a central server in order to minimize the total regret. In our model, an adversary chooses a distribution on the set of possible contexts and the agents observe only the context distribution and the exact context is unknown to the agents. Such a situation arises, for instance, when the context itself is a noisy measurement or based on a prediction mechanism as in weather forecasting or stock market prediction. Our goal is to develop a distributed algorithm that selects a sequence of optimal actions to maximize the cumulative reward. By performing a feature vector transformation and by leveraging the UCB algorithm, we propose a UCB algorithm for stochastic bandits with context distribution and prove that our algorithm achieves a regret and communications bounds of $O(d\\sqrt{MT}log^2T)$ and $O(M^{1.5}d^3)$, respectively, for linearly parametrized reward functions. We also consider a case where the agents observe the actual context after choosing the action. For this setting we presented a modified algorithm that utilizes the additional information to achieve a tighter regret bound. Finally, we validated the performance of our algorithms and compared it with other baseline approaches using extensive simulations on synthetic data and on the real world movielens dataset.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.107799746,
        "newsscientist":0.1396961605,
        "technologyreview":0.2357381241,
        "venturebeat":0.2351846446,
        "wired":0.1781143895,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14391v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659045611000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12655v1",
        "predicted_newsworthiness":0.4450142354,
        "title":"Semi-supervised 3D Object Detection with Proficient Teachers",
        "summary":"Dominated point cloud-based 3D object detectors in autonomous driving scenarios rely heavily on the huge amount of accurately labeled samples, however, 3D annotation in the point cloud is extremely tedious, expensive and time-consuming. To reduce the dependence on large supervision, semi-supervised learning (SSL) based approaches have been proposed. The Pseudo-Labeling methodology is commonly used for SSL frameworks, however, the low-quality predictions from the teacher model have seriously limited its performance. In this work, we propose a new Pseudo-Labeling framework for semi-supervised 3D object detection, by enhancing the teacher model to a proficient one with several necessary designs. First, to improve the recall of pseudo labels, a Spatialtemporal Ensemble (STE) module is proposed to generate sufficient seed boxes. Second, to improve the precision of recalled boxes, a Clusteringbased Box Voting (CBV) module is designed to get aggregated votes from the clustered seed boxes. This also eliminates the necessity of sophisticated thresholds to select pseudo labels. Furthermore, to reduce the negative influence of wrongly pseudo-labeled samples during the training, a soft supervision signal is proposed by considering Box-wise Contrastive Learning (BCL). The effectiveness of our model is verified on both ONCE and Waymo datasets. For example, on ONCE, our approach significantly improves the baseline by 9.51 mAP. Moreover, with half annotations, our model outperforms the oracle model with full annotations on Waymo.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0924462562,
        "newsscientist":0.1522122708,
        "technologyreview":0.2496344766,
        "venturebeat":0.2285211586,
        "wired":0.1965292493,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12655v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658811243000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14499v1",
        "predicted_newsworthiness":0.4448461589,
        "title":"Class-Difficulty Based Methods for Long-Tailed Visual Recognition",
        "summary":"Long-tailed datasets are very frequently encountered in real-world use cases where few classes or categories (known as majority or head classes) have higher number of data samples compared to the other classes (known as minority or tail classes). Training deep neural networks on such datasets gives results biased towards the head classes. So far, researchers have come up with multiple weighted loss and data re-sampling techniques in efforts to reduce the bias. However, most of such techniques assume that the tail classes are always the most difficult classes to learn and therefore need more weightage or attention. Here, we argue that the assumption might not always hold true. Therefore, we propose a novel approach to dynamically measure the instantaneous difficulty of each class during the training phase of the model. Further, we use the difficulty measures of each class to design a novel weighted loss technique called `class-wise difficulty based weighted (CDB-W) loss' and a novel data sampling technique called `class-wise difficulty based sampling (CDB-S)'. To verify the wide-scale usability of our CDB methods, we conducted extensive experiments on multiple tasks such as image classification, object detection, instance segmentation and video-action classification. Results verified that CDB-W loss and CDB-S could achieve state-of-the-art results on many class-imbalanced datasets such as ImageNet-LT, LVIS and EGTEA, that resemble real-world use cases.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0957112994,
        "newsscientist":0.1334388208,
        "technologyreview":0.2263991731,
        "venturebeat":0.1990829119,
        "wired":0.1401200285,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14499v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659076402000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11537v1",
        "predicted_newsworthiness":0.4446522141,
        "title":"RGB-D Robotic Pose Estimation For a Servicing Robotic Arm",
        "summary":"A large number of robotic and human-assisted missions to the Moon and Mars are forecast. NASA's efforts to learn about the geology and makeup of these celestial bodies rely heavily on the use of robotic arms. The safety and redundancy aspects will be crucial when humans will be working alongside the robotic explorers. Additionally, robotic arms are crucial to satellite servicing and planned orbit debris mitigation missions. The goal of this work is to create a custom Computer Vision (CV) based Artificial Neural Network (ANN) that would be able to rapidly identify the posture of a 7 Degree of Freedom (DoF) robotic arm from a single (RGB-D) image - just like humans can easily identify if an arm is pointing in some general direction. The Sawyer robotic arm is used for developing and training this intelligent algorithm. Since Sawyer's joint space spans 7 dimensions, it is an insurmountable task to cover the entire joint configuration space. In this work, orthogonal arrays are used, similar to the Taguchi method, to efficiently span the joint space with the minimal number of training images. This ``optimally'' generated database is used to train the custom ANN and its degree of accuracy is on average equal to twice the smallest joint displacement step used for database generation. A pre-trained ANN will be useful for estimating the postures of robotic manipulators used on space stations, spacecraft, and rovers as an auxiliary tool or for contingency plans.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0883668606,
        "newsscientist":0.1906604266,
        "technologyreview":0.2491648883,
        "venturebeat":0.2046195438,
        "wired":0.1877614801,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11537v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658588596000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13325v1",
        "predicted_newsworthiness":0.4446060395,
        "title":"SiRi: A Simple Selective Retraining Mechanism for Transformer-based Visual Grounding",
        "summary":"In this paper, we investigate how to achieve better visual grounding with modern vision-language transformers, and propose a simple yet powerful Selective Retraining (SiRi) mechanism for this challenging task. Particularly, SiRi conveys a significant principle to the research of visual grounding, i.e., a better initialized vision-language encoder would help the model converge to a better local minimum, advancing the performance accordingly. In specific, we continually update the parameters of the encoder as the training goes on, while periodically re-initialize rest of the parameters to compel the model to be better optimized based on an enhanced encoder. SiRi can significantly outperform previous approaches on three popular benchmarks. Specifically, our method achieves 83.04% Top1 accuracy on RefCOCO+ testA, outperforming the state-of-the-art approaches (training from scratch) by more than 10.21%. Additionally, we reveal that SiRi performs surprisingly superior even with limited training data. We also extend it to transformer-based visual grounding models and other vision-language tasks to verify the validity.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0794104554,
        "newsscientist":0.1474832515,
        "technologyreview":0.2414131873,
        "venturebeat":0.2230054705,
        "wired":0.1768238162,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13325v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658905261000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00748v1",
        "predicted_newsworthiness":0.4441765222,
        "title":"Efficient Long-Text Understanding with Short-Text Models",
        "summary":"Transformer-based pretrained language models (LMs) are ubiquitous across natural language understanding, but cannot be applied to long sequences such as stories, scientific articles and long documents, due to their quadratic complexity. While a myriad of efficient transformer variants have been proposed, they are typically based on custom implementations that require expensive pretraining from scratch. In this work, we propose SLED: SLiding-Encoder and Decoder, a simple approach for processing long sequences that re-uses and leverages battle-tested short-text pretrained LMs. Specifically, we partition the input into overlapping chunks, encode each with a short-text LM encoder and use the pretrained decoder to fuse information across chunks (fusion-in-decoder). We illustrate through controlled experiments that SLED offers a viable strategy for long text understanding and evaluate our approach on SCROLLS, a benchmark with seven datasets across a wide range of language understanding tasks. We find that SLED is competitive with specialized models that are up to 50x larger and require a dedicated and expensive pretraining step.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0978654175,
        "newsscientist":0.1258042822,
        "technologyreview":0.2108142212,
        "venturebeat":0.2227577122,
        "wired":0.1778185037,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00748v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai",
            "cs.lg"
        ],
        "published":1659352479000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00979v1",
        "predicted_newsworthiness":0.4440911694,
        "title":"Automatically Discovering Novel Visual Categories with Self-supervised Prototype Learning",
        "summary":"This paper tackles the problem of novel category discovery (NCD), which aims to discriminate unknown categories in large-scale image collections. The NCD task is challenging due to the closeness to the real-world scenarios, where we have only encountered some partial classes and images. Unlike other works on the NCD, we leverage the prototypes to emphasize the importance of category discrimination and alleviate the issue of missing annotations of novel classes. Concretely, we propose a novel adaptive prototype learning method consisting of two main stages: prototypical representation learning and prototypical self-training. In the first stage, we obtain a robust feature extractor, which could serve for all images with base and novel categories. This ability of instance and category discrimination of the feature extractor is boosted by self-supervised learning and adaptive prototypes. In the second stage, we utilize the prototypes again to rectify offline pseudo labels and train a final parametric classifier for category clustering. We conduct extensive experiments on four benchmark datasets and demonstrate the effectiveness and robustness of the proposed method with state-of-the-art performance.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.097528252,
        "newsscientist":0.1487233566,
        "technologyreview":0.2317924588,
        "venturebeat":0.2157938087,
        "wired":0.1629539304,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00979v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659371673000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13181v1",
        "predicted_newsworthiness":0.4440663209,
        "title":"Planning and Learning: A Review of Methods involving Path-Planning for Autonomous Vehicles",
        "summary":"This short review aims to make the reader familiar with state-of-the-art works relating to planning, scheduling and learning. First, we study state-of-the-art planning algorithms. We give a brief introduction of neural networks. Then we explore in more detail graph neural networks, a recent variant of neural networks suited for processing graph-structured inputs. We describe briefly the concept of reinforcement learning algorithms and some approaches designed to date. Next, we study some successful approaches combining neural networks for path-planning. Lastly, we focus on temporal planning problems with uncertainty.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0987075564,
        "newsscientist":0.156235381,
        "technologyreview":0.2687374266,
        "venturebeat":0.2379418067,
        "wired":0.1892911342,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13181v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1658868978000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.13082v1",
        "predicted_newsworthiness":0.4439143705,
        "title":"Offline Reinforcement Learning at Multiple Frequencies",
        "summary":"Leveraging many sources of offline robot data requires grappling with the heterogeneity of such data. In this paper, we focus on one particular aspect of heterogeneity: learning from offline data collected at different control frequencies. Across labs, the discretization of controllers, sampling rates of sensors, and demands of a task of interest may differ, giving rise to a mixture of frequencies in an aggregated dataset. We study how well offline reinforcement learning (RL) algorithms can accommodate data with a mixture of frequencies during training. We observe that the $Q$-value propagates at different rates for different discretizations, leading to a number of learning challenges for off-the-shelf offline RL. We present a simple yet effective solution that enforces consistency in the rate of $Q$-value updates to stabilize learning. By scaling the value of $N$ in $N$-step returns with the discretization size, we effectively balance $Q$-value propagation, leading to more stable convergence. On three simulated robotic control problems, we empirically find that this simple approach outperforms na\\\"ive mixing by 50% on average.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0859464783,
        "newsscientist":0.1303420666,
        "technologyreview":0.2260869036,
        "venturebeat":0.2077523443,
        "wired":0.1602692825,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13082v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.ro"
        ],
        "published":1658858089000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11846v1",
        "predicted_newsworthiness":0.4437869377,
        "title":"Mixture of Input-Output Hidden Markov Models for Heterogeneous Disease Progression Modeling",
        "summary":"A particular challenge for disease progression modeling is the heterogeneity of a disease and its manifestations in the patients. Existing approaches often assume the presence of a single disease progression characteristics which is unlikely for neurodegenerative disorders such as Parkinson's disease. In this paper, we propose a hierarchical time-series model that can discover multiple disease progression dynamics. The proposed model is an extension of an input-output hidden Markov model that takes into account the clinical assessments of patients' health status and prescribed medications. We illustrate the benefits of our model using a synthetically generated dataset and a real-world longitudinal dataset for Parkinson's disease.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1363226925,
        "newsscientist":0.1639848969,
        "technologyreview":0.1923347413,
        "venturebeat":0.1692365221,
        "wired":0.1379974871,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11846v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658704626000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14003v1",
        "predicted_newsworthiness":0.4428289008,
        "title":"Raising Student Completion Rates with Adaptive Curriculum and Contextual Bandits",
        "summary":"We present an adaptive learning Intelligent Tutoring System, which uses model-based reinforcement learning in the form of contextual bandits to assign learning activities to students. The model is trained on the trajectories of thousands of students in order to maximize their exercise completion rates and continues to learn online, automatically adjusting itself to new activities. A randomized controlled trial with students shows that our model leads to superior completion rates and significantly improved student engagement when compared to other approaches. Our approach is fully-automated unlocking new opportunities for learning experience personalization.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1533405145,
        "newsscientist":0.1656431579,
        "technologyreview":0.252661758,
        "venturebeat":0.258156905,
        "wired":0.2119914569,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14003v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai",
            "cs.cy",
            "cs.hc",
            "cs.lg"
        ],
        "published":1659005531000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14393v1",
        "predicted_newsworthiness":0.4427775157,
        "title":"LAD: Language Models as Data for Zero-Shot Dialog",
        "summary":"To facilitate zero-shot generalization in taskoriented dialog, this paper proposes Language Models as Data (LAD). LAD is a paradigm for creating diverse and accurate synthetic data which conveys the necessary structural constraints and can be used to train a downstream neural dialog model. LAD leverages GPT-3 to induce linguistic diversity. LAD achieves significant performance gains in zero-shot settings on intent prediction (+15%), slot filling (+31.4 F-1) and next action prediction (+11 F1). Furthermore, an interactive human evaluation shows that training with LAD is competitive with training on human dialogs. LAD is open-sourced, with the code and data available at https:\/\/github.com\/Shikib\/lad.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0968534482,
        "newsscientist":0.1149685591,
        "technologyreview":0.2215028856,
        "venturebeat":0.2438619069,
        "wired":0.183219525,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14393v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1659046245000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00785v1",
        "predicted_newsworthiness":0.442654321,
        "title":"Verification system based on long-range iris and Graph Siamese Neural Networks",
        "summary":"Biometric systems represent valid solutions in tasks like user authentication and verification, since they are able to analyze physical and behavioural features with high precision. However, especially when physical biometrics are used, as is the case of iris recognition, they require specific hardware such as retina scanners, sensors, or HD cameras to achieve relevant results. At the same time, they require the users to be very close to the camera to extract high-resolution information. For this reason, in this work, we propose a novel approach that uses long-range (LR) distance images for implementing an iris verification system. More specifically, we present a novel methodology for converting LR iris images into graphs and then use Graph Siamese Neural Networks (GSNN) to predict whether two graphs belong to the same person. In this study, we not only describe this methodology but also evaluate how the spectral components of these images can be used for improving the graph extraction and the final classification task. Results demonstrate the suitability of this approach, encouraging the community to explore graph application in biometric systems.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1108502111,
        "newsscientist":0.1754935009,
        "technologyreview":0.2576827951,
        "venturebeat":0.2254400648,
        "wired":0.1705681792,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00785v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659000131000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13264v1",
        "predicted_newsworthiness":0.4424475423,
        "title":"Instance-specific 6-DoF Object Pose Estimation from Minimal Annotations",
        "summary":"In many robotic applications, the environment setting in which the 6-DoF pose estimation of a known, rigid object and its subsequent grasping is to be performed, remains nearly unchanging and might even be known to the robot in advance. In this paper, we refer to this problem as instance-specific pose estimation: the robot is expected to estimate the pose with a high degree of accuracy in only a limited set of familiar scenarios. Minor changes in the scene, including variations in lighting conditions and background appearance, are acceptable but drastic alterations are not anticipated. To this end, we present a method to rapidly train and deploy a pipeline for estimating the continuous 6-DoF pose of an object from a single RGB image. The key idea is to leverage known camera poses and rigid body geometry to partially automate the generation of a large labeled dataset. The dataset, along with sufficient domain randomization, is then used to supervise the training of deep neural networks for predicting semantic keypoints. Experimentally, we demonstrate the convenience and effectiveness of our proposed method to accurately estimate object pose requiring only a very small amount of manual annotation for training.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0700564306,
        "newsscientist":0.1236219652,
        "technologyreview":0.2078896042,
        "venturebeat":0.1940863404,
        "wired":0.1627695752,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13264v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ro"
        ],
        "published":1658890828000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11737v1",
        "predicted_newsworthiness":0.4421378635,
        "title":"Towards Using Fully Observable Policies for POMDPs",
        "summary":"Partially Observable Markov Decision Process (POMDP) is a framework applicable to many real world problems. In this work, we propose an approach to solve POMDPs with multimodal belief by relying on a policy that solves the fully observable version. By defininig a new, mixture value function based on the value function from the fully observable variant, we can use the corresponding greedy policy to solve the POMDP itself. We develop the mathematical framework necessary for discussion, and introduce a benchmark built on the task of Reconnaissance Blind TicTacToe. On this benchmark, we show that our policy outperforms policies ignoring the existence of multiple modes.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0849063471,
        "newsscientist":0.1563181373,
        "technologyreview":0.2219875605,
        "venturebeat":0.1966717987,
        "wired":0.1718433905,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11737v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658668933000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11820v1",
        "predicted_newsworthiness":0.4419304611,
        "title":"Optimizing Resource Allocation and VNF Embedding in RAN Slicing",
        "summary":"5G radio access network (RAN) with network slicing methodology plays a key role in the development of the next-generation network system. RAN slicing focuses on splitting the substrate's resources into a set of self-contained programmable RAN slices. Leveraged by network function virtualization (NFV), a RAN slice is constituted by various virtual network functions (VNFs) and virtual links that are embedded as instances on substrate nodes. In this work, we focus on the following fundamental tasks: i) establishing the theoretical foundation for constructing a VNF mapping plan for RAN slice recovery optimization and ii) developing algorithms needed to map\/embed VNFs efficiently. In particular, we propose four efficient algorithms, including Resource-based Algorithm (RBA), Connectivity-based Algorithm (CBA), Group-based Algorithm (GBA), and Group-Connectivity-based Algorithm (GCBA) to solve the resource allocation and VNF mapping problem. Extensive experiments are also conducted to validate the robustness of RAN slicing via the proposed algorithms.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0741996972,
        "newsscientist":0.0820890507,
        "technologyreview":0.1336166734,
        "venturebeat":0.1709621861,
        "wired":0.1310132557,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11820v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658200350000,
        "published_hr":"Jul 18, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2208.01813v1",
        "predicted_newsworthiness":0.441734879,
        "title":"TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation",
        "summary":"Text-VQA aims at answering questions that require understanding the textual cues in an image. Despite the great progress of existing Text-VQA methods, their performance suffers from insufficient human-labeled question-answer (QA) pairs. However, we observe that, in general, the scene text is not fully exploited in the existing datasets -- only a small portion of text in each image participates in the annotated QA activities. This results in a huge waste of useful information. To address this deficiency, we develop a new method to generate high-quality and diverse QA pairs by explicitly utilizing the existing rich text available in the scene context of each image. Specifically, we propose, TAG, a text-aware visual question-answer generation architecture that learns to produce meaningful, and accurate QA samples using a multimodal transformer. The architecture exploits underexplored scene text information and enhances scene understanding of Text-VQA models by combining the generated QA pairs with the initial training data. Extensive experimental results on two well-known Text-VQA benchmarks (TextVQA and ST-VQA) demonstrate that our proposed TAG effectively enlarges the training data that helps improve the Text-VQA performance without extra labeling effort. Moreover, our model outperforms state-of-the-art approaches that are pre-trained with extra large-scale data. Code will be made publicly available.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1049296332,
        "newsscientist":0.1396409585,
        "technologyreview":0.2180292875,
        "venturebeat":0.2201843581,
        "wired":0.1824083318,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01813v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659493089000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01347v1",
        "predicted_newsworthiness":0.4416899867,
        "title":"How UMass-FSD Inadvertently Leverages Temporal Bias",
        "summary":"First Story Detection describes the task of identifying new events in a stream of documents. The UMass-FSD system is known for its strong performance in First Story Detection competitions. Recently, it has been frequently used as a high accuracy baseline in research publications. We are the first to discover that UMass-FSD inadvertently leverages temporal bias. Interestingly, the discovered bias contrasts previously known biases and performs significantly better. Our analysis reveals an increased contribution of temporally distant documents, resulting from an unusual way of handling incremental term statistics. We show that this form of temporal bias is also applicable to other well-known First Story Detection systems, where it improves the detection accuracy. To provide a more generalizable conclusion and demonstrate that the observed bias is not only an artefact of a particular implementation, we present a model that intentionally leverages a bias on temporal distance. Our model significantly improves the detection effectiveness of state-of-the-art First Story Detection systems.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1568402892,
        "newsscientist":0.17451031,
        "technologyreview":0.2528253014,
        "venturebeat":0.2713642248,
        "wired":0.258697352,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01347v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1659435569000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.14489v1",
        "predicted_newsworthiness":0.4416855346,
        "title":"StyleAM: Perception-Oriented Unsupervised Domain Adaption for Non-reference Image Quality Assessment",
        "summary":"Deep neural networks (DNNs) have shown great potential in non-reference image quality assessment (NR-IQA). However, the annotation of NR-IQA is labor-intensive and time-consuming, which severely limits their application especially for authentic images. To relieve the dependence on quality annotation, some works have applied unsupervised domain adaptation (UDA) to NR-IQA. However, the above methods ignore that the alignment space used in classification is sub-optimal, since the space is not elaborately designed for perception. To solve this challenge, we propose an effective perception-oriented unsupervised domain adaptation method StyleAM for NR-IQA, which transfers sufficient knowledge from label-rich source domain data to label-free target domain images via Style Alignment and Mixup. Specifically, we find a more compact and reliable space i.e., feature style space for perception-oriented UDA based on an interesting\/amazing observation, that the feature style (i.e., the mean and variance) of the deep layer in DNNs is exactly associated with the quality score in NR-IQA. Therefore, we propose to align the source and target domains in a more perceptual-oriented space i.e., the feature style space, to reduce the intervention from other quality-irrelevant feature factors. Furthermore, to increase the consistency between quality score and its feature style, we also propose a novel feature augmentation strategy Style Mixup, which mixes the feature styles (i.e., the mean and variance) before the last layer of DNNs together with mixing their labels. Extensive experimental results on two typical cross-domain settings (i.e., synthetic to authentic, and multiple distortions to one distortion) have demonstrated the effectiveness of our proposed StyleAM on NR-IQA.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0778801723,
        "newsscientist":0.1124862267,
        "technologyreview":0.1913266534,
        "venturebeat":0.1642697972,
        "wired":0.1315476686,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14489v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659073878000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11554v1",
        "predicted_newsworthiness":0.4415188173,
        "title":"Towards Open Set 3D Learning: A Benchmark on Object Point Clouds",
        "summary":"In the last years, there has been significant progress in the field of 3D learning on classification, detection and segmentation problems. The vast majority of the existing studies focus on canonical closed-set conditions, neglecting the intrinsic open nature of the real-world. This limits the abilities of autonomous systems involved in safety-critical applications that require managing novel and unknown signals. In this context exploiting 3D data can be a valuable asset since it conveys rich information about the geometry of sensed objects and scenes. This paper provides the first broad study on Open Set 3D learning. We introduce a novel testbed with settings of increasing difficulty in terms of category semantic shift and cover both in-domain (synthetic-to-synthetic) and cross-domain (synthetic-to-real) scenarios. Moreover, we investigate the related out-of-distribution and Open Set 2D literature to understand if and how their most recent approaches are effective on 3D data. Our extensive benchmark positions several algorithms in the same coherent picture, revealing their strengths and limitations. The results of our analysis may serve as a reliable foothold for future tailored Open Set 3D models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0846392058,
        "newsscientist":0.1436124619,
        "technologyreview":0.2171313015,
        "venturebeat":0.2132070371,
        "wired":0.1679238173,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11554v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658595645000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00928v1",
        "predicted_newsworthiness":0.4411016722,
        "title":"OmniCity: Omnipotent City Understanding with Multi-level and Multi-view Images",
        "summary":"This paper presents OmniCity, a new dataset for omnipotent city understanding from multi-level and multi-view images. More precisely, the OmniCity contains multi-view satellite images as well as street-level panorama and mono-view images, constituting over 100K pixel-wise annotated images that are well-aligned and collected from 25K geo-locations in New York City. To alleviate the substantial pixel-wise annotation efforts, we propose an efficient street-view image annotation pipeline that leverages the existing label maps of satellite view and the transformation relations between different views (satellite, panorama, and mono-view). With the new OmniCity dataset, we provide benchmarks for a variety of tasks including building footprint extraction, height estimation, and building plane\/instance\/fine-grained segmentation. We also analyze the impact of view on each task, the performance of different models, limitations of existing methods, etc. Compared with the existing multi-level and multi-view benchmarks, our OmniCity contains a larger number of images with richer annotation types and more views, provides more baseline results obtained from state-of-the-art models, and introduces a novel task for fine-grained building instance segmentation on street-level panorama images. Moreover, OmniCity provides new problem settings for existing tasks, such as cross-view image matching, synthesis, segmentation, detection, etc., and facilitates the developing of new methods for large-scale city understanding, reconstruction, and simulation. The OmniCity dataset as well as the benchmarks will be available at https:\/\/city-super.github.io\/omnicity.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.130467066,
        "newsscientist":0.150406559,
        "technologyreview":0.2126932693,
        "venturebeat":0.2026763192,
        "wired":0.1934381458,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00928v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659367165000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11382v2",
        "predicted_newsworthiness":0.4408684875,
        "title":"Density-Aware Personalized Training for Risk Prediction in Imbalanced Medical Data",
        "summary":"Medical events of interest, such as mortality, often happen at a low rate in electronic medical records, as most admitted patients survive. Training models with this imbalance rate (class density discrepancy) may lead to suboptimal prediction. Traditionally this problem is addressed through ad-hoc methods such as resampling or reweighting but performance in many cases is still limited. We propose a framework for training models for this imbalance issue: 1) we first decouple the feature extraction and classification process, adjusting training batches separately for each component to mitigate bias caused by class density discrepancy; 2) we train the network with both a density-aware loss and a learnable cost matrix for misclassifications. We demonstrate our model's improved performance in real-world medical datasets (TOPCAT and MIMIC-III) to show improved AUC-ROC, AUC-PRC, Brier Skill Score compared with the baselines in the domain.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1537832827,
        "newsscientist":0.1673813815,
        "technologyreview":0.2454647609,
        "venturebeat":0.2391711399,
        "wired":0.1659593927,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11382v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658536793000,
        "published_hr":"Jul 22, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14687v3",
        "predicted_newsworthiness":0.440842225,
        "title":"A Data-driven Latent Semantic Analysis for Automatic Text Summarization using LDA Topic Modelling",
        "summary":"With the advent and popularity of big data mining and huge text analysis in modern times, automated text summarization became prominent for extracting and retrieving important information from documents. This research investigates aspects of automatic text summarization from the perspectives of single and multiple documents. Summarization is a task of condensing huge text articles into short, summarized versions. The text is reduced in size for summarization purpose but preserving key vital information and retaining the meaning of the original document. This study presents the Latent Dirichlet Allocation (LDA) approach used to perform topic modelling from summarised medical science journal articles with topics related to genes and diseases. In this study, PyLDAvis web-based interactive visualization tool was used to visualise the selected topics. The visualisation provides an overarching view of the main topics while allowing and attributing deep meaning to the prevalence individual topic. This study presents a novel approach to summarization of single and multiple documents. The results suggest the terms ranked purely by considering their probability of the topic prevalence within the processed document using extractive summarization technique. PyLDAvis visualization describes the flexibility of exploring the terms of the topics' association to the fitted LDA model. The topic modelling result shows prevalence within topics 1 and 2. This association reveals that there is similarity between the terms in topic 1 and 2 in this study. The efficacy of the LDA and the extractive summarization methods were measured using Latent Semantic Analysis (LSA) and Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics to evaluate the reliability and validity of the model.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1492968436,
        "newsscientist":0.1521145473,
        "technologyreview":0.1750074787,
        "venturebeat":0.1601941744,
        "wired":0.1444111943,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14687v3",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.lg"
        ],
        "published":1658574243000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.00847v1",
        "predicted_newsworthiness":0.440760973,
        "title":"MAFW: A Large-scale, Multi-modal, Compound Affective Database for Dynamic Facial Expression Recognition in the Wild",
        "summary":"Dynamic facial expression recognition (FER) databases provide important data support for affective computing and applications. However, most FER databases are annotated with several basic mutually exclusive emotional categories and contain only one modality, e.g., videos. The monotonous labels and modality cannot accurately imitate human emotions and fulfill applications in the real world. In this paper, we propose MAFW, a large-scale multi-modal compound affective database with 10,045 video-audio clips in the wild. Each clip is annotated with a compound emotional category and a couple of sentences that describe the subjects' affective behaviors in the clip. For the compound emotion annotation, each clip is categorized into one or more of the 11 widely-used emotions, i.e., anger, disgust, fear, happiness, neutral, sadness, surprise, contempt, anxiety, helplessness, and disappointment. To ensure high quality of the labels, we filter out the unreliable annotations by an Expectation Maximization (EM) algorithm, and then obtain 11 single-label emotion categories and 32 multi-label emotion categories. To the best of our knowledge, MAFW is the first in-the-wild multi-modal database annotated with compound emotion annotations and emotion-related captions. Additionally, we also propose a novel Transformer-based expression snippet feature learning method to recognize the compound emotions leveraging the expression-change relations among different emotions and modalities. Extensive experiments on MAFW database show the advantages of the proposed method over other state-of-the-art methods for both uni- and multi-modal FER. Our MAFW database is publicly available from https:\/\/mafw-database.github.io\/MAFW.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1420752191,
        "newsscientist":0.1611405265,
        "technologyreview":0.2441583776,
        "venturebeat":0.2358489836,
        "wired":0.2022760428,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00847v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659360873000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13647v2",
        "predicted_newsworthiness":0.4404634518,
        "title":"NAUTS: Negotiation for Adaptation to Unstructured Terrain Surfaces",
        "summary":"When robots operate in real-world off-road environments with unstructured terrains, the ability to adapt their navigational policy is critical for effective and safe navigation. However, off-road terrains introduce several challenges to robot navigation, including dynamic obstacles and terrain uncertainty, leading to inefficient traversal or navigation failures. To address these challenges, we introduce a novel approach for adaptation by negotiation that enables a ground robot to adjust its navigational behaviors through a negotiation process. Our approach first learns prediction models for various navigational policies to function as a terrain-aware joint local controller and planner. Then, through a new negotiation process, our approach learns from various policies' interactions with the environment to agree on the optimal combination of policies in an online fashion to adapt robot navigation to unstructured off-road terrains on the fly. Additionally, we implement a new optimization algorithm that offers the optimal solution for robot negotiation in real-time during execution. Experimental results have validated that our method for adaptation by negotiation outperforms previous methods for robot navigation, especially over unseen and uncertain dynamic terrains.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1126607119,
        "newsscientist":0.1607774359,
        "technologyreview":0.2533592728,
        "venturebeat":0.2238140702,
        "wired":0.2081436437,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13647v2",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658941726000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.14425v1",
        "predicted_newsworthiness":0.4403461954,
        "title":"3D Cartoon Face Generation with Controllable Expressions from a Single GAN Image",
        "summary":"In this paper, we investigate an open research task of generating 3D cartoon face shapes from single 2D GAN generated human faces and without 3D supervision, where we can also manipulate the facial expressions of the 3D shapes. To this end, we discover the semantic meanings of StyleGAN latent space, such that we are able to produce face images of various expressions, poses, and lighting by controlling the latent codes. Specifically, we first finetune the pretrained StyleGAN face model on the cartoon datasets. By feeding the same latent codes to face and cartoon generation models, we aim to realize the translation from 2D human face images to cartoon styled avatars. We then discover semantic directions of the GAN latent space, in an attempt to change the facial expressions while preserving the original identity. As we do not have any 3D annotations for cartoon faces, we manipulate the latent codes to generate images with different poses and lighting, such that we can reconstruct the 3D cartoon face shapes. We validate the efficacy of our method on three cartoon datasets qualitatively and quantitatively.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1071818511,
        "newsscientist":0.1400291688,
        "technologyreview":0.2139202447,
        "venturebeat":0.2010804431,
        "wired":0.1856457064,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14425v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659056781000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12377v3",
        "predicted_newsworthiness":0.4402205011,
        "title":"A novel Deep Learning approach for one-step Conformal Prediction approximation",
        "summary":"Deep Learning predictions with measurable confidence are increasingly desirable for real-world problems, especially in high-risk settings. The Conformal Prediction (CP) framework is a versatile solution that automatically guarantees a maximum error rate. However, CP suffers from computational inefficiencies that limit its application to large-scale datasets. In this paper, we propose a novel conformal loss function that approximates the traditionally two-step CP approach in a single step. By evaluating and penalising deviations from the stringent expected CP output distribution, a Deep Learning model may learn the direct relationship between input data and conformal p-values. Our approach achieves significant training time reductions up to 86% compared to Aggregated Conformal Prediction (ACP), an accepted CP approximation variant. In terms of approximate validity and predictive efficiency, we carry out a comprehensive empirical evaluation to show our novel loss function's competitiveness with ACP for binary and multi-class classification on the well-established MNIST dataset.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.092302188,
        "newsscientist":0.13759484,
        "technologyreview":0.232710129,
        "venturebeat":0.2110271189,
        "wired":0.1490272478,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12377v3",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658771169000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12397v1",
        "predicted_newsworthiness":0.4401052907,
        "title":"C3-SL: Circular Convolution-Based Batch-Wise Compression for Communication-Efficient Split Learning",
        "summary":"Most existing studies improve the efficiency of Split learning (SL) by compressing the transmitted features. However, most works focus on dimension-wise compression that transforms high-dimensional features into a low-dimensional space. In this paper, we propose circular convolution-based batch-wise compression for SL (C3-SL) to compress multiple features into one single feature. To avoid information loss while merging multiple features, we exploit the quasi-orthogonality of features in high-dimensional space with circular convolution and superposition. To the best of our knowledge, we are the first to explore the potential of batch-wise compression under the SL scenario. Based on the simulation results on CIFAR-10 and CIFAR-100, our method achieves a 16x compression ratio with negligible accuracy drops compared with the vanilla SL. Moreover, C3-SL significantly reduces 1152x memory and 2.25x computation overhead compared to the state-of-the-art dimension-wise compression method.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0715487144,
        "newsscientist":0.1187234597,
        "technologyreview":0.2024453562,
        "venturebeat":0.2119591496,
        "wired":0.1632753906,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12397v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1658771942000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01134v1",
        "predicted_newsworthiness":0.4399904711,
        "title":"Improving the Trainability of Deep Neural Networks through Layerwise Batch-Entropy Regularization",
        "summary":"Training deep neural networks is a very demanding task, especially challenging is how to adapt architectures to improve the performance of trained models. We can find that sometimes, shallow networks generalize better than deep networks, and the addition of more layers results in higher training and test errors. The deep residual learning framework addresses this degradation problem by adding skip connections to several neural network layers. It would at first seem counter-intuitive that such skip connections are needed to train deep networks successfully as the expressivity of a network would grow exponentially with depth. In this paper, we first analyze the flow of information through neural networks. We introduce and evaluate the batch-entropy which quantifies the flow of information through each layer of a neural network. We prove empirically and theoretically that a positive batch-entropy is required for gradient descent-based training approaches to optimize a given loss function successfully. Based on those insights, we introduce batch-entropy regularization to enable gradient descent-based training algorithms to optimize the flow of information through each hidden layer individually. With batch-entropy regularization, gradient descent optimizers can transform untrainable networks into trainable networks. We show empirically that we can therefore train a \"vanilla\" fully connected network and convolutional neural network -- no skip connections, batch normalization, dropout, or any other architectural tweak -- with 500 layers by simply adding the batch-entropy regularization term to the loss function. The effect of batch-entropy regularization is not only evaluated on vanilla neural networks, but also on residual networks, autoencoders, and also transformer models over a wide range of computer vision as well as natural language processing tasks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1050725476,
        "newsscientist":0.1640782191,
        "technologyreview":0.2986578949,
        "venturebeat":0.2725795376,
        "wired":0.1880882531,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01134v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659385918000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00250v1",
        "predicted_newsworthiness":0.4399808311,
        "title":"A Bayesian Approach to Learning Bandit Structure in Markov Decision Processes",
        "summary":"In the reinforcement learning literature, there are many algorithms developed for either Contextual Bandit (CB) or Markov Decision Processes (MDP) environments. However, when deploying reinforcement learning algorithms in the real world, even with domain expertise, it is often difficult to know whether it is appropriate to treat a sequential decision making problem as a CB or an MDP. In other words, do actions affect future states, or only the immediate rewards? Making the wrong assumption regarding the nature of the environment can lead to inefficient learning, or even prevent the algorithm from ever learning an optimal policy, even with infinite data. In this work we develop an online algorithm that uses a Bayesian hypothesis testing approach to learn the nature of the environment. Our algorithm allows practitioners to incorporate prior knowledge about whether the environment is that of a CB or an MDP, and effectively interpolate between classical CB and MDP-based algorithms to mitigate against the effects of misspecifying the environment. We perform simulations and demonstrate that in CB settings our algorithm achieves lower regret than MDP-based algorithms, while in non-bandit MDP settings our algorithm is able to learn the optimal policy, often achieving comparable regret to MDP-based algorithms.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1142852063,
        "newsscientist":0.150994386,
        "technologyreview":0.2363189638,
        "venturebeat":0.2235579639,
        "wired":0.1667659867,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00250v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659194291000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14500v1",
        "predicted_newsworthiness":0.4399281251,
        "title":"A Transfer Learning-Based Approach to Marine Vessel Re-Identification",
        "summary":"Marine vessel re-identification technology is an important component of intelligent shipping systems and an important part of the visual perception tasks required for marine surveillance. However, unlike the situation on land, the maritime environment is complex and variable with fewer samples, and it is more difficult to perform vessel re-identification at sea. Therefore, this paper proposes a transfer dynamic alignment algorithm and simulates the swaying situation of vessels at sea, using a well-camouflaged and similar warship as the test target to improve the recognition difficulty and thus cope with the impact caused by complex sea conditions, and discusses the effect of different types of vessels as transfer objects. The experimental results show that the improved algorithm improves the mean average accuracy (mAP) by 10.2% and the first hit rate (Rank1) by 4.9% on average.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0962111422,
        "newsscientist":0.1697046762,
        "technologyreview":0.2068540808,
        "venturebeat":0.1921185629,
        "wired":0.1576647324,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14500v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659076570000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14119v1",
        "predicted_newsworthiness":0.4398789201,
        "title":"A Survey of Syntactic Modelling Structures in Biomedical Ontologies",
        "summary":"Despite the large-scale uptake of semantic technologies in the biomedical domain, little is known about common modelling practices in published ontologies. OWL ontologies are often published only in the crude form of sets of axioms leaving the underlying design opaque. However, a principled and systematic ontology development life cycle is likely to be reflected in regularities of the ontology's emergent syntactic structure. To develop an understanding of this emergent structure, we propose to reverse-engineer ontologies taking a syntax-directed approach for identifying and analysing regularities for axioms and sets of axioms. We survey BioPortal in terms of syntactic modelling trends and common practices for OWL axioms and class frames. Our findings suggest that biomedical ontologies only share simple syntactic structures in which OWL constructors are not deeply nested or combined in a complex manner. While such simple structures often account for large proportions of axioms in a given ontology, many ontologies also contain non-trivial amounts of more complex syntactic structures that are not common across ontologies.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1519554222,
        "newsscientist":0.1710111808,
        "technologyreview":0.2021315265,
        "venturebeat":0.1833967783,
        "wired":0.1623504349,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14119v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1659018780000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.13919v1",
        "predicted_newsworthiness":0.4392938272,
        "title":"Persona-Knowledge Dialogue Multi-Context Retrieval and Enhanced Decoding Methods",
        "summary":"Persona and Knowledge dual context open-domain chat is a novel dialogue generation task introduced recently. While Persona and Knowledge is each interesting context of open-domain dialogue, the combination of both has not been well studied. We tackle Persona-Knowledge identification and response generation tasks in this paper. We design an informed data augmentation strategy that is compatible with neural Q&A retrieval models. With the augmented data, we perform permutative Persona-Knowledge evaluation and successive Persona search fine-tuning. Furthermore, we perform dialogue generation with various decoding techniques and illustrate crucial elements. We achieve SOTA across official metrics with 93.99% Grounding accuracy average and 23.62 SacreBLEU score.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1069584288,
        "newsscientist":0.1205082197,
        "technologyreview":0.2219653794,
        "venturebeat":0.2451133201,
        "wired":0.1954572822,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13919v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ir"
        ],
        "published":1658992748000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00438v1",
        "predicted_newsworthiness":0.4392719976,
        "title":"Toward Understanding WordArt: Corner-Guided Transformer for Scene Text Recognition",
        "summary":"Artistic text recognition is an extremely challenging task with a wide range of applications. However, current scene text recognition methods mainly focus on irregular text while have not explored artistic text specifically. The challenges of artistic text recognition include the various appearance with special-designed fonts and effects, the complex connections and overlaps between characters, and the severe interference from background patterns. To alleviate these problems, we propose to recognize the artistic text at three levels. Firstly, corner points are applied to guide the extraction of local features inside characters, considering the robustness of corner structures to appearance and shape. In this way, the discreteness of the corner points cuts off the connection between characters, and the sparsity of them improves the robustness for background interference. Secondly, we design a character contrastive loss to model the character-level feature, improving the feature representation for character classification. Thirdly, we utilize Transformer to learn the global feature on image-level and model the global relationship of the corner points, with the assistance of a corner-query cross-attention mechanism. Besides, we provide an artistic text dataset to benchmark the performance. Experimental results verify the significant superiority of our proposed method on artistic text recognition and also achieve state-of-the-art performance on several blurred and perspective datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1268875514,
        "newsscientist":0.1553530421,
        "technologyreview":0.2197955055,
        "venturebeat":0.2018097055,
        "wired":0.1831391189,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00438v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659276665000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12647v1",
        "predicted_newsworthiness":0.4392638688,
        "title":"Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering",
        "summary":"Existing visual question answering methods tend to capture the spurious correlations from visual and linguistic modalities, and fail to discover the true casual mechanism that facilitates reasoning truthfully based on the dominant visual evidence and the correct question intention. Additionally, the existing methods usually ignore the complex event-level understanding in multi-modal settings that requires a strong cognitive capability of causal inference to jointly model cross-modal event temporality, causality, and dynamics. In this work, we focus on event-level visual question answering from a new perspective, i.e., cross-modal causal relational reasoning, by introducing causal intervention methods to mitigate the spurious correlations and discover the true causal structures for the integration of visual and linguistic modalities. Specifically, we propose a novel event-level visual question answering framework named Cross-Modal Causal RelatIonal Reasoning (CMCIR), to achieve robust casuality-aware visual-linguistic question answering. To uncover the causal structures for visual and linguistic modalities, the novel Causality-aware Visual-Linguistic Reasoning (CVLR) module is proposed to collaboratively disentangle the visual and linguistic spurious correlations via elaborately designed front-door and back-door causal intervention modules. To discover the fine-grained interactions between linguistic semantics and spatial-temporal representations, we build a novel Spatial-Temporal Transformer (STT) that builds the multi-modal co-occurrence interactions between visual and linguistic content. Extensive experiments on large-scale event-level urban dataset SUTD-TrafficQA and three benchmark real-world datasets TGIF-QA, MSVD-QA, and MSRVTT-QA demonstrate the effectiveness of our CMCIR for discovering visual-linguistic causal structures.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1375867964,
        "newsscientist":0.1574347685,
        "technologyreview":0.2250894045,
        "venturebeat":0.2125754825,
        "wired":0.1815931752,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12647v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658809554000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12967v1",
        "predicted_newsworthiness":0.439176763,
        "title":"TransFiner: A Full-Scale Refinement Approach for Multiple Object Tracking",
        "summary":"Multiple object tracking (MOT) is the task containing detection and association. Plenty of trackers have achieved competitive performance. Unfortunately, for the lack of informative exchange on these subtasks, they are often biased toward one of the two and remain underperforming in complex scenarios, such as the expected false negatives and mistaken trajectories of targets when passing each other. In this paper, we propose TransFiner, a transformer-based post-refinement approach for MOT. It is a generic attachment framework that leverages the images and tracking results (locations and class predictions) from the original tracker as inputs, which are then used to launch TransFiner powerfully. Moreover, TransFiner depends on query pairs, which produce pairs of detection and motion through the fusion decoder and achieve comprehensive tracking improvement. We also provide targeted refinement by labeling query pairs according to different refinement levels. Experiments show that our design is effective, on the MOT17 benchmark, we elevate the CenterTrack from 67.8% MOTA and 64.7% IDF1 to 71.5% MOTA and 66.8% IDF1.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0772756801,
        "newsscientist":0.1335199543,
        "technologyreview":0.205613349,
        "venturebeat":0.2119666197,
        "wired":0.1715645104,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12967v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658848902000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12572v1",
        "predicted_newsworthiness":0.4388156908,
        "title":"Translating a Visual LEGO Manual to a Machine-Executable Plan",
        "summary":"We study the problem of translating an image-based, step-by-step assembly manual created by human designers into machine-interpretable instructions. We formulate this problem as a sequential prediction task: at each step, our model reads the manual, locates the components to be added to the current shape, and infers their 3D poses. This task poses the challenge of establishing a 2D-3D correspondence between the manual image and the real 3D object, and 3D pose estimation for unseen 3D objects, since a new component to be added in a step can be an object built from previous steps. To address these two challenges, we present a novel learning-based framework, the Manual-to-Executable-Plan Network (MEPNet), which reconstructs the assembly steps from a sequence of manual images. The key idea is to integrate neural 2D keypoint detection modules and 2D-3D projection algorithms for high-precision prediction and strong generalization to unseen components. The MEPNet outperforms existing methods on three newly collected LEGO manual datasets and a Minecraft house dataset.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0909564808,
        "newsscientist":0.1481407247,
        "technologyreview":0.2384154413,
        "venturebeat":0.2212607649,
        "wired":0.1913988495,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12572v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658792146000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13738v1",
        "predicted_newsworthiness":0.4385992366,
        "title":"Break and Make: Interactive Structural Understanding Using LEGO Bricks",
        "summary":"Visual understanding of geometric structures with complex spatial relationships is a fundamental component of human intelligence. As children, we learn how to reason about structure not only from observation, but also by interacting with the world around us -- by taking things apart and putting them back together again. The ability to reason about structure and compositionality allows us to not only build things, but also understand and reverse-engineer complex systems. In order to advance research in interactive reasoning for part-based geometric understanding, we propose a challenging new assembly problem using LEGO bricks that we call Break and Make. In this problem an agent is given a LEGO model and attempts to understand its structure by interactively inspecting and disassembling it. After this inspection period, the agent must then prove its understanding by rebuilding the model from scratch using low-level action primitives. In order to facilitate research on this problem we have built LTRON, a fully interactive 3D simulator that allows learning agents to assemble, disassemble and manipulate LEGO models. We pair this simulator with a new dataset of fan-made LEGO creations that have been uploaded to the internet in order to provide complex scenes containing over a thousand unique brick shapes. We take a first step towards solving this problem using sequence-to-sequence models that provide guidance for how to make progress on this challenging problem. Our simulator and data are available at github.com\/aaronwalsman\/ltron. Additional training code and PyTorch examples are available at github.com\/aaronwalsman\/ltron-torch-eccv22.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1350807078,
        "newsscientist":0.2075317884,
        "technologyreview":0.2796635558,
        "venturebeat":0.2474508977,
        "wired":0.239157223,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13738v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658946789000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00369v1",
        "predicted_newsworthiness":0.4380554864,
        "title":"Exploring Attention-Aware Network Resource Allocation for Customized Metaverse Services",
        "summary":"Emerging with the support of computing and communications technologies, Metaverse is expected to bring users unprecedented service experiences. However, the increase in the number of Metaverse users places a heavy demand on network resources, especially for Metaverse services that are based on graphical extended reality and require rendering a plethora of virtual objects. To make efficient use of network resources and improve the Quality-of-Experience (QoE), we design an attention-aware network resource allocation scheme to achieve customized Metaverse services. The aim is to allocate more network resources to virtual objects in which users are more interested. We first discuss several key techniques related to Metaverse services, including QoE analysis, eye-tracking, and remote rendering. We then review existing datasets and propose the user-object-attention level (UOAL) dataset that contains the ground truth attention of 30 users to 96 objects in 1,000 images. A tutorial on how to use UOAL is presented. With the help of UOAL, we propose an attention-aware network resource allocation algorithm that has two steps, i.e., attention prediction and QoE maximization. Specially, we provide an overview of the designs of two types of attention prediction methods, i.e., interest-aware and time-aware prediction. By using the predicted user-object-attention values, network resources such as the rendering capacity of edge devices can be allocated optimally to maximize the QoE. Finally, we propose promising research directions related to Metaverse services.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1466206908,
        "newsscientist":0.2114866456,
        "technologyreview":0.2758931392,
        "venturebeat":0.3476819927,
        "wired":0.2972823447,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00369v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni",
            "cs.ai"
        ],
        "published":1659247455000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2208.00690v2",
        "predicted_newsworthiness":0.4380250515,
        "title":"Generative Bias for Visual Question Answering",
        "summary":"The task of Visual Question Answering (VQA) is known to be plagued by the issue of VQA models exploiting biases within the dataset to make its final prediction. Many previous ensemble based debiasing methods have been proposed where an additional model is purposefully trained to be biased in order to aid in training a robust target model. However, these methods compute the bias for a model from the label statistics of the training data or directly from single modal branches. In contrast, in this work, in order to better learn the bias a target VQA model suffers from, we propose a generative method to train the bias model \\emph{directly from the target model}, called GenB. In particular, GenB employs a generative network to learn the bias through a combination of the adversarial objective and knowledge distillation. We then debias our target model with GenB as a bias model, and show through extensive experiments the effects of our method on various VQA bias datasets including VQA-CP2, VQA-CP1, GQA-OOD, and VQA-CE.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.118055879,
        "newsscientist":0.1427982115,
        "technologyreview":0.2591056666,
        "venturebeat":0.2265532154,
        "wired":0.1703450804,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00690v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.cl",
            "cs.lg"
        ],
        "published":1659344282000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14172v1",
        "predicted_newsworthiness":0.4376683924,
        "title":"Semantic-Aligned Matching for Enhanced DETR Convergence and Multi-Scale Feature Fusion",
        "summary":"The recently proposed DEtection TRansformer (DETR) has established a fully end-to-end paradigm for object detection. However, DETR suffers from slow training convergence, which hinders its applicability to various detection tasks. We observe that DETR's slow convergence is largely attributed to the difficulty in matching object queries to relevant regions due to the unaligned semantics between object queries and encoded image features. With this observation, we design Semantic-Aligned-Matching DETR++ (SAM-DETR++) to accelerate DETR's convergence and improve detection performance. The core of SAM-DETR++ is a plug-and-play module that projects object queries and encoded image features into the same feature embedding space, where each object query can be easily matched to relevant regions with similar semantics. Besides, SAM-DETR++ searches for multiple representative keypoints and exploits their features for semantic-aligned matching with enhanced representation capacity. Furthermore, SAM-DETR++ can effectively fuse multi-scale features in a coarse-to-fine manner on the basis of the designed semantic-aligned matching. Extensive experiments show that the proposed SAM-DETR++ achieves superior convergence speed and competitive detection accuracy. Additionally, as a plug-and-play method, SAM-DETR++ can complement existing DETR convergence solutions with even better performance, achieving 44.8% AP with merely 12 training epochs and 49.1% AP with 50 training epochs on COCO val2017 with ResNet-50. Codes are available at https:\/\/github.com\/ZhangGongjie\/SAM-DETR .",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0802291916,
        "newsscientist":0.1245328355,
        "technologyreview":0.2088432649,
        "venturebeat":0.2071910534,
        "wired":0.1615365283,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14172v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659022469000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00399v1",
        "predicted_newsworthiness":0.4375930731,
        "title":"Neural Knowledge Bank for Pretrained Transformers",
        "summary":"The ability of pretrained Transformers to remember factual knowledge is essential for knowledge-intense downstream tasks such as closed-book question answering. Existing work has shown that pretrained Transformers can recall or leverage factual knowledge that appears in the pretraining corpus to some degree. However, due to the limit of the model capacity, the ability of pretrained models to remember factual knowledge is also limited. Dai et al. (2022) find that the Feed-Forward Networks (FFNs) in pretrained Transformers store factual knowledge in a memory-like manner. Inspired by this finding, we propose a Neural Knowledge Bank (NKB) to store extra factual knowledge for pretrained Transformers. To be specific, we also regard FFNs as key-value memories, and extend them with additional memory slots. During knowledge injection, we fix the original model and inject factual knowledge into the extended memory slots, so there will be no catastrophic forgetting for the pretrained model. In addition, the view of FFNs as key-value memories makes the NKB highly interpretable. We use three closed-book question answering datasets to show our strong ability to store extra factual knowledge. Also, we prove that the NKB will not degrade the general language generation ability of pretrained models through two representative generation tasks, summarization and machine translation. Further, we thoroughly analyze the NKB to reveal its working mechanism and present the meaning of its keys and values in a human-readable way. On top of it, we perform a preliminary attempt to directly update the factual knowledge in the NKB without any additional training.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1090155009,
        "newsscientist":0.1449089931,
        "technologyreview":0.2370878632,
        "venturebeat":0.2232267595,
        "wired":0.1631167674,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00399v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1659258874000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.13337v1",
        "predicted_newsworthiness":0.4372850742,
        "title":"Two-Stream UNET Networks for Semantic Segmentation in Medical Images",
        "summary":"Recent advances of semantic image segmentation greatly benefit from deeper and larger Convolutional Neural Network (CNN) models. Compared to image segmentation in the wild, properties of both medical images themselves and of existing medical datasets hinder training deeper and larger models because of overfitting. To this end, we propose a novel two-stream UNET architecture for automatic end-to-end medical image segmentation, in which intensity value and gradient vector flow (GVF) are two inputs for each stream, respectively. We demonstrate that two-stream CNNs with more low-level features greatly benefit semantic segmentation for imperfect medical image datasets. Our proposed two-stream networks are trained and evaluated on the popular medical image segmentation benchmarks, and the results are competitive with the state of the art. The code will be released soon.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0901041566,
        "newsscientist":0.1304426578,
        "technologyreview":0.2149224513,
        "venturebeat":0.1947752649,
        "wired":0.1354669743,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13337v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658907911000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12350v1",
        "predicted_newsworthiness":0.4372670714,
        "title":"Energy-efficient DNN Inference on Approximate Accelerators Through Formal Property Exploration",
        "summary":"Deep Neural Networks (DNNs) are being heavily utilized in modern applications and are putting energy-constraint devices to the test. To bypass high energy consumption issues, approximate computing has been employed in DNN accelerators to balance out the accuracy-energy reduction trade-off. However, the approximation-induced accuracy loss can be very high and drastically degrade the performance of the DNN. Therefore, there is a need for a fine-grain mechanism that would assign specific DNN operations to approximation in order to maintain acceptable DNN accuracy, while also achieving low energy consumption. In this paper, we present an automated framework for weight-to-approximation mapping enabling formal property exploration for approximate DNN accelerators. At the MAC unit level, our experimental evaluation surpassed already energy-efficient mappings by more than $\\times2$ in terms of energy gains, while also supporting significantly more fine-grain control over the introduced approximation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0667945667,
        "newsscientist":0.1162232328,
        "technologyreview":0.2085985683,
        "venturebeat":0.1965833053,
        "wired":0.141147042,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12350v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658768820000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01836v1",
        "predicted_newsworthiness":0.4372042258,
        "title":"EMC2A-Net: An Efficient Multibranch Cross-channel Attention Network for SAR Target Classification",
        "summary":"In recent years, convolutional neural networks (CNNs) have shown great potential in synthetic aperture radar (SAR) target recognition. SAR images have a strong sense of granularity and have different scales of texture features, such as speckle noise, target dominant scatterers and target contours, which are rarely considered in the traditional CNN model. This paper proposed two residual blocks, namely EMC2A blocks with multiscale receptive fields(RFs), based on a multibranch structure and then designed an efficient isotopic architecture deep CNN (DCNN), EMC2A-Net. EMC2A blocks utilize parallel dilated convolution with different dilation rates, which can effectively capture multiscale context features without significantly increasing the computational burden. To further improve the efficiency of multiscale feature fusion, this paper proposed a multiscale feature cross-channel attention module, namely the EMC2A module, adopting a local multiscale feature interaction strategy without dimensionality reduction. This strategy adaptively adjusts the weights of each channel through efficient one-dimensional (1D)-circular convolution and sigmoid function to guide attention at the global channel wise level. The comparative results on the MSTAR dataset show that EMC2A-Net outperforms the existing available models of the same type and has relatively lightweight network structure. The ablation experiment results show that the EMC2A module significantly improves the performance of the model by using only a few parameters and appropriate cross-channel interactions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1053418532,
        "newsscientist":0.1467806075,
        "technologyreview":0.2055291699,
        "venturebeat":0.1761837201,
        "wired":0.152082484,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01836v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659501112000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13798v2",
        "predicted_newsworthiness":0.4371992217,
        "title":"Look at Adjacent Frames: Video Anomaly Detection without Offline Training",
        "summary":"We propose a solution to detect anomalous events in videos without the need to train a model offline. Specifically, our solution is based on a randomly-initialized multilayer perceptron that is optimized online to reconstruct video frames, pixel-by-pixel, from their frequency information. Based on the information shifts between adjacent frames, an incremental learner is used to update parameters of the multilayer perceptron after observing each frame, thus allowing to detect anomalous events along the video stream. Traditional solutions that require no offline training are limited to operating on videos with only a few abnormal frames. Our solution breaks this limit and achieves strong performance on benchmark datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1158047914,
        "newsscientist":0.167956848,
        "technologyreview":0.2380225622,
        "venturebeat":0.2184886335,
        "wired":0.1959236285,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13798v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658956738000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12966v1",
        "predicted_newsworthiness":0.4371237734,
        "title":"Nondestructive Quality Control in Powder Metallurgy using Hyperspectral Imaging",
        "summary":"Measuring the purity in the metal powder is critical for preserving the quality of additive manufacturing products. Contamination is one of the most headache problems which can be caused by multiple reasons and lead to the as-built components cracking and malfunctions. Existing methods for metallurgical condition assessment are mostly time-consuming and mainly focus on the physical integrity of structure rather than material composition. Through capturing spectral data from a wide frequency range along with the spatial information, hyperspectral imaging (HSI) can detect minor differences in terms of temperature, moisture and chemical composition. Therefore, HSI can provide a unique way to tackle this challenge. In this paper, with the use of a near-infrared HSI camera, applications of HSI for the non-destructive inspection of metal powders are introduced. Technical assumptions and solutions on three step-by-step case studies are presented in detail, including powder characterization, contamination detection, and band selection analysis. Experimental results have fully demonstrated the great potential of HSI and related AI techniques for NDT of powder metallurgy, especially the potential to satisfy the industrial manufacturing environment.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0682776803,
        "newsscientist":0.1267556388,
        "technologyreview":0.1290858761,
        "venturebeat":0.0982807807,
        "wired":0.0868002495,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12966v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658848835000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12978v1",
        "predicted_newsworthiness":0.4366473005,
        "title":"Tracking Every Thing in the Wild",
        "summary":"Current multi-category Multiple Object Tracking (MOT) metrics use class labels to group tracking results for per-class evaluation. Similarly, MOT methods typically only associate objects with the same class predictions. These two prevalent strategies in MOT implicitly assume that the classification performance is near-perfect. However, this is far from the case in recent large-scale MOT datasets, which contain large numbers of classes with many rare or semantically similar categories. Therefore, the resulting inaccurate classification leads to sub-optimal tracking and inadequate benchmarking of trackers. We address these issues by disentangling classification from tracking. We introduce a new metric, Track Every Thing Accuracy (TETA), breaking tracking measurement into three sub-factors: localization, association, and classification, allowing comprehensive benchmarking of tracking performance even under inaccurate classification. TETA also deals with the challenging incomplete annotation problem in large-scale tracking datasets. We further introduce a Track Every Thing tracker (TETer), that performs association using Class Exemplar Matching (CEM). Our experiments show that TETA evaluates trackers more comprehensively, and TETer achieves significant improvements on the challenging large-scale datasets BDD100K and TAO compared to the state-of-the-art.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0956324586,
        "newsscientist":0.1487414108,
        "technologyreview":0.2165038454,
        "venturebeat":0.2177867966,
        "wired":0.1855296074,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12978v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658849839000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14370v1",
        "predicted_newsworthiness":0.4360800186,
        "title":"Improving Few-shot News Recommendation via Cross-lingual Transfer",
        "summary":"The cold-start problem has been commonly recognized in recommendation systems and studied by following a general idea to leverage the abundant interaction records of warm users to infer the preference of cold users. However, the performance of these solutions is limited by the amount of records available from warm users to use. Thus, building a recommendation system based on few interaction records from a few users still remains a challenging problem for unpopular or early-stage recommendation platforms. This paper focuses on solving the few-shot recommendation problem for news recommendation based on two observations. First, news at different platforms (even in different languages) may share similar topics. Second, the user preference over these topics is transferable across different platforms. Therefore, we propose to solve the few-shot news recommendation problem by transferring the user-news preference from a rich source domain to a low-resource target domain. To bridge two domains in different languages without any overlapping users and news, we propose a novel unsupervised cross-lingual transfer model as the news encoder that aligns semantically similar news in two domains. A user encoder is constructed on top of the aligned news encoding and transfers the user preference from the source to the target domain. Experimental results on two real-world news recommendation datasets show the superior performance of our proposed method on addressing few-shot news recommendation, comparing to the state-of-the-art baselines.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.124705518,
        "newsscientist":0.1194743438,
        "technologyreview":0.2001180491,
        "venturebeat":0.2233244521,
        "wired":0.2012827583,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14370v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.ai"
        ],
        "published":1659039793000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.14476v1",
        "predicted_newsworthiness":0.435477392,
        "title":"Centrality and Consistency: Two-Stage Clean Samples Identification for Learning with Instance-Dependent Noisy Labels",
        "summary":"Deep models trained with noisy labels are prone to over-fitting and struggle in generalization. Most existing solutions are based on an ideal assumption that the label noise is class-conditional, i.e., instances of the same class share the same noise model, and are independent of features. While in practice, the real-world noise patterns are usually more fine-grained as instance-dependent ones, which poses a big challenge, especially in the presence of inter-class imbalance. In this paper, we propose a two-stage clean samples identification method to address the aforementioned challenge. First, we employ a class-level feature clustering procedure for the early identification of clean samples that are near the class-wise prediction centers. Notably, we address the class imbalance problem by aggregating rare classes according to their prediction entropy. Second, for the remaining clean samples that are close to the ground truth class boundary (usually mixed with the samples with instance-dependent noises), we propose a novel consistency-based classification method that identifies them using the consistency of two classifier heads: the higher the consistency, the larger the probability that a sample is clean. Extensive experiments on several challenging benchmarks demonstrate the superior performance of our method against the state-of-the-art.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0877758647,
        "newsscientist":0.1383117788,
        "technologyreview":0.2320649464,
        "venturebeat":0.2137650556,
        "wired":0.1411727485,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14476v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659070497000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11942v1",
        "predicted_newsworthiness":0.4353282198,
        "title":"Scalable Fiducial Tag Localization on a 3D Prior Map via Graph-Theoretic Global Tag-Map Registration",
        "summary":"This paper presents an accurate and scalable method for fiducial tag localization on a 3D prior environmental map. The proposed method comprises three steps: 1) visual odometry-based landmark SLAM for estimating the relative poses between fiducial tags, 2) geometrical matching-based global tag-map registration via maximum clique finding, and 3) tag pose refinement based on direct camera-map alignment with normalized information distance. Through simulation-based evaluations, the proposed method achieved a 98 \\% global tag-map registration success rate and an average tag pose estimation accuracy of a few centimeters. Experimental results in a real environment demonstrated that it enables to localize over 110 fiducial tags placed in an environment in 25 minutes for data recording and post-processing.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0853064137,
        "newsscientist":0.1237252523,
        "technologyreview":0.1614223187,
        "venturebeat":0.1641272588,
        "wired":0.1575096742,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11942v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658733530000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00874v1",
        "predicted_newsworthiness":0.4344599922,
        "title":"S$^2$Contact: Graph-based Network for 3D Hand-Object Contact Estimation with Semi-Supervised Learning",
        "summary":"Despite the recent efforts in accurate 3D annotations in hand and object datasets, there still exist gaps in 3D hand and object reconstructions. Existing works leverage contact maps to refine inaccurate hand-object pose estimations and generate grasps given object models. However, they require explicit 3D supervision which is seldom available and therefore, are limited to constrained settings, e.g., where thermal cameras observe residual heat left on manipulated objects. In this paper, we propose a novel semi-supervised framework that allows us to learn contact from monocular images. Specifically, we leverage visual and geometric consistency constraints in large-scale datasets for generating pseudo-labels in semi-supervised learning and propose an efficient graph-based network to infer contact. Our semi-supervised learning framework achieves a favourable improvement over the existing supervised learning methods trained on data with `limited' annotations. Notably, our proposed model is able to achieve superior results with less than half the network parameters and memory access cost when compared with the commonly-used PointNet-based approach. We show benefits from using a contact map that rules hand-object interactions to produce more accurate reconstructions. We further demonstrate that training with pseudo-labels can extend contact map estimations to out-of-domain objects and generalise better across multiple datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0902180496,
        "newsscientist":0.1383497892,
        "technologyreview":0.2015325961,
        "venturebeat":0.1774395853,
        "wired":0.1567534418,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00874v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659362723000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00815v1",
        "predicted_newsworthiness":0.4342962812,
        "title":"Dynamic Batch Adaptation",
        "summary":"Current deep learning adaptive optimizer methods adjust the step magnitude of parameter updates by altering the effective learning rate used by each parameter. Motivated by the known inverse relation between batch size and learning rate on update step magnitudes, we introduce a novel training procedure that dynamically decides the dimension and the composition of the current update step. Our procedure, Dynamic Batch Adaptation (DBA) analyzes the gradients of every sample and selects the subset that best improves certain metrics such as gradient variance for each layer of the network. We present results showing DBA significantly improves the speed of model convergence. Additionally, we find that DBA produces an increased improvement over standard optimizers when used in data scarce conditions where, in addition to convergence speed, it also significantly improves model generalization, managing to train a network with a single fully connected hidden layer using only 1% of the MNIST dataset to reach 97.79% test accuracy. In an even more extreme scenario, it manages to reach 97.44% test accuracy using only 10 samples per class. These results represent a relative error rate reduction of 81.78% and 88.07% respectively, compared to the standard optimizers, Stochastic Gradient Descent (SGD) and Adam.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0927006863,
        "newsscientist":0.1469181111,
        "technologyreview":0.2777748698,
        "venturebeat":0.2625737801,
        "wired":0.1641603971,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00815v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cv"
        ],
        "published":1659358329000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14251v1",
        "predicted_newsworthiness":0.4342195046,
        "title":"Measuring Causal Effects of Data Statistics on Language Model's `Factual' Predictions",
        "summary":"Large amounts of training data are one of the major reasons for the high performance of state-of-the-art NLP models. But what exactly in the training data causes a model to make a certain prediction? We seek to answer this question by providing a language for describing how training data influences predictions, through a causal framework. Importantly, our framework bypasses the need to retrain expensive models and allows us to estimate causal effects based on observational data alone. Addressing the problem of extracting factual knowledge from pretrained language models (PLMs), we focus on simple data statistics such as co-occurrence counts and show that these statistics do influence the predictions of PLMs, suggesting that such models rely on shallow heuristics. Our causal framework and our results demonstrate the importance of studying datasets and the benefits of causality for understanding NLP models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1283342828,
        "newsscientist":0.1494560628,
        "technologyreview":0.248865491,
        "venturebeat":0.2399302201,
        "wired":0.1816839718,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14251v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659029784000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.13988v1",
        "predicted_newsworthiness":0.4336518747,
        "title":"Sequence to sequence pretraining for a less-resourced Slovenian language",
        "summary":"Large pretrained language models have recently conquered the area of natural language processing. As an alternative to predominant masked language modelling introduced in BERT, the T5 model has introduced a more general training objective, namely sequence to sequence transformation, which includes masked language model but more naturally fits text generation tasks such as machine translation, summarization, open-domain question answering, text simplification, dialogue systems, etc. The monolingual variants of T5 models have been limited to well-resourced languages, while the massively multilingual T5 model supports 101 languages. In contrast, we trained two different sized T5-type sequence to sequence models for morphologically rich Slovene language with much less resources and analyzed their behavior. Concerning classification tasks, the SloT5 models mostly lag behind the monolingual Slovene SloBERTa model but are to be considered for the generative tasks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.108567157,
        "newsscientist":0.1168795362,
        "technologyreview":0.1978011002,
        "venturebeat":0.2028811483,
        "wired":0.1455725507,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13988v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659002930000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.12033v1",
        "predicted_newsworthiness":0.4334106199,
        "title":"Contrastive Learning for Interactive Recommendation in Fashion",
        "summary":"Recommender systems and search are both indispensable in facilitating personalization and ease of browsing in online fashion platforms. However, the two tools often operate independently, failing to combine the strengths of recommender systems to accurately capture user tastes with search systems' ability to process user queries. We propose a novel remedy to this problem by automatically recommending personalized fashion items based on a user-provided text request. Our proposed model, WhisperLite, uses contrastive learning to capture user intent from natural language text and improves the recommendation quality of fashion products. WhisperLite combines the strength of CLIP embeddings with additional neural network layers for personalization, and is trained using a composite loss function based on binary cross entropy and contrastive loss. The model demonstrates a significant improvement in offline recommendation retrieval metrics when tested on a real-world dataset collected from an online retail fashion store, as well as widely used open-source datasets in different e-commerce domains, such as restaurants, movies and TV shows, clothing and shoe reviews. We additionally conduct a user study that captures user judgements on the relevance of the model's recommended items, confirming the relevancy of WhisperLite's recommendations in an online setting.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1341701774,
        "newsscientist":0.154834618,
        "technologyreview":0.2501654977,
        "venturebeat":0.2741344685,
        "wired":0.225499185,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12033v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1658744156000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.00173v1",
        "predicted_newsworthiness":0.4332180583,
        "title":"A Survey on Masked Autoencoder for Self-supervised Learning in Vision and Beyond",
        "summary":"Masked autoencoders are scalable vision learners, as the title of MAE \\cite{he2022masked}, which suggests that self-supervised learning (SSL) in vision might undertake a similar trajectory as in NLP. Specifically, generative pretext tasks with the masked prediction (e.g., BERT) have become a de facto standard SSL practice in NLP. By contrast, early attempts at generative methods in vision have been buried by their discriminative counterparts (like contrastive learning); however, the success of mask image modeling has revived the masking autoencoder (often termed denoising autoencoder in the past). As a milestone to bridge the gap with BERT in NLP, masked autoencoder has attracted unprecedented attention for SSL in vision and beyond. This work conducts a comprehensive survey of masked autoencoders to shed insight on a promising direction of SSL. As the first to review SSL with masked autoencoders, this work focuses on its application in vision by discussing its historical developments, recent progress, and implications for diverse applications.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1189876006,
        "newsscientist":0.1664987287,
        "technologyreview":0.2776139786,
        "venturebeat":0.2533965311,
        "wired":0.1917849353,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00173v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1659175168000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02029v1",
        "predicted_newsworthiness":0.4327611477,
        "title":"Supervised and Reinforcement Learning from Observations in Reconnaissance Blind Chess",
        "summary":"In this work, we adapt a training approach inspired by the original AlphaGo system to play the imperfect information game of Reconnaissance Blind Chess. Using only the observations instead of a full description of the game state, we first train a supervised agent on publicly available game records. Next, we increase the performance of the agent through self-play with the on-policy reinforcement learning algorithm Proximal Policy Optimization. We do not use any search to avoid problems caused by the partial observability of game states and only use the policy network to generate moves when playing. With this approach, we achieve an ELO of 1330 on the RBC leaderboard, which places our agent at position 27 at the time of this writing. We see that self-play significantly improves performance and that the agent plays acceptably well without search and without making assumptions about the true game state.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1157130292,
        "newsscientist":0.1638650606,
        "technologyreview":0.2719974105,
        "venturebeat":0.2433651112,
        "wired":0.1914815708,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02029v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1659531019000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.11067v2",
        "predicted_newsworthiness":0.4324901269,
        "title":"Latent Space Unsupervised Semantic Segmentation",
        "summary":"The development of compact and energy-efficient wearable sensors has led to an increase in the availability of biosignals. To analyze these continuously recorded, and often multidimensional, time series at scale, being able to conduct meaningful unsupervised data segmentation is an auspicious target. A common way to achieve this is to identify change-points within the time series as the segmentation basis. However, traditional change-point detection algorithms often come with drawbacks, limiting their real-world applicability. Notably, they generally rely on the complete time series to be available and thus cannot be used for real-time applications. Another common limitation is that they poorly (or cannot) handle the segmentation of multidimensional time series. Consequently, the main contribution of this work is to propose a novel unsupervised segmentation algorithm for multidimensional time series named Latent Space Unsupervised Semantic Segmentation (LS-USS), which was designed to work easily with both online and batch data. When comparing LS-USS against other state-of-the-art change-point detection algorithms on a variety of real-world datasets, in both the offline and real-time setting, LS-USS systematically achieves on par or better performances.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.106410595,
        "newsscientist":0.1596284914,
        "technologyreview":0.1885044505,
        "venturebeat":0.1991295526,
        "wired":0.155702708,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11067v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658495502000,
        "published_hr":"Jul 22, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11995v2",
        "predicted_newsworthiness":0.4324040609,
        "title":"3D Siamese Transformer Network for Single Object Tracking on Point Clouds",
        "summary":"Siamese network based trackers formulate 3D single object tracking as cross-correlation learning between point features of a template and a search area. Due to the large appearance variation between the template and search area during tracking, how to learn the robust cross correlation between them for identifying the potential target in the search area is still a challenging problem. In this paper, we explicitly use Transformer to form a 3D Siamese Transformer network for learning robust cross correlation between the template and the search area of point clouds. Specifically, we develop a Siamese point Transformer network to learn shape context information of the target. Its encoder uses self-attention to capture non-local information of point clouds to characterize the shape information of the object, and the decoder utilizes cross-attention to upsample discriminative point features. After that, we develop an iterative coarse-to-fine correlation network to learn the robust cross correlation between the template and the search area. It formulates the cross-feature augmentation to associate the template with the potential target in the search area via cross attention. To further enhance the potential target, it employs the ego-feature augmentation that applies self-attention to the local k-NN graph of the feature space to aggregate target features. Experiments on the KITTI, nuScenes, and Waymo datasets show that our method achieves state-of-the-art performance on the 3D single object tracking task.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0781193466,
        "newsscientist":0.1369726162,
        "technologyreview":0.2097806163,
        "venturebeat":0.2030043065,
        "wired":0.162229665,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11995v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658740110000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14799v1",
        "predicted_newsworthiness":0.4322935025,
        "title":"A Hybrid Complex-valued Neural Network Framework with Applications to Electroencephalogram (EEG)",
        "summary":"In this article, we present a new EEG signal classification framework by integrating the complex-valued and real-valued Convolutional Neural Network(CNN) with discrete Fourier transform (DFT). The proposed neural network architecture consists of one complex-valued convolutional layer, two real-valued convolutional layers, and three fully connected layers. Our method can efficiently utilize the phase information contained in the DFT. We validate our approach using two simulated EEG signals and a benchmark data set and compare it with two widely used frameworks. Our method drastically reduces the number of parameters used and improves accuracy when compared with the existing methods in classifying benchmark data sets, and significantly improves performance in classifying simulated EEG signals.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0947737416,
        "newsscientist":0.1555935351,
        "technologyreview":0.2077099688,
        "venturebeat":0.1804287315,
        "wired":0.136534725,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14799v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658969467000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11586v1",
        "predicted_newsworthiness":0.4319641251,
        "title":"Path Tracing in 2D, 3D, and Physicalized Networks",
        "summary":"It is common to advise against using 3D to visualize abstract data such as networks, however Ware and Mitchell's 2008 study showed that path tracing in a network is less error prone in 3D than in 2D. It is unclear, however, if 3D retains its advantage when the 2D presentation of a network is improved using edge-routing, and when simple interaction techniques for exploring the network are available. We address this with two studies of path tracing under new conditions. The first study was preregistered, involved 34 users, and compared 2D and 3D layouts that the user could rotate and move in virtual reality with a handheld controller. Error rates were lower in 3D than in 2D, despite the use of edge-routing in 2D and the use of mouse-driven interactive highlighting of edges. The second study involved 12 users and investigated data physicalization, comparing 3D layouts in virtual reality versus physical 3D printouts of networks augmented with a Microsoft HoloLens headset. No difference was found in error rate, but users performed a variety of actions with their fingers in the physical condition which can inform new interaction techniques.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1577300365,
        "newsscientist":0.1933344695,
        "technologyreview":0.2550832823,
        "venturebeat":0.309462074,
        "wired":0.2626580682,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11586v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1658604953000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.01021v1",
        "predicted_newsworthiness":0.4319567566,
        "title":"Learning Multi-Object Symbols for Manipulation with Attentive Deep Effect Predictors",
        "summary":"In this paper, we propose a concept learning architecture that enables a robot to build symbols through self-exploration by interacting with a varying number of objects. Our aim is to allow a robot to learn concepts without constraints, such as a fixed number of interacted objects or pre-defined symbolic structures. As such, the sought architecture should be able to build symbols for objects such as single objects that can be grasped, object stacks that cannot be grasped together, or other composite dynamic structures. Towards this end, we propose a novel architecture, a self-attentive predictive encoder-decoder network with binary activation layers. We show the validity of the proposed network through a robotic manipulation setup involving a varying number of rigid objects. The continuous sensorimotor experience of the robot is used by the proposed network to form effect predictors and symbolic structures that describe the interaction of the robot in a discrete way. We showed that the robot acquired reasoning capabilities to encode interaction dynamics of a varying number of objects in different configurations using the discovered symbols. For example, the robot could reason that (possible multiple numbers of) objects on top of another object would move together if the object below is moved by the robot. We also showed that the discovered symbols can be used for planning to reach goals by training a higher-level neural network that makes pure symbolic reasoning.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1019556497,
        "newsscientist":0.192583795,
        "technologyreview":0.3002903752,
        "venturebeat":0.2457186112,
        "wired":0.2024163066,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01021v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659376439000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00929v2",
        "predicted_newsworthiness":0.4316788571,
        "title":"giMLPs: Gate with Inhibition Mechanism in MLPs",
        "summary":"This paper presents a new model architecture, gate with inhibition MLP (giMLP).The gate with inhibition on CycleMLP (gi-CycleMLP) can produce equal performance on the ImageNet classification task, and it also improves the BERT, Roberta, and DeBERTaV3 models depending on two novel techniques. The first is the gating MLP, where matrix multiplications between the MLP and the trunk Attention input in further adjust models' adaptation. The second is inhibition which inhibits or enhances the branch adjustment, and with the inhibition levels increasing, it offers models more muscular features restriction. We show that the giCycleMLP with a lower inhibition level can be competitive with the original CycleMLP in terms of ImageNet classification accuracy. In addition, we also show through a comprehensive empirical study that these techniques significantly improve the performance of fine-tuning NLU downstream tasks. As for the gate with inhibition MLPs on DeBERTa (giDeBERTa) fine-tuning, we find it can achieve appealing results on most parts of NLU tasks without any extra pretraining again. We also find that with the use of Gate With Inhibition, the activation function should have a short and smooth negative tail, with which the unimportant features or the features that hurt models can be moderately inhibited. The experiments on ImageNet and twelve language downstream tasks demonstrate the effectiveness of Gate With Inhibition, both for image classification and for enhancing the capacity of nature language fine-tuning without any extra pretraining.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0875982683,
        "newsscientist":0.1487224792,
        "technologyreview":0.2561572219,
        "venturebeat":0.224971131,
        "wired":0.1573733113,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00929v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659367431000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.12783v1",
        "predicted_newsworthiness":0.4314898661,
        "title":"Equivariant and Invariant Grounding for Video Question Answering",
        "summary":"Video Question Answering (VideoQA) is the task of answering the natural language questions about a video. Producing an answer requires understanding the interplay across visual scenes in video and linguistic semantics in question. However, most leading VideoQA models work as black boxes, which make the visual-linguistic alignment behind the answering process obscure. Such black-box nature calls for visual explainability that reveals ``What part of the video should the model look at to answer the question?''. Only a few works present the visual explanations in a post-hoc fashion, which emulates the target model's answering process via an additional method. Nonetheless, the emulation struggles to faithfully exhibit the visual-linguistic alignment during answering. Instead of post-hoc explainability, we focus on intrinsic interpretability to make the answering process transparent. At its core is grounding the question-critical cues as the causal scene to yield answers, while rolling out the question-irrelevant information as the environment scene. Taking a causal look at VideoQA, we devise a self-interpretable framework, Equivariant and Invariant Grounding for Interpretable VideoQA (EIGV). Specifically, the equivariant grounding encourages the answering to be sensitive to the semantic changes in the causal scene and question; in contrast, the invariant grounding enforces the answering to be insensitive to the changes in the environment scene. By imposing them on the answering process, EIGV is able to distinguish the causal scene from the environment information, and explicitly present the visual-linguistic alignment. Extensive experiments on three benchmark datasets justify the superiority of EIGV in terms of accuracy and visual interpretability over the leading baselines.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1205011733,
        "newsscientist":0.1544062804,
        "technologyreview":0.2453602545,
        "venturebeat":0.221387791,
        "wired":0.1798017544,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12783v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658829662000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11584v1",
        "predicted_newsworthiness":0.4314870287,
        "title":"Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning",
        "summary":"Practising and honing skills forms a fundamental component of how humans learn, yet artificial agents are rarely specifically trained to perform them. Instead, they are usually trained end-to-end, with the hope being that useful skills will be implicitly learned in order to maximise discounted return of some extrinsic reward function. In this paper, we investigate how skills can be incorporated into the training of reinforcement learning (RL) agents in complex environments with large state-action spaces and sparse rewards. To this end, we created SkillHack, a benchmark of tasks and associated skills based on the game of NetHack. We evaluate a number of baselines on this benchmark, as well as our own novel skill-based method Hierarchical Kickstarting (HKS), which is shown to outperform all other evaluated methods. Our experiments show that learning with a prior knowledge of useful skills can significantly improve the performance of agents on complex problems. We ultimately argue that utilising predefined skills provides a useful inductive bias for RL problems, especially those with large state-action spaces and sparse rewards.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1294567265,
        "newsscientist":0.1895184257,
        "technologyreview":0.320968289,
        "venturebeat":0.2872883094,
        "wired":0.2197255207,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11584v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658604209000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01561v1",
        "predicted_newsworthiness":0.4313206524,
        "title":"Lost in Space Marking",
        "summary":"We look at a decision taken early in training a subword tokenizer, namely whether it should be the word-initial token that carries a special mark, or the word-final one. Based on surface-level considerations of efficiency and cohesion, as well as morphological coverage, we find that a Unigram LM tokenizer trained on pre-tokenized English text is better off marking the word-initial token, while one trained on raw text benefits from marking word ends. Our findings generalize across domains.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0801817167,
        "newsscientist":0.1141914588,
        "technologyreview":0.1772580597,
        "venturebeat":0.1831859943,
        "wired":0.1429862934,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01561v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659456451000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.13005v1",
        "predicted_newsworthiness":0.4311763168,
        "title":"Hansel: A Chinese Few-Shot and Zero-Shot Entity Linking Benchmark",
        "summary":"Modern Entity Linking (EL) systems entrench a popularity bias, yet there is no dataset focusing on tail and emerging entities in languages other than English. We present Hansel, a new benchmark in Chinese that fills the vacancy of non-English few-shot and zero-shot EL challenges. The test set of Hansel is human annotated and reviewed, created with a novel method for collecting zero-shot EL datasets. It covers 10K diverse documents in news, social media posts and other web articles, with Wikidata as its target Knowledge Base. We demonstrate that the existing state-of-the-art EL system performs poorly on Hansel (R@1 of 36.6% on Few-Shot). We then establish a strong baseline that scores a R@1 of 46.2% on Few-Shot and 76.6% on Zero-Shot on our dataset. We also show that our baseline achieves competitive results on TAC-KBP2015 Chinese Entity Linking task.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1217569991,
        "newsscientist":0.1238702896,
        "technologyreview":0.2037140584,
        "venturebeat":0.1946482058,
        "wired":0.182231074,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13005v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658851747000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00627v1",
        "predicted_newsworthiness":0.4309989136,
        "title":"A Rotation Meanout Network with Invariance for Dermoscopy Image Classification and Retrieval",
        "summary":"The computer-aided diagnosis (CAD) system can provide a reference basis for the clinical diagnosis of skin diseases. Convolutional neural networks (CNNs) can not only extract visual elements such as colors and shapes but also semantic features. As such they have made great improvements in many tasks of dermoscopy images. The imaging of dermoscopy has no main direction, indicating that there are a large number of skin lesion target rotations in the datasets. However, CNNs lack anti-rotation ability, which is bound to affect the feature extraction ability of CNNs. We propose a rotation meanout (RM) network to extract rotation invariance features from dermoscopy images. In RM, each set of rotated feature maps corresponds to a set of weight-sharing convolution outputs and they are fused using meanout operation to obtain the final feature maps. Through theoretical derivation, the proposed RM network is rotation-equivariant and can extract rotation-invariant features when being followed by the global average pooling (GAP) operation. The extracted rotation-invariant features can better represent the original data in classification and retrieval tasks for dermoscopy images. The proposed RM is a general operation, which does not change the network structure or increase any parameter, and can be flexibly embedded in any part of CNNs. Extensive experiments are conducted on a dermoscopy image dataset. The results show our method outperforms other anti-rotation methods and achieves great improvements in dermoscopy image classification and retrieval tasks, indicating the potential of rotation invariance in the field of dermoscopy images.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0798584382,
        "newsscientist":0.1361653199,
        "technologyreview":0.1990264823,
        "venturebeat":0.1710854524,
        "wired":0.1222129233,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00627v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659334552000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13514v1",
        "predicted_newsworthiness":0.4308910651,
        "title":"UNIMIB at TREC 2021 Clinical Trials Track",
        "summary":"This contribution summarizes the participation of the UNIMIB team to the TREC 2021 Clinical Trials Track. We have investigated the effect of different query representations combined with several retrieval models on the retrieval performance. First, we have implemented a neural re-ranking approach to study the effectiveness of dense text representations. Additionally, we have investigated the effectiveness of a novel decision-theoretic model for relevance estimation. Finally, both of the above relevance models have been compared with standard retrieval approaches. In particular, we combined a keyword extraction method with a standard retrieval process based on the BM25 model and a decision-theoretic relevance model that exploits the characteristics of this particular search task. The obtained results show that the proposed keyword extraction method improves 84% of the queries over the TREC's median NDCG@10 measure when combined with either traditional or decision-theoretic relevance models. Moreover, regarding RPEC@10, the employed decision-theoretic model improves 85% of the queries over the reported TREC's median value.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1369123622,
        "newsscientist":0.1567283886,
        "technologyreview":0.2290861171,
        "venturebeat":0.242904679,
        "wired":0.1685568538,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13514v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.cl"
        ],
        "published":1658929170000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.02169v1",
        "predicted_newsworthiness":0.4306996305,
        "title":"SpanDrop: Simple and Effective Counterfactual Learning for Long Sequences",
        "summary":"Distilling supervision signal from a long sequence to make predictions is a challenging task in machine learning, especially when not all elements in the input sequence contribute equally to the desired output. In this paper, we propose SpanDrop, a simple and effective data augmentation technique that helps models identify the true supervision signal in a long sequence with very few examples. By directly manipulating the input sequence, SpanDrop randomly ablates parts of the sequence at a time and ask the model to perform the same task to emulate counterfactual learning and achieve input attribution. Based on theoretical analysis of its properties, we also propose a variant of SpanDrop based on the beta-Bernoulli distribution, which yields diverse augmented sequences while providing a learning objective that is more consistent with the original dataset. We demonstrate the effectiveness of SpanDrop on a set of carefully designed toy tasks, as well as various natural language processing tasks that require reasoning over long sequences to arrive at the correct answer, and show that it helps models improve performance both when data is scarce and abundant.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1290291148,
        "newsscientist":0.1941656132,
        "technologyreview":0.2968918275,
        "venturebeat":0.2792197595,
        "wired":0.200232432,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02169v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cl"
        ],
        "published":1659542317000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11871v1",
        "predicted_newsworthiness":0.4306403232,
        "title":"Towards Complex Document Understanding By Discrete Reasoning",
        "summary":"Document Visual Question Answering (VQA) aims to understand visually-rich documents to answer questions in natural language, which is an emerging research topic for both Natural Language Processing and Computer Vision. In this work, we introduce a new Document VQA dataset, named TAT-DQA, which consists of 3,067 document pages comprising semi-structured table(s) and unstructured text as well as 16,558 question-answer pairs by extending the TAT-QA dataset. These documents are sampled from real-world financial reports and contain lots of numbers, which means discrete reasoning capability is demanded to answer questions on this dataset. Based on TAT-DQA, we further develop a novel model named MHST that takes into account the information in multi-modalities, including text, layout and visual image, to intelligently address different types of questions with corresponding strategies, i.e., extraction or reasoning. Extensive experiments show that the MHST model significantly outperforms the baseline methods, demonstrating its effectiveness. However, the performance still lags far behind that of expert humans. We expect that our new TAT-DQA dataset would facilitate the research on deep understanding of visually-rich documents combining vision and language, especially for scenarios that require discrete reasoning. Also, we hope the proposed model would inspire researchers to design more advanced Document VQA models in future.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0965902897,
        "newsscientist":0.1325156628,
        "technologyreview":0.2345624729,
        "venturebeat":0.2345902347,
        "wired":0.1465322108,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11871v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658713399000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12504v1",
        "predicted_newsworthiness":0.4305940206,
        "title":"Unsupervised Speaker Diarization that is Agnostic to Language, Overlap-Aware, and Tuning Free",
        "summary":"Podcasts are conversational in nature and speaker changes are frequent -- requiring speaker diarization for content understanding. We propose an unsupervised technique for speaker diarization without relying on language-specific components. The algorithm is overlap-aware and does not require information about the number of speakers. Our approach shows 79% improvement on purity scores (34% on F-score) against the Google Cloud Platform solution on podcast data.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1261087457,
        "newsscientist":0.1322247023,
        "technologyreview":0.1959439011,
        "venturebeat":0.2063622157,
        "wired":0.2061120837,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12504v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658779634000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00817v1",
        "predicted_newsworthiness":0.4303265898,
        "title":"DSLA: Dynamic smooth label assignment for efficient anchor-free object detection",
        "summary":"Anchor-free detectors basically formulate object detection as dense classification and regression. For popular anchor-free detectors, it is common to introduce an individual prediction branch to estimate the quality of localization. The following inconsistencies are observed when we delve into the practices of classification and quality estimation. Firstly, for some adjacent samples which are assigned completely different labels, the trained model would produce similar classification scores. This violates the training objective and leads to performance degradation. Secondly, it is found that detected bounding boxes with higher confidences contrarily have smaller overlaps with the corresponding ground-truth. Accurately localized bounding boxes would be suppressed by less accurate ones in the Non-Maximum Suppression (NMS) procedure. To address the inconsistency problems, the Dynamic Smooth Label Assignment (DSLA) method is proposed. Based on the concept of centerness originally developed in FCOS, a smooth assignment strategy is proposed. The label is smoothed to a continuous value in [0, 1] to make a steady transition between positive and negative samples. Intersection-of-Union (IoU) is predicted dynamically during training and is coupled with the smoothed label. The dynamic smooth label is assigned to supervise the classification branch. Under such supervision, quality estimation branch is naturally merged into the classification branch, which simplifies the architecture of anchor-free detector. Comprehensive experiments are conducted on the MS COCO benchmark. It is demonstrated that, DSLA can significantly boost the detection accuracy by alleviating the above inconsistencies for anchor-free detectors. Our codes are released at https:\/\/github.com\/YonghaoHe\/DSLA.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.080016219,
        "newsscientist":0.1398574428,
        "technologyreview":0.2143736801,
        "venturebeat":0.1908103279,
        "wired":0.1626749044,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00817v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659358604000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12302v1",
        "predicted_newsworthiness":0.4302902024,
        "title":"Exploiting Diversity of Unlabeled Data for Label-Efficient Semi-Supervised Active Learning",
        "summary":"The availability of large labeled datasets is the key component for the success of deep learning. However, annotating labels on large datasets is generally time-consuming and expensive. Active learning is a research area that addresses the issues of expensive labeling by selecting the most important samples for labeling. Diversity-based sampling algorithms are known as integral components of representation-based approaches for active learning. In this paper, we introduce a new diversity-based initial dataset selection algorithm to select the most informative set of samples for initial labeling in the active learning setting. Self-supervised representation learning is used to consider the diversity of samples in the initial dataset selection algorithm. Also, we propose a novel active learning query strategy, which uses diversity-based sampling on consistency-based embeddings. By considering the consistency information with the diversity in the consistency-based embedding scheme, the proposed method could select more informative samples for labeling in the semi-supervised learning setting. Comparative experiments show that the proposed method achieves compelling results on CIFAR-10 and Caltech-101 datasets compared with previous active learning approaches by utilizing the diversity of unlabeled data.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0824324196,
        "newsscientist":0.1119122877,
        "technologyreview":0.1995045016,
        "venturebeat":0.1943520739,
        "wired":0.1210782273,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12302v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658765515000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14473v1",
        "predicted_newsworthiness":0.4298793459,
        "title":"Benchmarking Azerbaijani Neural Machine Translation",
        "summary":"Little research has been done on Neural Machine Translation (NMT) for Azerbaijani. In this paper, we benchmark the performance of Azerbaijani-English NMT systems on a range of techniques and datasets. We evaluate which segmentation techniques work best on Azerbaijani translation and benchmark the performance of Azerbaijani NMT models across several domains of text. Our results show that while Unigram segmentation improves NMT performance and Azerbaijani translation models scale better with dataset quality than quantity, cross-domain generalization remains a challenge",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1091587742,
        "newsscientist":0.1028160201,
        "technologyreview":0.178490325,
        "venturebeat":0.1732489585,
        "wired":0.1325814774,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14473v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659068983000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00223v1",
        "predicted_newsworthiness":0.4298717871,
        "title":"PolarMix: A General Data Augmentation Technique for LiDAR Point Clouds",
        "summary":"LiDAR point clouds, which are usually scanned by rotating LiDAR sensors continuously, capture precise geometry of the surrounding environment and are crucial to many autonomous detection and navigation tasks. Though many 3D deep architectures have been developed, efficient collection and annotation of large amounts of point clouds remain one major challenge in the analytic and understanding of point cloud data. This paper presents PolarMix, a point cloud augmentation technique that is simple and generic but can mitigate the data constraint effectively across different perception tasks and scenarios. PolarMix enriches point cloud distributions and preserves point cloud fidelity via two cross-scan augmentation strategies that cut, edit, and mix point clouds along the scanning direction. The first is scene-level swapping which exchanges point cloud sectors of two LiDAR scans that are cut along the azimuth axis. The second is instance-level rotation and paste which crops point instances from one LiDAR scan, rotates them by multiple angles (to create multiple copies), and paste the rotated point instances into other scans. Extensive experiments show that PolarMix achieves superior performance consistently across different perception tasks and scenarios. In addition, it can work as plug-and-play for various 3D deep architectures and also performs well for unsupervised domain adaptation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1091247525,
        "newsscientist":0.1653659385,
        "technologyreview":0.2388396318,
        "venturebeat":0.239554523,
        "wired":0.2029911871,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00223v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1659189139000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13532v1",
        "predicted_newsworthiness":0.4293763989,
        "title":"Contrastive Masked Autoencoders are Stronger Vision Learners",
        "summary":"Masked image modeling (MIM) has achieved promising results on various vision tasks. However, the limited discriminability of learned representation manifests there is still plenty to go for making a stronger vision learner. Towards this goal, we propose Contrastive Masked Autoencoders (CMAE), a new self-supervised pre-training method for learning more comprehensive and capable vision representations. By elaboratively unifying contrastive learning (CL) and masked image model (MIM) through novel designs, CMAE leverages their respective advantages and learns representations with both strong instance discriminability and local perceptibility. Specifically, CMAE consists of two branches where the online branch is an asymmetric encoder-decoder and the target branch is a momentum updated encoder. During training, the online encoder reconstructs original images from latent representations of masked images to learn holistic features. The target encoder, fed with the full images, enhances the feature discriminability via contrastive learning with its online counterpart. To make CL compatible with MIM, CMAE introduces two new components, i.e. pixel shift for generating plausible positive views and feature decoder for complementing features of contrastive pairs. Thanks to these novel designs, CMAE effectively improves the representation quality and transfer performance over its MIM counterpart. CMAE achieves the state-of-the-art performance on highly competitive benchmarks of image classification, semantic segmentation and object detection. Notably, CMAE-Base achieves $85.3\\%$ top-1 accuracy on ImageNet and $52.5\\%$ mIoU on ADE20k, surpassing previous best results by $0.7\\%$ and $1.8\\%$ respectively. Codes will be made publicly available.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0812446496,
        "newsscientist":0.1338347282,
        "technologyreview":0.2317457905,
        "venturebeat":0.2040391059,
        "wired":0.1526809554,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13532v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658930662000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11785v2",
        "predicted_newsworthiness":0.4293480619,
        "title":"Model-based Unbiased Learning to Rank",
        "summary":"Unbiased Learning to Rank~(ULTR) that learns to rank documents with biased user feedback data is a well-known challenge in information retrieval. Existing methods in unbiased learning to rank typically rely on click modeling or inverse propensity weighting~(IPW). Unfortunately, the search engines are faced with severe long-tail query distribution, where neither click modeling nor IPW can handle well. Click modeling suffers from data sparsity problem since the same query-document pair appears limited times on tail queries; IPW suffers from high variance problem since it is highly sensitive to small propensity score values. Therefore, a general debiasing framework that works well under tail queries is in desperate need. To address this problem, we propose a model-based unbiased learning-to-rank framework. Specifically, we develop a general context-aware user simulator to generate pseudo clicks for unobserved ranked lists to train rankers, which addresses the data sparsity problem. In addition, considering the discrepancy between pseudo clicks and actual clicks, we take the observation of a ranked list as the treatment variable and further incorporate inverse propensity weighting with pseudo labels in a doubly robust way. The derived bias and variance indicate that the proposed model-based method is more robust than existing methods. Finally, extensive experiments on benchmark datasets, including simulated datasets and real click logs, demonstrate that the proposed model-based method consistently performs outperforms state-of-the-art methods in various scenarios.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1158638831,
        "newsscientist":0.1197188719,
        "technologyreview":0.2051913918,
        "venturebeat":0.2164204297,
        "wired":0.1739295414,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11785v2",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1658687198000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.12678v1",
        "predicted_newsworthiness":0.4293479177,
        "title":"Analyzing Sharpness along GD Trajectory: Progressive Sharpening and Edge of Stability",
        "summary":"Recent findings (e.g., arXiv:2103.00065) demonstrate that modern neural networks trained by full-batch gradient descent typically enter a regime called Edge of Stability (EOS). In this regime, the sharpness, i.e., the maximum Hessian eigenvalue, first increases to the value 2\/(step size) (the progressive sharpening phase) and then oscillates around this value (the EOS phase). This paper aims to analyze the GD dynamics and the sharpness along the optimization trajectory. Our analysis naturally divides the GD trajectory into four phases depending on the change of the sharpness. We empirically identify the norm of output layer weight as an interesting indicator of sharpness dynamics. Based on this empirical observation, we attempt to theoretically and empirically explain the dynamics of various key quantities that lead to the change of sharpness in each phase of EOS. Moreover, based on certain assumptions, we provide a theoretical proof of the sharpness behavior in EOS regime in two-layer fully-connected linear neural networks. We also discuss some other empirical findings and the limitation of our theoretical results.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.072459493,
        "newsscientist":0.1097381925,
        "technologyreview":0.1974995433,
        "venturebeat":0.1689759514,
        "wired":0.1284512711,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12678v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658817478000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00609v1",
        "predicted_newsworthiness":0.4287936639,
        "title":"Accurate Polygonal Mapping of Buildings in Satellite Imagery",
        "summary":"This paper studies the problem of polygonal mapping of buildings by tackling the issue of mask reversibility that leads to a notable performance gap between the predicted masks and polygons from the learning-based methods. We addressed such an issue by exploiting the hierarchical supervision (of bottom-level vertices, mid-level line segments and the high-level regional masks) and proposed a novel interaction mechanism of feature embedding sourced from different levels of supervision signals to obtain reversible building masks for polygonal mapping of buildings. As a result, we show that the learned reversible building masks take all the merits of the advances of deep convolutional neural networks for high-performing polygonal mapping of buildings. In the experiments, we evaluated our method on the two public benchmarks of AICrowd and Inria. On the AICrowd dataset, our proposed method obtains unanimous improvements on the metrics of AP, APboundary and PoLiS. For the Inria dataset, our proposed method also obtains very competitive results on the metrics of IoU and Accuracy. The models and source code are available at https:\/\/github.com\/SarahwXU.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1313616379,
        "newsscientist":0.1681719914,
        "technologyreview":0.2525554819,
        "venturebeat":0.2163725904,
        "wired":0.1903249503,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00609v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659329695000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12152v1",
        "predicted_newsworthiness":0.4287387384,
        "title":"Deep Laparoscopic Stereo Matching with Transformers",
        "summary":"The self-attention mechanism, successfully employed with the transformer structure is shown promise in many computer vision tasks including image recognition, and object detection. Despite the surge, the use of the transformer for the problem of stereo matching remains relatively unexplored. In this paper, we comprehensively investigate the use of the transformer for the problem of stereo matching, especially for laparoscopic videos, and propose a new hybrid deep stereo matching framework (HybridStereoNet) that combines the best of the CNN and the transformer in a unified design. To be specific, we investigate several ways to introduce transformers to volumetric stereo matching pipelines by analyzing the loss landscape of the designs and in-domain\/cross-domain accuracy. Our analysis suggests that employing transformers for feature representation learning, while using CNNs for cost aggregation will lead to faster convergence, higher accuracy and better generalization than other options. Our extensive experiments on Sceneflow, SCARED2019 and dVPN datasets demonstrate the superior performance of our HybridStereoNet.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0946344721,
        "newsscientist":0.1488894523,
        "technologyreview":0.237572688,
        "venturebeat":0.2180402551,
        "wired":0.1811034349,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12152v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658753672000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13887v1",
        "predicted_newsworthiness":0.4280309482,
        "title":"Adaptive Second Order Coresets for Data-efficient Machine Learning",
        "summary":"Training machine learning models on massive datasets incurs substantial computational costs. To alleviate such costs, there has been a sustained effort to develop data-efficient training methods that can carefully select subsets of the training examples that generalize on par with the full training data. However, existing methods are limited in providing theoretical guarantees for the quality of the models trained on the extracted subsets, and may perform poorly in practice. We propose AdaCore, a method that leverages the geometry of the data to extract subsets of the training examples for efficient machine learning. The key idea behind our method is to dynamically approximate the curvature of the loss function via an exponentially-averaged estimate of the Hessian to select weighted subsets (coresets) that provide a close approximation of the full gradient preconditioned with the Hessian. We prove rigorous guarantees for the convergence of various first and second-order methods applied to the subsets chosen by AdaCore. Our extensive experiments show that AdaCore extracts coresets with higher quality compared to baselines and speeds up training of convex and non-convex machine learning models, such as logistic regression and neural networks, by over 2.9x over the full data and 4.5x over random subsets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0775841308,
        "newsscientist":0.1254080291,
        "technologyreview":0.2551346604,
        "venturebeat":0.2520633627,
        "wired":0.1614951774,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13887v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658986989000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13856v1",
        "predicted_newsworthiness":0.4279117358,
        "title":"Learning to Adapt Classifier for Imbalanced Semi-supervised Learning",
        "summary":"Pseudo-labeling has proven to be a promising semi-supervised learning (SSL) paradigm. Existing pseudo-labeling methods commonly assume that the class distributions of training data are balanced. However, such an assumption is far from realistic scenarios and existing pseudo-labeling methods suffer from severe performance degeneration in the context of class-imbalance. In this work, we investigate pseudo-labeling under imbalanced semi-supervised setups. The core idea is to automatically assimilate the training bias arising from class-imbalance, using a bias adaptive classifier that equips the original linear classifier with a bias attractor. The bias attractor is designed to be a light-weight residual network for adapting to the training bias. Specifically, the bias attractor is learned through a bi-level learning framework such that the bias adaptive classifier is able to fit imbalanced training data, while the linear classifier can give unbiased label prediction for each class. We conduct extensive experiments under various imbalanced semi-supervised setups, and the results demonstrate that our method can be applicable to different pseudo-labeling models and superior to the prior arts.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1177156827,
        "newsscientist":0.1467268307,
        "technologyreview":0.253900372,
        "venturebeat":0.2389312951,
        "wired":0.1724213752,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13856v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658974547000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13909v1",
        "predicted_newsworthiness":0.4274891592,
        "title":"Exploiting Negative Preference in Content-based Music Recommendation with Contrastive Learning",
        "summary":"Advanced music recommendation systems are being introduced along with the development of machine learning. However, it is essential to design a music recommendation system that can increase user satisfaction by understanding users' music tastes, not by the complexity of models. Although several studies related to music recommendation systems exploiting negative preferences have shown performance improvements, there was a lack of explanation on how they led to better recommendations. In this work, we analyze the role of negative preference in users' music tastes by comparing music recommendation models with contrastive learning exploiting preference (CLEP) but with three different training strategies - exploiting preferences of both positive and negative (CLEP-PN), positive only (CLEP-P), and negative only (CLEP-N). We evaluate the effectiveness of the negative preference by validating each system with a small amount of personalized data obtained via survey and further illuminate the possibility of exploiting negative preference in music recommendations. Our experimental results show that CLEP-N outperforms the other two in accuracy and false positive rate. Furthermore, the proposed training strategies produced a consistent tendency regardless of different types of front-end musical feature extractors, proving the stability of the proposed method.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1308862864,
        "newsscientist":0.160424882,
        "technologyreview":0.2315446819,
        "venturebeat":0.2455191082,
        "wired":0.2039505404,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13909v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.lg"
        ],
        "published":1658991768000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.00439v1",
        "predicted_newsworthiness":0.4274499068,
        "title":"Design What You Desire: Icon Generation from Orthogonal Application and Theme Labels",
        "summary":"Generative adversarial networks (GANs) have been trained to be professional artists able to create stunning artworks such as face generation and image style transfer. In this paper, we focus on a realistic business scenario: automated generation of customizable icons given desired mobile applications and theme styles. We first introduce a theme-application icon dataset, termed AppIcon, where each icon has two orthogonal theme and app labels. By investigating a strong baseline StyleGAN2, we observe mode collapse caused by the entanglement of the orthogonal labels. To solve this challenge, we propose IconGAN composed of a conditional generator and dual discriminators with orthogonal augmentations, and a contrastive feature disentanglement strategy is further designed to regularize the feature space of the two discriminators. Compared with other approaches, IconGAN indicates a superior advantage on the AppIcon benchmark. Further analysis also justifies the effectiveness of disentangling app and theme representations. Our project will be released at: https:\/\/github.com\/architect-road\/IconGAN.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1077159905,
        "newsscientist":0.1577482984,
        "technologyreview":0.2560558227,
        "venturebeat":0.244396635,
        "wired":0.2195293001,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00439v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659277124000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11862v1",
        "predicted_newsworthiness":0.426996683,
        "title":"Improving Bot Response Contradiction Detection via Utterance Rewriting",
        "summary":"Though chatbots based on large neural models can often produce fluent responses in open domain conversations, one salient error type is contradiction or inconsistency with the preceding conversation turns. Previous work has treated contradiction detection in bot responses as a task similar to natural language inference, e.g., detect the contradiction between a pair of bot utterances. However, utterances in conversations may contain co-references or ellipsis, and using these utterances as is may not always be sufficient for identifying contradictions. This work aims to improve the contradiction detection via rewriting all bot utterances to restore antecedents and ellipsis. We curated a new dataset for utterance rewriting and built a rewriting model on it. We empirically demonstrate that this model can produce satisfactory rewrites to make bot utterances more complete. Furthermore, using rewritten utterances improves contradiction detection performance significantly, e.g., the AUPR and joint accuracy scores (detecting contradiction along with evidence) increase by 6.5% and 4.5% (absolute increase), respectively.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1280980462,
        "newsscientist":0.1499139336,
        "technologyreview":0.2648992357,
        "venturebeat":0.2590993778,
        "wired":0.2203312166,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11862v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1658710470000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.01482v1",
        "predicted_newsworthiness":0.4267424338,
        "title":"Folding Knots Using a Team of Aerial Robots",
        "summary":"From ancient times, humans have been using cables and ropes to tie, carry, and manipulate objects by folding knots. However, automating knot folding is challenging because it requires dexterity to move a cable over and under itself. In this paper, we propose a method to fold knots in midair using a team of aerial vehicles. We take advantage of the fact that vehicles are able to fly in between cable segments without any re-grasping. So the team grasps the cable from the floor, and releases it once the knot is folded. Based on a composition of catenary curves, we simplify the complexity of dealing with an infinite-dimensional configuration space of the cable, and formally propose a new knot representation. Such representation allows us to design a trajectory that can be used to fold knots using a leader-follower approach. We show that our method works for different types of knots in simulations. Additionally, we show that our solution is also computationally efficient and can be executed in real-time.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0916883004,
        "newsscientist":0.1747515985,
        "technologyreview":0.2089339727,
        "venturebeat":0.1768101782,
        "wired":0.2146008218,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01482v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659450690000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01265v1",
        "predicted_newsworthiness":0.4267397678,
        "title":"Explicit Use of Fourier Spectrum in Generative Adversarial Networks",
        "summary":"Generative Adversarial Networks have got the researchers' attention due to their state-of-the-art performance in generating new images with only a dataset of the target distribution. It has been shown that there is a dissimilarity between the spectrum of authentic images and fake ones. Since the Fourier transform is a bijective mapping, saying that the model has a significant problem in learning the original distribution is a fair conclusion. In this work, we investigate the possible reasons for the mentioned drawback in the architecture and mathematical theory of the current GANs. Then we propose a new model to reduce the discrepancies between the spectrum of the actual and fake images. To that end, we design a brand new architecture for the frequency domain using the blueprint of geometric deep learning. Then, we experimentally show promising improvements in the quality of the generated images by considering the Fourier domain representation of the original data as a principal feature in the training process.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1030511899,
        "newsscientist":0.1663920243,
        "technologyreview":0.2594468257,
        "venturebeat":0.206999819,
        "wired":0.1789841647,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01265v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659421604000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12100v1",
        "predicted_newsworthiness":0.4266738141,
        "title":"IGFormer: Interaction Graph Transformer for Skeleton-based Human Interaction Recognition",
        "summary":"Human interaction recognition is very important in many applications. One crucial cue in recognizing an interaction is the interactive body parts. In this work, we propose a novel Interaction Graph Transformer (IGFormer) network for skeleton-based interaction recognition via modeling the interactive body parts as graphs. More specifically, the proposed IGFormer constructs interaction graphs according to the semantic and distance correlations between the interactive body parts, and enhances the representation of each person by aggregating the information of the interactive body parts based on the learned graphs. Furthermore, we propose a Semantic Partition Module to transform each human skeleton sequence into a Body-Part-Time sequence to better capture the spatial and temporal information of the skeleton sequence for learning the graphs. Extensive experiments on three benchmark datasets demonstrate that our model outperforms the state-of-the-art with a significant margin.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0960645761,
        "newsscientist":0.142822371,
        "technologyreview":0.178364021,
        "venturebeat":0.163591046,
        "wired":0.1397763465,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12100v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658751075000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11455v1",
        "predicted_newsworthiness":0.4266455204,
        "title":"UC-OWOD: Unknown-Classified Open World Object Detection",
        "summary":"Open World Object Detection (OWOD) is a challenging computer vision problem that requires detecting unknown objects and gradually learning the identified unknown classes. However, it cannot distinguish unknown instances as multiple unknown classes. In this work, we propose a novel OWOD problem called Unknown-Classified Open World Object Detection (UC-OWOD). UC-OWOD aims to detect unknown instances and classify them into different unknown classes. Besides, we formulate the problem and devise a two-stage object detector to solve UC-OWOD. First, unknown label-aware proposal and unknown-discriminative classification head are used to detect known and unknown objects. Then, similarity-based unknown classification and unknown clustering refinement modules are constructed to distinguish multiple unknown classes. Moreover, two novel evaluation protocols are designed to evaluate unknown-class detection. Abundant experiments and visualizations prove the effectiveness of the proposed method. Code is available at https:\/\/github.com\/JohnWuzh\/UC-OWOD.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0685333825,
        "newsscientist":0.1430225912,
        "technologyreview":0.1790547339,
        "venturebeat":0.1630989303,
        "wired":0.123879867,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11455v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658564130000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13364v1",
        "predicted_newsworthiness":0.4266347736,
        "title":"Deep Clustering with Features from Self-Supervised Pretraining",
        "summary":"A deep clustering model conceptually consists of a feature extractor that maps data points to a latent space, and a clustering head that groups data points into clusters in the latent space. Although the two components used to be trained jointly in an end-to-end fashion, recent works have proved it beneficial to train them separately in two stages. In the first stage, the feature extractor is trained via self-supervised learning, which enables the preservation of the cluster structures among the data points. To preserve the cluster structures even better, we propose to replace the first stage with another model that is pretrained on a much larger dataset via self-supervised learning. The method is simple and might suffer from domain shift. Nonetheless, we have empirically shown that it can achieve superior clustering performance. When a vision transformer (ViT) architecture is used for feature extraction, our method has achieved clustering accuracy 94.0%, 55.6% and 97.9% on CIFAR-10, CIFAR-100 and STL-10 respectively. The corresponding previous state-of-the-art results are 84.3%, 47.7% and 80.8%. Our code will be available online with the publication of the paper.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0830188257,
        "newsscientist":0.1393841818,
        "technologyreview":0.2414908661,
        "venturebeat":0.226612729,
        "wired":0.1732926472,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13364v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658911125000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01372v1",
        "predicted_newsworthiness":0.4265773737,
        "title":"A Riemannian Take on Human Motion Analysis and Retargeting",
        "summary":"Dynamic motions of humans and robots are widely driven by posture-dependent nonlinear interactions between their degrees of freedom. However, these dynamical effects remain mostly overlooked when studying the mechanisms of human movement generation. Inspired by recent works, we hypothesize that human motions are planned as sequences of geodesic synergies, and thus correspond to coordinated joint movements achieved with piecewise minimum energy. The underlying computational model is built on Riemannian geometry to account for the inertial characteristics of the body. Through the analysis of various human arm motions, we find that our model segments motions into geodesic synergies, and successfully predicts observed arm postures, hand trajectories, as well as their respective velocity profiles. Moreover, we show that our analysis can further be exploited to transfer arm motions to robots by reproducing individual human synergies as geodesic paths in the robot configuration space.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0953808157,
        "newsscientist":0.158248477,
        "technologyreview":0.1801114256,
        "venturebeat":0.1569480901,
        "wired":0.1642489304,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01372v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659440217000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00221v1",
        "predicted_newsworthiness":0.4265632885,
        "title":"Bipedal Locomotion Optimization by Exploitation of the Full Dynamics in DCM Trajectory Planning",
        "summary":"Walking motion planning based on Divergent Component of Motion (DCM) and Linear Inverted Pendulum Model (LIPM) is one of the alternatives that could be implemented to generate online humanoid robot gait trajectories. This algorithm requires different parameters to be adjusted. Herein, we developed a framework to attain optimal parameters to achieve a stable and energy-efficient trajectory for real robot's gait. To find the optimal trajectory, four cost functions representing energy consumption, the sum of joints velocity and applied torque at each lower limb joint of the robot, and a cost function based on the Zero Moment Point (ZMP) stability criterion were considered. Genetic algorithm was employed in the framework to optimize each of these cost functions. Although the trajectory planning was done with the help of the simplified model, the values of each cost function were obtained by considering the full dynamics model and foot-ground contact model in Bullet physics engine simulator. The results of this optimization yield that walking with the most stability and walking in the most efficient way are in contrast with each other. Therefore, in another attempt, multi-objective optimization for ZMP and energy cost functions at three different speeds was performed. Finally, we compared the designed trajectory, which was generated using optimal parameters, with the simulation results in Choreonoid simulator.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0628678284,
        "newsscientist":0.1144333962,
        "technologyreview":0.1320347882,
        "venturebeat":0.1115604425,
        "wired":0.1140905421,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00221v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659189004000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00963v1",
        "predicted_newsworthiness":0.4265569952,
        "title":"FrOoDo: Framework for Out-of-Distribution Detection",
        "summary":"FrOoDo is an easy-to-use and flexible framework for Out-of-Distribution detection tasks in digital pathology. It can be used with PyTorch classification and segmentation models, and its modular design allows for easy extension. The goal is to automate the task of OoD Evaluation such that research can focus on the main goal of either designing new models, new methods or evaluating a new dataset. The code can be found at https:\/\/github.com\/MECLabTUDA\/FrOoDo.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1116473798,
        "newsscientist":0.1767044573,
        "technologyreview":0.1808917079,
        "venturebeat":0.1485899066,
        "wired":0.1207030696,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00963v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659370281000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00487v1",
        "predicted_newsworthiness":0.4264904789,
        "title":"One Object at a Time: Accurate and Robust Structure From Motion for Robots",
        "summary":"A gaze-fixating robot perceives distance to the fixated object and relative positions of surrounding objects immediately, accurately, and robustly. We show how fixation, which is the act of looking at one object while moving, exploits regularities in the geometry of 3D space to obtain this information. These regularities introduce rotation-translation couplings that are not commonly used in structure from motion. To validate, we use a Franka Emika Robot with an RGB camera. We a) find that error in distance estimate is less than 5 mm at a distance of 15 cm, and b) show how relative position can be used to find obstacles under challenging scenarios. We combine accurate distance estimates and obstacle information into a reactive robot behavior that is able to pick up objects of unknown size, while impeded by unforeseen obstacles.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0874485757,
        "newsscientist":0.1781245986,
        "technologyreview":0.2364646684,
        "venturebeat":0.2047507135,
        "wired":0.1882056856,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00487v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv"
        ],
        "published":1659291424000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.02128v1",
        "predicted_newsworthiness":0.4264723584,
        "title":"Distributed On-Demand Routing for LEO Mega-Constellations: A Starlink Case Study",
        "summary":"The design and launch of large-scale satellite networks create an imminent demand for efficient and delay-minimising routing methods. With the rising number of satellites in such constellations, pre-computing all shortest routes between all satellites and for all times becomes more and more infeasible due to space and time limitations. Even though distributed on-demand routing methods were developed for specific LEO satellite network configurations, they are not suited for increasingly popular mega-constellations based on Walker Delta formations. The contributions of this paper are twofold. First, we introduce a formal model that mathematically captures the time-evolving locations of satellites in a Walker Delta constellation and use it to establish a formula to compute the minimum number of ISL hops between two given satellites. In the second part, we present an on-demand hop-count-based routing algorithm that approximates the optimal path while achieving superior performance compared to classical shortest-path algorithms like Dijkstra.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0710032952,
        "newsscientist":0.124625126,
        "technologyreview":0.1485922732,
        "venturebeat":0.145626174,
        "wired":0.1354213295,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02128v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1659539237000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2208.00712v1",
        "predicted_newsworthiness":0.4263514252,
        "title":"Cross Attention Based Style Distribution for Controllable Person Image Synthesis",
        "summary":"Controllable person image synthesis task enables a wide range of applications through explicit control over body pose and appearance. In this paper, we propose a cross attention based style distribution module that computes between the source semantic styles and target pose for pose transfer. The module intentionally selects the style represented by each semantic and distributes them according to the target pose. The attention matrix in cross attention expresses the dynamic similarities between the target pose and the source styles for all semantics. Therefore, it can be utilized to route the color and texture from the source image, and is further constrained by the target parsing map to achieve a clearer objective. At the same time, to encode the source appearance accurately, the self attention among different semantic styles is also added. The effectiveness of our model is validated quantitatively and qualitatively on pose transfer and virtual try-on tasks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0832368251,
        "newsscientist":0.1202164747,
        "technologyreview":0.166610113,
        "venturebeat":0.1600818059,
        "wired":0.1524478991,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00712v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659347439000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00088v1",
        "predicted_newsworthiness":0.4262540331,
        "title":"Improved Policy Optimization for Online Imitation Learning",
        "summary":"We consider online imitation learning (OIL), where the task is to find a policy that imitates the behavior of an expert via active interaction with the environment. We aim to bridge the gap between the theory and practice of policy optimization algorithms for OIL by analyzing one of the most popular OIL algorithms, DAGGER. Specifically, if the class of policies is sufficiently expressive to contain the expert policy, we prove that DAGGER achieves constant regret. Unlike previous bounds that require the losses to be strongly-convex, our result only requires the weaker assumption that the losses be strongly-convex with respect to the policy's sufficient statistics (not its parameterization). In order to ensure convergence for a wider class of policies and losses, we augment DAGGER with an additional regularization term. In particular, we propose a variant of Follow-the-Regularized-Leader (FTRL) and its adaptive variant for OIL and develop a memory-efficient implementation, which matches the memory requirements of FTL. Assuming that the loss functions are smooth and convex with respect to the parameters of the policy, we also prove that FTRL achieves constant regret for any sufficiently expressive policy class, while retaining $O(\\sqrt{T})$ regret in the worst-case. We demonstrate the effectiveness of these algorithms with experiments on synthetic and high-dimensional control tasks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.093886521,
        "newsscientist":0.1387688311,
        "technologyreview":0.2624383533,
        "venturebeat":0.2209817605,
        "wired":0.1681950206,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00088v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659132134000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13464v1",
        "predicted_newsworthiness":0.4260607494,
        "title":"Towards the Probabilistic Fusion of Learned Priors into Standard Pipelines for 3D Reconstruction",
        "summary":"The best way to combine the results of deep learning with standard 3D reconstruction pipelines remains an open problem. While systems that pass the output of traditional multi-view stereo approaches to a network for regularisation or refinement currently seem to get the best results, it may be preferable to treat deep neural networks as separate components whose results can be probabilistically fused into geometry-based systems. Unfortunately, the error models required to do this type of fusion are not well understood, with many different approaches being put forward. Recently, a few systems have achieved good results by having their networks predict probability distributions rather than single values. We propose using this approach to fuse a learned single-view depth prior into a standard 3D reconstruction system. Our system is capable of incrementally producing dense depth maps for a set of keyframes. We train a deep neural network to predict discrete, nonparametric probability distributions for the depth of each pixel from a single image. We then fuse this \"probability volume\" with another probability volume based on the photometric consistency between subsequent frames and the keyframe image. We argue that combining the probability volumes from these two sources will result in a volume that is better conditioned. To extract depth maps from the volume, we minimise a cost function that includes a regularisation term based on network predicted surface normals and occlusion boundaries. Through a series of experiments, we demonstrate that each of these components improves the overall performance of the system.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0815910842,
        "newsscientist":0.1275079926,
        "technologyreview":0.1935816894,
        "venturebeat":0.1742546811,
        "wired":0.1481305133,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13464v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ro"
        ],
        "published":1658921329000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12599v1",
        "predicted_newsworthiness":0.4259498918,
        "title":"A Survey of Explainable Graph Neural Networks: Taxonomy and Evaluation Metrics",
        "summary":"Graph neural networks (GNNs) have demonstrated a significant boost in prediction performance on graph data. At the same time, the predictions made by these models are often hard to interpret. In that regard, many efforts have been made to explain the prediction mechanisms of these models from perspectives such as GNNExplainer, XGNN and PGExplainer. Although such works present systematic frameworks to interpret GNNs, a holistic review for explainable GNNs is unavailable. In this survey, we present a comprehensive review of explainability techniques developed for GNNs. We focus on explainable graph neural networks and categorize them based on the use of explainable methods. We further provide the common performance metrics for GNNs explanations and point out several future research directions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1244841606,
        "newsscientist":0.1684627762,
        "technologyreview":0.2654548293,
        "venturebeat":0.2500975849,
        "wired":0.1949320517,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12599v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658799954000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01605v1",
        "predicted_newsworthiness":0.4257936215,
        "title":"Learning Skill-based Industrial Robot Tasks with User Priors",
        "summary":"Robot skills systems are meant to reduce robot setup time for new manufacturing tasks. Yet, for dexterous, contact-rich tasks, it is often difficult to find the right skill parameters. One strategy is to learn these parameters by allowing the robot system to learn directly on the task. For a learning problem, a robot operator can typically specify the type and range of values of the parameters. Nevertheless, given their prior experience, robot operators should be able to help the learning process further by providing educated guesses about where in the parameter space potential optimal solutions could be found. Interestingly, such prior knowledge is not exploited in current robot learning frameworks. We introduce an approach that combines user priors and Bayesian optimization to allow fast optimization of robot industrial tasks at robot deployment time. We evaluate our method on three tasks that are learned in simulation as well as on two tasks that are learned directly on a real robot system. Additionally, we transfer knowledge from the corresponding simulation tasks by automatically constructing priors from well-performing configurations for learning on the real system. To handle potentially contradicting task objectives, the tasks are modeled as multi-objective problems. Our results show that operator priors, both user-specified and transferred, vastly accelerate the discovery of rich Pareto fronts, and typically produce final performance far superior to proposed baselines.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0729546638,
        "newsscientist":0.1379989749,
        "technologyreview":0.2679261731,
        "venturebeat":0.2308535577,
        "wired":0.1800181974,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01605v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659460839000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13684v1",
        "predicted_newsworthiness":0.425705806,
        "title":"The Surface Edge Explorer (SEE): A measurement-direct approach to next best view planning",
        "summary":"High-quality observations of the real world are crucial for a variety of applications, including producing 3D printed replicas of small-scale scenes and conducting inspections of large-scale infrastructure. These 3D observations are commonly obtained by combining multiple sensor measurements from different views. Guiding the selection of suitable views is known as the Next Best View (NBV) planning problem. Most NBV approaches reason about measurements using rigid data structures (e.g., surface meshes or voxel grids). This simplifies next best view selection but can be computationally expensive, reduces real-world fidelity, and couples the selection of a next best view with the final data processing. This paper presents the Surface Edge Explorer (SEE), a NBV approach that selects new observations directly from previous sensor measurements without requiring rigid data structures. SEE uses measurement density to propose next best views that increase coverage of insufficiently observed surfaces while avoiding potential occlusions. Statistical results from simulated experiments show that SEE can attain better surface coverage in less computational time and sensor travel distance than evaluated volumetric approaches on both small- and large-scale scenes. Real-world experiments demonstrate SEE autonomously observing a deer statue using a 3D sensor affixed to a robotic arm.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0938453737,
        "newsscientist":0.1757426388,
        "technologyreview":0.2269310272,
        "venturebeat":0.218106124,
        "wired":0.2077451659,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13684v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658944494000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00702v1",
        "predicted_newsworthiness":0.4256662677,
        "title":"A New Calibration Method for Industrial Robot Based on Step-Size Levenberg-Marquardt Algorithm",
        "summary":"Industrial robots play a vital role in automatic production, which have been widely utilized in industrial production activities, like handling and welding. However, due to an uncalibrated robot with machining tolerance and assembly tolerance, it suffers from low absolute positioning accuracy, which cannot satisfy the requirements of high-precision manufacture. To address this hot issue, we propose a novel calibration method based on an unscented Kalman filter and variable step-size Levenberg-Marquardt algorithm. This work has three ideas: a) proposing a novel variable step-size Levenberg-Marquardt algorithm to addresses the issue of local optimum in a Levenberg-Marquardt algorithm; b) employing an unscented Kalman filter to reduce the influence of the measurement noises; and c) developing a novel calibration method incorporating an unscented Kalman filter with a variable step-size Levenberg-Marquardt algorithm. Furthermore, we conduct enough experiments on an ABB IRB 120 industrial robot. From the experimental results, the proposed method achieves much higher calibration accuracy than some state-of-the-art calibration methods. Hence, this work is an important milestone in the field of robot calibration.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0506268361,
        "newsscientist":0.1112251205,
        "technologyreview":0.1852023556,
        "venturebeat":0.1447480871,
        "wired":0.1362797178,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00702v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659345494000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00946v1",
        "predicted_newsworthiness":0.4252053414,
        "title":"Motion-aware Memory Network for Fast Video Salient Object Detection",
        "summary":"Previous methods based on 3DCNN, convLSTM, or optical flow have achieved great success in video salient object detection (VSOD). However, they still suffer from high computational costs or poor quality of the generated saliency maps. To solve these problems, we design a space-time memory (STM)-based network, which extracts useful temporal information of the current frame from adjacent frames as the temporal branch of VSOD. Furthermore, previous methods only considered single-frame prediction without temporal association. As a result, the model may not focus on the temporal information sufficiently. Thus, we initially introduce object motion prediction between inter-frame into VSOD. Our model follows standard encoder--decoder architecture. In the encoding stage, we generate high-level temporal features by using high-level features from the current and its adjacent frames. This approach is more efficient than the optical flow-based methods. In the decoding stage, we propose an effective fusion strategy for spatial and temporal branches. The semantic information of the high-level features is used to fuse the object details in the low-level features, and then the spatiotemporal features are obtained step by step to reconstruct the saliency maps. Moreover, inspired by the boundary supervision commonly used in image salient object detection (ISOD), we design a motion-aware loss for predicting object boundary motion and simultaneously perform multitask learning for VSOD and object motion prediction, which can further facilitate the model to extract spatiotemporal features accurately and maintain the object integrity. Extensive experiments on several datasets demonstrated the effectiveness of our method and can achieve state-of-the-art metrics on some datasets. The proposed model does not require optical flow or other preprocessing, and can reach a speed of nearly 100 FPS during inference.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0813604954,
        "newsscientist":0.1307389504,
        "technologyreview":0.1784124866,
        "venturebeat":0.178326098,
        "wired":0.1526579366,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00946v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659369379000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01375v1",
        "predicted_newsworthiness":0.4249385065,
        "title":"BERT4Loc: BERT for Location -- POI Recommender System",
        "summary":"Recommending points of interest is a difficult problem that requires precise location information to be extracted from a location-based social media platform. Another challenging and critical problem for such a location-aware recommendation system is modelling users' preferences based on their historical behaviors. We propose a location-aware recommender system based on Bidirectional Encoder Representations from Transformers for the purpose of providing users with location-based recommendations. The proposed model incorporates location data and user preferences. When compared to predicting the next item of interest (location) at each position in a sequence, our model can provide the user with more relevant results. Extensive experiments on a benchmark dataset demonstrate that our model consistently outperforms a variety of state-of-the-art sequential models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1255291605,
        "newsscientist":0.1390838004,
        "technologyreview":0.21184169,
        "venturebeat":0.2466101986,
        "wired":0.2224826017,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01375v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.ai"
        ],
        "published":1659440819000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.14561v1",
        "predicted_newsworthiness":0.4249140189,
        "title":"Cyclic Policy Distillation: Sample-Efficient Sim-to-Real Reinforcement Learning with Domain Randomization",
        "summary":"Deep reinforcement learning with domain randomization learns a control policy in various simulations with randomized physical and sensor model parameters to become transferable to the real world in a zero-shot setting. However, a huge number of samples are often required to learn an effective policy when the range of randomized parameters is extensive due to the instability of policy updates. To alleviate this problem, we propose a sample-efficient method named Cyclic Policy Distillation (CPD). CPD divides the range of randomized parameters into several small sub-domains and assigns a local policy to each sub-domain. Then, the learning of local policies is performed while {\\it cyclically} transitioning the target sub-domain to neighboring sub-domains and exploiting the learned values\/policies of the neighbor sub-domains with a monotonic policy-improvement scheme. Finally, all of the learned local policies are distilled into a global policy for sim-to-real transfer. The effectiveness and sample efficiency of CPD are demonstrated through simulations with four tasks (Pendulum from OpenAIGym and Pusher, Swimmer, and HalfCheetah from Mujoco), and a real-robot ball-dispersal task.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0984485156,
        "newsscientist":0.1460666271,
        "technologyreview":0.2498165534,
        "venturebeat":0.2108559874,
        "wired":0.175856029,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14561v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.lg"
        ],
        "published":1659086573000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01912v1",
        "predicted_newsworthiness":0.4248984754,
        "title":"Cross-Lingual Knowledge Transfer for Clinical Phenotyping",
        "summary":"Clinical phenotyping enables the automatic extraction of clinical conditions from patient records, which can be beneficial to doctors and clinics worldwide. However, current state-of-the-art models are mostly applicable to clinical notes written in English. We therefore investigate cross-lingual knowledge transfer strategies to execute this task for clinics that do not use the English language and have a small amount of in-domain data available. We evaluate these strategies for a Greek and a Spanish clinic leveraging clinical notes from different clinical domains such as cardiology, oncology and the ICU. Our results reveal two strategies that outperform the state-of-the-art: Translation-based methods in combination with domain-specific encoders and cross-lingual encoders plus adapters. We find that these strategies perform especially well for classifying rare phenotypes and we advise on which method to prefer in which situation. Our results show that using multilingual data overall improves clinical phenotyping models and can compensate for data sparseness.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1563565802,
        "newsscientist":0.1782544741,
        "technologyreview":0.2274646895,
        "venturebeat":0.2161839346,
        "wired":0.1486517518,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01912v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659515601000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11762v1",
        "predicted_newsworthiness":0.4248258463,
        "title":"Anti-Overestimation Dialogue Policy Learning for Task-Completion Dialogue System",
        "summary":"A dialogue policy module is an essential part of task-completion dialogue systems. Recently, increasing interest has focused on reinforcement learning (RL)-based dialogue policy. Its favorable performance and wise action decisions rely on an accurate estimation of action values. The overestimation problem is a widely known issue of RL since its estimate of the maximum action value is larger than the ground truth, which results in an unstable learning process and suboptimal policy. This problem is detrimental to RL-based dialogue policy learning. To mitigate this problem, this paper proposes a dynamic partial average estimator (DPAV) of the ground truth maximum action value. DPAV calculates the partial average between the predicted maximum action value and minimum action value, where the weights are dynamically adaptive and problem-dependent. We incorporate DPAV into a deep Q-network as the dialogue policy and show that our method can achieve better or comparable results compared to top baselines on three dialogue datasets of different domains with a lower computational load. In addition, we also theoretically prove the convergence and derive the upper and lower bounds of the bias compared with those of other methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1201383372,
        "newsscientist":0.1342042356,
        "technologyreview":0.2667750468,
        "venturebeat":0.2539077195,
        "wired":0.1866381576,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11762v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1658677088000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.12396v1",
        "predicted_newsworthiness":0.4247744296,
        "title":"Exploring CLIP for Assessing the Look and Feel of Images",
        "summary":"Measuring the perception of visual content is a long-standing problem in computer vision. Many mathematical models have been developed to evaluate the look or quality of an image. Despite the effectiveness of such tools in quantifying degradations such as noise and blurriness levels, such quantification is loosely coupled with human language. When it comes to more abstract perception about the feel of visual content, existing methods can only rely on supervised models that are explicitly trained with labeled data collected via laborious user study. In this paper, we go beyond the conventional paradigms by exploring the rich visual language prior encapsulated in Contrastive Language-Image Pre-training (CLIP) models for assessing both the quality perception (look) and abstract perception (feel) of images in a zero-shot manner. In particular, we discuss effective prompt designs and show an effective prompt pairing strategy to harness the prior. We also provide extensive experiments on controlled datasets and Image Quality Assessment (IQA) benchmarks. Our results show that CLIP captures meaningful priors that generalize well to different perceptual assessments. Code will be avaliable at https:\/\/github.com\/IceClear\/CLIP-IQA.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1074034615,
        "newsscientist":0.1570299808,
        "technologyreview":0.2392614132,
        "venturebeat":0.2147881717,
        "wired":0.1914870171,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12396v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658771896000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01182v1",
        "predicted_newsworthiness":0.4241294333,
        "title":"Mitigating Biases in Student Performance Prediction via Attention-Based Personalized Federated Learning",
        "summary":"Traditional learning-based approaches to student modeling generalize poorly to underrepresented student groups due to biases in data availability. In this paper, we propose a methodology for predicting student performance from their online learning activities that optimizes inference accuracy over different demographic groups such as race and gender. Building upon recent foundations in federated learning, in our approach, personalized models for individual student subgroups are derived from a global model aggregated across all student models via meta-gradient updates that account for subgroup heterogeneity. To learn better representations of student activity, we augment our approach with a self-supervised behavioral pretraining methodology that leverages multiple modalities of student behavior (e.g., visits to lecture videos and participation on forums), and include a neural network attention mechanism in the model aggregation stage. Through experiments on three real-world datasets from online courses, we demonstrate that our approach obtains substantial improvements over existing student modeling baselines in predicting student learning outcomes for all subgroups. Visual analysis of the resulting student embeddings confirm that our personalization methodology indeed identifies different activity patterns within different subgroups, consistent with its stronger inference ability compared with the baselines.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1926773975,
        "newsscientist":0.1840934684,
        "technologyreview":0.2795503636,
        "venturebeat":0.2722410701,
        "wired":0.2183384808,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01182v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cy"
        ],
        "published":1659399740000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01172v1",
        "predicted_newsworthiness":0.4241011963,
        "title":"MV6D: Multi-View 6D Pose Estimation on RGB-D Frames Using a Deep Point-wise Voting Network",
        "summary":"Estimating 6D poses of objects is an essential computer vision task. However, most conventional approaches rely on camera data from a single perspective and therefore suffer from occlusions. We overcome this issue with our novel multi-view 6D pose estimation method called MV6D which accurately predicts the 6D poses of all objects in a cluttered scene based on RGB-D images from multiple perspectives. We base our approach on the PVN3D network that uses a single RGB-D image to predict keypoints of the target objects. We extend this approach by using a combined point cloud from multiple views and fusing the images from each view with a DenseFusion layer. In contrast to current multi-view pose detection networks such as CosyPose, our MV6D can learn the fusion of multiple perspectives in an end-to-end manner and does not require multiple prediction stages or subsequent fine tuning of the prediction. Furthermore, we present three novel photorealistic datasets of cluttered scenes with heavy occlusions. All of them contain RGB-D images from multiple perspectives and the ground truth for instance semantic segmentation and 6D pose estimation. MV6D significantly outperforms the state-of-the-art in multi-view 6D pose estimation even in cases where the camera poses are known inaccurately. Furthermore, we show that our approach is robust towards dynamic camera setups and that its accuracy increases incrementally with an increasing number of perspectives.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0862080862,
        "newsscientist":0.1259432587,
        "technologyreview":0.1918479276,
        "venturebeat":0.2043161814,
        "wired":0.1632164141,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01172v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1659396883000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00544v1",
        "predicted_newsworthiness":0.4240282533,
        "title":"Analysis of Semi-Supervised Methods for Facial Expression Recognition",
        "summary":"Training deep neural networks for image recognition often requires large-scale human annotated data. To reduce the reliance of deep neural solutions on labeled data, state-of-the-art semi-supervised methods have been proposed in the literature. Nonetheless, the use of such semi-supervised methods has been quite rare in the field of facial expression recognition (FER). In this paper, we present a comprehensive study on recently proposed state-of-the-art semi-supervised learning methods in the context of FER. We conduct comparative study on eight semi-supervised learning methods, namely Pi-Model, Pseudo-label, Mean-Teacher, VAT, MixMatch, ReMixMatch, UDA, and FixMatch, on three FER datasets (FER13, RAF-DB, and AffectNet), when various amounts of labeled samples are used. We also compare the performance of these methods against fully-supervised training. Our study shows that when training existing semi-supervised methods on as little as 250 labeled samples per class can yield comparable performances to that of fully-supervised methods trained on the full labeled datasets. To facilitate further research in this area, we make our code publicly available at: https:\/\/github.com\/ShuvenduRoy\/SSL_FER",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1047046428,
        "newsscientist":0.1315785052,
        "technologyreview":0.2207346452,
        "venturebeat":0.2033318875,
        "wired":0.1622794051,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00544v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659311915000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01949v1",
        "predicted_newsworthiness":0.4239506533,
        "title":"Negative Frames Matter in Egocentric Visual Query 2D Localization",
        "summary":"The recently released Ego4D dataset and benchmark significantly scales and diversifies the first-person visual perception data. In Ego4D, the Visual Queries 2D Localization task aims to retrieve objects appeared in the past from the recording in the first-person view. This task requires a system to spatially and temporally localize the most recent appearance of a given object query, where query is registered by a single tight visual crop of the object in a different scene. Our study is based on the three-stage baseline introduced in the Episodic Memory benchmark. The baseline solves the problem by detection and tracking: detect the similar objects in all the frames, then run a tracker from the most confident detection result. In the VQ2D challenge, we identified two limitations of the current baseline. (1) The training configuration has redundant computation. Although the training set has millions of instances, most of them are repetitive and the number of unique object is only around 14.6k. The repeated gradient computation of the same object lead to an inefficient training; (2) The false positive rate is high on background frames. This is due to the distribution gap between training and evaluation. During training, the model is only able to see the clean, stable, and labeled frames, but the egocentric videos also have noisy, blurry, or unlabeled background frames. To this end, we developed a more efficient and effective solution. Concretely, we bring the training loop from ~15 days to less than 24 hours, and we achieve 0.17% spatial-temporal AP, which is 31% higher than the baseline. Our solution got the first ranking on the public leaderboard. Our code is publicly available at https:\/\/github.com\/facebookresearch\/vq2d_cvpr.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0927149059,
        "newsscientist":0.1567937831,
        "technologyreview":0.216460453,
        "venturebeat":0.2139503656,
        "wired":0.192729399,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01949v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659520491000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11447v1",
        "predicted_newsworthiness":0.4235099459,
        "title":"Handling Data Heterogeneity in Federated Learning via Knowledge Fusion",
        "summary":"Federated learning (FL) supports distributed training of a global machine learning model across multiple clients with the help from a central server. The local dataset held by each client is never exchanged in FL, so the local dataset privacy is protected. Although FL is increasingly popular, data heterogeneity across different clients leads to the client model drift issue and results in model performance degradation and poor model fairness. To address the issue, we design Federated learning with global-local Knowledge Fusion (FedKF) scheme in this paper. The key idea in FedKF is to let the server return the global knowledge to be fused with the local knowledge in each training round so that the local model can be regularized towards the global optima. Thus, the client model drift issue can be mitigated. In FedKF, we first propose the active-inactive model aggregation technique that supports a precise global knowledge representation. Then, we propose a data-free knowledge distillation (KD) approach to facilitate the KD from the global model to the local model while the local model can still learn the local knowledge (embedded in the local dataset) simultaneously, thereby realizing the global-local knowledge fusion process. The theoretical analysis and intensive experiments demonstrate that FedKF achieves high model performance, high fairness, and privacy-preserving simultaneously. The project source codes will be released on GitHub after the paper review.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0981318118,
        "newsscientist":0.1346205909,
        "technologyreview":0.2500336122,
        "venturebeat":0.2632379754,
        "wired":0.1786684251,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11447v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658560822000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11512v1",
        "predicted_newsworthiness":0.4234132071,
        "title":"Combining Hybrid Architecture and Pseudo-label for Semi-supervised Abdominal Organ Segmentation",
        "summary":"Abdominal organ segmentation has many important clinical applications, such as organ quantification, surgical planning, and disease diagnosis. However, manually annotating organs from CT scans is time-consuming and labor-intensive. Semi-supervised learning has shown the potential to alleviate this challenge by learning from a large set of unlabeled images and limited labeled samples. In this work, we follow the self-training strategy and employ a hybrid architecture (PHTrans) with CNN and Transformer for both teacher and student models to generate precise pseudo-labels. Afterward, we introduce them with label data together into a two-stage segmentation framework with lightweight PHTrans for training to improve the performance and generalization ability of the model while remaining efficient. Experiments on the validation set of FLARE2022 demonstrate that our method achieves excellent segmentation performance as well as fast and low-resource model inference. The average DSC and HSD are 0.8956 and 0.9316, respectively. Under our development environments, the average inference time is 18.62 s, the average maximum GPU memory is 1995.04 MB, and the area under the GPU memory-time curve and the average area under the CPU utilization-time curve are 23196.84 and 319.67.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0871168997,
        "newsscientist":0.1385865581,
        "technologyreview":0.2092969302,
        "venturebeat":0.1789270171,
        "wired":0.1329748808,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11512v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658581363000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11753v1",
        "predicted_newsworthiness":0.4233031486,
        "title":"Label-Guided Auxiliary Training Improves 3D Object Detector",
        "summary":"Detecting 3D objects from point clouds is a practical yet challenging task that has attracted increasing attention recently. In this paper, we propose a Label-Guided auxiliary training method for 3D object detection (LG3D), which serves as an auxiliary network to enhance the feature learning of existing 3D object detectors. Specifically, we propose two novel modules: a Label-Annotation-Inducer that maps annotations and point clouds in bounding boxes to task-specific representations and a Label-Knowledge-Mapper that assists the original features to obtain detection-critical representations. The proposed auxiliary network is discarded in inference and thus has no extra computational cost at test time. We conduct extensive experiments on both indoor and outdoor datasets to verify the effectiveness of our approach. For example, our proposed LG3D improves VoteNet by 2.5% and 3.1% mAP on the SUN RGB-D and ScanNetV2 datasets, respectively.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0902422563,
        "newsscientist":0.1392574835,
        "technologyreview":0.2139647599,
        "venturebeat":0.2008043659,
        "wired":0.1600441735,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11753v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658672541000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00408v1",
        "predicted_newsworthiness":0.4225395615,
        "title":"DA$^2$ Dataset: Toward Dexterity-Aware Dual-Arm Grasping",
        "summary":"In this paper, we introduce DA$^2$, the first large-scale dual-arm dexterity-aware dataset for the generation of optimal bimanual grasping pairs for arbitrary large objects. The dataset contains about 9M pairs of parallel-jaw grasps, generated from more than 6000 objects and each labeled with various grasp dexterity measures. In addition, we propose an end-to-end dual-arm grasp evaluation model trained on the rendered scenes from this dataset. We utilize the evaluation model as our baseline to show the value of this novel and nontrivial dataset by both online analysis and real robot experiments. All data and related code will be open-sourced at https:\/\/sites.google.com\/view\/da2dataset.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0727024509,
        "newsscientist":0.138906299,
        "technologyreview":0.1979574292,
        "venturebeat":0.167866989,
        "wired":0.1471439168,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00408v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv"
        ],
        "published":1659261747000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00571v1",
        "predicted_newsworthiness":0.4219374596,
        "title":"CLIFF: Carrying Location Information in Full Frames into Human Pose and Shape Estimation",
        "summary":"Top-down methods dominate the field of 3D human pose and shape estimation, because they are decoupled from human detection and allow researchers to focus on the core problem. However, cropping, their first step, discards the location information from the very beginning, which makes themselves unable to accurately predict the global rotation in the original camera coordinate system. To address this problem, we propose to Carry Location Information in Full Frames (CLIFF) into this task. Specifically, we feed more holistic features to CLIFF by concatenating the cropped-image feature with its bounding box information. We calculate the 2D reprojection loss with a broader view of the full frame, taking a projection process similar to that of the person projected in the image. Fed and supervised by global-location-aware information, CLIFF directly predicts the global rotation along with more accurate articulated poses. Besides, we propose a pseudo-ground-truth annotator based on CLIFF, which provides high-quality 3D annotations for in-the-wild 2D datasets and offers crucial full supervision for regression-based methods. Extensive experiments on popular benchmarks show that CLIFF outperforms prior arts by a significant margin, and reaches the first place on the AGORA leaderboard (the SMPL-Algorithms track). The code and data are available at https:\/\/github.com\/huawei-noah\/noah-research\/tree\/master\/CLIFF.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1005518701,
        "newsscientist":0.1312353891,
        "technologyreview":0.1874653223,
        "venturebeat":0.1886494588,
        "wired":0.1637666488,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00571v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659319726000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12029v1",
        "predicted_newsworthiness":0.4218788094,
        "title":"Evaluating the Accuracy of Stochastic Geometry Based Models for LEO Satellite Networks Analysis",
        "summary":"This paper investigates the accuracy of recently proposed stochastic geometry-based modeling of low earth orbit (LEO) satellite networks. In particular, we use the Wasserstein Distance-inspired method to analyze the distances between different models, including Fibonacci lattice and orbit models. We propose an algorithm to calculate the distance between the generated point sets. Next, we test the algorithm's performance and analyze the distance between the stochastic geometry model and other more widely acceptable models using numerical results.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0826188222,
        "newsscientist":0.1235879927,
        "technologyreview":0.152341166,
        "venturebeat":0.1184869115,
        "wired":0.1309625592,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12029v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658743932000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.12646v1",
        "predicted_newsworthiness":0.421871527,
        "title":"Learning Hierarchy Aware Features for Reducing Mistake Severity",
        "summary":"Label hierarchies are often available apriori as part of biological taxonomy or language datasets WordNet. Several works exploit these to learn hierarchy aware features in order to improve the classifier to make semantically meaningful mistakes while maintaining or reducing the overall error. In this paper, we propose a novel approach for learning Hierarchy Aware Features (HAF) that leverages classifiers at each level of the hierarchy that are constrained to generate predictions consistent with the label hierarchy. The classifiers are trained by minimizing a Jensen-Shannon Divergence with target soft labels obtained from the fine-grained classifiers. Additionally, we employ a simple geometric loss that constrains the feature space geometry to capture the semantic structure of the label space. HAF is a training time approach that improves the mistakes while maintaining top-1 error, thereby, addressing the problem of cross-entropy loss that treats all mistakes as equal. We evaluate HAF on three hierarchical datasets and achieve state-of-the-art results on the iNaturalist-19 and CIFAR-100 datasets. The source code is available at https:\/\/github.com\/07Agarg\/HAF",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0996359176,
        "newsscientist":0.1390885276,
        "technologyreview":0.2202443793,
        "venturebeat":0.1966870727,
        "wired":0.1383853066,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12646v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658809487000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12842v1",
        "predicted_newsworthiness":0.4218533018,
        "title":"Unsupervised Domain Adaptation for Video Transformers in Action Recognition",
        "summary":"Over the last few years, Unsupervised Domain Adaptation (UDA) techniques have acquired remarkable importance and popularity in computer vision. However, when compared to the extensive literature available for images, the field of videos is still relatively unexplored. On the other hand, the performance of a model in action recognition is heavily affected by domain shift. In this paper, we propose a simple and novel UDA approach for video action recognition. Our approach leverages recent advances on spatio-temporal transformers to build a robust source model that better generalises to the target domain. Furthermore, our architecture learns domain invariant features thanks to the introduction of a novel alignment loss term derived from the Information Bottleneck principle. We report results on two video action recognition benchmarks for UDA, showing state-of-the-art performance on HMDB$\\leftrightarrow$UCF, as well as on Kinetics$\\rightarrow$NEC-Drone, which is more challenging. This demonstrates the effectiveness of our method in handling different levels of domain shift. The source code is available at https:\/\/github.com\/vturrisi\/UDAVT.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0867375653,
        "newsscientist":0.1233208189,
        "technologyreview":0.1900703972,
        "venturebeat":0.1786435701,
        "wired":0.1605418621,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12842v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658837859000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00329v1",
        "predicted_newsworthiness":0.421708294,
        "title":"PASTA: A Dataset for Modeling Participant States in Narratives",
        "summary":"The events in a narrative can be understood as a coherent whole via the underlying states of its participants. Often, these participant states are not explicitly mentioned in the narrative, left to be filled in via common-sense or inference. A model that understands narratives should be able to infer these implicit participant states and reason about the impact of changes to these states on the narrative. To facilitate this goal, we introduce a new crowdsourced Participants States dataset, PASTA. This dataset contains valid, inferable participant states; a counterfactual perturbation to the state; and the changes to the story that would be necessary if the counterfactual was true. We introduce three state-based reasoning tasks that test for the ability to infer when a state is entailed by a story, revise a story for a counterfactual state, and to explain the most likely state change given a revised story. Our benchmarking experiments show that while today's LLMs are able to reason about states to some degree, there is a large room for improvement, suggesting potential avenues for future research.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1564410585,
        "newsscientist":0.1672384875,
        "technologyreview":0.2394797936,
        "venturebeat":0.2288217066,
        "wired":0.2165661373,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00329v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659230508000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.01944v1",
        "predicted_newsworthiness":0.4216125166,
        "title":"PalQuant: Accelerating High-precision Networks on Low-precision Accelerators",
        "summary":"Recently low-precision deep learning accelerators (DLAs) have become popular due to their advantages in chip area and energy consumption, yet the low-precision quantized models on these DLAs bring in severe accuracy degradation. One way to achieve both high accuracy and efficient inference is to deploy high-precision neural networks on low-precision DLAs, which is rarely studied. In this paper, we propose the PArallel Low-precision Quantization (PalQuant) method that approximates high-precision computations via learning parallel low-precision representations from scratch. In addition, we present a novel cyclic shuffle module to boost the cross-group information communication between parallel low-precision groups. Extensive experiments demonstrate that PalQuant has superior performance to state-of-the-art quantization methods in both accuracy and inference speed, e.g., for ResNet-18 network quantization, PalQuant can obtain 0.52\\% higher accuracy and 1.78$\\times$ speedup simultaneously over their 4-bit counter-part on a state-of-the-art 2-bit accelerator. Code is available at \\url{https:\/\/github.com\/huqinghao\/PalQuant}.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0758200759,
        "newsscientist":0.1253321319,
        "technologyreview":0.2156152432,
        "venturebeat":0.2046498997,
        "wired":0.1466256495,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01944v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659519853000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11919v1",
        "predicted_newsworthiness":0.421534527,
        "title":"Patchwork++: Fast and Robust Ground Segmentation Solving Partial Under-Segmentation Using 3D Point Cloud",
        "summary":"In the field of 3D perception using 3D LiDAR sensors, ground segmentation is an essential task for various purposes, such as traversable area detection and object recognition. Under these circumstances, several ground segmentation methods have been proposed. However, some limitations are still encountered. First, some ground segmentation methods require fine-tuning of parameters depending on the surroundings, which is excessively laborious and time-consuming. Moreover, even if the parameters are well adjusted, a partial under-segmentation problem can still emerge, which implies ground segmentation failures in some regions. Finally, ground segmentation methods typically fail to estimate an appropriate ground plane when the ground is above another structure, such as a retaining wall. To address these problems, we propose a robust ground segmentation method called Patchwork++, an extension of Patchwork. Patchwork++ exploits adaptive ground likelihood estimation (A-GLE) to calculate appropriate parameters adaptively based on the previous ground segmentation results. Moreover, temporal ground revert (TGR) alleviates a partial under-segmentation problem by using the temporary ground property. Also, region-wise vertical plane fitting (R-VPF) is introduced to segment the ground plane properly even if the ground is elevated with different layers. Finally, we present reflected noise removal (RNR) to eliminate virtual noise points efficiently based on the 3D LiDAR reflection model. We demonstrate the qualitative and quantitative evaluations using a SemanticKITTI dataset. Our code is available at https:\/\/github.com\/url-kaist\/patchwork-plusplus",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0940759783,
        "newsscientist":0.1510478731,
        "technologyreview":0.1940392278,
        "venturebeat":0.1813459054,
        "wired":0.1707791543,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11919v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv"
        ],
        "published":1658729342000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01840v1",
        "predicted_newsworthiness":0.4214480437,
        "title":"'Labelling the Gaps': A Weakly Supervised Automatic Eye Gaze Estimation",
        "summary":"Over the past few years, there has been an increasing interest to interpret gaze direction in an unconstrained environment with limited supervision. Owing to data curation and annotation issues, replicating gaze estimation method to other platforms, such as unconstrained outdoor or AR\/VR, might lead to significant drop in performance due to insufficient availability of accurately annotated data for model training. In this paper, we explore an interesting yet challenging problem of gaze estimation method with a limited amount of labelled data. The proposed method distills knowledge from the labelled subset with visual features; including identity-specific appearance, gaze trajectory consistency and motion features. Given a gaze trajectory, the method utilizes label information of only the start and the end frames of a gaze sequence. An extension of the proposed method further reduces the requirement of labelled frames to only the start frame with a minor drop in the generated label's quality. We evaluate the proposed method on four benchmark datasets (CAVE, TabletGaze, MPII and Gaze360) as well as web-crawled YouTube videos. Our proposed method reduces the annotation effort to as low as 2.67%, with minimal impact on performance; indicating the potential of our model enabling gaze estimation 'in-the-wild' setup.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1031583847,
        "newsscientist":0.1448624374,
        "technologyreview":0.2095866266,
        "venturebeat":0.2124464907,
        "wired":0.1837343092,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01840v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659502316000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12598v1",
        "predicted_newsworthiness":0.4211459881,
        "title":"Classifier-Free Diffusion Guidance",
        "summary":"Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1023843982,
        "newsscientist":0.12526511,
        "technologyreview":0.1677427066,
        "venturebeat":0.1354067977,
        "wired":0.1101423852,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12598v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658799727000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13186v1",
        "predicted_newsworthiness":0.4210039286,
        "title":"On Missing Labels, Long-tails and Propensities in Extreme Multi-label Classification",
        "summary":"The propensity model introduced by Jain et al. 2016 has become a standard approach for dealing with missing and long-tail labels in extreme multi-label classification (XMLC). In this paper, we critically revise this approach showing that despite its theoretical soundness, its application in contemporary XMLC works is debatable. We exhaustively discuss the flaws of the propensity-based approach, and present several recipes, some of them related to solutions used in search engines and recommender systems, that we believe constitute promising alternatives to be followed in XMLC.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1129578182,
        "newsscientist":0.1197699038,
        "technologyreview":0.1710243886,
        "venturebeat":0.1823031508,
        "wired":0.158986009,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13186v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ir"
        ],
        "published":1658870603000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14552v1",
        "predicted_newsworthiness":0.4209848337,
        "title":"ScaleFormer: Revisiting the Transformer-based Backbones from a Scale-wise Perspective for Medical Image Segmentation",
        "summary":"Recently, a variety of vision transformers have been developed as their capability of modeling long-range dependency. In current transformer-based backbones for medical image segmentation, convolutional layers were replaced with pure transformers, or transformers were added to the deepest encoder to learn global context. However, there are mainly two challenges in a scale-wise perspective: (1) intra-scale problem: the existing methods lacked in extracting local-global cues in each scale, which may impact the signal propagation of small objects; (2) inter-scale problem: the existing methods failed to explore distinctive information from multiple scales, which may hinder the representation learning from objects with widely variable size, shape and location. To address these limitations, we propose a novel backbone, namely ScaleFormer, with two appealing designs: (1) A scale-wise intra-scale transformer is designed to couple the CNN-based local features with the transformer-based global cues in each scale, where the row-wise and column-wise global dependencies can be extracted by a lightweight Dual-Axis MSA. (2) A simple and effective spatial-aware inter-scale transformer is designed to interact among consensual regions in multiple scales, which can highlight the cross-scale dependency and resolve the complex scale variations. Experimental results on different benchmarks demonstrate that our Scale-Former outperforms the current state-of-the-art methods. The code is publicly available at: https:\/\/github.com\/ZJUGiveLab\/ScaleFormer.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0826499206,
        "newsscientist":0.1288577725,
        "technologyreview":0.1830926656,
        "venturebeat":0.164272805,
        "wired":0.1276282823,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14552v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659084900000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01296v1",
        "predicted_newsworthiness":0.4206290389,
        "title":"Silo NLP's Participation at WAT2022",
        "summary":"This paper provides the system description of \"Silo NLP's\" submission to the Workshop on Asian Translation (WAT2022). We have participated in the Indic Multimodal tasks (English->Hindi, English->Malayalam, and English->Bengali Multimodal Translation). For text-only translation, we trained Transformers from scratch and fine-tuned mBART-50 models. For multimodal translation, we used the same mBART architecture and extracted object tags from the images to use as visual features concatenated with the text sequence. Our submission tops many tasks including English->Hindi multimodal translation (evaluation test), English->Malayalam text-only and multimodal translation (evaluation test), English->Bengali multimodal translation (challenge test), and English->Bengali text-only translation (evaluation test).",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1101076645,
        "newsscientist":0.1270682355,
        "technologyreview":0.1992115845,
        "venturebeat":0.2132100539,
        "wired":0.1732512694,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01296v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659426573000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.01954v1",
        "predicted_newsworthiness":0.4206258664,
        "title":"Dilated Context Integrated Network with Cross-Modal Consensus for Temporal Emotion Localization in Videos",
        "summary":"Understanding human emotions is a crucial ability for intelligent robots to provide better human-robot interactions. The existing works are limited to trimmed video-level emotion classification, failing to locate the temporal window corresponding to the emotion. In this paper, we introduce a new task, named Temporal Emotion Localization in videos~(TEL), which aims to detect human emotions and localize their corresponding temporal boundaries in untrimmed videos with aligned subtitles. TEL presents three unique challenges compared to temporal action localization: 1) The emotions have extremely varied temporal dynamics; 2) The emotion cues are embedded in both appearances and complex plots; 3) The fine-grained temporal annotations are complicated and labor-intensive. To address the first two challenges, we propose a novel dilated context integrated network with a coarse-fine two-stream architecture. The coarse stream captures varied temporal dynamics by modeling multi-granularity temporal contexts. The fine stream achieves complex plots understanding by reasoning the dependency between the multi-granularity temporal contexts from the coarse stream and adaptively integrates them into fine-grained video segment features. To address the third challenge, we introduce a cross-modal consensus learning paradigm, which leverages the inherent semantic consensus between the aligned video and subtitle to achieve weakly-supervised learning. We contribute a new testing set with 3,000 manually-annotated temporal boundaries so that future research on the TEL problem can be quantitatively evaluated. Extensive experiments show the effectiveness of our approach on temporal emotion localization. The repository of this work is at https:\/\/github.com\/YYJMJC\/Temporal-Emotion-Localization-in-Videos.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1233947351,
        "newsscientist":0.1464950711,
        "technologyreview":0.2260409407,
        "venturebeat":0.2136041888,
        "wired":0.1795874506,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01954v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659520849000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12013v1",
        "predicted_newsworthiness":0.42058543,
        "title":"Effective and Interpretable Information Aggregation with Capacity Networks",
        "summary":"How to aggregate information from multiple instances is a key question multiple instance learning. Prior neural models implement different variants of the well-known encoder-decoder strategy according to which all input features are encoded a single, high-dimensional embedding which is then decoded to generate an output. In this work, inspired by Choquet capacities, we propose Capacity networks. Unlike encoder-decoders, Capacity networks generate multiple interpretable intermediate results which can be aggregated in a semantically meaningful space to obtain the final output. Our experiments show that implementing this simple inductive bias leads to improvements over different encoder-decoder architectures in a wide range of experiments. Moreover, the interpretable intermediate results make Capacity networks interpretable by design, which allows a semantically meaningful inspection, evaluation, and regularization of the network internals.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1028318588,
        "newsscientist":0.1485198336,
        "technologyreview":0.2601154665,
        "venturebeat":0.2396616663,
        "wired":0.1616157058,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12013v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658742316000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00361v1",
        "predicted_newsworthiness":0.4205622354,
        "title":"One for All: One-stage Referring Expression Comprehension with Dynamic Reasoning",
        "summary":"Referring Expression Comprehension (REC) is one of the most important tasks in visual reasoning that requires a model to detect the target object referred by a natural language expression. Among the proposed pipelines, the one-stage Referring Expression Comprehension (OSREC) has become the dominant trend since it merges the region proposal and selection stages. Many state-of-the-art OSREC models adopt a multi-hop reasoning strategy because a sequence of objects is frequently mentioned in a single expression which needs multi-hop reasoning to analyze the semantic relation. However, one unsolved issue of these models is that the number of reasoning steps needs to be pre-defined and fixed before inference, ignoring the varying complexity of expressions. In this paper, we propose a Dynamic Multi-step Reasoning Network, which allows the reasoning steps to be dynamically adjusted based on the reasoning state and expression complexity. Specifically, we adopt a Transformer module to memorize & process the reasoning state and a Reinforcement Learning strategy to dynamically infer the reasoning steps. The work achieves the state-of-the-art performance or significant improvements on several REC datasets, ranging from RefCOCO (+, g) with short expressions, to Ref-Reasoning, a dataset with long and complex compositional expressions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0792592775,
        "newsscientist":0.1167269663,
        "technologyreview":0.2047660816,
        "venturebeat":0.198009744,
        "wired":0.1328352681,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00361v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659243087000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13240v1",
        "predicted_newsworthiness":0.4205321733,
        "title":"Contrastive Image Synthesis and Self-supervised Feature Adaptation for Cross-Modality Biomedical Image Segmentation",
        "summary":"This work presents a novel framework CISFA (Contrastive Image synthesis and Self-supervised Feature Adaptation)that builds on image domain translation and unsupervised feature adaptation for cross-modality biomedical image segmentation. Different from existing works, we use a one-sided generative model and add a weighted patch-wise contrastive loss between sampled patches of the input image and the corresponding synthetic image, which serves as shape constraints. Moreover, we notice that the generated images and input images share similar structural information but are in different modalities. As such, we enforce contrastive losses on the generated images and the input images to train the encoder of a segmentation model to minimize the discrepancy between paired images in the learned embedding space. Compared with existing works that rely on adversarial learning for feature adaptation, such a method enables the encoder to learn domain-independent features in a more explicit way. We extensively evaluate our methods on segmentation tasks containing CT and MRI images for abdominal cavities and whole hearts. Experimental results show that the proposed framework not only outputs synthetic images with less distortion of organ shapes, but also outperforms state-of-the-art domain adaptation methods by a large margin.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0745758171,
        "newsscientist":0.1217260712,
        "technologyreview":0.1803249099,
        "venturebeat":0.1383224761,
        "wired":0.1098650807,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13240v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658886566000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13085v2",
        "predicted_newsworthiness":0.4204211119,
        "title":"Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment",
        "summary":"Detection Transformer (DETR) relies on One-to-One assignment, i.e., assigning one ground-truth object to only one positive object query, for end-to-end object detection and lacks the capability of exploiting multiple positive object queries. We present a novel DETR training approach, named {\\em Group DETR}, to support Group-wise One-to-Many assignment. We make simple modifications during training: (i) adopt $K$ groups of object queries; (ii) conduct decoder self-attention on each group of object queries with the same parameters; (iii) perform One-to-One label assignment for each group, leading to $K$ positive object queries for each ground-truth object. In inference, we only use one group of object queries, making no modifications to DETR architecture and processes. We validate the effectiveness of the proposed approach on DETR variants, including Conditional DETR, DAB-DETR, DN-DETR, and DINO. Code will be available.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0947974673,
        "newsscientist":0.1445010455,
        "technologyreview":0.2301107479,
        "venturebeat":0.2139713861,
        "wired":0.1699738624,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13085v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658858278000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00475v1",
        "predicted_newsworthiness":0.4199498705,
        "title":"Augmenting Vision Language Pretraining by Learning Codebook with Visual Semantics",
        "summary":"Language modality within the vision language pretraining framework is innately discretized, endowing each word in the language vocabulary a semantic meaning. In contrast, visual modality is inherently continuous and high-dimensional, which potentially prohibits the alignment as well as fusion between vision and language modalities. We therefore propose to \"discretize\" the visual representation by joint learning a codebook that imbues each visual token a semantic. We then utilize these discretized visual semantics as self-supervised ground-truths for building our Masked Image Modeling objective, a counterpart of Masked Language Modeling which proves successful for language models. To optimize the codebook, we extend the formulation of VQ-VAE which gives a theoretic guarantee. Experiments validate the effectiveness of our approach across common vision-language benchmarks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.095884126,
        "newsscientist":0.1262165404,
        "technologyreview":0.2394180453,
        "venturebeat":0.2190396961,
        "wired":0.1675709129,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00475v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659288969000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12644v1",
        "predicted_newsworthiness":0.4198088528,
        "title":"Learning Bipedal Walking On Planned Footsteps For Humanoid Robots",
        "summary":"Deep reinforcement learning (RL) based controllers for legged robots have demonstrated impressive robustness for walking in different environments for several robot platforms. To enable the application of RL policies for humanoid robots in real-world settings, it is crucial to build a system that can achieve robust walking in any direction, on 2D and 3D terrains, and be controllable by a user-command. In this paper, we tackle this problem by learning a policy to follow a given step sequence. The policy is trained with the help of a set of procedurally generated step sequences (also called footstep plans). We show that simply feeding the upcoming 2 steps to the policy is sufficient to achieve omnidirectional walking, turning in place, standing, and climbing stairs. Our method employs curriculum learning on the complexity of terrains, and circumvents the need for reference motions or pre-trained weights. We demonstrate the application of our proposed method to learn RL policies for 2 new robot platforms - HRP5P and JVRC-1 - in the MuJoCo simulation environment. The code for training and evaluation is available online.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0876416663,
        "newsscientist":0.1571595422,
        "technologyreview":0.2460497744,
        "venturebeat":0.2186775946,
        "wired":0.191641419,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12644v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai",
            "cs.lg"
        ],
        "published":1658808960000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.14545v1",
        "predicted_newsworthiness":0.4193843828,
        "title":"A One-Shot Reparameterization Method for Reducing the Loss of Tile Pruning on DNNs",
        "summary":"Recently, tile pruning has been widely studied to accelerate the inference of deep neural networks (DNNs). However, we found that the loss due to tile pruning, which can eliminate important elements together with unimportant elements, is large on trained DNNs. In this study, we propose a one-shot reparameterization method, called TileTrans, to reduce the loss of tile pruning. Specifically, we repermute the rows or columns of the weight matrix such that the model architecture can be kept unchanged after reparameterization. This repermutation realizes the reparameterization of the DNN model without any retraining. The proposed reparameterization method combines important elements into the same tile; thus, preserving the important elements after the tile pruning. Furthermore, TileTrans can be seamlessly integrated into existing tile pruning methods because it is a pre-processing method executed before pruning, which is orthogonal to most existing methods. The experimental results demonstrate that our method is essential in reducing the loss of tile pruning on DNNs. Specifically, the accuracy is improved by up to 17% for AlexNet while 5% for ResNet-34, where both models are pre-trained on ImageNet.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0947743876,
        "newsscientist":0.1232254143,
        "technologyreview":0.2344586387,
        "venturebeat":0.209160815,
        "wired":0.1412684135,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14545v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659083235000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13475v1",
        "predicted_newsworthiness":0.4191471107,
        "title":"PASTA-GAN++: A Versatile Framework for High-Resolution Unpaired Virtual Try-on",
        "summary":"Image-based virtual try-on is one of the most promising applications of human-centric image generation due to its tremendous real-world potential. In this work, we take a step forwards to explore versatile virtual try-on solutions, which we argue should possess three main properties, namely, they should support unsupervised training, arbitrary garment categories, and controllable garment editing. To this end, we propose a characteristic-preserving end-to-end network, the PAtch-routed SpaTially-Adaptive GAN++ (PASTA-GAN++), to achieve a versatile system for high-resolution unpaired virtual try-on. Specifically, our PASTA-GAN++ consists of an innovative patch-routed disentanglement module to decouple the intact garment into normalized patches, which is capable of retaining garment style information while eliminating the garment spatial information, thus alleviating the overfitting issue during unsupervised training. Furthermore, PASTA-GAN++ introduces a patch-based garment representation and a patch-guided parsing synthesis block, allowing it to handle arbitrary garment categories and support local garment editing. Finally, to obtain try-on results with realistic texture details, PASTA-GAN++ incorporates a novel spatially-adaptive residual module to inject the coarse warped garment feature into the generator. Extensive experiments on our newly collected UnPaired virtual Try-on (UPT) dataset demonstrate the superiority of PASTA-GAN++ over existing SOTAs and its ability for controllable garment editing.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0876602964,
        "newsscientist":0.1351642101,
        "technologyreview":0.1987402571,
        "venturebeat":0.1752693114,
        "wired":0.1540164003,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13475v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658922469000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13861v1",
        "predicted_newsworthiness":0.4190080891,
        "title":"DnSwin: Toward Real-World Denoising via Continuous Wavelet Sliding-Transformer",
        "summary":"Real-world image denoising is a practical image restoration problem that aims to obtain clean images from in-the-wild noisy input. Recently, Vision Transformer (ViT) exhibits a strong ability to capture long-range dependencies and many researchers attempt to apply ViT to image denoising tasks. However, real-world image is an isolated frame that makes the ViT build the long-range dependencies on the internal patches, which divides images into patches and disarranges the noise pattern and gradient continuity. In this article, we propose to resolve this issue by using a continuous Wavelet Sliding-Transformer that builds frequency correspondence under real-world scenes, called DnSwin. Specifically, we first extract the bottom features from noisy input images by using a CNN encoder. The key to DnSwin is to separate high-frequency and low-frequency information from the features and build frequency dependencies. To this end, we propose Wavelet Sliding-Window Transformer that utilizes discrete wavelet transform, self-attention and inverse discrete wavelet transform to extract deep features. Finally, we reconstruct the deep features into denoised images using a CNN decoder. Both quantitative and qualitative evaluations on real-world denoising benchmarks demonstrate that the proposed DnSwin performs favorably against the state-of-the-art methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0920549956,
        "newsscientist":0.1285395418,
        "technologyreview":0.1947449323,
        "venturebeat":0.1705431564,
        "wired":0.1425832373,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13861v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658975637000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00453v1",
        "predicted_newsworthiness":0.4189317478,
        "title":"One-Shot Medical Landmark Localization by Edge-Guided Transform and Noisy Landmark Refinement",
        "summary":"As an important upstream task for many medical applications, supervised landmark localization still requires non-negligible annotation costs to achieve desirable performance. Besides, due to cumbersome collection procedures, the limited size of medical landmark datasets impacts the effectiveness of large-scale self-supervised pre-training methods. To address these challenges, we propose a two-stage framework for one-shot medical landmark localization, which first infers landmarks by unsupervised registration from the labeled exemplar to unlabeled targets, and then utilizes these noisy pseudo labels to train robust detectors. To handle the significant structure variations, we learn an end-to-end cascade of global alignment and local deformations, under the guidance of novel loss functions which incorporate edge information. In stage II, we explore self-consistency for selecting reliable pseudo labels and cross-consistency for semi-supervised learning. Our method achieves state-of-the-art performances on public datasets of different body parts, which demonstrates its general applicability.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.086693737,
        "newsscientist":0.1381359189,
        "technologyreview":0.1860537665,
        "venturebeat":0.1566087011,
        "wired":0.1356625664,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00453v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659282148000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.08215v3",
        "predicted_newsworthiness":0.418608712,
        "title":"Optimizing out-of-plane stiffness for soft grippers",
        "summary":"In this paper, we presented a data-driven framework to optimize the out-of-plane stiffness for soft grippers to achieve mechanical properties as hard-to-twist and easy-to-bend. The effectiveness of this method is demonstrated in the design of a soft pneumatic bending actuator (SPBA). First, a new objective function is defined to quantitatively evaluate the out-of-plane stiffness as well as the bending performance. Then, sensitivity analysis is conducted on the parametric model of an SPBA design to determine the optimized design parameters with the help of Finite Element Analysis (FEA). To enable the computation of numerical optimization, a data-driven approach is employed to learn a cost function that directly represents the out-of-plane stiffness as a differentiable function of the design variables. A gradient-based method is used to maximize the out-of-plane stiffness of the SPBA while ensuring specific bending performance. The effectiveness of our method has been demonstrated in physical experiments taken on 3D-printed grippers.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0447306557,
        "newsscientist":0.1024670078,
        "technologyreview":0.1216739003,
        "venturebeat":0.1012077021,
        "wired":0.1121406209,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.08215v3",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658074745000,
        "published_hr":"Jul 17, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01898v1",
        "predicted_newsworthiness":0.4185759583,
        "title":"XCon: Learning with Experts for Fine-grained Category Discovery",
        "summary":"We address the problem of generalized category discovery (GCD) in this paper, i.e. clustering the unlabeled images leveraging the information from a set of seen classes, where the unlabeled images could contain both seen classes and unseen classes. The seen classes can be seen as an implicit criterion of classes, which makes this setting different from unsupervised clustering where the cluster criteria may be ambiguous. We mainly concern the problem of discovering categories within a fine-grained dataset since it is one of the most direct applications of category discovery, i.e. helping experts discover novel concepts within an unlabeled dataset using the implicit criterion set forth by the seen classes. State-of-the-art methods for generalized category discovery leverage contrastive learning to learn the representations, but the large inter-class similarity and intra-class variance pose a challenge for the methods because the negative examples may contain irrelevant cues for recognizing a category so the algorithms may converge to a local-minima. We present a novel method called Expert-Contrastive Learning (XCon) to help the model to mine useful information from the images by first partitioning the dataset into sub-datasets using k-means clustering and then performing contrastive learning on each of the sub-datasets to learn fine-grained discriminative features. Experiments on fine-grained datasets show a clear improved performance over the previous best methods, indicating the effectiveness of our method.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0883654628,
        "newsscientist":0.1462417732,
        "technologyreview":0.2249179605,
        "venturebeat":0.2029494248,
        "wired":0.148101106,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01898v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659513792000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00593v1",
        "predicted_newsworthiness":0.4182293884,
        "title":"Long Short-Term Preference Modeling for Continuous-Time Sequential Recommendation",
        "summary":"Modeling the evolution of user preference is essential in recommender systems. Recently, dynamic graph-based methods have been studied and achieved SOTA for recommendation, majority of which focus on user's stable long-term preference. However, in real-world scenario, user's short-term preference evolves over time dynamically. Although there exists sequential methods that attempt to capture it, how to model the evolution of short-term preference with dynamic graph-based methods has not been well-addressed yet. In particular: 1) existing methods do not explicitly encode and capture the evolution of short-term preference as sequential methods do; 2) simply using last few interactions is not enough for modeling the changing trend. In this paper, we propose Long Short-Term Preference Modeling for Continuous-Time Sequential Recommendation (LSTSR) to capture the evolution of short-term preference under dynamic graph. Specifically, we explicitly encode short-term preference and optimize it via memory mechanism, which has three key operations: Message, Aggregate and Update. Our memory mechanism can not only store one-hop information, but also trigger with new interactions online. Extensive experiments conducted on five public datasets show that LSTSR consistently outperforms many state-of-the-art recommendation methods across various lines.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.09287337,
        "newsscientist":0.1119028279,
        "technologyreview":0.1782661917,
        "venturebeat":0.2119937952,
        "wired":0.1765921459,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00593v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.lg"
        ],
        "published":1659325495000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.01913v1",
        "predicted_newsworthiness":0.4179530195,
        "title":"EgPDE-Net: Building Continuous Neural Networks for Time Series Prediction with Exogenous Variables",
        "summary":"While exogenous variables have a major impact on performance improvement in time series analysis, inter-series correlation and time dependence among them are rarely considered in the present continuous methods. The dynamical systems of multivariate time series could be modelled with complex unknown partial differential equations (PDEs) which play a prominent role in many disciplines of science and engineering. In this paper, we propose a continuous-time model for arbitrary-step prediction to learn an unknown PDE system in multivariate time series whose governing equations are parameterised by self-attention and gated recurrent neural networks. The proposed model, \\underline{E}xogenous-\\underline{g}uided \\underline{P}artial \\underline{D}ifferential \\underline{E}quation Network (EgPDE-Net), takes account of the relationships among the exogenous variables and their effects on the target series. Importantly, the model can be reduced into a regularised ordinary differential equation (ODE) problem with special designed regularisation guidance, which makes the PDE problem tractable to obtain numerical solutions and feasible to predict multiple future values of the target series at arbitrary time points. Extensive experiments demonstrate that our proposed model could achieve competitive accuracy over strong baselines: on average, it outperforms the best baseline by reducing $9.85\\%$ on RMSE and $13.98\\%$ on MAE for arbitrary-step prediction.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.136130996,
        "newsscientist":0.1560266703,
        "technologyreview":0.2181575836,
        "venturebeat":0.1988734693,
        "wired":0.1471353402,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01913v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659515671000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01739v1",
        "predicted_newsworthiness":0.4179279966,
        "title":"Reconstructing Sparse Illicit Supply Networks: A Case Study of Multiplex Drug Trafficking Networks",
        "summary":"The network structure provides critical information for law enforcement agencies to develop effective strategies to interdict illicit supply networks. However, the complete structure of covert networks is often unavailable, thus it is crucially important to develop approaches to infer a more complete structure of covert networks. In this paper, we work on real-world multiplex drug trafficking networks extracted from an investigation report. A statistical approach built on the EM algorithm (DegEM) as well as other methods based on structural similarity are applied to reconstruct the multiplex drug trafficking network given different fractions of observed nodes and links. It is found that DegEM approach achieves the best predictive performance in terms of several accuracy metrics. Meanwhile, structural similarity-based methods perform poorly in reconstructing the drug trafficking networks due to the sparsity of links between nodes in the network. The inferred multiplex networks can be leveraged to (i) inform the decision-making on monitoring covert networks as well as allocating limited resources for collecting additional information to improve the reconstruction accuracy and (ii) develop more effective interdiction strategies.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1731042841,
        "newsscientist":0.1511356481,
        "technologyreview":0.2258174379,
        "venturebeat":0.2000547674,
        "wired":0.1914722969,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01739v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.lg"
        ],
        "published":1659110810000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.12224v1",
        "predicted_newsworthiness":0.4178730299,
        "title":"Learning Human Body Motions from Skeleton-Based Observations for Robot-Assisted Therapy",
        "summary":"Robots applied in therapeutic scenarios, for instance in the therapy of individuals with Autism Spectrum Disorder, are sometimes used for imitation learning activities in which a person needs to repeat motions by the robot. To simplify the task of incorporating new types of motions that a robot can perform, it is desirable that the robot has the ability to learn motions by observing demonstrations from a human, such as a therapist. In this paper, we investigate an approach for acquiring motions from skeleton observations of a human, which are collected by a robot-centric RGB-D camera. Given a sequence of observations of various joints, the joint positions are mapped to match the configuration of a robot before being executed by a PID position controller. We evaluate the method, in particular the reproduction error, by performing a study with QTrobot in which the robot acquired different upper-body dance moves from multiple participants. The results indicate the method's overall feasibility, but also indicate that the reproduction quality is affected by noise in the skeleton observations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1121624912,
        "newsscientist":0.1887386663,
        "technologyreview":0.2294863757,
        "venturebeat":0.1962660739,
        "wired":0.1721048145,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12224v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658759046000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13038v1",
        "predicted_newsworthiness":0.417576574,
        "title":"Text-Guided Synthesis of Artistic Images with Retrieval-Augmented Diffusion Models",
        "summary":"Novel architectures have recently improved generative image synthesis leading to excellent visual quality in various tasks. Of particular note is the field of ``AI-Art'', which has seen unprecedented growth with the emergence of powerful multimodal models such as CLIP. By combining speech and image synthesis models, so-called ``prompt-engineering'' has become established, in which carefully selected and composed sentences are used to achieve a certain visual style in the synthesized image. In this note, we present an alternative approach based on retrieval-augmented diffusion models (RDMs). In RDMs, a set of nearest neighbors is retrieved from an external database during training for each training instance, and the diffusion model is conditioned on these informative samples. During inference (sampling), we replace the retrieval database with a more specialized database that contains, for example, only images of a particular visual style. This provides a novel way to prompt a general trained model after training and thereby specify a particular visual style. As shown by our experiments, this approach is superior to specifying the visual style within the text prompt. We open-source code and model weights at https:\/\/github.com\/CompVis\/latent-diffusion .",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1182196728,
        "newsscientist":0.1642714083,
        "technologyreview":0.2423622916,
        "venturebeat":0.2165599642,
        "wired":0.2063988724,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13038v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658854611000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01150v1",
        "predicted_newsworthiness":0.417392931,
        "title":"Mitigating Shadows in Lidar Scan Matching using Spherical Voxels",
        "summary":"In this paper we propose an approach to mitigate shadowing errors in Lidar scan matching, by introducing a preprocessing step based on spherical gridding. Because the grid aligns with the Lidar beam, it is relatively easy to eliminate shadow edges which cause systematic errors in Lidar scan matching. As we show through simulation, our proposed algorithm provides better results than ground-plane removal, the most common existing strategy for shadow mitigation. Unlike ground plane removal, our method applies to arbitrary terrains (e.g. shadows on urban walls, shadows in hilly terrain) while retaining key Lidar points on the ground that are critical for estimating changes in height, pitch, and roll. Our preprocessing algorithm can be used with a range of scan-matching methods; however, for voxel-based scan matching methods, it provides additional benefits by reducing computation costs and more evenly distributing Lidar points among voxels.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1253994794,
        "newsscientist":0.1621628794,
        "technologyreview":0.1942901174,
        "venturebeat":0.1835371109,
        "wired":0.1915870358,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01150v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv"
        ],
        "published":1659390291000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.12628v1",
        "predicted_newsworthiness":0.4173745758,
        "title":"Bundle MCR: Towards Conversational Bundle Recommendation",
        "summary":"Bundle recommender systems recommend sets of items (e.g., pants, shirt, and shoes) to users, but they often suffer from two issues: significant interaction sparsity and a large output space. In this work, we extend multi-round conversational recommendation (MCR) to alleviate these issues. MCR, which uses a conversational paradigm to elicit user interests by asking user preferences on tags (e.g., categories or attributes) and handling user feedback across multiple rounds, is an emerging recommendation setting to acquire user feedback and narrow down the output space, but has not been explored in the context of bundle recommendation. In this work, we propose a novel recommendation task named Bundle MCR. We first propose a new framework to formulate Bundle MCR as Markov Decision Processes (MDPs) with multiple agents, for user modeling, consultation and feedback handling in bundle contexts. Under this framework, we propose a model architecture, called Bundle Bert (Bunt) to (1) recommend items, (2) post questions and (3) manage conversations based on bundle-aware conversation states. Moreover, to train Bunt effectively, we propose a two-stage training strategy. In an offline pre-training stage, Bunt is trained using multiple cloze tasks to mimic bundle interactions in conversations. Then in an online fine-tuning stage, Bunt agents are enhanced by user interactions. Our experiments on multiple offline datasets as well as the human evaluation show the value of extending MCR frameworks to bundle settings and the effectiveness of our Bunt design.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1022510153,
        "newsscientist":0.1431471394,
        "technologyreview":0.2312739556,
        "venturebeat":0.2671776533,
        "wired":0.2392287444,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12628v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.ai"
        ],
        "published":1658806122000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.13600v1",
        "predicted_newsworthiness":0.4172201208,
        "title":"Lightweight and Progressively-Scalable Networks for Semantic Segmentation",
        "summary":"Multi-scale learning frameworks have been regarded as a capable class of models to boost semantic segmentation. The problem nevertheless is not trivial especially for the real-world deployments, which often demand high efficiency in inference latency. In this paper, we thoroughly analyze the design of convolutional blocks (the type of convolutions and the number of channels in convolutions), and the ways of interactions across multiple scales, all from lightweight standpoint for semantic segmentation. With such in-depth comparisons, we conclude three principles, and accordingly devise Lightweight and Progressively-Scalable Networks (LPS-Net) that novelly expands the network complexity in a greedy manner. Technically, LPS-Net first capitalizes on the principles to build a tiny network. Then, LPS-Net progressively scales the tiny network to larger ones by expanding a single dimension (the number of convolutional blocks, the number of channels, or the input resolution) at one time to meet the best speed\/accuracy tradeoff. Extensive experiments conducted on three datasets consistently demonstrate the superiority of LPS-Net over several efficient semantic segmentation methods. More remarkably, our LPS-Net achieves 73.4% mIoU on Cityscapes test set, with the speed of 413.5FPS on an NVIDIA GTX 1080Ti, leading to a performance improvement by 1.5% and a 65% speed-up against the state-of-the-art STDC. Code is available at \\url{https:\/\/github.com\/YihengZhang-CV\/LPS-Net}.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.093466378,
        "newsscientist":0.1311961526,
        "technologyreview":0.2351947231,
        "venturebeat":0.2230115501,
        "wired":0.1695074453,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13600v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658937628000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01925v1",
        "predicted_newsworthiness":0.4166846683,
        "title":"SuperLine3D: Self-supervised Line Segmentation and Description for LiDAR Point Cloud",
        "summary":"Poles and building edges are frequently observable objects on urban roads, conveying reliable hints for various computer vision tasks. To repetitively extract them as features and perform association between discrete LiDAR frames for registration, we propose the first learning-based feature segmentation and description model for 3D lines in LiDAR point cloud. To train our model without the time consuming and tedious data labeling process, we first generate synthetic primitives for the basic appearance of target lines, and build an iterative line auto-labeling process to gradually refine line labels on real LiDAR scans. Our segmentation model can extract lines under arbitrary scale perturbations, and we use shared EdgeConv encoder layers to train the two segmentation and descriptor heads jointly. Base on the model, we can build a highly-available global registration module for point cloud registration, in conditions without initial transformation hints. Experiments have demonstrated that our line-based registration method is highly competitive to state-of-the-art point-based approaches. Our code is available at https:\/\/github.com\/zxrzju\/SuperLine3D.git.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0987814236,
        "newsscientist":0.1436493729,
        "technologyreview":0.2006184141,
        "venturebeat":0.1955626826,
        "wired":0.1749955063,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01925v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659517574000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11895v1",
        "predicted_newsworthiness":0.4161911226,
        "title":"On Mitigating Hard Clusters for Face Clustering",
        "summary":"Face clustering is a promising way to scale up face recognition systems using large-scale unlabeled face images. It remains challenging to identify small or sparse face image clusters that we call hard clusters, which is caused by the heterogeneity, \\ie, high variations in size and sparsity, of the clusters. Consequently, the conventional way of using a uniform threshold (to identify clusters) often leads to a terrible misclassification for the samples that should belong to hard clusters. We tackle this problem by leveraging the neighborhood information of samples and inferring the cluster memberships (of samples) in a probabilistic way. We introduce two novel modules, Neighborhood-Diffusion-based Density (NDDe) and Transition-Probability-based Distance (TPDi), based on which we can simply apply the standard Density Peak Clustering algorithm with a uniform threshold. Our experiments on multiple benchmarks show that each module contributes to the final performance of our method, and by incorporating them into other advanced face clustering methods, these two modules can boost the performance of these methods to a new state-of-the-art. Code is available at: https:\/\/github.com\/echoanran\/On-Mitigating-Hard-Clusters.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0881832742,
        "newsscientist":0.1184034984,
        "technologyreview":0.1715059516,
        "venturebeat":0.1561228534,
        "wired":0.1373709022,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11895v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658721315000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12268v1",
        "predicted_newsworthiness":0.4160920527,
        "title":"What is Healthy? Generative Counterfactual Diffusion for Lesion Localization",
        "summary":"Reducing the requirement for densely annotated masks in medical image segmentation is important due to cost constraints. In this paper, we consider the problem of inferring pixel-level predictions of brain lesions by only using image-level labels for training. By leveraging recent advances in generative diffusion probabilistic models (DPM), we synthesize counterfactuals of \"How would a patient appear if X pathology was not present?\". The difference image between the observed patient state and the healthy counterfactual can be used for inferring the location of pathology. We generate counterfactuals that correspond to the minimal change of the input such that it is transformed to healthy domain. This requires training with healthy and unhealthy data in DPMs. We improve on previous counterfactual DPMs by manipulating the generation process with implicit guidance along with attention conditioning instead of using classifiers. Code is available at https:\/\/github.com\/vios-s\/Diff-SCM.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1075128467,
        "newsscientist":0.1466441277,
        "technologyreview":0.190302023,
        "venturebeat":0.1503014634,
        "wired":0.1356359488,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12268v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658763672000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00449v1",
        "predicted_newsworthiness":0.4157818804,
        "title":"SdAE: Self-distillated Masked Autoencoder",
        "summary":"With the development of generative-based self-supervised learning (SSL) approaches like BeiT and MAE, how to learn good representations by masking random patches of the input image and reconstructing the missing information has grown in concern. However, BeiT and PeCo need a \"pre-pretraining\" stage to produce discrete codebooks for masked patches representing. MAE does not require a pre-training codebook process, but setting pixels as reconstruction targets may introduce an optimization gap between pre-training and downstream tasks that good reconstruction quality may not always lead to the high descriptive capability for the model. Considering the above issues, in this paper, we propose a simple Self-distillated masked AutoEncoder network, namely SdAE. SdAE consists of a student branch using an encoder-decoder structure to reconstruct the missing information, and a teacher branch producing latent representation of masked tokens. We also analyze how to build good views for the teacher branch to produce latent representation from the perspective of information bottleneck. After that, we propose a multi-fold masking strategy to provide multiple masked views with balanced information for boosting the performance, which can also reduce the computational complexity. Our approach generalizes well: with only 300 epochs pre-training, a vanilla ViT-Base model achieves an 84.1% fine-tuning accuracy on ImageNet-1k classification, 48.6 mIOU on ADE20K segmentation, and 48.9 mAP on COCO detection, which surpasses other methods by a considerable margin. Code is available at https:\/\/github.com\/AbrahamYabo\/SdAE.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0872004615,
        "newsscientist":0.1228879901,
        "technologyreview":0.218854743,
        "venturebeat":0.186279752,
        "wired":0.1399908742,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00449v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659280045000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14674v1",
        "predicted_newsworthiness":0.4157238438,
        "title":"Enhanced Laser-Scan Matching with Online Error Estimation for Highway and Tunnel Driving",
        "summary":"Lidar data can be used to generate point clouds for the navigation of autonomous vehicles or mobile robotics platforms. Scan matching, the process of estimating the rigid transformation that best aligns two point clouds, is the basis for lidar odometry, a form of dead reckoning. Lidar odometry is particularly useful when absolute sensors, like GPS, are not available. Here we propose the Iterative Closest Ellipsoidal Transform (ICET), a scan matching algorithm which provides two novel improvements over the current state-of-the-art Normal Distributions Transform (NDT). Like NDT, ICET decomposes lidar data into voxels and fits a Gaussian distribution to the points within each voxel. The first innovation of ICET reduces geometric ambiguity along large flat surfaces by suppressing the solution along those directions. The second innovation of ICET is to infer the output error covariance associated with the position and orientation transformation between successive point clouds; the error covariance is particularly useful when ICET is incorporated into a state-estimation routine such as an extended Kalman filter. We constructed a simulation to compare the performance of ICET and NDT in 2D space both with and without geometric ambiguity and found that ICET produces superior estimates while accurately predicting solution accuracy.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0787901362,
        "newsscientist":0.1381814767,
        "technologyreview":0.2144257135,
        "venturebeat":0.200907953,
        "wired":0.1857364274,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14674v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv"
        ],
        "published":1659102152000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13820v1",
        "predicted_newsworthiness":0.4154280744,
        "title":"Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers",
        "summary":"Transformer encoder architectures have recently achieved state-of-the-art results on monocular 3D human mesh reconstruction, but they require a substantial number of parameters and expensive computations. Due to the large memory overhead and slow inference speed, it is difficult to deploy such models for practical use. In this paper, we propose a novel transformer encoder-decoder architecture for 3D human mesh reconstruction from a single image, called FastMETRO. We identify the performance bottleneck in the encoder-based transformers is caused by the token design which introduces high complexity interactions among input tokens. We disentangle the interactions via an encoder-decoder architecture, which allows our model to demand much fewer parameters and shorter inference time. In addition, we impose the prior knowledge of human body's morphological relationship via attention masking and mesh upsampling operations, which leads to faster convergence with higher accuracy. Our FastMETRO improves the Pareto-front of accuracy and efficiency, and clearly outperforms image-based methods on Human3.6M and 3DPW. Furthermore, we validate its generalizability on FreiHAND.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0769613749,
        "newsscientist":0.1369103248,
        "technologyreview":0.1930702809,
        "venturebeat":0.1745214108,
        "wired":0.1422759584,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13820v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1658962449000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01633v1",
        "predicted_newsworthiness":0.4150564586,
        "title":"UnrealEgo: A New Dataset for Robust Egocentric 3D Human Motion Capture",
        "summary":"We present UnrealEgo, i.e., a new large-scale naturalistic dataset for egocentric 3D human pose estimation. UnrealEgo is based on an advanced concept of eyeglasses equipped with two fisheye cameras that can be used in unconstrained environments. We design their virtual prototype and attach them to 3D human models for stereo view capture. We next generate a large corpus of human motions. As a consequence, UnrealEgo is the first dataset to provide in-the-wild stereo images with the largest variety of motions among existing egocentric datasets. Furthermore, we propose a new benchmark method with a simple but effective idea of devising a 2D keypoint estimation module for stereo inputs to improve 3D human pose estimation. The extensive experiments show that our approach outperforms the previous state-of-the-art methods qualitatively and quantitatively. UnrealEgo and our source codes are available on our project web page.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0850468604,
        "newsscientist":0.1370220091,
        "technologyreview":0.202382348,
        "venturebeat":0.2232617405,
        "wired":0.187208412,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01633v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659463194000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14202v1",
        "predicted_newsworthiness":0.4150412795,
        "title":"Progressive Voronoi Diagram Subdivision: Towards A Holistic Geometric Framework for Exemplar-free Class-Incremental Learning",
        "summary":"Exemplar-free Class-incremental Learning (CIL) is a challenging problem because rehearsing data from previous phases is strictly prohibited, causing catastrophic forgetting of Deep Neural Networks (DNNs). In this paper, we present iVoro, a holistic framework for CIL, derived from computational geometry. We found Voronoi Diagram (VD), a classical model for space subdivision, is especially powerful for solving the CIL problem, because VD itself can be constructed favorably in an incremental manner -- the newly added sites (classes) will only affect the proximate classes, making the non-contiguous classes hardly forgettable. Further, in order to find a better set of centers for VD construction, we colligate DNN with VD using Power Diagram and show that the VD structure can be optimized by integrating local DNN models using a divide-and-conquer algorithm. Moreover, our VD construction is not restricted to the deep feature space, but is also applicable to multiple intermediate feature spaces, promoting VD to be multi-centered VD (CIVD) that efficiently captures multi-grained features from DNN. Importantly, iVoro is also capable of handling uncertainty-aware test-time Voronoi cell assignment and has exhibited high correlations between geometric uncertainty and predictive accuracy (up to ~0.9). Putting everything together, iVoro achieves up to 25.26%, 37.09%, and 33.21% improvements on CIFAR-100, TinyImageNet, and ImageNet-Subset, respectively, compared to the state-of-the-art non-exemplar CIL approaches. In conclusion, iVoro enables highly accurate, privacy-preserving, and geometrically interpretable CIL that is particularly useful when cross-phase data sharing is forbidden, e.g. in medical applications. Our code is available at https:\/\/machunwei.github.io\/ivoro.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1036320214,
        "newsscientist":0.1628867158,
        "technologyreview":0.2601449377,
        "venturebeat":0.2357880884,
        "wired":0.1851442017,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14202v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659024917000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01195v1",
        "predicted_newsworthiness":0.4148864541,
        "title":"Making the Best of Both Worlds: A Domain-Oriented Transformer for Unsupervised Domain Adaptation",
        "summary":"Extensive studies on Unsupervised Domain Adaptation (UDA) have propelled the deployment of deep learning from limited experimental datasets into real-world unconstrained domains. Most UDA approaches align features within a common embedding space and apply a shared classifier for target prediction. However, since a perfectly aligned feature space may not exist when the domain discrepancy is large, these methods suffer from two limitations. First, the coercive domain alignment deteriorates target domain discriminability due to lacking target label supervision. Second, the source-supervised classifier is inevitably biased to source data, thus it may underperform in target domain. To alleviate these issues, we propose to simultaneously conduct feature alignment in two individual spaces focusing on different domains, and create for each space a domain-oriented classifier tailored specifically for that domain. Specifically, we design a Domain-Oriented Transformer (DOT) that has two individual classification tokens to learn different domain-oriented representations, and two classifiers to preserve domain-wise discriminability. Theoretical guaranteed contrastive-based alignment and the source-guided pseudo-label refinement strategy are utilized to explore both domain-invariant and specific information. Comprehensive experiments validate that our method achieves state-of-the-art on several benchmarks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0851587491,
        "newsscientist":0.116231146,
        "technologyreview":0.2201467004,
        "venturebeat":0.2062010088,
        "wired":0.1444158699,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01195v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659404317000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13963v1",
        "predicted_newsworthiness":0.4148270107,
        "title":"Meta-Learning based Degradation Representation for Blind Super-Resolution",
        "summary":"The most of CNN based super-resolution (SR) methods assume that the degradation is known (\\eg, bicubic). These methods will suffer a severe performance drop when the degradation is different from their assumption. Therefore, some approaches attempt to train SR networks with the complex combination of multiple degradations to cover the real degradation space. To adapt to multiple unknown degradations, introducing an explicit degradation estimator can actually facilitate SR performance. However, previous explicit degradation estimation methods usually predict Gaussian blur with the supervision of groundtruth blur kernels, and estimation errors may lead to SR failure. Thus, it is necessary to design a method that can extract implicit discriminative degradation representation. To this end, we propose a Meta-Learning based Region Degradation Aware SR Network (MRDA), including Meta-Learning Network (MLN), Degradation Extraction Network (DEN), and Region Degradation Aware SR Network (RDAN). To handle the lack of groundtruth degradation, we use the MLN to rapidly adapt to the specific complex degradation after several iterations and extract implicit degradation information. Subsequently, a teacher network MRDA$_{T}$ is designed to further utilize the degradation information extracted by MLN for SR. However, MLN requires iterating on paired low-resolution (LR) and corresponding high-resolution (HR) images, which is unavailable in the inference phase. Therefore, we adopt knowledge distillation (KD) to make the student network learn to directly extract the same implicit degradation representation (IDR) as the teacher from LR images.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0702572859,
        "newsscientist":0.100512313,
        "technologyreview":0.1442799098,
        "venturebeat":0.1065874458,
        "wired":0.0834266957,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13963v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658998980000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01448v2",
        "predicted_newsworthiness":0.4146977844,
        "title":"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model",
        "summary":"In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1140174024,
        "newsscientist":0.1325100123,
        "technologyreview":0.2238266036,
        "venturebeat":0.228152771,
        "wired":0.1697420884,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01448v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1659447007000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11770v1",
        "predicted_newsworthiness":0.4144690661,
        "title":"Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head Synthesis",
        "summary":"Talking head synthesis is an emerging technology with wide applications in film dubbing, virtual avatars and online education. Recent NeRF-based methods generate more natural talking videos, as they better capture the 3D structural information of faces. However, a specific model needs to be trained for each identity with a large dataset. In this paper, we propose Dynamic Facial Radiance Fields (DFRF) for few-shot talking head synthesis, which can rapidly generalize to an unseen identity with few training data. Different from the existing NeRF-based methods which directly encode the 3D geometry and appearance of a specific person into the network, our DFRF conditions face radiance field on 2D appearance images to learn the face prior. Thus the facial radiance field can be flexibly adjusted to the new identity with few reference images. Additionally, for better modeling of the facial deformations, we propose a differentiable face warping module conditioned on audio signals to deform all reference images to the query space. Extensive experiments show that with only tens of seconds of training clip available, our proposed DFRF can synthesize natural and high-quality audio-driven talking head videos for novel identities with only 40k iterations. We highly recommend readers view our supplementary video for intuitive comparisons. Code is available in https:\/\/sstzal.github.io\/DFRF\/.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1059962437,
        "newsscientist":0.1507476222,
        "technologyreview":0.2327722864,
        "venturebeat":0.2326908226,
        "wired":0.20661125,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11770v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658681163000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12042v1",
        "predicted_newsworthiness":0.4140303312,
        "title":"Revisiting AP Loss for Dense Object Detection: Adaptive Ranking Pair Selection",
        "summary":"Average precision (AP) loss has recently shown promising performance on the dense object detection task. However,a deep understanding of how AP loss affects the detector from a pairwise ranking perspective has not yet been developed.In this work, we revisit the average precision (AP)loss and reveal that the crucial element is that of selecting the ranking pairs between positive and negative samples.Based on this observation, we propose two strategies to improve the AP loss. The first of these is a novel Adaptive Pairwise Error (APE) loss that focusing on ranking pairs in both positive and negative samples. Moreover,we select more accurate ranking pairs by exploiting the normalized ranking scores and localization scores with a clustering algorithm. Experiments conducted on the MSCOCO dataset support our analysis and demonstrate the superiority of our proposed method compared with current classification and ranking loss. The code is available at https:\/\/github.com\/Xudangliatiger\/APE-Loss.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0770755219,
        "newsscientist":0.1248563632,
        "technologyreview":0.181536125,
        "venturebeat":0.1755589372,
        "wired":0.1493765743,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12042v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658745186000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13516v1",
        "predicted_newsworthiness":0.4139802343,
        "title":"Online Continual Learning with Contrastive Vision Transformer",
        "summary":"Online continual learning (online CL) studies the problem of learning sequential tasks from an online data stream without task boundaries, aiming to adapt to new data while alleviating catastrophic forgetting on the past tasks. This paper proposes a framework Contrastive Vision Transformer (CVT), which designs a focal contrastive learning strategy based on a transformer architecture, to achieve a better stability-plasticity trade-off for online CL. Specifically, we design a new external attention mechanism for online CL that implicitly captures previous tasks' information. Besides, CVT contains learnable focuses for each class, which could accumulate the knowledge of previous classes to alleviate forgetting. Based on the learnable focuses, we design a focal contrastive loss to rebalance contrastive learning between new and past classes and consolidate previously learned representations. Moreover, CVT contains a dual-classifier structure for decoupling learning current classes and balancing all observed classes. The extensive experimental results show that our approach achieves state-of-the-art performance with even fewer parameters on online CL benchmarks and effectively alleviates the catastrophic forgetting.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0899850757,
        "newsscientist":0.1380383794,
        "technologyreview":0.223853328,
        "venturebeat":0.2058872554,
        "wired":0.1526511188,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13516v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cv"
        ],
        "published":1658652662000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12537v1",
        "predicted_newsworthiness":0.4138345329,
        "title":"Live Stream Temporally Embedded 3D Human Body Pose and Shape Estimation",
        "summary":"3D Human body pose and shape estimation within a temporal sequence can be quite critical for understanding human behavior. Despite the significant progress in human pose estimation in the recent years, which are often based on single images or videos, human motion estimation on live stream videos is still a rarely-touched area considering its special requirements for real-time output and temporal consistency. To address this problem, we present a temporally embedded 3D human body pose and shape estimation (TePose) method to improve the accuracy and temporal consistency of pose estimation in live stream videos. TePose uses previous predictions as a bridge to feedback the error for better estimation in the current frame and to learn the correspondence between data frames and predictions in the history. A multi-scale spatio-temporal graph convolutional network is presented as the motion discriminator for adversarial training using datasets without any 3D labeling. We propose a sequential data loading strategy to meet the special start-to-end data processing requirement of live stream. We demonstrate the importance of each proposed module with extensive experiments. The results show the effectiveness of TePose on widely-used human pose benchmarks with state-of-the-art performance.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1109805809,
        "newsscientist":0.1407789056,
        "technologyreview":0.2028383131,
        "venturebeat":0.1939792623,
        "wired":0.1701138741,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12537v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658784119000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00934v1",
        "predicted_newsworthiness":0.413624856,
        "title":"Video Question Answering with Iterative Video-Text Co-Tokenization",
        "summary":"Video question answering is a challenging task that requires understanding jointly the language input, the visual information in individual video frames, as well as the temporal information about the events occurring in the video. In this paper, we propose a novel multi-stream video encoder for video question answering that uses multiple video inputs and a new video-text iterative co-tokenization approach to answer a variety of questions related to videos. We experimentally evaluate the model on several datasets, such as MSRVTT-QA, MSVD-QA, IVQA, outperforming the previous state-of-the-art by large margins. Simultaneously, our model reduces the required GFLOPs from 150-360 to only 67, producing a highly efficient video question answering model.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0982470459,
        "newsscientist":0.1175034735,
        "technologyreview":0.1961917876,
        "venturebeat":0.2083658583,
        "wired":0.1741667718,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00934v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659368138000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14745v1",
        "predicted_newsworthiness":0.4133485357,
        "title":"Collision detection and identification for a legged manipulator",
        "summary":"To safely deploy legged robots in the real world it is necessary to provide them with the ability to reliably detect unexpected contacts and accurately estimate the corresponding contact force. In this paper, we propose a collision detection and identification pipeline for a quadrupedal manipulator. We first introduce an approach to estimate the collision time span based on band-pass filtering and show that this information is key for obtaining accurate collision force estimates. We then improve the accuracy of the identified force magnitude by compensating for model inaccuracies, unmodeled loads, and any other potential source of quasi-static disturbances acting on the robot. We validate our framework with extensive hardware experiments in various scenarios, including trotting and additional unmodeled load on the robot.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.085046337,
        "newsscientist":0.1744501482,
        "technologyreview":0.2013773852,
        "venturebeat":0.1649899534,
        "wired":0.1705845891,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14745v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659109043000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00287v2",
        "predicted_newsworthiness":0.4130256541,
        "title":"Simplex Clustering via sBeta with Applications to Online Adjustment of Black-Box Predictions",
        "summary":"We explore clustering the softmax predictions of deep neural networks and introduce a novel probabilistic clustering method, referred to as k-sBetas. In the general context of clustering distributions, the existing methods focused on exploring distortion measures tailored to simplex data, such as the KL divergence, as alternatives to the standard Euclidean distance. We provide a general perspective of clustering distributions, which emphasizes that the statistical models underlying distortion-based methods may not be descriptive enough. Instead, we optimize a mixed-variable objective measuring the conformity of data within each cluster to the introduced sBeta density function, whose parameters are constrained and estimated jointly with binary assignment variables. Our versatile formulation approximates a variety of parametric densities for modeling cluster data, and enables to control the cluster-balance bias. This yields highly competitive performances for efficient unsupervised adjustment of black-box predictions in a variety of scenarios, including one-shot classification and unsupervised domain adaptation in real-time for road segmentation. Implementation is available at https:\/\/github.com\/fchiaroni\/Clustering_Softmax_Predictions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.085578886,
        "newsscientist":0.1300330404,
        "technologyreview":0.2065335478,
        "venturebeat":0.1940257301,
        "wired":0.1511575236,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00287v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1659205751000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12888v1",
        "predicted_newsworthiness":0.4128032042,
        "title":"LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection",
        "summary":"Visual question answering (VQA) often requires an understanding of visual concepts and language semantics, which relies on external knowledge. Most existing methods exploit pre-trained language models or\/and unstructured text, but the knowledge in these resources are often incomplete and noisy. Some methods prefer to use knowledge graphs (KGs) which often have intensive structured knowledge, but the research is still quite preliminary. In this paper, we propose LaKo, a knowledge-driven VQA method via Late Knowledge-to-text Injection. To effectively incorporate an external KG, we transfer triples into text and propose a late injection mechanism. Finally we address VQA as a text generation task with an effective encoder-decoder paradigm. In the evaluation with OKVQA datasets, our method achieves state-of-the-art results.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0853601435,
        "newsscientist":0.1183543498,
        "technologyreview":0.2013301892,
        "venturebeat":0.1963615181,
        "wired":0.1499780653,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12888v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658842191000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13611v2",
        "predicted_newsworthiness":0.4127131638,
        "title":"A Semi-automatic Cell Tracking Process Towards Completing the 4D Atlas of C. elegans Development",
        "summary":"The nematode Caenorhabditis elegans (C. elegans) is used as a model organism to better understand developmental biology and neurobiology. C. elegans features an invariant cell lineage, which has been catalogued and observed using fluorescence microscopy images. However, established methods to track cells in late-stage development fail to generalize once sporadic muscular twitching has begun. We build upon methodology which uses skin cells as fiducial markers to carry out cell tracking despite random twitching. In particular, we present a cell nucleus segmentation and tracking procedure which was integrated into a 3D rendering GUI to improve efficiency in tracking cells across late-stage development. Results on images depicting aforementioned muscle cell nuclei across three test embryos suggest the fiducial markers in conjunction with a classic tracking paradigm overcome sporadic twitching.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1202344242,
        "newsscientist":0.2266294753,
        "technologyreview":0.2244619408,
        "venturebeat":0.1714920983,
        "wired":0.1798867256,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13611v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658938912000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12126v1",
        "predicted_newsworthiness":0.4125975418,
        "title":"PirouNet: Creating Intentional Dance with Semi-Supervised Conditional Recurrent Variational Autoencoders",
        "summary":"Using Artificial Intelligence (AI) to create dance choreography with intention is still at an early stage. Methods that conditionally generate dance sequences remain limited in their ability to follow choreographer-specific creative intentions, often relying on external prompts or supervised learning. In the same vein, fully annotated dance datasets are rare and labor intensive. To fill this gap and help leverage deep learning as a meaningful tool for choreographers, we propose \"PirouNet\", a semi-supervised conditional recurrent variational autoencoder together with a dance labeling web application. PirouNet allows dance professionals to annotate data with their own subjective creative labels and subsequently generate new bouts of choreography based on their aesthetic criteria. Thanks to the proposed semi-supervised approach, PirouNet only requires a small portion of the dataset to be labeled, typically on the order of 1%. We demonstrate PirouNet's capabilities as it generates original choreography based on the \"Laban Time Effort\", an established dance notion describing intention for a movement's time dynamics. We extensively evaluate PirouNet's dance creations through a series of qualitative and quantitative metrics, validating its applicability as a tool for choreographers.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1641392835,
        "newsscientist":0.1932761066,
        "technologyreview":0.2578803632,
        "venturebeat":0.2441058138,
        "wired":0.2414145668,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12126v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658426699000,
        "published_hr":"Jul 21, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12759v1",
        "predicted_newsworthiness":0.4124439903,
        "title":"Training Effective Neural Sentence Encoders from Automatically Mined Paraphrases",
        "summary":"Sentence embeddings are commonly used in text clustering and semantic retrieval tasks. State-of-the-art sentence representation methods are based on artificial neural networks fine-tuned on large collections of manually labeled sentence pairs. Sufficient amount of annotated data is available for high-resource languages such as English or Chinese. In less popular languages, multilingual models have to be used, which offer lower performance. In this publication, we address this problem by proposing a method for training effective language-specific sentence encoders without manually labeled data. Our approach is to automatically construct a dataset of paraphrase pairs from sentence-aligned bilingual text corpora. We then use the collected data to fine-tune a Transformer language model with an additional recurrent pooling layer. Our sentence encoder can be trained in less than a day on a single graphics card, achieving high performance on a diverse set of sentence-level tasks. We evaluate our method on eight linguistic tasks in Polish, comparing it with the best available multilingual sentence encoders.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1068204498,
        "newsscientist":0.1158021082,
        "technologyreview":0.2141162317,
        "venturebeat":0.2160097529,
        "wired":0.1538952989,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12759v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658826536000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11463v1",
        "predicted_newsworthiness":0.4123826024,
        "title":"When Counting Meets HMER: Counting-Aware Network for Handwritten Mathematical Expression Recognition",
        "summary":"Recently, most handwritten mathematical expression recognition (HMER) methods adopt the encoder-decoder networks, which directly predict the markup sequences from formula images with the attention mechanism. However, such methods may fail to accurately read formulas with complicated structure or generate long markup sequences, as the attention results are often inaccurate due to the large variance of writing styles or spatial layouts. To alleviate this problem, we propose an unconventional network for HMER named Counting-Aware Network (CAN), which jointly optimizes two tasks: HMER and symbol counting. Specifically, we design a weakly-supervised counting module that can predict the number of each symbol class without the symbol-level position annotations, and then plug it into a typical attention-based encoder-decoder model for HMER. Experiments on the benchmark datasets for HMER validate that both joint optimization and counting results are beneficial for correcting the prediction errors of encoder-decoder models, and CAN consistently outperforms the state-of-the-art methods. In particular, compared with an encoder-decoder model for HMER, the extra time cost caused by the proposed counting module is marginal. The source code is available at https:\/\/github.com\/LBH1024\/CAN.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0976292563,
        "newsscientist":0.130640849,
        "technologyreview":0.2002811952,
        "venturebeat":0.1955655475,
        "wired":0.144887919,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11463v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658565572000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00275v1",
        "predicted_newsworthiness":0.412330779,
        "title":"Revisiting the Critical Factors of Augmentation-Invariant Representation Learning",
        "summary":"We focus on better understanding the critical factors of augmentation-invariant representation learning. We revisit MoCo v2 and BYOL and try to prove the authenticity of the following assumption: different frameworks bring about representations of different characteristics even with the same pretext task. We establish the first benchmark for fair comparisons between MoCo v2 and BYOL, and observe: (i) sophisticated model configurations enable better adaptation to pre-training dataset; (ii) mismatched optimization strategies of pre-training and fine-tuning hinder model from achieving competitive transfer performances. Given the fair benchmark, we make further investigation and find asymmetry of network structure endows contrastive frameworks to work well under the linear evaluation protocol, while may hurt the transfer performances on long-tailed classification tasks. Moreover, negative samples do not make models more sensible to the choice of data augmentations, nor does the asymmetric network structure. We believe our findings provide useful information for future work.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1080992559,
        "newsscientist":0.1502421305,
        "technologyreview":0.2660424503,
        "venturebeat":0.2338563308,
        "wired":0.1612604875,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00275v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659200833000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13492v1",
        "predicted_newsworthiness":0.4122714595,
        "title":"Time to augment contrastive learning",
        "summary":"Biological vision systems are unparalleled in their ability to learn visual representations without supervision. In machine learning, contrastive learning (CL) has led to major advances in forming object representations in an unsupervised fashion. These systems learn representations invariant to augmentation operations over images, like cropping or flipping. In contrast, biological vision systems exploit the temporal structure of the visual experience. This gives access to augmentations not commonly used in CL, like watching the same object from multiple viewpoints or against different backgrounds. Here, we systematically investigate and compare the potential benefits of such time-based augmentations for learning object categories. Our results show that time-based augmentations achieve large performance gains over state-of-the-art image augmentations. Specifically, our analyses reveal that: 1) 3-D object rotations drastically improve the learning of object categories; 2) viewing objects against changing backgrounds is vital for learning to discard background-related information. Overall, we conclude that time-based augmentations can greatly improve contrastive learning, narrowing the gap between artificial and biological vision systems.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1177410276,
        "newsscientist":0.2087118246,
        "technologyreview":0.2872357809,
        "venturebeat":0.2431039886,
        "wired":0.2150258935,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13492v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1658924877000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12767v1",
        "predicted_newsworthiness":0.4117949479,
        "title":"Criteria Comparative Learning for Real-scene Image Super-Resolution",
        "summary":"Real-scene image super-resolution aims to restore real-world low-resolution images into their high-quality versions. A typical RealSR framework usually includes the optimization of multiple criteria which are designed for different image properties, by making the implicit assumption that the ground-truth images can provide a good trade-off between different criteria. However, this assumption could be easily violated in practice due to the inherent contrastive relationship between different image properties. Contrastive learning (CL) provides a promising recipe to relieve this problem by learning discriminative features using the triplet contrastive losses. Though CL has achieved significant success in many computer vision tasks, it is non-trivial to introduce CL to RealSR due to the difficulty in defining valid positive image pairs in this case. Inspired by the observation that the contrastive relationship could also exist between the criteria, in this work, we propose a novel training paradigm for RealSR, named Criteria Comparative Learning (Cria-CL), by developing contrastive losses defined on criteria instead of image patches. In addition, a spatial projector is proposed to obtain a good view for Cria-CL in RealSR. Our experiments demonstrate that compared with the typical weighted regression strategy, our method achieves a significant improvement under similar parameter settings.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0741076707,
        "newsscientist":0.1038572526,
        "technologyreview":0.1791659903,
        "venturebeat":0.1433672302,
        "wired":0.1155769906,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12767v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658827372000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01849v1",
        "predicted_newsworthiness":0.41152592,
        "title":"Coarse-to-Fine Knowledge-Enhanced Multi-Interest Learning Framework for Multi-Behavior Recommendation",
        "summary":"Multi-types of behaviors (e.g., clicking, adding to cart, purchasing, etc.) widely exist in most real-world recommendation scenarios, which are beneficial to learn users' multi-faceted preferences. As dependencies are explicitly exhibited by the multiple types of behaviors, effectively modeling complex behavior dependencies is crucial for multi-behavior prediction. The state-of-the-art multi-behavior models learn behavior dependencies indistinguishably with all historical interactions as input. However, different behaviors may reflect different aspects of user preference, which means that some irrelevant interactions may play as noises to the target behavior to be predicted. To address the aforementioned limitations, we introduce multi-interest learning to the multi-behavior recommendation. More specifically, we propose a novel Coarse-to-fine Knowledge-enhanced Multi-interest Learning (CKML) framework to learn shared and behavior-specific interests for different behaviors. CKML introduces two advanced modules, namely Coarse-grained Interest Extracting (CIE) and Fine-grained Behavioral Correlation (FBC), which work jointly to capture fine-grained behavioral dependencies. CIE uses knowledge-aware information to extract initial representations of each interest. FBC incorporates a dynamic routing scheme to further assign each behavior among interests. Additionally, we use the self-attention mechanism to correlate different behavioral information at the interest level. Empirical results on three real-world datasets verify the effectiveness and efficiency of our model in exploiting multi-behavior data. Further experiments demonstrate the effectiveness of each module and the robustness and superiority of the shared and specific modelling paradigm for multi-behavior data.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1005348282,
        "newsscientist":0.1257798144,
        "technologyreview":0.2023189753,
        "venturebeat":0.2256115399,
        "wired":0.1791951591,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01849v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.ai"
        ],
        "published":1659504494000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.01521v1",
        "predicted_newsworthiness":0.4114950134,
        "title":"DSR -- A dual subspace re-projection network for surface anomaly detection",
        "summary":"The state-of-the-art in discriminative unsupervised surface anomaly detection relies on external datasets for synthesizing anomaly-augmented training images. Such approaches are prone to failure on near-in-distribution anomalies since these are difficult to be synthesized realistically due to their similarity to anomaly-free regions. We propose an architecture based on quantized feature space representation with dual decoders, DSR, that avoids the image-level anomaly synthesis requirement. Without making any assumptions about the visual properties of anomalies, DSR generates the anomalies at the feature level by sampling the learned quantized feature space, which allows a controlled generation of near-in-distribution anomalies. DSR achieves state-of-the-art results on the KSDD2 and MVTec anomaly detection datasets. The experiments on the challenging real-world KSDD2 dataset show that DSR significantly outperforms other unsupervised surface anomaly detection methods, improving the previous top-performing methods by 10% AP in anomaly detection and 35% AP in anomaly localization.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.099382166,
        "newsscientist":0.1576487693,
        "technologyreview":0.2152554633,
        "venturebeat":0.1924062041,
        "wired":0.1605682109,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01521v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659453329000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12120v1",
        "predicted_newsworthiness":0.4114782722,
        "title":"Efficient Graph-Friendly COCO Metric Computation for Train-Time Model Evaluation",
        "summary":"Evaluating the COCO mean average precision (MaP) and COCO recall metrics as part of the static computation graph of modern deep learning frameworks poses a unique set of challenges. These challenges include the need for maintaining a dynamic-sized state to compute mean average precision, reliance on global dataset-level statistics to compute the metrics, and managing differing numbers of bounding boxes between images in a batch. As a consequence, it is common practice for researchers and practitioners to evaluate COCO metrics as a post training evaluation step. With a graph-friendly algorithm to compute COCO Mean Average Precision and recall, these metrics could be evaluated at training time, improving visibility into the evolution of the metrics through training curve plots, and decreasing iteration time when prototyping new model versions. Our contributions include an accurate approximation algorithm for Mean Average Precision, an open source implementation of both COCO mean average precision and COCO recall, extensive numerical benchmarks to verify the accuracy of our implementations, and an open-source training loop that include train-time evaluation of mean average precision and recall.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0876288616,
        "newsscientist":0.1429917271,
        "technologyreview":0.259400545,
        "venturebeat":0.2427209366,
        "wired":0.1729970911,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12120v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658443140000,
        "published_hr":"Jul 21, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14678v1",
        "predicted_newsworthiness":0.4109935127,
        "title":"AlphaVC: High-Performance and Efficient Learned Video Compression",
        "summary":"Recently, learned video compression has drawn lots of attention and show a rapid development trend with promising results. However, the previous works still suffer from some criticial issues and have a performance gap with traditional compression standards in terms of widely used PSNR metric. In this paper, we propose several techniques to effectively improve the performance. First, to address the problem of accumulative error, we introduce a conditional-I-frame as the first frame in the GoP, which stabilizes the reconstructed quality and saves the bit-rate. Second, to efficiently improve the accuracy of inter prediction without increasing the complexity of decoder, we propose a pixel-to-feature motion prediction method at encoder side that helps us to obtain high-quality motion information. Third, we propose a probability-based entropy skipping method, which not only brings performance gain, but also greatly reduces the runtime of entropy coding. With these powerful techniques, this paper proposes AlphaVC, a high-performance and efficient learned video compression scheme. To the best of our knowledge, AlphaVC is the first E2E AI codec that exceeds the latest compression standard VVC on all common test datasets for both PSNR (-28.2% BD-rate saving) and MSSSIM (-52.2% BD-rate saving), and has very fast encoding (0.001x VVC) and decoding (1.69x VVC) speeds.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0736196587,
        "newsscientist":0.1151719898,
        "technologyreview":0.18235225,
        "venturebeat":0.1775131006,
        "wired":0.1574128773,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14678v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659102764000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14812v1",
        "predicted_newsworthiness":0.4108432635,
        "title":"GLEAN: Generative Latent Bank for Image Super-Resolution and Beyond",
        "summary":"We show that pre-trained Generative Adversarial Networks (GANs) such as StyleGAN and BigGAN can be used as a latent bank to improve the performance of image super-resolution. While most existing perceptual-oriented approaches attempt to generate realistic outputs through learning with adversarial loss, our method, Generative LatEnt bANk (GLEAN), goes beyond existing practices by directly leveraging rich and diverse priors encapsulated in a pre-trained GAN. But unlike prevalent GAN inversion methods that require expensive image-specific optimization at runtime, our approach only needs a single forward pass for restoration. GLEAN can be easily incorporated in a simple encoder-bank-decoder architecture with multi-resolution skip connections. Employing priors from different generative models allows GLEAN to be applied to diverse categories (\\eg~human faces, cats, buildings, and cars). We further present a lightweight version of GLEAN, named LightGLEAN, which retains only the critical components in GLEAN. Notably, LightGLEAN consists of only 21% of parameters and 35% of FLOPs while achieving comparable image quality. We extend our method to different tasks including image colorization and blind image restoration, and extensive experiments show that our proposed models perform favorably in comparison to existing methods. Codes and models are available at https:\/\/github.com\/open-mmlab\/mmediting.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0826669232,
        "newsscientist":0.1231027404,
        "technologyreview":0.2052839079,
        "venturebeat":0.1774803143,
        "wired":0.149030465,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14812v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659117541000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02070v1",
        "predicted_newsworthiness":0.4108409249,
        "title":"Efficient Fine-Tuning of Compressed Language Models with Learners",
        "summary":"Fine-tuning BERT-based models is resource-intensive in memory, computation, and time. While many prior works aim to improve inference efficiency via compression techniques, e.g., pruning, these works do not explicitly address the computational challenges of training to downstream tasks. We introduce Learner modules and priming, novel methods for fine-tuning that exploit the overparameterization of pre-trained language models to gain benefits in convergence speed and resource utilization. Learner modules navigate the double bind of 1) training efficiently by fine-tuning a subset of parameters, and 2) training effectively by ensuring quick convergence and high metric scores. Our results on DistilBERT demonstrate that learners perform on par with or surpass the baselines. Learners train 7x fewer parameters than state-of-the-art methods on GLUE. On CoLA, learners fine-tune 20% faster, and have significantly lower resource utilization.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0792520185,
        "newsscientist":0.1010379614,
        "technologyreview":0.2088676946,
        "venturebeat":0.2148511047,
        "wired":0.153755832,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02070v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1659534150000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.13309v1",
        "predicted_newsworthiness":0.4106810918,
        "title":"Federated Selective Aggregation for Knowledge Amalgamation",
        "summary":"In this paper, we explore a new knowledge-amalgamation problem, termed Federated Selective Aggregation (FedSA). The goal of FedSA is to train a student model for a new task with the help of several decentralized teachers, whose pre-training tasks and data are different and agnostic. Our motivation for investigating such a problem setup stems from a recent dilemma of model sharing. Many researchers or institutes have spent enormous resources on training large and competent networks. Due to the privacy, security, or intellectual property issues, they are, however, not able to share their own pre-trained models, even if they wish to contribute to the community. The proposed FedSA offers a solution to this dilemma and makes it one step further since, again, the learned student may specialize in a new task different from all of the teachers. To this end, we proposed a dedicated strategy for handling FedSA. Specifically, our student-training process is driven by a novel saliency-based approach that adaptively selects teachers as the participants and integrates their representative capabilities into the student. To evaluate the effectiveness of FedSA, we conduct experiments on both single-task and multi-task settings. Experimental results demonstrate that FedSA effectively amalgamates knowledge from decentralized models and achieves competitive performance to centralized baselines.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1456722573,
        "newsscientist":0.1779524548,
        "technologyreview":0.290480912,
        "venturebeat":0.2770918711,
        "wired":0.2143993712,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13309v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658900210000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14007v1",
        "predicted_newsworthiness":0.4105954377,
        "title":"Learning Dynamic Manipulation Skills from Haptic-Play",
        "summary":"In this paper, we propose a data-driven skill learning approach to solve highly dynamic manipulation tasks entirely from offline teleoperated play data. We use a bilateral teleoperation system to continuously collect a large set of dexterous and agile manipulation behaviors, which is enabled by providing direct force feedback to the operator. We jointly learn the state conditional latent skill distribution and skill decoder network in the form of goal-conditioned policy and skill conditional state transition dynamics using a two-stage generative modeling framework. This allows one to perform robust model-based planning, both online and offline planning methods, in the learned skill-space to accomplish any given downstream tasks at test time. We provide both simulated and real-world dual-arm box manipulation experiments showing that a sequence of force-controlled dynamic manipulation skills can be composed in real-time to successfully configure the box to the randomly selected target position and orientation; please refer to the supplementary video, https:\/\/youtu.be\/LA5B236ILzM.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0931623826,
        "newsscientist":0.1573979232,
        "technologyreview":0.2528859015,
        "venturebeat":0.2166805101,
        "wired":0.184930306,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14007v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659006189000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01573v1",
        "predicted_newsworthiness":0.4101806752,
        "title":"Stochastic Deep Networks with Linear Competing Units for Model-Agnostic Meta-Learning",
        "summary":"This work addresses meta-learning (ML) by considering deep networks with stochastic local winner-takes-all (LWTA) activations. This type of network units results in sparse representations from each model layer, as the units are organized into blocks where only one unit generates a non-zero output. The main operating principle of the introduced units rely on stochastic principles, as the network performs posterior sampling over competing units to select the winner. Therefore, the proposed networks are explicitly designed to extract input data representations of sparse stochastic nature, as opposed to the currently standard deterministic representation paradigm. Our approach produces state-of-the-art predictive accuracy on few-shot image classification and regression experiments, as well as reduced predictive error on an active learning setting; these improvements come with an immensely reduced computational cost.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0801389193,
        "newsscientist":0.1326473341,
        "technologyreview":0.2289022246,
        "venturebeat":0.1987802729,
        "wired":0.1427491773,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01573v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659457194000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00843v1",
        "predicted_newsworthiness":0.4095450051,
        "title":"Relay Hindsight Experience Replay: Continual Reinforcement Learning for Robot Manipulation Tasks with Sparse Rewards",
        "summary":"Learning with sparse rewards is usually inefficient in Reinforcement Learning (RL). Hindsight Experience Replay (HER) has been shown an effective solution to handle the low sample efficiency that results from sparse rewards by goal relabeling. However, the HER still has an implicit virtual-positive sparse reward problem caused by invariant achieved goals, especially for robot manipulation tasks. To solve this problem, we propose a novel model-free continual RL algorithm, called Relay-HER (RHER). The proposed method first decomposes and rearranges the original long-horizon task into new sub-tasks with incremental complexity. Subsequently, a multi-task network is designed to learn the sub-tasks in ascending order of complexity. To solve the virtual-positive sparse reward problem, we propose a Random-Mixed Exploration Strategy (RMES), in which the achieved goals of the sub-task with higher complexity are quickly changed under the guidance of the one with lower complexity. The experimental results indicate the significant improvements in sample efficiency of RHER compared to vanilla-HER in five typical robot manipulation tasks, including Push, PickAndPlace, Drawer, Insert, and ObstaclePush. The proposed RHER method has also been applied to learn a contact-rich push task on a physical robot from scratch, and the success rate reached 10\/10 with only 250 episodes.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0832008147,
        "newsscientist":0.1536082624,
        "technologyreview":0.2467794255,
        "venturebeat":0.2041874671,
        "wired":0.1666484592,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00843v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai"
        ],
        "published":1659360601000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00662v1",
        "predicted_newsworthiness":0.4092317982,
        "title":"Local Perception-Aware Transformer for Aerial Tracking",
        "summary":"Transformer-based visual object tracking has been utilized extensively. However, the Transformer structure is lack of enough inductive bias. In addition, only focusing on encoding the global feature does harm to modeling local details, which restricts the capability of tracking in aerial robots. Specifically, with local-modeling to global-search mechanism, the proposed tracker replaces the global encoder by a novel local-recognition encoder. In the employed encoder, a local-recognition attention and a local element correction network are carefully designed for reducing the global redundant information interference and increasing local inductive bias. Meanwhile, the latter can model local object details precisely under aerial view through detail-inquiry net. The proposed method achieves competitive accuracy and robustness in several authoritative aerial benchmarks with 316 sequences in total. The proposed tracker's practicability and efficiency have been validated by the real-world tests.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0734828782,
        "newsscientist":0.1628531506,
        "technologyreview":0.2248923479,
        "venturebeat":0.1964511868,
        "wired":0.1922805436,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00662v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659340455000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13543v1",
        "predicted_newsworthiness":0.4087836733,
        "title":"Abstracting Sketches through Simple Primitives",
        "summary":"Humans show high-level of abstraction capabilities in games that require quickly communicating object information. They decompose the message content into multiple parts and communicate them in an interpretable protocol. Toward equipping machines with such capabilities, we propose the Primitive-based Sketch Abstraction task where the goal is to represent sketches using a fixed set of drawing primitives under the influence of a budget. To solve this task, our Primitive-Matching Network (PMN), learns interpretable abstractions of a sketch in a self supervised manner. Specifically, PMN maps each stroke of a sketch to its most similar primitive in a given set, predicting an affine transformation that aligns the selected primitive to the target stroke. We learn this stroke-to-primitive mapping end-to-end with a distance-transform loss that is minimal when the original sketch is precisely reconstructed with the predicted primitives. Our PMN abstraction empirically achieves the highest performance on sketch recognition and sketch-based image retrieval given a communication budget, while at the same time being highly interpretable. This opens up new possibilities for sketch analysis, such as comparing sketches by extracting the most relevant primitives that define an object category. Code is available at https:\/\/github.com\/ExplainableML\/sketch-primitives.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1089266901,
        "newsscientist":0.153242273,
        "technologyreview":0.2404146302,
        "venturebeat":0.2099119331,
        "wired":0.195136402,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13543v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658932359000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12872v1",
        "predicted_newsworthiness":0.4086343018,
        "title":"Generalized Probabilistic U-Net for medical image segementation",
        "summary":"We propose the Generalized Probabilistic U-Net, which extends the Probabilistic U-Net by allowing more general forms of the Gaussian distribution as the latent space distribution that can better approximate the uncertainty in the reference segmentations. We study the effect the choice of latent space distribution has on capturing the uncertainty in the reference segmentations using the LIDC-IDRI dataset. We show that the choice of distribution affects the sample diversity of the predictions and their overlap with respect to the reference segmentations. For the LIDC-IDRI dataset, we show that using a mixture of Gaussians results in a statistically significant improvement in the generalized energy distance (GED) metric with respect to the standard Probabilistic U-Net. We have made our implementation available at https:\/\/github.com\/ishaanb92\/GeneralizedProbabilisticUNet",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0690539829,
        "newsscientist":0.1122476221,
        "technologyreview":0.1770424498,
        "venturebeat":0.1513753697,
        "wired":0.1058040467,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12872v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658840617000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12235v1",
        "predicted_newsworthiness":0.4083865026,
        "title":"Advancing Semi-Supervised Task Oriented Dialog Systems by JSA Learning of Discrete Latent Variable Models",
        "summary":"Developing semi-supervised task-oriented dialog (TOD) systems by leveraging unlabeled dialog data has attracted increasing interests. For semi-supervised learning of latent state TOD models, variational learning is often used, but suffers from the annoying high-variance of the gradients propagated through discrete latent variables and the drawback of indirectly optimizing the target log-likelihood. Recently, an alternative algorithm, called joint stochastic approximation (JSA), has emerged for learning discrete latent variable models with impressive performances. In this paper, we propose to apply JSA to semi-supervised learning of the latent state TOD models, which is referred to as JSA-TOD. To our knowledge, JSA-TOD represents the first work in developing JSA based semi-supervised learning of discrete latent variable conditional models for such long sequential generation problems like in TOD systems. Extensive experiments show that JSA-TOD significantly outperforms its variational learning counterpart. Remarkably, semi-supervised JSA-TOD using 20% labels performs close to the full-supervised baseline on MultiWOZ2.1.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0778888949,
        "newsscientist":0.096119693,
        "technologyreview":0.1914480954,
        "venturebeat":0.2147145957,
        "wired":0.1601547207,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12235v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658759770000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.02140v1",
        "predicted_newsworthiness":0.4081847431,
        "title":"KPI-BERT: A Joint Named Entity Recognition and Relation Extraction Model for Financial Reports",
        "summary":"We present KPI-BERT, a system which employs novel methods of named entity recognition (NER) and relation extraction (RE) to extract and link key performance indicators (KPIs), e.g. \"revenue\" or \"interest expenses\", of companies from real-world German financial documents. Specifically, we introduce an end-to-end trainable architecture that is based on Bidirectional Encoder Representations from Transformers (BERT) combining a recurrent neural network (RNN) with conditional label masking to sequentially tag entities before it classifies their relations. Our model also introduces a learnable RNN-based pooling mechanism and incorporates domain expert knowledge by explicitly filtering impossible relations. We achieve a substantially higher prediction performance on a new practical dataset of German financial reports, outperforming several strong baselines including a competing state-of-the-art span-based entity tagging approach.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1267715025,
        "newsscientist":0.1234308806,
        "technologyreview":0.224018388,
        "venturebeat":0.2604038282,
        "wired":0.1728975051,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02140v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai",
            "cs.lg"
        ],
        "published":1659540088000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11773v1",
        "predicted_newsworthiness":0.408147168,
        "title":"N-LIMB: Neural Limb Optimization for Efficient Morphological Design",
        "summary":"A robot's ability to complete a task is heavily dependent on its physical design. However, identifying an optimal physical design and its corresponding control policy is inherently challenging. The freedom to choose the number of links, their type, and how they are connected results in a combinatorial design space, and the evaluation of any design in that space requires deriving its optimal controller. In this work, we present N-LIMB, an efficient approach to optimizing the design and control of a robot over large sets of morphologies. Central to our framework is a universal, design-conditioned control policy capable of controlling a diverse sets of designs. This policy greatly improves the sample efficiency of our approach by allowing the transfer of experience across designs and reducing the cost to evaluate new designs. We train this policy to maximize expected return over a distribution of designs, which is simultaneously updated towards higher performing designs under the universal policy. In this way, our approach converges towards a design distribution peaked around high-performing designs and a controller that is effectively fine-tuned for those designs. We demonstrate the potential of our approach on a series of locomotion tasks across varying terrains and show the discovery novel and high-performing design-control pairs.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0959070431,
        "newsscientist":0.1887286768,
        "technologyreview":0.2474957032,
        "venturebeat":0.1853055111,
        "wired":0.1846235161,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11773v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658681878000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.14298v1",
        "predicted_newsworthiness":0.4080901872,
        "title":"Learning Personalized Representations using Graph Convolutional Network",
        "summary":"Generating representations that precisely reflect customers' behavior is an important task for providing personalized skill routing experience in Alexa. Currently, Dynamic Routing (DR) team, which is responsible for routing Alexa traffic to providers or skills, relies on two features to be served as personal signals: absolute traffic count and normalized traffic count of every skill usage per customer. Neither of them considers the network based structure for interactions between customers and skills, which contain richer information for customer preferences. In this work, we first build a heterogeneous edge attributed graph based customers' past interactions with the invoked skills, in which the user requests (utterances) are modeled as edges. Then we propose a graph convolutional network(GCN) based model, namely Personalized Dynamic Routing Feature Encoder(PDRFE), that generates personalized customer representations learned from the built graph. Compared with existing models, PDRFE is able to further capture contextual information in the graph convolutional function. The performance of our proposed model is evaluated by a downstream task, defect prediction, that predicts the defect label from the learned embeddings of customers and their triggered skills. We observe up to 41% improvements on the cross entropy metric for our proposed models compared to the baselines.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1315346064,
        "newsscientist":0.1799931033,
        "technologyreview":0.3168067614,
        "venturebeat":0.3413488773,
        "wired":0.261945907,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14298v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659007898000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00233v1",
        "predicted_newsworthiness":0.4080839468,
        "title":"Learning Pseudo Front Depth for 2D Forward-Looking Sonar-based Multi-view Stereo",
        "summary":"Retrieving the missing dimension information in acoustic images from 2D forward-looking sonar is a well-known problem in the field of underwater robotics. There are works attempting to retrieve 3D information from a single image which allows the robot to generate 3D maps with fly-through motion. However, owing to the unique image formulation principle, estimating 3D information from a single image faces severe ambiguity problems. Classical methods of multi-view stereo can avoid the ambiguity problems, but may require a large number of viewpoints to generate an accurate model. In this work, we propose a novel learning-based multi-view stereo method to estimate 3D information. To better utilize the information from multiple frames, an elevation plane sweeping method is proposed to generate the depth-azimuth-elevation cost volume. The volume after regularization can be considered as a probabilistic volumetric representation of the target. Instead of performing regression on the elevation angles, we use pseudo front depth from the cost volume to represent the 3D information which can avoid the 2D-3D problem in acoustic imaging. High-accuracy results can be generated with only two or three images. Synthetic datasets were generated to simulate various underwater targets. We also built the first real dataset with accurate ground truth in a large scale water tank. Experimental results demonstrate the superiority of our method, compared to other state-of-the-art methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0851448335,
        "newsscientist":0.1641406245,
        "technologyreview":0.177747719,
        "venturebeat":0.1752192236,
        "wired":0.1700727096,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00233v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ro"
        ],
        "published":1659191721000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12710v1",
        "predicted_newsworthiness":0.4070405039,
        "title":"Active Learning of Ordinal Embeddings: A User Study on Football Data",
        "summary":"Humans innately measure distance between instances in an unlabeled dataset using an unknown similarity function. Distance metrics can only serve as proxy for similarity in information retrieval of similar instances. Learning a good similarity function from human annotations improves the quality of retrievals. This work uses deep metric learning to learn these user-defined similarity functions from few annotations for a large football trajectory dataset. We adapt an entropy-based active learning method with recent work from triplet mining to collect easy-to-answer but still informative annotations from human participants and use them to train a deep convolutional network that generalizes to unseen samples. Our user study shows that our approach improves the quality of the information retrieval compared to a previous deep metric learning approach that relies on a Siamese network. Specifically, we shed light on the strengths and weaknesses of passive sampling heuristics and active learners alike by analyzing the participants' response efficacy. To this end, we collect accuracy, algorithmic time complexity, the participants' fatigue and time-to-response, qualitative self-assessment and statements, as well as the effects of mixed-expertise annotators and their consistency on model performance and transfer-learning.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1285034892,
        "newsscientist":0.1606653282,
        "technologyreview":0.2567665698,
        "venturebeat":0.2432330371,
        "wired":0.1850400892,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12710v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658822123000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14626v1",
        "predicted_newsworthiness":0.4068986247,
        "title":"Restoring Vision in Adverse Weather Conditions with Patch-Based Denoising Diffusion Models",
        "summary":"Image restoration under adverse weather conditions has been of significant interest for various computer vision applications. Recent successful methods rely on the current progress in deep neural network architectural designs (e.g., with vision transformers). Motivated by the recent progress achieved with state-of-the-art conditional generative models, we present a novel patch-based image restoration algorithm based on denoising diffusion probabilistic models. Our patch-based diffusion modeling approach enables size-agnostic image restoration by using a guided denoising process with smoothed noise estimates across overlapping patches during inference. We empirically evaluate our model on benchmark datasets for image desnowing, combined deraining and dehazing, and raindrop removal. We demonstrate our approach to achieve state-of-the-art performances on both weather-specific and multi-weather image restoration, and qualitatively show strong generalization to real-world test images.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1101668864,
        "newsscientist":0.1448591098,
        "technologyreview":0.1917380581,
        "venturebeat":0.1586792292,
        "wired":0.1463997719,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14626v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659095561000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01320v1",
        "predicted_newsworthiness":0.40678939,
        "title":"Compound Density Networks for Risk Prediction using Electronic Health Records",
        "summary":"Electronic Health Records (EHRs) exhibit a high amount of missing data due to variations of patient conditions and treatment needs. Imputation of missing values has been considered an effective approach to deal with this challenge. Existing work separates imputation method and prediction model as two independent parts of an EHR-based machine learning system. We propose an integrated end-to-end approach by utilizing a Compound Density Network (CDNet) that allows the imputation method and prediction model to be tuned together within a single framework. CDNet consists of a Gated recurrent unit (GRU), a Mixture Density Network (MDN), and a Regularized Attention Network (RAN). The GRU is used as a latent variable model to model EHR data. The MDN is designed to sample latent variables generated by GRU. The RAN serves as a regularizer for less reliable imputed values. The architecture of CDNet enables GRU and MDN to iteratively leverage the output of each other to impute missing values, leading to a more accurate and robust prediction. We validate CDNet on the mortality prediction task on the MIMIC-III dataset. Our model outperforms state-of-the-art models by significant margins. We also empirically show that regularizing imputed values is a key factor for superior prediction performance. Analysis of prediction uncertainty shows that our model can capture both aleatoric and epistemic uncertainties, which offers model users a better understanding of the model results.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1638659628,
        "newsscientist":0.17678797,
        "technologyreview":0.2489107872,
        "venturebeat":0.2451032726,
        "wired":0.1712235455,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01320v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659431060000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00220v1",
        "predicted_newsworthiness":0.4066678407,
        "title":"HPO X ELA: Investigating Hyperparameter Optimization Landscapes by Means of Exploratory Landscape Analysis",
        "summary":"Hyperparameter optimization (HPO) is a key component of machine learning models for achieving peak predictive performance. While numerous methods and algorithms for HPO have been proposed over the last years, little progress has been made in illuminating and examining the actual structure of these black-box optimization problems. Exploratory landscape analysis (ELA) subsumes a set of techniques that can be used to gain knowledge about properties of unknown optimization problems. In this paper, we evaluate the performance of five different black-box optimizers on 30 HPO problems, which consist of two-, three- and five-dimensional continuous search spaces of the XGBoost learner trained on 10 different data sets. This is contrasted with the performance of the same optimizers evaluated on 360 problem instances from the black-box optimization benchmark (BBOB). We then compute ELA features on the HPO and BBOB problems and examine similarities and differences. A cluster analysis of the HPO and BBOB problems in ELA feature space allows us to identify how the HPO problems compare to the BBOB problems on a structural meta-level. We identify a subset of BBOB problems that are close to the HPO problems in ELA feature space and show that optimizer performance is comparably similar on these two sets of benchmark problems. We highlight open challenges of ELA for HPO and discuss potential directions of future research and applications.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0935794143,
        "newsscientist":0.1396791946,
        "technologyreview":0.2551712026,
        "venturebeat":0.2635615888,
        "wired":0.16451469,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00220v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659188806000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12622v1",
        "predicted_newsworthiness":0.406361918,
        "title":"Multi-Attention Network for Compressed Video Referring Object Segmentation",
        "summary":"Referring video object segmentation aims to segment the object referred by a given language expression. Existing works typically require compressed video bitstream to be decoded to RGB frames before being segmented, which increases computation and storage requirements and ultimately slows the inference down. This may hamper its application in real-world computing resource limited scenarios, such as autonomous cars and drones. To alleviate this problem, in this paper, we explore the referring object segmentation task on compressed videos, namely on the original video data flow. Besides the inherent difficulty of the video referring object segmentation task itself, obtaining discriminative representation from compressed video is also rather challenging. To address this problem, we propose a multi-attention network which consists of dual-path dual-attention module and a query-based cross-modal Transformer module. Specifically, the dual-path dual-attention module is designed to extract effective representation from compressed data in three modalities, i.e., I-frame, Motion Vector and Residual. The query-based cross-modal Transformer firstly models the correlation between linguistic and visual modalities, and then the fused multi-modality features are used to guide object queries to generate a content-aware dynamic kernel and to predict final segmentation masks. Different from previous works, we propose to learn just one kernel, which thus removes the complicated post mask-matching procedure of existing methods. Extensive promising experimental results on three challenging datasets show the effectiveness of our method compared against several state-of-the-art methods which are proposed for processing RGB data. Source code is available at: https:\/\/github.com\/DexiangHong\/MANet.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.095618242,
        "newsscientist":0.1282740109,
        "technologyreview":0.2046860114,
        "venturebeat":0.1924998333,
        "wired":0.1534579502,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12622v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658804452000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12346v1",
        "predicted_newsworthiness":0.4062436539,
        "title":"Contrastive Knowledge-Augmented Meta-Learning for Few-Shot Classification",
        "summary":"Model agnostic meta-learning algorithms aim to infer priors from several observed tasks that can then be used to adapt to a new task with few examples. Given the inherent diversity of tasks arising in existing benchmarks, recent methods use separate, learnable structure, such as hierarchies or graphs, for enabling task-specific adaptation of the prior. While these approaches have produced significantly better meta learners, our goal is to improve their performance when the heterogeneous task distribution contains challenging distribution shifts and semantic disparities. To this end, we introduce CAML (Contrastive Knowledge-Augmented Meta Learning), a novel approach for knowledge-enhanced few-shot learning that evolves a knowledge graph to effectively encode historical experience, and employs a contrastive distillation strategy to leverage the encoded knowledge for task-aware modulation of the base learner. Using standard benchmarks, we evaluate the performance of CAML in different few-shot learning scenarios. In addition to the standard few-shot task adaptation, we also consider the more challenging multi-domain task adaptation and few-shot dataset generalization settings in our empirical studies. Our results shows that CAML consistently outperforms best known approaches and achieves improved generalization.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0917782064,
        "newsscientist":0.1338352261,
        "technologyreview":0.2286478133,
        "venturebeat":0.2009728748,
        "wired":0.1417241191,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12346v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658768489000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00495v1",
        "predicted_newsworthiness":0.4059415799,
        "title":"Multi-Modal Multi-Agent Optimization for LIMMS, A Modular Robotics Approach to Delivery Automation",
        "summary":"In this paper we present a motion planner for LIMMS, a modular multi-agent, multi-modal package delivery platform. A single LIMMS unit is a robot that can operate as an arm or leg depending on how and what it is attached to, e.g., a manipulator when it is anchored to walls within a delivery vehicle or a quadruped robot when 4 are attached to a box. Coordinating amongst multiple LIMMS, when each one can take on vastly different roles, can quickly become complex. For such a planning problem we first compose the necessary logic and constraints. The formulation is then solved for skill exploration and can be implemented on hardware after refinement. To solve this optimization problem we use alternating direction method of multipliers (ADMM). The proposed planner is experimented under various scenarios which shows the capability of LIMMS to enter into different modes or combinations of them to achieve their goal of moving shipping boxes.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0765753429,
        "newsscientist":0.1298199231,
        "technologyreview":0.2252527331,
        "venturebeat":0.1892802744,
        "wired":0.172241839,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00495v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659294935000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13450v1",
        "predicted_newsworthiness":0.4052955627,
        "title":"Skimming, Locating, then Perusing: A Human-Like Framework for Natural Language Video Localization",
        "summary":"This paper addresses the problem of natural language video localization (NLVL). Almost all existing works follow the \"only look once\" framework that exploits a single model to directly capture the complex cross- and self-modal relations among video-query pairs and retrieve the relevant segment. However, we argue that these methods have overlooked two indispensable characteristics of an ideal localization method: 1) Frame-differentiable: considering the imbalance of positive\/negative video frames, it is effective to highlight positive frames and weaken negative ones during the localization. 2) Boundary-precise: to predict the exact segment boundary, the model should capture more fine-grained differences between consecutive frames since their variations are often smooth. To this end, inspired by how humans perceive and localize a segment, we propose a two-step human-like framework called Skimming-Locating-Perusing (SLP). SLP consists of a Skimming-and-Locating (SL) module and a Bi-directional Perusing (BP) module. The SL module first refers to the query semantic and selects the best matched frame from the video while filtering out irrelevant frames. Then, the BP module constructs an initial segment based on this frame, and dynamically updates it by exploring its adjacent frames until no frame shares the same activity semantic. Experimental results on three challenging benchmarks show that our SLP is superior to the state-of-the-art methods and localizes more precise segment boundaries.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0961044143,
        "newsscientist":0.1280144034,
        "technologyreview":0.1835276669,
        "venturebeat":0.1907714402,
        "wired":0.1676386145,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13450v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658919573000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11725v1",
        "predicted_newsworthiness":0.405215909,
        "title":"Combining Internal and External Constraints for Unrolling Shutter in Videos",
        "summary":"Videos obtained by rolling-shutter (RS) cameras result in spatially-distorted frames. These distortions become significant under fast camera\/scene motions. Undoing effects of RS is sometimes addressed as a spatial problem, where objects need to be rectified\/displaced in order to generate their correct global shutter (GS) frame. However, the cause of the RS effect is inherently temporal, not spatial. In this paper we propose a space-time solution to the RS problem. We observe that despite the severe differences between their xy frames, a RS video and its corresponding GS video tend to share the exact same xt slices -- up to a known sub-frame temporal shift. Moreover, they share the same distribution of small 2D xt-patches, despite the strong temporal aliasing within each video. This allows to constrain the GS output video using video-specific constraints imposed by the RS input video. Our algorithm is composed of 3 main components: (i) Dense temporal upsampling between consecutive RS frames using an off-the-shelf method, (which was trained on regular video sequences), from which we extract GS \"proposals\". (ii) Learning to correctly merge an ensemble of such GS \"proposals\" using a dedicated MergeNet. (iii) A video-specific zero-shot optimization which imposes the similarity of xt-patches between the GS output video and the RS input video. Our method obtains state-of-the-art results on benchmark datasets, both numerically and visually, despite being trained on a small synthetic RS\/GS dataset. Moreover, it generalizes well to new complex RS videos with motion types outside the distribution of the training set (e.g., complex non-rigid motions) -- videos which competing methods trained on much more data cannot handle well. We attribute these generalization capabilities to the combination of external and internal constraints.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.07415394,
        "newsscientist":0.1187822076,
        "technologyreview":0.1417974861,
        "venturebeat":0.1435777277,
        "wired":0.1530090266,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11725v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658664087000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13955v1",
        "predicted_newsworthiness":0.4051417822,
        "title":"Neural Architecture Search on Efficient Transformers and Beyond",
        "summary":"Recently, numerous efficient Transformers have been proposed to reduce the quadratic computational complexity of standard Transformers caused by the Softmax attention. However, most of them simply swap Softmax with an efficient attention mechanism without considering the customized architectures specially for the efficient attention. In this paper, we argue that the handcrafted vanilla Transformer architectures for Softmax attention may not be suitable for efficient Transformers. To address this issue, we propose a new framework to find optimal architectures for efficient Transformers with the neural architecture search (NAS) technique. The proposed method is validated on popular machine translation and image classification tasks. We observe that the optimal architecture of the efficient Transformer has the reduced computation compared with that of the standard Transformer, but the general accuracy is less comparable. It indicates that the Softmax attention and efficient attention have their own distinctions but neither of them can simultaneously balance the accuracy and efficiency well. This motivates us to mix the two types of attention to reduce the performance imbalance. Besides the search spaces that commonly used in existing NAS Transformer approaches, we propose a new search space that allows the NAS algorithm to automatically search the attention variants along with architectures. Extensive experiments on WMT' 14 En-De and CIFAR-10 demonstrate that our searched architecture maintains comparable accuracy to the standard Transformer with notably improved computational efficiency.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0791950447,
        "newsscientist":0.132679576,
        "technologyreview":0.2395839149,
        "venturebeat":0.2234728865,
        "wired":0.1496255965,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13955v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658997701000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11549v1",
        "predicted_newsworthiness":0.4050154038,
        "title":"Self-Support Few-Shot Semantic Segmentation",
        "summary":"Existing few-shot segmentation methods have achieved great progress based on the support-query matching framework. But they still heavily suffer from the limited coverage of intra-class variations from the few-shot supports provided. Motivated by the simple Gestalt principle that pixels belonging to the same object are more similar than those to different objects of same class, we propose a novel self-support matching strategy to alleviate this problem, which uses query prototypes to match query features, where the query prototypes are collected from high-confidence query predictions. This strategy can effectively capture the consistent underlying characteristics of the query objects, and thus fittingly match query features. We also propose an adaptive self-support background prototype generation module and self-support loss to further facilitate the self-support matching procedure. Our self-support network substantially improves the prototype quality, benefits more improvement from stronger backbones and more supports, and achieves SOTA on multiple datasets. Codes are at \\url{https:\/\/github.com\/fanq15\/SSP}.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0702968422,
        "newsscientist":0.1156439175,
        "technologyreview":0.1821654194,
        "venturebeat":0.1828890806,
        "wired":0.1445709283,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11549v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658593687000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12258v1",
        "predicted_newsworthiness":0.4044528273,
        "title":"Equivariance and Invariance Inductive Bias for Learning from Insufficient Data",
        "summary":"We are interested in learning robust models from insufficient data, without the need for any externally pre-trained checkpoints. First, compared to sufficient data, we show why insufficient data renders the model more easily biased to the limited training environments that are usually different from testing. For example, if all the training swan samples are \"white\", the model may wrongly use the \"white\" environment to represent the intrinsic class swan. Then, we justify that equivariance inductive bias can retain the class feature while invariance inductive bias can remove the environmental feature, leaving the class feature that generalizes to any environmental changes in testing. To impose them on learning, for equivariance, we demonstrate that any off-the-shelf contrastive-based self-supervised feature learning method can be deployed; for invariance, we propose a class-wise invariant risk minimization (IRM) that efficiently tackles the challenge of missing environmental annotation in conventional IRM. State-of-the-art experimental results on real-world benchmarks (VIPriors, ImageNet100 and NICO) validate the great potential of equivariance and invariance in data-efficient learning. The code is available at https:\/\/github.com\/Wangt-CN\/EqInv",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1094728052,
        "newsscientist":0.1585445408,
        "technologyreview":0.2738830608,
        "venturebeat":0.2335555808,
        "wired":0.1677894533,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12258v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1658762779000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14467v1",
        "predicted_newsworthiness":0.4042175697,
        "title":"GTrans: Grouping and Fusing Transformer Layers for Neural Machine Translation",
        "summary":"Transformer structure, stacked by a sequence of encoder and decoder network layers, achieves significant development in neural machine translation. However, vanilla Transformer mainly exploits the top-layer representation, assuming the lower layers provide trivial or redundant information and thus ignoring the bottom-layer feature that is potentially valuable. In this work, we propose the Group-Transformer model (GTrans) that flexibly divides multi-layer representations of both encoder and decoder into different groups and then fuses these group features to generate target words. To corroborate the effectiveness of the proposed method, extensive experiments and analytic experiments are conducted on three bilingual translation benchmarks and two multilingual translation tasks, including the IWLST-14, IWLST-17, LDC, WMT-14 and OPUS-100 benchmark. Experimental and analytical results demonstrate that our model outperforms its Transformer counterparts by a consistent gain. Furthermore, it can be successfully scaled up to 60 encoder layers and 36 decoder layers.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0908624028,
        "newsscientist":0.1110125028,
        "technologyreview":0.2105992453,
        "venturebeat":0.2042837604,
        "wired":0.1423347349,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14467v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1659067836000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11821v1",
        "predicted_newsworthiness":0.4037275075,
        "title":"Maximizing Entanglement Routing Rate in Quantum Networks: Approximation Algorithms",
        "summary":"There will be a fast-paced shift from conventional network systems to novel quantum networks that are supported by the quantum entanglement and teleportation, key technologies of the quantum era, to enable secured data transmissions in the next-generation of the Internet. Despite this prospect, migration to quantum networks cannot be done at once, especially on the aspect of quantum routing. In this paper, we study the maximizing entangled routing rate (MERR) problem. In particular, given a set of demands, we try to determine entangled routing paths for the maximum number of demands in the quantum network while meeting the network's fidelity. We first formulate the MERR problem using an integer linear programming (ILP) model to capture the traffic patent for all demands in the network. We then leverage the theory of relaxation of ILP to devise two efficient algorithms including HBRA and RRA with provable approximation ratios for the objective function. To deal with the challenge of the combinatorial optimization problem in big scale networks, we also propose the path-length-based approach (PLBA) to solve the MERR problem. Using both simulations and an open quantum network simulator platform to conduct experiments with real-world topologies and traffic matrices, we evaluate the performance of our algorithms and show up the success of maximizing entangled routing rate.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0740662277,
        "newsscientist":0.1213239721,
        "technologyreview":0.1493746076,
        "venturebeat":0.1226118888,
        "wired":0.1205034332,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11821v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658199294000,
        "published_hr":"Jul 18, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.11515v1",
        "predicted_newsworthiness":0.4035163079,
        "title":"Marior: Margin Removal and Iterative Content Rectification for Document Dewarping in the Wild",
        "summary":"Camera-captured document images usually suffer from perspective and geometric deformations. It is of great value to rectify them when considering poor visual aesthetics and the deteriorated performance of OCR systems. Recent learning-based methods intensively focus on the accurately cropped document image. However, this might not be sufficient for overcoming practical challenges, including document images either with large marginal regions or without margins. Due to this impracticality, users struggle to crop documents precisely when they encounter large marginal regions. Simultaneously, dewarping images without margins is still an insurmountable problem. To the best of our knowledge, there is still no complete and effective pipeline for rectifying document images in the wild. To address this issue, we propose a novel approach called Marior (Margin Removal and \\Iterative Content Rectification). Marior follows a progressive strategy to iteratively improve the dewarping quality and readability in a coarse-to-fine manner. Specifically, we divide the pipeline into two modules: margin removal module (MRM) and iterative content rectification module (ICRM). First, we predict the segmentation mask of the input image to remove the margin, thereby obtaining a preliminary result. Then we refine the image further by producing dense displacement flows to achieve content-aware rectification. We determine the number of refinement iterations adaptively. Experiments demonstrate the state-of-the-art performance of our method on public benchmarks. The resources are available at https:\/\/github.com\/ZZZHANG-jx\/Marior for further comparison.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0981016428,
        "newsscientist":0.1236795911,
        "technologyreview":0.1807588682,
        "venturebeat":0.1798816894,
        "wired":0.151895373,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11515v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658582067000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01159v2",
        "predicted_newsworthiness":0.4030617624,
        "title":"BATMAN: Bilateral Attention Transformer in Motion-Appearance Neighboring Space for Video Object Segmentation",
        "summary":"Video Object Segmentation (VOS) is fundamental to video understanding. Transformer-based methods show significant performance improvement on semi-supervised VOS. However, existing work faces challenges segmenting visually similar objects in close proximity of each other. In this paper, we propose a novel Bilateral Attention Transformer in Motion-Appearance Neighboring space (BATMAN) for semi-supervised VOS. It captures object motion in the video via a novel optical flow calibration module that fuses the segmentation mask with optical flow estimation to improve within-object optical flow smoothness and reduce noise at object boundaries. This calibrated optical flow is then employed in our novel bilateral attention, which computes the correspondence between the query and reference frames in the neighboring bilateral space considering both motion and appearance. Extensive experiments validate the effectiveness of BATMAN architecture by outperforming all existing state-of-the-art on all four popular VOS benchmarks: Youtube-VOS 2019 (85.0%), Youtube-VOS 2018 (85.3%), DAVIS 2017Val\/Testdev (86.2%\/82.2%), and DAVIS 2016 (92.5%).",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0717593272,
        "newsscientist":0.1250207673,
        "technologyreview":0.1701071769,
        "venturebeat":0.1784341394,
        "wired":0.1533157881,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01159v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659392494000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11709v1",
        "predicted_newsworthiness":0.4030154934,
        "title":"Keypoint-less Camera Calibration for Sports Field Registration in Soccer",
        "summary":"Sports field registration in broadcast videos is typically interpreted as the task of homography estimation, which provides a mapping between a planar field and the corresponding visible area of the image. In contrast to previous approaches, we consider the task as a camera calibration problem. First, we introduce a differentiable objective function that is able to learn the camera pose and focal length from segment correspondences (e.g., lines, point clouds), based on pixel-level annotations for segments of a known calibration object, i.e., the sports field. The calibration module iteratively minimizes the segment reprojection error induced by the estimated camera parameters. Second, we propose a novel approach for 3D sports field registration from broadcast soccer images. The calibration module does not require any training data and compared to the typical solution, which subsequently refines an initial estimation, our solution does it in one step. The proposed method is evaluated for sports field registration on two datasets and achieves superior results compared to two state-of-the-art approaches.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0889643143,
        "newsscientist":0.11499629,
        "technologyreview":0.156279664,
        "venturebeat":0.1942816663,
        "wired":0.1539186161,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11709v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658658685000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14657v1",
        "predicted_newsworthiness":0.4024575512,
        "title":"Multi-Agent Path Finding Based on Subdimensional Expansion with Bypass",
        "summary":"Multi-agent path finding (MAPF) is an active area in artificial intelligence, which has many real-world applications such as warehouse management, traffic control, robotics, etc. Recently, M* and its variants have greatly improved the ability to solve the MAPF problem. Although subdimensional expansion used in those approaches significantly decreases the dimensionality of the joint search space and reduces the branching factor, they do not make full use of the possible non-uniqueness of the optimal path of each agent. As a result, the updating of the collision sets may bring a large number of redundant computation. In this paper, the idea of bypass is introduced into subdimensional expansion to reduce the redundant computation. Specifically, we propose the BPM* algorithm, which is an implementation of subdimensional expansion with bypass in M*. In the experiments, we show that BPM* outperforms the state-of-the-art in solving several MAPF benchmark problems.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0743777163,
        "newsscientist":0.116871048,
        "technologyreview":0.1846941512,
        "venturebeat":0.1712852753,
        "wired":0.1354774141,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14657v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659099838000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00400v2",
        "predicted_newsworthiness":0.4022858151,
        "title":"FixMatchSeg: Fixing FixMatch for Semi-Supervised Semantic Segmentation",
        "summary":"Supervised deep learning methods for semantic medical image segmentation are getting increasingly popular in the past few years.However, in resource constrained settings, getting large number of annotated images is very difficult as it mostly requires experts, is expensive and time-consuming.Semi-supervised segmentation can be an attractive solution where a very few labeled images are used along with a large number of unlabeled ones. While the gap between supervised and semi-supervised methods have been dramatically reduced for classification problems in the past couple of years, there still remains a larger gap in segmentation methods. In this work, we adapt a state-of-the-art semi-supervised classification method FixMatch to semantic segmentation task, introducing FixMatchSeg. FixMatchSeg is evaluated in four different publicly available datasets of different anatomy and different modality: cardiac ultrasound, chest X-ray, retinal fundus image, and skin images. When there are few labels, we show that FixMatchSeg performs on par with strong supervised baselines.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0827318236,
        "newsscientist":0.1192742891,
        "technologyreview":0.1981937099,
        "venturebeat":0.184804843,
        "wired":0.1203397606,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00400v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659258892000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12280v1",
        "predicted_newsworthiness":0.4019005083,
        "title":"ArtFID: Quantitative Evaluation of Neural Style Transfer",
        "summary":"The field of neural style transfer has experienced a surge of research exploring different avenues ranging from optimization-based approaches and feed-forward models to meta-learning methods. The developed techniques have not just progressed the field of style transfer, but also led to breakthroughs in other areas of computer vision, such as all of visual synthesis. However, whereas quantitative evaluation and benchmarking have become pillars of computer vision research, the reproducible, quantitative assessment of style transfer models is still lacking. Even in comparison to other fields of visual synthesis, where widely used metrics exist, the quantitative evaluation of style transfer is still lagging behind. To support the automatic comparison of different style transfer approaches and to study their respective strengths and weaknesses, the field would greatly benefit from a quantitative measurement of stylization performance. Therefore, we propose a method to complement the currently mostly qualitative evaluation schemes. We provide extensive evaluations and a large-scale user study to show that the proposed metric strongly coincides with human judgment.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1060889515,
        "newsscientist":0.160058834,
        "technologyreview":0.2336537718,
        "venturebeat":0.211158847,
        "wired":0.1933026953,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12280v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658764374000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13316v1",
        "predicted_newsworthiness":0.4018128724,
        "title":"NICEST: Noisy Label Correction and Training for Robust Scene Graph Generation",
        "summary":"Nearly all existing scene graph generation (SGG) models have overlooked the ground-truth annotation qualities of mainstream SGG datasets, i.e., they assume: 1) all the manually annotated positive samples are equally correct; 2) all the un-annotated negative samples are absolutely background. In this paper, we argue that neither of the assumptions applies to SGG: there are numerous noisy ground-truth predicate labels that break these two assumptions and harm the training of unbiased SGG models. To this end, we propose a novel NoIsy label CorrEction and Sample Training strategy for SGG: NICEST. Specifically, it consists of two parts: NICE and NIST, which rule out these noisy label issues by generating high-quality samples and the effective training strategy, respectively. NICE first detects noisy samples and then reassigns them more high-quality soft predicate labels. NIST is a multi-teacher knowledge distillation based training strategy, which enables the model to learn unbiased fusion knowledge. And a dynamic trade-off weighting strategy in NIST is designed to penalize the bias of different teachers. Due to the model-agnostic nature of both NICE and NIST, our NICEST can be seamlessly incorporated into any SGG architecture to boost its performance on different predicate categories. In addition, to better evaluate the generalization of SGG models, we further propose a new benchmark VG-OOD, by re-organizing the prevalent VG dataset and deliberately making the predicate distributions of the training and test sets as different as possible for each subject-object category pair. This new benchmark helps disentangle the influence of subject-object category based frequency biases. Extensive ablations and results on different backbones and tasks have attested to the effectiveness and generalization ability of each component of NICEST.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0929577394,
        "newsscientist":0.1266175332,
        "technologyreview":0.2294113971,
        "venturebeat":0.197573003,
        "wired":0.1608731689,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13316v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658903147000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12520v1",
        "predicted_newsworthiness":0.4017395568,
        "title":"3D Lidar Reconstruction with Probabilistic Depth Completion for Robotic Navigation",
        "summary":"Safe motion planning in robotics requires planning into space which has been verified to be free of obstacles. However, obtaining such environment representations using lidars is challenging by virtue of the sparsity of their depth measurements. We present a learning-aided 3D lidar reconstruction framework that upsamples sparse lidar depth measurements with the aid of overlapping camera images so as to generate denser reconstructions with more definitively free space than can be achieved with the raw lidar measurements alone. We use a neural network with an encoder-decoder structure to predict dense depth images along with depth uncertainty estimates which are fused using a volumetric mapping system. We conduct experiments on real-world outdoor datasets captured using a handheld sensing device and a legged robot. Using input data from a 16-beam lidar mapping a building network, our experiments showed that the amount of estimated free space was increased by more than 40% with our approach. We also show that our approach trained on a synthetic dataset generalises well to real-world outdoor scenes without additional fine-tuning. Finally, we demonstrate how motion planning tasks can benefit from these denser reconstructions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1036931094,
        "newsscientist":0.16864743,
        "technologyreview":0.2443372593,
        "venturebeat":0.217376259,
        "wired":0.2039554616,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12520v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658781270000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.12389v1",
        "predicted_newsworthiness":0.4017151017,
        "title":"MemSAC: Memory Augmented Sample Consistency for Large Scale Domain Adaptation",
        "summary":"Practical real world datasets with plentiful categories introduce new challenges for unsupervised domain adaptation like small inter-class discriminability, that existing approaches relying on domain invariance alone cannot handle sufficiently well. In this work we propose MemSAC, which exploits sample level similarity across source and target domains to achieve discriminative transfer, along with architectures that scale to a large number of categories. For this purpose, we first introduce a memory augmented approach to efficiently extract pairwise similarity relations between labeled source and unlabeled target domain instances, suited to handle an arbitrary number of classes. Next, we propose and theoretically justify a novel variant of the contrastive loss to promote local consistency among within-class cross domain samples while enforcing separation between classes, thus preserving discriminative transfer from source to target. We validate the advantages of MemSAC with significant improvements over previous state-of-the-art on multiple challenging transfer tasks designed for large-scale adaptation, such as DomainNet with 345 classes and fine-grained adaptation on Caltech-UCSD birds dataset with 200 classes. We also provide in-depth analysis and insights into the effectiveness of MemSAC.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0742200417,
        "newsscientist":0.1076802937,
        "technologyreview":0.1720482167,
        "venturebeat":0.1528681793,
        "wired":0.1105421359,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12389v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1658771728000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02178v1",
        "predicted_newsworthiness":0.4015270777,
        "title":"KD-SCFNet: Towards More Accurate and Efficient Salient Object Detection via Knowledge Distillation",
        "summary":"Most existing salient object detection (SOD) models are difficult to apply due to the complex and huge model structures. Although some lightweight models are proposed, the accuracy is barely satisfactory. In this paper, we design a novel semantics-guided contextual fusion network (SCFNet) that focuses on the interactive fusion of multi-level features for accurate and efficient salient object detection. Furthermore, we apply knowledge distillation to SOD task and provide a sizeable dataset KD-SOD80K. In detail, we transfer the rich knowledge from a seasoned teacher to the untrained SCFNet through unlabeled images, enabling SCFNet to learn a strong generalization ability to detect salient objects more accurately. The knowledge distillation based SCFNet (KDSCFNet) achieves comparable accuracy to the state-of-the-art heavyweight methods with less than 1M parameters and 174 FPS real-time detection speed. Extensive experiments demonstrate the robustness and effectiveness of the proposed distillation method and SOD framework. Code and data: https:\/\/github.com\/zhangjinCV\/KD-SCFNet.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0905268512,
        "newsscientist":0.1316846745,
        "technologyreview":0.2200995211,
        "venturebeat":0.2086478615,
        "wired":0.1723103863,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02178v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659542591000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11660v1",
        "predicted_newsworthiness":0.4014609257,
        "title":"MAR: Masked Autoencoders for Efficient Action Recognition",
        "summary":"Standard approaches for video recognition usually operate on the full input videos, which is inefficient due to the widely present spatio-temporal redundancy in videos. Recent progress in masked video modelling, i.e., VideoMAE, has shown the ability of vanilla Vision Transformers (ViT) to complement spatio-temporal contexts given only limited visual contents. Inspired by this, we propose propose Masked Action Recognition (MAR), which reduces the redundant computation by discarding a proportion of patches and operating only on a part of the videos. MAR contains the following two indispensable components: cell running masking and bridging classifier. Specifically, to enable the ViT to perceive the details beyond the visible patches easily, cell running masking is presented to preserve the spatio-temporal correlations in videos, which ensures the patches at the same spatial location can be observed in turn for easy reconstructions. Additionally, we notice that, although the partially observed features can reconstruct semantically explicit invisible patches, they fail to achieve accurate classification. To address this, a bridging classifier is proposed to bridge the semantic gap between the ViT encoded features for reconstruction and the features specialized for classification. Our proposed MAR reduces the computational cost of ViT by 53% and extensive experiments show that MAR consistently outperforms existing ViT models with a notable margin. Especially, we found a ViT-Large trained by MAR outperforms the ViT-Huge trained by a standard training scheme by convincing margins on both Kinetics-400 and Something-Something v2 datasets, while our computation overhead of ViT-Large is only 14.5% of ViT-Huge.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0921833215,
        "newsscientist":0.1269182771,
        "technologyreview":0.2004525391,
        "venturebeat":0.1839809994,
        "wired":0.1547284558,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11660v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658636856000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12660v1",
        "predicted_newsworthiness":0.4004957887,
        "title":"Bilateral Self-unbiased Learning from Biased Implicit Feedback",
        "summary":"Implicit feedback has been widely used to build commercial recommender systems. Because observed feedback represents users' click logs, there is a semantic gap between true relevance and observed feedback. More importantly, observed feedback is usually biased towards popular items, thereby overestimating the actual relevance of popular items. Although existing studies have developed unbiased learning methods using inverse propensity weighting (IPW) or causal reasoning, they solely focus on eliminating the popularity bias of items. In this paper, we propose a novel unbiased recommender learning model, namely BIlateral SElf-unbiased Recommender (BISER), to eliminate the exposure bias of items caused by recommender models. Specifically, BISER consists of two key components: (i) self-inverse propensity weighting (SIPW) to gradually mitigate the bias of items without incurring high computational costs; and (ii) bilateral unbiased learning (BU) to bridge the gap between two complementary models in model predictions, i.e., user- and item-based autoencoders, alleviating the high variance of SIPW. Extensive experiments show that BISER consistently outperforms state-of-the-art unbiased recommender models over several datasets, including Coat, Yahoo! R3, MovieLens, and CiteULike.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1263764889,
        "newsscientist":0.1319851542,
        "technologyreview":0.2201851988,
        "venturebeat":0.231352771,
        "wired":0.1973787955,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12660v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.lg"
        ],
        "published":1658812662000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.00632v1",
        "predicted_newsworthiness":0.4002556519,
        "title":"Multi-spectral Vehicle Re-identification with Cross-directional Consistency Network and a High-quality Benchmark",
        "summary":"To tackle the challenge of vehicle re-identification (Re-ID) in complex lighting environments and diverse scenes, multi-spectral sources like visible and infrared information are taken into consideration due to their excellent complementary advantages. However, multi-spectral vehicle Re-ID suffers cross-modality discrepancy caused by heterogeneous properties of different modalities as well as a big challenge of the diverse appearance with different views in each identity. Meanwhile, diverse environmental interference leads to heavy sample distributional discrepancy in each modality. In this work, we propose a novel cross-directional consistency network to simultaneously overcome the discrepancies from both modality and sample aspects. In particular, we design a new cross-directional center loss to pull the modality centers of each identity close to mitigate cross-modality discrepancy, while the sample centers of each identity close to alleviate the sample discrepancy. Such strategy can generate discriminative multi-spectral feature representations for vehicle Re-ID. In addition, we design an adaptive layer normalization unit to dynamically adjust individual feature distribution to handle distributional discrepancy of intra-modality features for robust learning. To provide a comprehensive evaluation platform, we create a high-quality RGB-NIR-TIR multi-spectral vehicle Re-ID benchmark (MSVR310), including 310 different vehicles from a broad range of viewpoints, time spans and environmental complexities. Comprehensive experiments on both created and public datasets demonstrate the effectiveness of the proposed approach comparing to the state-of-the-art methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0965014616,
        "newsscientist":0.121568521,
        "technologyreview":0.1964724495,
        "venturebeat":0.1824058139,
        "wired":0.1619571,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00632v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659335732000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14268v1",
        "predicted_newsworthiness":0.4002509854,
        "title":"MonteBoxFinder: Detecting and Filtering Primitives to Fit a Noisy Point Cloud",
        "summary":"We present MonteBoxFinder, a method that, given a noisy input point cloud, fits cuboids to the input scene. Our primary contribution is a discrete optimization algorithm that, from a dense set of initially detected cuboids, is able to efficiently filter good boxes from the noisy ones. Inspired by recent applications of MCTS to scene understanding problems, we develop a stochastic algorithm that is, by design, more efficient for our task. Indeed, the quality of a fit for a cuboid arrangement is invariant to the order in which the cuboids are added into the scene. We develop several search baselines for our problem and demonstrate, on the ScanNet dataset, that our approach is more efficient and precise. Finally, we strongly believe that our core algorithm is very general and that it could be extended to many other problems in 3D scene understanding.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0857112944,
        "newsscientist":0.1569602848,
        "technologyreview":0.2091073403,
        "venturebeat":0.1848202269,
        "wired":0.1710498133,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14268v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659030727000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01823v1",
        "predicted_newsworthiness":0.3999097153,
        "title":"Statistical Attention Localization (SAL): Methodology and Application to Object Classification",
        "summary":"A statistical attention localization (SAL) method is proposed to facilitate the object classification task in this work. SAL consists of three steps: 1) preliminary attention window selection via decision statistics, 2) attention map refinement, and 3) rectangular attention region finalization. SAL computes soft-decision scores of local squared windows and uses them to identify salient regions in Step 1. To accommodate object of various sizes and shapes, SAL refines the preliminary result and obtain an attention map of more flexible shape in Step 2. Finally, SAL yields a rectangular attention region using the refined attention map and bounding box regularization in Step 3. As an application, we adopt E-PixelHop, which is an object classification solution based on successive subspace learning (SSL), as the baseline. We apply SAL so as to obtain a cropped-out and resized attention region as an alternative input. Classification results of the whole image as well as the attention region are ensembled to achieve the highest classification accuracy. Experiments on the CIFAR-10 dataset are given to demonstrate the advantage of the SAL-assisted object classification method.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0920737712,
        "newsscientist":0.1470261554,
        "technologyreview":0.2170929261,
        "venturebeat":0.1977196176,
        "wired":0.1506887789,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01823v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659495311000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12909v1",
        "predicted_newsworthiness":0.3996239085,
        "title":"AlignSDF: Pose-Aligned Signed Distance Fields for Hand-Object Reconstruction",
        "summary":"Recent work achieved impressive progress towards joint reconstruction of hands and manipulated objects from monocular color images. Existing methods focus on two alternative representations in terms of either parametric meshes or signed distance fields (SDFs). On one side, parametric models can benefit from prior knowledge at the cost of limited shape deformations and mesh resolutions. Mesh models, hence, may fail to precisely reconstruct details such as contact surfaces of hands and objects. SDF-based methods, on the other side, can represent arbitrary details but are lacking explicit priors. In this work we aim to improve SDF models using priors provided by parametric representations. In particular, we propose a joint learning framework that disentangles the pose and the shape. We obtain hand and object poses from parametric models and use them to align SDFs in 3D space. We show that such aligned SDFs better focus on reconstructing shape details and improve reconstruction accuracy both for hands and objects. We evaluate our method and demonstrate significant improvements over the state of the art on the challenging ObMan and DexYCB benchmarks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0761616061,
        "newsscientist":0.1227860802,
        "technologyreview":0.170514929,
        "venturebeat":0.1597574275,
        "wired":0.1258872794,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12909v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658843939000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00338v1",
        "predicted_newsworthiness":0.3996181026,
        "title":"Symmetry Regularization and Saturating Nonlinearity for Robust Quantization",
        "summary":"Robust quantization improves the tolerance of networks for various implementations, allowing reliable output in different bit-widths or fragmented low-precision arithmetic. In this work, we perform extensive analyses to identify the sources of quantization error and present three insights to robustify a network against quantization: reduction of error propagation, range clamping for error minimization, and inherited robustness against quantization. Based on these insights, we propose two novel methods called symmetry regularization (SymReg) and saturating nonlinearity (SatNL). Applying the proposed methods during training can enhance the robustness of arbitrary neural networks against quantization on existing post-training quantization (PTQ) and quantization-aware training (QAT) algorithms and enables us to obtain a single weight flexible enough to maintain the output quality under various conditions. We conduct extensive studies on CIFAR and ImageNet datasets and validate the effectiveness of the proposed methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0832878424,
        "newsscientist":0.1225001452,
        "technologyreview":0.21949191,
        "venturebeat":0.1786528932,
        "wired":0.1326150906,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00338v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659233548000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14074v1",
        "predicted_newsworthiness":0.3993452933,
        "title":"PEA: Improving the Performance of ReLU Networks for Free by Using Progressive Ensemble Activations",
        "summary":"In recent years novel activation functions have been proposed to improve the performance of neural networks, and they show superior performance compared to the ReLU counterpart. However, there are environments, where the availability of complex activations is limited, and usually only the ReLU is supported. In this paper we propose methods that can be used to improve the performance of ReLU networks by using these efficient novel activations during model training. More specifically, we propose ensemble activations that are composed of the ReLU and one of these novel activations. Furthermore, the coefficients of the ensemble are neither fixed nor learned, but are progressively updated during the training process in a way that by the end of the training only the ReLU activations remain active in the network and the other activations can be removed. This means that in inference time the network contains ReLU activations only. We perform extensive evaluations on the ImageNet classification task using various compact network architectures and various novel activation functions. Results show 0.2-0.8% top-1 accuracy gain, which confirms the applicability of the proposed methods. Furthermore, we demonstrate the proposed methods on semantic segmentation and we boost the performance of a compact segmentation network by 0.34% mIOU on the Cityscapes dataset.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.086693328,
        "newsscientist":0.1208509896,
        "technologyreview":0.2288048104,
        "venturebeat":0.2203861522,
        "wired":0.1363571329,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14074v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659014947000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00561v1",
        "predicted_newsworthiness":0.3992769316,
        "title":"AvatarGen: a 3D Generative Model for Animatable Human Avatars",
        "summary":"Unsupervised generation of clothed virtual humans with various appearance and animatable poses is important for creating 3D human avatars and other AR\/VR applications. Existing methods are either limited to rigid object modeling, or not generative and thus unable to synthesize high-quality virtual humans and animate them. In this work, we propose AvatarGen, the first method that enables not only non-rigid human generation with diverse appearance but also full control over poses and viewpoints, while only requiring 2D images for training. Specifically, it extends the recent 3D GANs to clothed human generation by utilizing a coarse human body model as a proxy to warp the observation space into a standard avatar under a canonical space. To model non-rigid dynamics, it introduces a deformation network to learn pose-dependent deformations in the canonical space. To improve geometry quality of the generated human avatars, it leverages signed distance field as geometric representation, which allows more direct regularization from the body model on the geometry learning. Benefiting from these designs, our method can generate animatable human avatars with high-quality appearance and geometry modeling, significantly outperforming previous 3D GANs. Furthermore, it is competent for many applications, e.g., single-view reconstruction, reanimation, and text-guided synthesis. Code and pre-trained model will be available.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0925268345,
        "newsscientist":0.147533304,
        "technologyreview":0.2090252258,
        "venturebeat":0.2237343199,
        "wired":0.1906503286,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00561v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659317222000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01302v1",
        "predicted_newsworthiness":0.3988479363,
        "title":"Overlooked Poses Actually Make Sense: Distilling Privileged Knowledge for Human Motion Prediction",
        "summary":"Previous works on human motion prediction follow the pattern of building a mapping relation between the sequence observed and the one to be predicted. However, due to the inherent complexity of multivariate time series data, it still remains a challenge to find the extrapolation relation between motion sequences. In this paper, we present a new prediction pattern, which introduces previously overlooked human poses, to implement the prediction task from the view of interpolation. These poses exist after the predicted sequence, and form the privileged sequence. To be specific, we first propose an InTerPolation learning Network (ITP-Network) that encodes both the observed sequence and the privileged sequence to interpolate the in-between predicted sequence, wherein the embedded Privileged-sequence-Encoder (Priv-Encoder) learns the privileged knowledge (PK) simultaneously. Then, we propose a Final Prediction Network (FP-Network) for which the privileged sequence is not observable, but is equipped with a novel PK-Simulator that distills PK learned from the previous network. This simulator takes as input the observed sequence, but approximates the behavior of Priv-Encoder, enabling FP-Network to imitate the interpolation process. Extensive experimental results demonstrate that our prediction pattern achieves state-of-the-art performance on benchmarked H3.6M, CMU-Mocap and 3DPW datasets in both short-term and long-term predictions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1045980735,
        "newsscientist":0.1625121497,
        "technologyreview":0.235242048,
        "venturebeat":0.2100522841,
        "wired":0.1716347414,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01302v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659428023000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00098v1",
        "predicted_newsworthiness":0.3987884236,
        "title":"Weakly Supervised Deep Instance Nuclei Detection using Points Annotation in 3D Cardiovascular Immunofluorescent Images",
        "summary":"Two major causes of death in the United States and worldwide are stroke and myocardial infarction. The underlying cause of both is thrombi released from ruptured or eroded unstable atherosclerotic plaques that occlude vessels in the heart (myocardial infarction) or the brain (stroke). Clinical studies show that plaque composition plays a more important role than lesion size in plaque rupture or erosion events. To determine the plaque composition, various cell types in 3D cardiovascular immunofluorescent images of plaque lesions are counted. However, counting these cells manually is expensive, time-consuming, and prone to human error. These challenges of manual counting motivate the need for an automated approach to localize and count the cells in images. The purpose of this study is to develop an automatic approach to accurately detect and count cells in 3D immunofluorescent images with minimal annotation effort. In this study, we used a weakly supervised learning approach to train the HoVer-Net segmentation model using point annotations to detect nuclei in fluorescent images. The advantage of using point annotations is that they require less effort as opposed to pixel-wise annotation. To train the HoVer-Net model using point annotations, we adopted a popularly used cluster labeling approach to transform point annotations into accurate binary masks of cell nuclei. Traditionally, these approaches have generated binary masks from point annotations, leaving a region around the object unlabeled (which is typically ignored during model training). However, these areas may contain important information that helps determine the boundary between cells. Therefore, we used the entropy minimization loss function in these areas to encourage the model to output more confident predictions on the unlabeled areas. Our comparison studies indicate that the HoVer-Net model trained using our weakly ...",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0984632823,
        "newsscientist":0.163162938,
        "technologyreview":0.194132076,
        "venturebeat":0.1553160022,
        "wired":0.1268922697,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00098v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659136018000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12536v1",
        "predicted_newsworthiness":0.3986015407,
        "title":"Lumen Shape Reconstruction using a Soft Robotic Balloon Catheter and Electrical Impedance Tomography",
        "summary":"Incorrectly sized balloon catheters can lead to increased post-surgical complications, yet even with preoperative imaging, correct selection remains a challenge. With limited feedback during surgery, it is difficult to verify correct deployment. We propose the use of integrated impedance measurements and Electrical Impedance Tomography (EIT) imaging to assess the deformation of the balloon and determine the size and shape of the surrounding lumen. Previous work using single impedance measurements, or pressure data and analytical models, whilst demonstrating high sizing accuracy, have assumed a circular cross section. Here we extend these methods by adding a multitude of electrodes to detect elliptical and occluded lumen and obtain EIT images to localise deformations. Using a 14 Fr (5.3 mm) catheter as an example, numerical simulations were performed to find the optimal electrode configuration of two rings of 8 electrodes spaced 10 mm apart. The simulations predicted that the maximum detectable aspect ratio decreased from 0.9 for a 14mm balloon to 0.5 at 30mm. The sizing and ellipticity detection results were verified experimentally. A prototype robotic balloon catheter was constructed to automatically inflate a compliant balloon while simultaneously recording EIT and pressure data. Data were collected in experiments replicating stenotic vessels with an elliptical and asymmetrical profile, and the widening of a lumen during angioplasty. After calibration, the system was able to correctly localise the occlusion and detect aspect ratios of 0.75. EIT images further localised the occlusion and visualised the dilation of the lumen during balloon inflation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0707369461,
        "newsscientist":0.1453724988,
        "technologyreview":0.1536825277,
        "venturebeat":0.1026657651,
        "wired":0.1174365124,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12536v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658783860000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.12654v1",
        "predicted_newsworthiness":0.3983459083,
        "title":"ProposalContrast: Unsupervised Pre-training for LiDAR-based 3D Object Detection",
        "summary":"Existing approaches for unsupervised point cloud pre-training are constrained to either scene-level or point\/voxel-level instance discrimination. Scene-level methods tend to lose local details that are crucial for recognizing the road objects, while point\/voxel-level methods inherently suffer from limited receptive field that is incapable of perceiving large objects or context environments. Considering region-level representations are more suitable for 3D object detection, we devise a new unsupervised point cloud pre-training framework, called ProposalContrast, that learns robust 3D representations by contrasting region proposals. Specifically, with an exhaustive set of region proposals sampled from each point cloud, geometric point relations within each proposal are modeled for creating expressive proposal representations. To better accommodate 3D detection properties, ProposalContrast optimizes with both inter-cluster and inter-proposal separation, i.e., sharpening the discriminativeness of proposal representations across semantic classes and object instances. The generalizability and transferability of ProposalContrast are verified on various 3D detectors (i.e., PV-RCNN, CenterPoint, PointPillars and PointRCNN) and datasets (i.e., KITTI, Waymo and ONCE).",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.089589832,
        "newsscientist":0.1430342909,
        "technologyreview":0.2029723876,
        "venturebeat":0.1899330935,
        "wired":0.1662933316,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12654v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658810749000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12824v1",
        "predicted_newsworthiness":0.3983201522,
        "title":"Compositional Human-Scene Interaction Synthesis with Semantic Control",
        "summary":"Synthesizing natural interactions between virtual humans and their 3D environments is critical for numerous applications, such as computer games and AR\/VR experiences. Our goal is to synthesize humans interacting with a given 3D scene controlled by high-level semantic specifications as pairs of action categories and object instances, e.g., \"sit on the chair\". The key challenge of incorporating interaction semantics into the generation framework is to learn a joint representation that effectively captures heterogeneous information, including human body articulation, 3D object geometry, and the intent of the interaction. To address this challenge, we design a novel transformer-based generative model, in which the articulated 3D human body surface points and 3D objects are jointly encoded in a unified latent space, and the semantics of the interaction between the human and objects are embedded via positional encoding. Furthermore, inspired by the compositional nature of interactions that humans can simultaneously interact with multiple objects, we define interaction semantics as the composition of varying numbers of atomic action-object pairs. Our proposed generative model can naturally incorporate varying numbers of atomic interactions, which enables synthesizing compositional human-scene interactions without requiring composite interaction data. We extend the PROX dataset with interaction semantic labels and scene instance segmentation to evaluate our method and demonstrate that our method can generate realistic human-scene interactions with semantic control. Our perceptual study shows that our synthesized virtual humans can naturally interact with 3D scenes, considerably outperforming existing methods. We name our method COINS, for COmpositional INteraction Synthesis with Semantic Control. Code and data are available at https:\/\/github.com\/zkf1997\/COINS.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1028882078,
        "newsscientist":0.1554486399,
        "technologyreview":0.2189544879,
        "venturebeat":0.2362683829,
        "wired":0.1983382931,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12824v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658835464000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01254v1",
        "predicted_newsworthiness":0.3980039668,
        "title":"A Robust Morphological Approach for Semantic Segmentation of Very High Resolution Images",
        "summary":"State-of-the-art methods for semantic segmentation of images involve computationally intensive neural network architectures. Most of these methods are not adaptable to high-resolution image segmentation due to memory and other computational issues. Typical approaches in literature involve design of neural network architectures that can fuse global information from low-resolution images and local information from the high-resolution counterparts. However, architectures designed for processing high resolution images are unnecessarily complex and involve a lot of hyper parameters that can be difficult to tune. Also, most of these architectures require ground truth annotations of the high resolution images to train, which can be hard to obtain. In this article, we develop a robust pipeline based on mathematical morphological (MM) operators that can seamlessly extend any existing semantic segmentation algorithm to high resolution images. Our method does not require the ground truth annotations of the high resolution images. It is based on efficiently utilizing information from the low-resolution counterparts, and gradient information on the high-resolution images. We obtain high quality seeds from the inferred labels on low-resolution images using traditional morphological operators and propagate seed labels using a random walker to refine the semantic labels at the boundaries. We show that the semantic segmentation results obtained by our method beat the existing state-of-the-art algorithms on high-resolution images. We empirically prove the robustness of our approach to the hyper parameters used in our pipeline. Further, we characterize some necessary conditions under which our pipeline is applicable and provide an in-depth analysis of the proposed approach.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0883586644,
        "newsscientist":0.139327454,
        "technologyreview":0.2056962921,
        "venturebeat":0.178425681,
        "wired":0.1343029745,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01254v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659417935000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12808v1",
        "predicted_newsworthiness":0.3979957812,
        "title":"Class-Aware Universum Inspired Re-Balance Learning for Long-Tailed Recognition",
        "summary":"Data augmentation for minority classes is an effective strategy for long-tailed recognition, thus developing a large number of methods. Although these methods all ensure the balance in sample quantity, the quality of the augmented samples is not always satisfactory for recognition, being prone to such problems as over-fitting, lack of diversity, semantic drift, etc. For these issues, we propose the Class-aware Universum Inspired Re-balance Learning(CaUIRL) for long-tailed recognition, which endows the Universum with class-aware ability to re-balance individual minority classes from both sample quantity and quality. In particular, we theoretically prove that the classifiers learned by CaUIRL are consistent with those learned under the balanced condition from a Bayesian perspective. In addition, we further develop a higher-order mixup approach, which can automatically generate class-aware Universum(CaU) data without resorting to any external data. Unlike the traditional Universum, such generated Universum additionally takes the domain similarity, class separability, and sample diversity into account. Extensive experiments on benchmark datasets demonstrate the surprising advantages of our method, especially the top1 accuracy in minority classes is improved by 1.9% 6% compared to the state-of-the-art method.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0881322142,
        "newsscientist":0.1291473518,
        "technologyreview":0.1958325417,
        "venturebeat":0.1767807984,
        "wired":0.1274630067,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12808v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658833419000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13191v1",
        "predicted_newsworthiness":0.3975653154,
        "title":"GCN-WP -- Semi-Supervised Graph Convolutional Networks for Win Prediction in Esports",
        "summary":"Win prediction is crucial to understanding skill modeling, teamwork and matchmaking in esports. In this paper we propose GCN-WP, a semi-supervised win prediction model for esports based on graph convolutional networks. This model learns the structure of an esports league over the course of a season (1 year) and makes predictions on another similar league. This model integrates over 30 features about the match and players and employs graph convolution to classify games based on their neighborhood. Our model achieves state-of-the-art prediction accuracy when compared to machine learning or skill rating models for LoL. The framework is generalizable so it can easily be extended to other multiplayer online games.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1234412921,
        "newsscientist":0.1486066165,
        "technologyreview":0.2383142158,
        "venturebeat":0.2909168494,
        "wired":0.1899115622,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13191v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.si"
        ],
        "published":1658871487000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12131v1",
        "predicted_newsworthiness":0.3974762098,
        "title":"Complementing Semi-Supervised Learning with Uncertainty Quantification",
        "summary":"The problem of fully supervised classification is that it requires a tremendous amount of annotated data, however, in many datasets a large portion of data is unlabeled. To alleviate this problem semi-supervised learning (SSL) leverages the knowledge of the classifier on the labeled domain and extrapolates it to the unlabeled domain which has a supposedly similar distribution as annotated data. Recent success on SSL methods crucially hinges on thresholded pseudo labeling and thereby consistency regularization for the unlabeled domain. However, the existing methods do not incorporate the uncertainty of the pseudo labels or unlabeled samples in the training process which are due to the noisy labels or out of distribution samples owing to strong augmentations. Inspired by the recent developments in SSL, our goal in this paper is to propose a novel unsupervised uncertainty-aware objective that relies on aleatoric and epistemic uncertainty quantification. Complementing the recent techniques in SSL with the proposed uncertainty-aware loss function our approach outperforms or is on par with the state-of-the-art over standard SSL benchmarks while being computationally lightweight. Our results outperform the state-of-the-art results on complex datasets such as CIFAR-100 and Mini-ImageNet.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0961668248,
        "newsscientist":0.1260991818,
        "technologyreview":0.2146035139,
        "venturebeat":0.2061185397,
        "wired":0.1497083099,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12131v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658448902000,
        "published_hr":"Jul 21, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13242v1",
        "predicted_newsworthiness":0.3972304867,
        "title":"Uncertainty-based Visual Question Answering: Estimating Semantic Inconsistency between Image and Knowledge Base",
        "summary":"Knowledge-based visual question answering (KVQA) task aims to answer questions that require additional external knowledge as well as an understanding of images and questions. Recent studies on KVQA inject an external knowledge in a multi-modal form, and as more knowledge is used, irrelevant information may be added and can confuse the question answering. In order to properly use the knowledge, this study proposes the following: 1) we introduce a novel semantic inconsistency measure computed from caption uncertainty and semantic similarity; 2) we suggest a new external knowledge assimilation method based on the semantic inconsistency measure and apply it to integrate explicit knowledge and implicit knowledge for KVQA; 3) the proposed method is evaluated with the OK-VQA dataset and achieves the state-of-the-art performance.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0914797251,
        "newsscientist":0.1332873405,
        "technologyreview":0.1879987583,
        "venturebeat":0.1800107593,
        "wired":0.1377442819,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13242v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.cl",
            "cs.lg"
        ],
        "published":1658887109000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01960v1",
        "predicted_newsworthiness":0.3970552159,
        "title":"Learning Object Manipulation Skills from Video via Approximate Differentiable Physics",
        "summary":"We aim to teach robots to perform simple object manipulation tasks by watching a single video demonstration. Towards this goal, we propose an optimization approach that outputs a coarse and temporally evolving 3D scene to mimic the action demonstrated in the input video. Similar to previous work, a differentiable renderer ensures perceptual fidelity between the 3D scene and the 2D video. Our key novelty lies in the inclusion of a differentiable approach to solve a set of Ordinary Differential Equations (ODEs) that allows us to approximately model laws of physics such as gravity, friction, and hand-object or object-object interactions. This not only enables us to dramatically improve the quality of estimated hand and object states, but also produces physically admissible trajectories that can be directly translated to a robot without the need for costly reinforcement learning. We evaluate our approach on a 3D reconstruction task that consists of 54 video demonstrations sourced from 9 actions such as pull something from right to left or put something in front of something. Our approach improves over previous state-of-the-art by almost 30%, demonstrating superior quality on especially challenging actions involving physical interactions of two objects such as put something onto something. Finally, we showcase the learned skills on a Franka Emika Panda robot.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.10104575,
        "newsscientist":0.1803922102,
        "technologyreview":0.2640400772,
        "venturebeat":0.2107544414,
        "wired":0.2060140408,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01960v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv",
            "cs.lg"
        ],
        "published":1659522107000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.02131v1",
        "predicted_newsworthiness":0.3967408686,
        "title":"Masked Vision and Language Modeling for Multi-modal Representation Learning",
        "summary":"In this paper, we study how to use masked signal modeling in vision and language (V+L) representation learning. Instead of developing masked language modeling (MLM) and masked image modeling (MIM) independently, we propose to build joint masked vision and language modeling, where the masked signal of one modality is reconstructed with the help from another modality. This is motivated by the nature of image-text paired data that both of the image and the text convey almost the same information but in different formats. The masked signal reconstruction of one modality conditioned on another modality can also implicitly learn cross-modal alignment between language tokens and image patches. Our experiments on various V+L tasks show that the proposed method not only achieves state-of-the-art performances by using a large amount of data, but also outperforms the other competitors by a significant margin in the regimes of limited training data.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0934793151,
        "newsscientist":0.1161827489,
        "technologyreview":0.2126205898,
        "venturebeat":0.1876771748,
        "wired":0.151796441,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02131v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.cl",
            "cs.lg"
        ],
        "published":1659539461000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00311v1",
        "predicted_newsworthiness":0.3961583912,
        "title":"Delving into Effective Gradient Matching for Dataset Condensation",
        "summary":"As deep learning models and datasets rapidly scale up, network training is extremely time-consuming and resource-costly. Instead of training on the entire dataset, learning with a small synthetic dataset becomes an efficient solution. Extensive research has been explored in the direction of dataset condensation, among which gradient matching achieves state-of-the-art performance. The gradient matching method directly targets the training dynamics by matching the gradient when training on the original and synthetic datasets. However, there are limited deep investigations into the principle and effectiveness of this method. In this work, we delve into the gradient matching method from a comprehensive perspective and answer the critical questions of what, how, and where to match. We propose to match the multi-level gradients to involve both intra-class and inter-class gradient information. We demonstrate that the distance function should focus on the angle, considering the magnitude simultaneously to delay the overfitting. An overfitting-aware adaptive learning step strategy is also proposed to trim unnecessary optimization steps for algorithmic efficiency improvement. Ablation and comparison experiments demonstrate that our proposed methodology shows superior accuracy, efficiency, and generalization compared to prior work.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0936194608,
        "newsscientist":0.1378612248,
        "technologyreview":0.2724094524,
        "venturebeat":0.2542191166,
        "wired":0.1635278066,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00311v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1659216670000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01735v1",
        "predicted_newsworthiness":0.3957765977,
        "title":"V-Coder: Adaptive AutoEncoder for Semantic Disclosure in Knowledge Graphs",
        "summary":"Semantic Web or Knowledge Graphs (KG) emerged to one of the most important information source for intelligent systems requiring access to structured knowledge. One of the major challenges is the extraction and processing of unambiguous information from textual data. Following the human perception, overlapping semantic linkages between two named entities become clear due to our common-sense about the context a relationship lives in which is not the case when we look at it from an automatically driven process of a machine. In this work, we are interested in the problem of Relational Resolution within the scope of KGs, i.e, we are investigating the inherent semantic of relationships between entities within a network. We propose a new adaptive AutoEncoder, called V-Coder, to identify relations inherently connecting entities from different domains. Those relations can be considered as being ambiguous and are candidates for disentanglement. Likewise to the Adaptive Learning Theory (ART), our model learns new patterns from the KG by increasing units in a competitive layer without discarding the previous observed patterns whilst learning the quality of each relation separately. The evaluation on real-world datasets of Freebase, Yago and NELL shows that the V-Coder is not only able to recover links from corrupted input data, but also shows that the semantic disclosure of relations in a KG show the tendency to improve link prediction. A semantic evaluation wraps the evaluation up.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1105000009,
        "newsscientist":0.1443712937,
        "technologyreview":0.2443424993,
        "venturebeat":0.2369788984,
        "wired":0.1643463759,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01735v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.lg"
        ],
        "published":1658501506000,
        "published_hr":"Jul 22, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.12244v1",
        "predicted_newsworthiness":0.3955442168,
        "title":"DeepFusion: Real-Time Dense 3D Reconstruction for Monocular SLAM using Single-View Depth and Gradient Predictions",
        "summary":"While the keypoint-based maps created by sparse monocular simultaneous localisation and mapping (SLAM) systems are useful for camera tracking, dense 3D reconstructions may be desired for many robotic tasks. Solutions involving depth cameras are limited in range and to indoor spaces, and dense reconstruction systems based on minimising the photometric error between frames are typically poorly constrained and suffer from scale ambiguity. To address these issues, we propose a 3D reconstruction system that leverages the output of a convolutional neural network (CNN) to produce fully dense depth maps for keyframes that include metric scale. Our system, DeepFusion, is capable of producing real-time dense reconstructions on a GPU. It fuses the output of a semi-dense multiview stereo algorithm with the depth and gradient predictions of a CNN in a probabilistic fashion, using learned uncertainties produced by the network. While the network only needs to be run once per keyframe, we are able to optimise for the depth map with each new frame so as to constantly make use of new geometric constraints. Based on its performance on synthetic and real-world datasets, we demonstrate that DeepFusion is capable of performing at least as well as other comparable systems.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0833870569,
        "newsscientist":0.1237282658,
        "technologyreview":0.1969233217,
        "venturebeat":0.2023117049,
        "wired":0.1667122666,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12244v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ro"
        ],
        "published":1658760926000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01380v1",
        "predicted_newsworthiness":0.3954938153,
        "title":"GaitGL: Learning Discriminative Global-Local Feature Representations for Gait Recognition",
        "summary":"Existing gait recognition methods either directly establish Global Feature Representation (GFR) from original gait sequences or generate Local Feature Representation (LFR) from several local parts. However, GFR tends to neglect local details of human postures as the receptive fields become larger in the deeper network layers. Although LFR allows the network to focus on the detailed posture information of each local region, it neglects the relations among different local parts and thus only exploits limited local information of several specific regions. To solve these issues, we propose a global-local based gait recognition network, named GaitGL, to generate more discriminative feature representations. To be specific, a novel Global and Local Convolutional Layer (GLCL) is developed to take full advantage of both global visual information and local region details in each layer. GLCL is a dual-branch structure that consists of a GFR extractor and a mask-based LFR extractor. GFR extractor aims to extract contextual information, e.g., the relationship among various body parts, and the mask-based LFR extractor is presented to exploit the detailed posture changes of local regions. In addition, we introduce a novel mask-based strategy to improve the local feature extraction capability. Specifically, we design pairs of complementary masks to randomly occlude feature maps, and then train our mask-based LFR extractor on various occluded feature maps. In this manner, the LFR extractor will learn to fully exploit local information. Extensive experiments demonstrate that GaitGL achieves better performance than state-of-the-art gait recognition methods. The average rank-1 accuracy on CASIA-B, OU-MVLP, GREW and Gait3D is 93.6%, 98.7%, 68.0% and 63.8%, respectively, significantly outperforming the competing methods. The proposed method has won the first prize in two competitions: HID 2020 and HID 2021.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0981442974,
        "newsscientist":0.1417810569,
        "technologyreview":0.2012305878,
        "venturebeat":0.1888419501,
        "wired":0.1571335377,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01380v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659441021000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14491v1",
        "predicted_newsworthiness":0.3951523208,
        "title":"Conservative Generator, Progressive Discriminator: Coordination of Adversaries in Few-shot Incremental Image Synthesis",
        "summary":"The capacity to learn incrementally from an online stream of data is an envied trait of human learners, as deep neural networks typically suffer from catastrophic forgetting and stability-plasticity dilemma. Several works have previously explored incremental few-shot learning, a task with greater challenges due to data constraint, mostly in classification setting with mild success. In this work, we study the underrepresented task of generative incremental few-shot learning. To effectively handle the inherent challenges of incremental learning and few-shot learning, we propose a novel framework named ConPro that leverages the two-player nature of GANs. Specifically, we design a conservative generator that preserves past knowledge in parameter and compute efficient manner, and a progressive discriminator that learns to reason semantic distances between past and present task samples, minimizing overfitting with few data points and pursuing good forward transfer. We present experiments to validate the effectiveness of ConPro.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0979801069,
        "newsscientist":0.1408514571,
        "technologyreview":0.2446860423,
        "venturebeat":0.2132458406,
        "wired":0.175471322,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14491v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659074429000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14347v1",
        "predicted_newsworthiness":0.3951189077,
        "title":"Training a universal instance segmentation network for live cell images of various cell types and imaging modalities",
        "summary":"We share our recent findings in an attempt to train a universal segmentation network for various cell types and imaging modalities. Our method was built on the generalized U-Net architecture, which allows the evaluation of each component individually. We modified the traditional binary training targets to include three classes for direct instance segmentation. Detailed experiments were performed regarding training schemes, training settings, network backbones, and individual modules on the segmentation performance. Our proposed training scheme draws minibatches in turn from each dataset, and the gradients are accumulated before an optimization step. We found that the key to training a universal network is all-time supervision on all datasets, and it is necessary to sample each dataset in an unbiased way. Our experiments also suggest that there might exist common features to define cell boundaries across cell types and imaging modalities, which could allow application of trained models to totally unseen datasets. A few training tricks can further boost the segmentation performance, including uneven class weights in the cross-entropy loss function, well-designed learning rate scheduler, larger image crops for contextual information, and additional loss terms for unbalanced classes. We also found that segmentation performance can benefit from group normalization layer and Atrous Spatial Pyramid Pooling module, thanks to their more reliable statistics estimation and improved semantic understanding, respectively. We participated in the 6th Cell Tracking Challenge (CTC) held at IEEE International Symposium on Biomedical Imaging (ISBI) 2021 using one of the developed variants. Our method was evaluated as the best runner up during the initial submission for the primary track, and also secured the 3rd place in an additional round of competition in preparation for the summary publication.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0843098901,
        "newsscientist":0.1536945451,
        "technologyreview":0.2142053665,
        "venturebeat":0.184576724,
        "wired":0.1445694076,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14347v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659034650000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13317v2",
        "predicted_newsworthiness":0.3949270385,
        "title":"Convolutional Embedding Makes Hierarchical Vision Transformer Stronger",
        "summary":"Vision Transformers (ViTs) have recently dominated a range of computer vision tasks, yet it suffers from low training data efficiency and inferior local semantic representation capability without appropriate inductive bias. Convolutional neural networks (CNNs) inherently capture regional-aware semantics, inspiring researchers to introduce CNNs back into the architecture of the ViTs to provide desirable inductive bias for ViTs. However, is the locality achieved by the micro-level CNNs embedded in ViTs good enough? In this paper, we investigate the problem by profoundly exploring how the macro architecture of the hybrid CNNs\/ViTs enhances the performances of hierarchical ViTs. Particularly, we study the role of token embedding layers, alias convolutional embedding (CE), and systemically reveal how CE injects desirable inductive bias in ViTs. Besides, we apply the optimal CE configuration to 4 recently released state-of-the-art ViTs, effectively boosting the corresponding performances. Finally, a family of efficient hybrid CNNs\/ViTs, dubbed CETNets, are released, which may serve as generic vision backbones. Specifically, CETNets achieve 84.9% Top-1 accuracy on ImageNet-1K (training from scratch), 48.6% box mAP on the COCO benchmark, and 51.6% mIoU on the ADE20K, substantially improving the performances of the corresponding state-of-the-art baselines.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1024624318,
        "newsscientist":0.1478058189,
        "technologyreview":0.2590951841,
        "venturebeat":0.2377322326,
        "wired":0.182958493,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13317v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658903796000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14191v1",
        "predicted_newsworthiness":0.3947629135,
        "title":"Learning with Limited Annotations: A Survey on Deep Semi-Supervised Learning for Medical Image Segmentation",
        "summary":"Medical image segmentation is a fundamental and critical step in many image-guided clinical approaches. Recent success of deep learning-based segmentation methods usually relies on a large amount of labeled data, which is particularly difficult and costly to obtain especially in the medical imaging domain where only experts can provide reliable and accurate annotations. Semi-supervised learning has emerged as an appealing strategy and been widely applied to medical image segmentation tasks to train deep models with limited annotations. In this paper, we present a comprehensive review of recently proposed semi-supervised learning methods for medical image segmentation and summarized both the technical novelties and empirical results. Furthermore, we analyze and discuss the limitations and several unsolved problems of existing approaches. We hope this review could inspire the research community to explore solutions for this challenge and further promote the developments in medical image segmentation field.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0635228462,
        "newsscientist":0.108299896,
        "technologyreview":0.1747263291,
        "venturebeat":0.1508935393,
        "wired":0.1016078323,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14191v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659023866000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02148v1",
        "predicted_newsworthiness":0.3944647542,
        "title":"GPPF: A General Perception Pre-training Framework via Sparsely Activated Multi-Task Learning",
        "summary":"Pre-training over mixtured multi-task, multi-domain, and multi-modal data remains an open challenge in vision perception pre-training. In this paper, we propose GPPF, a General Perception Pre-training Framework, that pre-trains a task-level dynamic network, which is composed by knowledge \"legos\" in each layers, on labeled multi-task and multi-domain datasets. By inspecting humans' innate ability to learn in complex environment, we recognize and transfer three critical elements to deep networks: (1) simultaneous exposure to diverse cross-task and cross-domain information in each batch. (2) partitioned knowledge storage in separate lego units driven by knowledge sharing. (3) sparse activation of a subset of lego units for both pre-training and downstream tasks. Noteworthy, the joint training of disparate vision tasks is non-trivial due to their differences in input shapes, loss functions, output formats, data distributions, etc. Therefore, we innovatively develop a plug-and-play multi-task training algorithm, which supports Single Iteration Multiple Tasks (SIMT) concurrently training. SIMT lays the foundation of pre-training with large-scale multi-task multi-domain datasets and is proved essential for stable training in our GPPF experiments. Excitingly, the exhaustive experiments show that, our GPPF-R50 model achieves significant improvements of 2.5-5.8 over a strong baseline of the 8 pre-training tasks in GPPF-15M and harvests a range of SOTAs over the 22 downstream tasks with similar computation budgets. We also validate the generalization ability of GPPF to SOTA vision transformers with consistent improvements. These solid experimental results fully prove the effective knowledge learning, storing, sharing, and transfer provided by our novel GPPF framework.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0826381605,
        "newsscientist":0.1447825023,
        "technologyreview":0.2544517845,
        "venturebeat":0.2217431634,
        "wired":0.1630958562,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02148v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659540875000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01838v1",
        "predicted_newsworthiness":0.394072166,
        "title":"Re-Attention Transformer for Weakly Supervised Object Localization",
        "summary":"Weakly supervised object localization is a challenging task which aims to localize objects with coarse annotations such as image categories. Existing deep network approaches are mainly based on class activation map, which focuses on highlighting discriminative local region while ignoring the full object. In addition, the emerging transformer-based techniques constantly put a lot of emphasis on the backdrop that impedes the ability to identify complete objects. To address these issues, we present a re-attention mechanism termed token refinement transformer (TRT) that captures the object-level semantics to guide the localization well. Specifically, TRT introduces a novel module named token priority scoring module (TPSM) to suppress the effects of background noise while focusing on the target object. Then, we incorporate the class activation map as the semantically aware input to restrain the attention map to the target object. Extensive experiments on two benchmarks showcase the superiority of our proposed method against existing methods with image category annotations. Source code is available in \\url{https:\/\/github.com\/su-hui-zz\/ReAttentionTransformer}.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0785223013,
        "newsscientist":0.1279217505,
        "technologyreview":0.1935879876,
        "venturebeat":0.1727274485,
        "wired":0.1385411272,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01838v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659501268000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13080v1",
        "predicted_newsworthiness":0.3937055551,
        "title":"DETRs with Hybrid Matching",
        "summary":"One-to-one set matching is a key design for DETR to establish its end-to-end capability, so that object detection does not require a hand-crafted NMS (non-maximum suppression) method to remove duplicate detections. This end-to-end signature is important for the versatility of DETR, and it has been generalized to a wide range of visual problems, including instance\/semantic segmentation, human pose estimation, and point cloud\/multi-view-images based detection, etc. However, we note that because there are too few queries assigned as positive samples, the one-to-one set matching significantly reduces the training efficiency of positive samples. This paper proposes a simple yet effective method based on a hybrid matching scheme that combines the original one-to-one matching branch with auxiliary queries that use one-to-many matching loss during training. This hybrid strategy has been shown to significantly improve training efficiency and improve accuracy. In inference, only the original one-to-one match branch is used, thus maintaining the end-to-end merit and the same inference efficiency of DETR. The method is named $\\mathcal{H}$-DETR, and it shows that a wide range of representative DETR methods can be consistently improved across a wide range of visual tasks, including Deformable-DETR, 3DETR\/PETRv2, PETR, and TransTrack, among others. Code will be available at: https:\/\/github.com\/HDETR",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0861251843,
        "newsscientist":0.1311837704,
        "technologyreview":0.2125806322,
        "venturebeat":0.2007291245,
        "wired":0.158546887,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13080v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658857934000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01710v1",
        "predicted_newsworthiness":0.3935142387,
        "title":"Smart Visual Beacons with Asynchronous Optical Communications using Event Cameras",
        "summary":"Event cameras are bio-inspired dynamic vision sensors that respond to changes in image intensity with a high temporal resolution, high dynamic range and low latency. These sensor characteristics are ideally suited to enable visual target tracking in concert with a broadcast visual communication channel for smart visual beacons with applications in distributed robotics. Visual beacons can be constructed by high-frequency modulation of Light Emitting Diodes (LEDs) such as vehicle headlights, Internet of Things (IoT) LEDs, smart building lights, etc., that are already present in many real-world scenarios. The high temporal resolution characteristic of the event cameras allows them to capture visual signals at far higher data rates compared to classical frame-based cameras. In this paper, we propose a novel smart visual beacon architecture with both LED modulation and event camera demodulation algorithms. We quantitatively evaluate the relationship between LED transmission rate, communication distance and the message transmission accuracy for the smart visual beacon communication system that we prototyped. The proposed method achieves up to 4 kbps in an indoor environment and lossless transmission over a distance of 100 meters, at a transmission rate of 500 bps, in full sunlight, demonstrating the potential of the technology in an outdoor environment.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0952540549,
        "newsscientist":0.1630622437,
        "technologyreview":0.2142442623,
        "venturebeat":0.2346240325,
        "wired":0.2123289668,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01710v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659469592000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13457v1",
        "predicted_newsworthiness":0.3933670995,
        "title":"Reducing the Vision and Language Bias for Temporal Sentence Grounding",
        "summary":"Temporal sentence grounding (TSG) is an important yet challenging task in multimedia information retrieval. Although previous TSG methods have achieved decent performance, they tend to capture the selection biases of frequently appeared video-query pairs in the dataset rather than present robust multimodal reasoning abilities, especially for the rarely appeared pairs. In this paper, we study the above issue of selection biases and accordingly propose a Debiasing-TSG (D-TSG) model to filter and remove the negative biases in both vision and language modalities for enhancing the model generalization ability. Specifically, we propose to alleviate the issue from two perspectives: 1) Feature distillation. We built a multi-modal debiasing branch to firstly capture the vision and language biases, and then apply a bias identification module to explicitly recognize the true negative biases and remove them from the benign multi-modal representations. 2) Contrastive sample generation. We construct two types of negative samples to enforce the model to accurately learn the aligned multi-modal semantics and make complete semantic reasoning. We apply the proposed model to both commonly and rarely appeared TSG cases, and demonstrate its effectiveness by achieving the state-of-the-art performance on three benchmark datasets (ActivityNet Caption, TACoS, and Charades-STA).",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1155394954,
        "newsscientist":0.1226574408,
        "technologyreview":0.2115839764,
        "venturebeat":0.2050348613,
        "wired":0.1832288043,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13457v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658920725000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01009v1",
        "predicted_newsworthiness":0.3933457708,
        "title":"Few-shot Adaptation Works with UnpredicTable Data",
        "summary":"Prior work on language models (LMs) shows that training on a large number of diverse tasks improves few-shot learning (FSL) performance on new tasks. We take this to the extreme, automatically extracting 413,299 tasks from internet tables - orders of magnitude more than the next-largest public datasets. Finetuning on the resulting dataset leads to improved FSL performance on Natural Language Processing (NLP) tasks, but not proportionally to dataset scale. In fact, we find that narrow subsets of our dataset sometimes outperform more diverse datasets. For example, finetuning on software documentation from support.google.com raises FSL performance by a mean of +7.5% on 52 downstream tasks, which beats training on 40 human-curated NLP datasets (+6.7%). Finetuning on various narrow datasets leads to similar broad improvements across test tasks, suggesting that the gains are not from domain adaptation but adapting to FSL in general. We do not observe clear patterns between the datasets that lead to FSL gains, leaving open questions about why certain data helps with FSL.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.119067068,
        "newsscientist":0.1571365556,
        "technologyreview":0.2827627121,
        "venturebeat":0.2805237596,
        "wired":0.2035534164,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01009v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai",
            "cs.lg"
        ],
        "published":1659375325000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00231v1",
        "predicted_newsworthiness":0.3930980686,
        "title":"Masked Autoencoders As The Unified Learners For Pre-Trained Sentence Representation",
        "summary":"Despite the progresses on pre-trained language models, there is a lack of unified frameworks for pre-trained sentence representation. As such, it calls for different pre-training methods for specific scenarios, and the pre-trained models are likely to be limited by their universality and representation quality. In this work, we extend the recently proposed MAE style pre-training strategy, RetroMAE, such that it may effectively support a wide variety of sentence representation tasks. The extended framework consists of two stages, with RetroMAE conducted throughout the process. The first stage performs RetroMAE over generic corpora, like Wikipedia, BookCorpus, etc., from which the base model is learned. The second stage takes place on domain-specific data, e.g., MS MARCO and NLI, where the base model is continuingly trained based on RetroMAE and contrastive learning. The pre-training outputs at the two stages may serve different applications, whose effectiveness are verified with comprehensive experiments. Concretely, the base model are proved to be effective for zero-shot retrieval, with remarkable performances achieved on BEIR benchmark. The continuingly pre-trained models further benefit more downstream tasks, including the domain-specific dense retrieval on MS MARCO, Natural Questions, and the sentence embeddings' quality for standard STS and transfer tasks in SentEval. The empirical insights of this work may inspire the future design of sentence representation pre-training. Our pre-trained models and source code will be released to the public communities.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1059086865,
        "newsscientist":0.1089872083,
        "technologyreview":0.1999000998,
        "venturebeat":0.1997067645,
        "wired":0.1596883388,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00231v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659191695000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14349v1",
        "predicted_newsworthiness":0.3930704729,
        "title":"Bridging the Gap between Deep Learning and Hypothesis-Driven Analysis via Permutation Testing",
        "summary":"A fundamental approach in neuroscience research is to test hypotheses based on neuropsychological and behavioral measures, i.e., whether certain factors (e.g., related to life events) are associated with an outcome (e.g., depression). In recent years, deep learning has become a potential alternative approach for conducting such analyses by predicting an outcome from a collection of factors and identifying the most \"informative\" ones driving the prediction. However, this approach has had limited impact as its findings are not linked to statistical significance of factors supporting hypotheses. In this article, we proposed a flexible and scalable approach based on the concept of permutation testing that integrates hypothesis testing into the data-driven deep learning analysis. We apply our approach to the yearly self-reported assessments of 621 adolescent participants of the National Consortium of Alcohol and Neurodevelopment in Adolescence (NCANDA) to predict negative valence, a symptom of major depressive disorder according to the NIMH Research Domain Criteria (RDoC). Our method successfully identifies categories of risk factors that further explain the symptom.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1885096831,
        "newsscientist":0.2140445107,
        "technologyreview":0.2725533386,
        "venturebeat":0.2389899864,
        "wired":0.198383615,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14349v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659034869000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02080v1",
        "predicted_newsworthiness":0.3930670134,
        "title":"A Feature-space Multimodal Data Augmentation Technique for Text-video Retrieval",
        "summary":"Every hour, huge amounts of visual contents are posted on social media and user-generated content platforms. To find relevant videos by means of a natural language query, text-video retrieval methods have received increased attention over the past few years. Data augmentation techniques were introduced to increase the performance on unseen test examples by creating new training samples with the application of semantics-preserving techniques, such as color space or geometric transformations on images. Yet, these techniques are usually applied on raw data, leading to more resource-demanding solutions and also requiring the shareability of the raw data, which may not always be true, e.g. copyright issues with clips from movies or TV series. To address this shortcoming, we propose a multimodal data augmentation technique which works in the feature space and creates new videos and captions by mixing semantically similar samples. We experiment our solution on a large scale public dataset, EPIC-Kitchens-100, and achieve considerable improvements over a baseline method, improved state-of-the-art performance, while at the same time performing multiple ablation studies. We release code and pretrained models on Github at https:\/\/github.com\/aranciokov\/FSMMDA_VideoRetrieval.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1427228149,
        "newsscientist":0.1559325538,
        "technologyreview":0.2546599831,
        "venturebeat":0.2439153648,
        "wired":0.2351630819,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02080v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659535520000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01709v1",
        "predicted_newsworthiness":0.3929239644,
        "title":"Adapting Triplet Importance of Implicit Feedback for Personalized Recommendation",
        "summary":"Implicit feedback is frequently used for developing personalized recommendation services due to its ubiquity and accessibility in real-world systems. In order to effectively utilize such information, most research adopts the pairwise ranking method on constructed training triplets (user, positive item, negative item) and aims to distinguish between positive items and negative items for each user. However, most of these methods treat all the training triplets equally, which ignores the subtle difference between different positive or negative items. On the other hand, even though some other works make use of the auxiliary information (e.g., dwell time) of user behaviors to capture this subtle difference, such auxiliary information is hard to obtain. To mitigate the aforementioned problems, we propose a novel training framework named Triplet Importance Learning (TIL), which adaptively learns the importance score of training triplets. We devise two strategies for the importance score generation and formulate the whole procedure as a bilevel optimization, which does not require any rule-based design. We integrate the proposed training procedure with several Matrix Factorization (MF)- and Graph Neural Network (GNN)-based recommendation models, demonstrating the compatibility of our framework. Via a comparison using three real-world datasets with many state-of-the-art methods, we show that our proposed method outperforms the best existing models by 3-21\\% in terms of Recall@k for the top-k recommendation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1034359159,
        "newsscientist":0.1387891334,
        "technologyreview":0.2239181681,
        "venturebeat":0.2440973287,
        "wired":0.2008395755,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01709v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.lg"
        ],
        "published":1659469487000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.14428v1",
        "predicted_newsworthiness":0.3928635833,
        "title":"Paired Cross-Modal Data Augmentation for Fine-Grained Image-to-Text Retrieval",
        "summary":"This paper investigates an open research problem of generating text-image pairs to improve the training of fine-grained image-to-text cross-modal retrieval task, and proposes a novel framework for paired data augmentation by uncovering the hidden semantic information of StyleGAN2 model. Specifically, we first train a StyleGAN2 model on the given dataset. We then project the real images back to the latent space of StyleGAN2 to obtain the latent codes. To make the generated images manipulatable, we further introduce a latent space alignment module to learn the alignment between StyleGAN2 latent codes and the corresponding textual caption features. When we do online paired data augmentation, we first generate augmented text through random token replacement, then pass the augmented text into the latent space alignment module to output the latent codes, which are finally fed to StyleGAN2 to generate the augmented images. We evaluate the efficacy of our augmented data approach on two public cross-modal retrieval datasets, in which the promising experimental results demonstrate the augmented text-image pair data can be trained together with the original data to boost the image-to-text cross-modal retrieval performance.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0976432549,
        "newsscientist":0.1316537637,
        "technologyreview":0.2322113675,
        "venturebeat":0.2169690488,
        "wired":0.1722250721,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14428v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659057714000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01316v1",
        "predicted_newsworthiness":0.3926483248,
        "title":"Joint Learning-based Causal Relation Extraction from Biomedical Literature",
        "summary":"Causal relation extraction of biomedical entities is one of the most complex tasks in biomedical text mining, which involves two kinds of information: entity relations and entity functions. One feasible approach is to take relation extraction and function detection as two independent sub-tasks. However, this separate learning method ignores the intrinsic correlation between them and leads to unsatisfactory performance. In this paper, we propose a joint learning model, which combines entity relation extraction and entity function detection to exploit their commonality and capture their inter-relationship, so as to improve the performance of biomedical causal relation extraction. Meanwhile, during the model training stage, different function types in the loss function are assigned different weights. Specifically, the penalty coefficient for negative function instances increases to effectively improve the precision of function detection. Experimental results on the BioCreative-V Track 4 corpus show that our joint learning model outperforms the separate models in BEL statement extraction, achieving the F1 scores of 58.4% and 37.3% on the test set in Stage 2 and Stage 1 evaluations, respectively. This demonstrates that our joint learning system reaches the state-of-the-art performance in Stage 2 compared with other systems.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1367435969,
        "newsscientist":0.1559869203,
        "technologyreview":0.2059482165,
        "venturebeat":0.1973134079,
        "wired":0.145475314,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01316v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1659430497000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00894v1",
        "predicted_newsworthiness":0.3926069219,
        "title":"Towards Computing an Optimal Abstraction for Structural Causal Models",
        "summary":"Working with causal models at different levels of abstraction is an important feature of science. Existing work has already considered the problem of expressing formally the relation of abstraction between causal models. In this paper, we focus on the problem of learning abstractions. We start by defining the learning problem formally in terms of the optimization of a standard measure of consistency. We then point out the limitation of this approach, and we suggest extending the objective function with a term accounting for information loss. We suggest a concrete measure of information loss, and we illustrate its contribution to learning new abstractions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1125721366,
        "newsscientist":0.1367059568,
        "technologyreview":0.1815908077,
        "venturebeat":0.143993428,
        "wired":0.1339851151,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00894v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1659364557000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.14465v1",
        "predicted_newsworthiness":0.3925261133,
        "title":"Fine-grained Retrieval Prompt Tuning",
        "summary":"Fine-grained object retrieval aims to learn discriminative representation to retrieve visually similar objects. However, existing top-performing works usually impose pairwise similarities on the semantic embedding spaces to continually fine-tune the entire model in limited-data regimes, thus resulting in easily converging to suboptimal solutions. In this paper, we develop Fine-grained Retrieval Prompt Tuning (FRPT), which steers a frozen pre-trained model to perform the fine-grained retrieval task from the perspectives of sample prompt and feature adaptation. Specifically, FRPT only needs to learn fewer parameters in the prompt and adaptation instead of fine-tuning the entire model, thus solving the convergence to suboptimal solutions caused by fine-tuning the entire model. Technically, as sample prompts, a structure perturbation prompt (SPP) is introduced to zoom and even exaggerate some pixels contributing to category prediction via a content-aware inhomogeneous sampling operation. In this way, SPP can make the fine-grained retrieval task aided by the perturbation prompts close to the solved task during the original pre-training. Besides, a category-specific awareness head is proposed and regarded as feature adaptation, which removes the species discrepancies in the features extracted by the pre-trained model using instance normalization, and thus makes the optimized features only include the discrepancies among subcategories. Extensive experiments demonstrate that our FRPT with fewer learnable parameters achieves the state-of-the-art performance on three widely-used fine-grained datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0697633414,
        "newsscientist":0.116059679,
        "technologyreview":0.1741053602,
        "venturebeat":0.1637404248,
        "wired":0.131142711,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14465v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659067804000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12217v1",
        "predicted_newsworthiness":0.3923322335,
        "title":"Finite-Time Analysis of Asynchronous Q-learning under Diminishing Step-Size from Control-Theoretic View",
        "summary":"Q-learning has long been one of the most popular reinforcement learning algorithms, and theoretical analysis of Q-learning has been an active research topic for decades. Although researches on asymptotic convergence analysis of Q-learning have a long tradition, non-asymptotic convergence has only recently come under active study. The main goal of this paper is to investigate new finite-time analysis of asynchronous Q-learning under Markovian observation models via a control system viewpoint. In particular, we introduce a discrete-time time-varying switching system model of Q-learning with diminishing step-sizes for our analysis, which significantly improves recent development of the switching system analysis with constant step-sizes, and leads to \\(\\mathcal{O}\\left( \\sqrt{\\frac{\\log k}{k}} \\right)\\) convergence rate that is comparable to or better than most of the state of the art results in the literature. In the mean while, a technique using the similarly transformation is newly applied to avoid the difficulty in the analysis posed by diminishing step-sizes. The proposed analysis brings in additional insights, covers different scenarios, and provides new simplified templates for analysis to deepen our understanding on Q-learning via its unique connection to discrete-time switching systems.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0671409981,
        "newsscientist":0.0961577883,
        "technologyreview":0.1370660992,
        "venturebeat":0.1158592471,
        "wired":0.1076349237,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12217v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.lg"
        ],
        "published":1658758555000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.14130v1",
        "predicted_newsworthiness":0.3923142246,
        "title":"FedVARP: Tackling the Variance Due to Partial Client Participation in Federated Learning",
        "summary":"Data-heterogeneous federated learning (FL) systems suffer from two significant sources of convergence error: 1) client drift error caused by performing multiple local optimization steps at clients, and 2) partial client participation error caused by the fact that only a small subset of the edge clients participate in every training round. We find that among these, only the former has received significant attention in the literature. To remedy this, we propose FedVARP, a novel variance reduction algorithm applied at the server that eliminates error due to partial client participation. To do so, the server simply maintains in memory the most recent update for each client and uses these as surrogate updates for the non-participating clients in every round. Further, to alleviate the memory requirement at the server, we propose a novel clustering-based variance reduction algorithm ClusterFedVARP. Unlike previously proposed methods, both FedVARP and ClusterFedVARP do not require additional computation at clients or communication of additional optimization parameters. Through extensive experiments, we show that FedVARP outperforms state-of-the-art methods, and ClusterFedVARP achieves performance comparable to FedVARP with much less memory requirements.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0846033039,
        "newsscientist":0.112652347,
        "technologyreview":0.2119045951,
        "venturebeat":0.2376189376,
        "wired":0.1485665639,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14130v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659019901000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13865v1",
        "predicted_newsworthiness":0.3919731977,
        "title":"Diversity Boosted Learning for Domain Generalization with Large Number of Domains",
        "summary":"Machine learning algorithms minimizing the average training loss usually suffer from poor generalization performance due to the greedy exploitation of correlations among the training data, which are not stable under distributional shifts. It inspires various works for domain generalization (DG), where a series of methods, such as Causal Matching and FISH, work by pairwise domain operations. They would need $O(n^2)$ pairwise domain operations with $n$ domains, where each one is often highly expensive. Moreover, while a common objective in the DG literature is to learn invariant representations against domain-induced spurious correlations, we highlight the importance of mitigating spurious correlations caused by objects. Based on the observation that diversity helps mitigate spurious correlations, we propose a Diversity boosted twO-level saMplIng framework (DOMI) utilizing Determinantal Point Processes (DPPs) to efficiently sample the most informative ones among large number of domains. We show that DOMI helps train robust models against spurious correlations from both domain-side and object-side, substantially enhancing the performance of the backbone DG algorithms on rotated MNIST, rotated Fashion MNIST, and iwildcam datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0939245414,
        "newsscientist":0.1296697846,
        "technologyreview":0.2263530331,
        "venturebeat":0.2121490091,
        "wired":0.1571383821,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13865v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658977097000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14698v1",
        "predicted_newsworthiness":0.3918091936,
        "title":"Can Shuffling Video Benefit Temporal Bias Problem: A Novel Training Framework for Temporal Grounding",
        "summary":"Temporal grounding aims to locate a target video moment that semantically corresponds to the given sentence query in an untrimmed video. However, recent works find that existing methods suffer a severe temporal bias problem. These methods do not reason the target moment locations based on the visual-textual semantic alignment but over-rely on the temporal biases of queries in training sets. To this end, this paper proposes a novel training framework for grounding models to use shuffled videos to address temporal bias problem without losing grounding accuracy. Our framework introduces two auxiliary tasks, cross-modal matching and temporal order discrimination, to promote the grounding model training. The cross-modal matching task leverages the content consistency between shuffled and original videos to force the grounding model to mine visual contents to semantically match queries. The temporal order discrimination task leverages the difference in temporal order to strengthen the understanding of long-term temporal contexts. Extensive experiments on Charades-STA and ActivityNet Captions demonstrate the effectiveness of our method for mitigating the reliance on temporal biases and strengthening the model's generalization ability against the different temporal distributions. Code is available at https:\/\/github.com\/haojc\/ShufflingVideosForTSG.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1162215248,
        "newsscientist":0.1478754955,
        "technologyreview":0.2182879128,
        "venturebeat":0.2156048918,
        "wired":0.1969126099,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14698v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659103908000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11735v1",
        "predicted_newsworthiness":0.3916631352,
        "title":"AMS-Net: Adaptive Multiscale Sparse Neural Network with Interpretable Basis Expansion for Multiphase Flow Problems",
        "summary":"In this work, we propose an adaptive sparse learning algorithm that can be applied to learn the physical processes and obtain a sparse representation of the solution given a large snapshot space. Assume that there is a rich class of precomputed basis functions that can be used to approximate the quantity of interest. We then design a neural network architecture to learn the coefficients of solutions in the spaces which are spanned by these basis functions. The information of the basis functions are incorporated in the loss function, which minimizes the differences between the downscaled reduced order solutions and reference solutions at multiple time steps. The network contains multiple submodules and the solutions at different time steps can be learned simultaneously. We propose some strategies in the learning framework to identify important degrees of freedom. To find a sparse solution representation, a soft thresholding operator is applied to enforce the sparsity of the output coefficient vectors of the neural network. To avoid over-simplification and enrich the approximation space, some degrees of freedom can be added back to the system through a greedy algorithm. In both scenarios, that is, removing and adding degrees of freedom, the corresponding network connections are pruned or reactivated guided by the magnitude of the solution coefficients obtained from the network outputs. The proposed adaptive learning process is applied to some toy case examples to demonstrate that it can achieve a good basis selection and accurate approximation. More numerical tests are performed on two-phase multiscale flow problems to show the capability and interpretability of the proposed method on complicated applications.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0693467365,
        "newsscientist":0.1042190205,
        "technologyreview":0.1403785345,
        "venturebeat":0.1133266209,
        "wired":0.0957196356,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11735v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658668363000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02129v1",
        "predicted_newsworthiness":0.3916521333,
        "title":"SC6D: Symmetry-agnostic and Correspondence-free 6D Object Pose Estimation",
        "summary":"This paper presents an efficient symmetry-agnostic and correspondence-free framework, referred to as SC6D, for 6D object pose estimation from a single monocular RGB image. SC6D requires neither the 3D CAD model of the object nor any prior knowledge of the symmetries. The pose estimation is decomposed into three sub-tasks: a) object 3D rotation representation learning and matching; b) estimation of the 2D location of the object center; and c) scale-invariant distance estimation (the translation along the z-axis) via classification. SC6D is evaluated on three benchmark datasets, T-LESS, YCB-V, and ITODD, and results in state-of-the-art performance on the T-LESS dataset. Moreover, SC6D is computationally much more efficient than the previous state-of-the-art method SurfEmb. The implementation and pre-trained models are publicly available at https:\/\/github.com\/dingdingcai\/SC6D-pose.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0630250269,
        "newsscientist":0.1046962026,
        "technologyreview":0.1640670126,
        "venturebeat":0.1762283419,
        "wired":0.140395235,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02129v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659539307000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13428v1",
        "predicted_newsworthiness":0.3912679137,
        "title":"Leveraging GAN Priors for Few-Shot Part Segmentation",
        "summary":"Few-shot part segmentation aims to separate different parts of an object given only a few annotated samples. Due to the challenge of limited data, existing works mainly focus on learning classifiers over pre-trained features, failing to learn task-specific features for part segmentation. In this paper, we propose to learn task-specific features in a \"pre-training\"-\"fine-tuning\" paradigm. We conduct prompt designing to reduce the gap between the pre-train task (i.e., image generation) and the downstream task (i.e., part segmentation), so that the GAN priors for generation can be leveraged for segmentation. This is achieved by projecting part segmentation maps into the RGB space and conducting interpolation between RGB segmentation maps and original images. Specifically, we design a fine-tuning strategy to progressively tune an image generator into a segmentation generator, where the supervision of the generator varying from images to segmentation maps by interpolation. Moreover, we propose a two-stream architecture, i.e., a segmentation stream to generate task-specific features, and an image stream to provide spatial constraints. The image stream can be regarded as a self-supervised auto-encoder, and this enables our model to benefit from large-scale support images. Overall, this work is an attempt to explore the internal relevance between generation tasks and perception tasks by prompt designing. Extensive experiments show that our model can achieve state-of-the-art performance on several part segmentation datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0756008111,
        "newsscientist":0.1199972185,
        "technologyreview":0.1950421569,
        "venturebeat":0.1704357291,
        "wired":0.1443961517,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13428v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658917027000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11782v1",
        "predicted_newsworthiness":0.3912173378,
        "title":"Enhancements to the BOUN Treebank Reflecting the Agglutinative Nature of Turkish",
        "summary":"In this study, we aim to offer linguistically motivated solutions to resolve the issues of the lack of representation of null morphemes, highly productive derivational processes, and syncretic morphemes of Turkish in the BOUN Treebank without diverging from the Universal Dependencies framework. In order to tackle these issues, new annotation conventions were introduced by splitting certain lemmas and employing the MISC (miscellaneous) tab in the UD framework to denote derivation. Representational capabilities of the re-annotated treebank were tested on a LSTM-based dependency parser and an updated version of the BoAT Tool is introduced.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.096284463,
        "newsscientist":0.089856415,
        "technologyreview":0.097604522,
        "venturebeat":0.1111992739,
        "wired":0.0909649086,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11782v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658685387000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00113v1",
        "predicted_newsworthiness":0.3910830146,
        "title":"Neural Correspondence Field for Object Pose Estimation",
        "summary":"We propose a method for estimating the 6DoF pose of a rigid object with an available 3D model from a single RGB image. Unlike classical correspondence-based methods which predict 3D object coordinates at pixels of the input image, the proposed method predicts 3D object coordinates at 3D query points sampled in the camera frustum. The move from pixels to 3D points, which is inspired by recent PIFu-style methods for 3D reconstruction, enables reasoning about the whole object, including its (self-)occluded parts. For a 3D query point associated with a pixel-aligned image feature, we train a fully-connected neural network to predict: (i) the corresponding 3D object coordinates, and (ii) the signed distance to the object surface, with the first defined only for query points in the surface vicinity. We call the mapping realized by this network as Neural Correspondence Field. The object pose is then robustly estimated from the predicted 3D-3D correspondences by the Kabsch-RANSAC algorithm. The proposed method achieves state-of-the-art results on three BOP datasets and is shown superior especially in challenging cases with occlusion. The project website is at: linhuang17.github.io\/NCF.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0669860788,
        "newsscientist":0.1224618089,
        "technologyreview":0.1916845915,
        "venturebeat":0.1891212096,
        "wired":0.1512185673,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00113v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg",
            "cs.ro"
        ],
        "published":1659145703000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01744v1",
        "predicted_newsworthiness":0.3910439473,
        "title":"Cross-Modal Alignment Learning of Vision-Language Conceptual Systems",
        "summary":"Human infants learn the names of objects and develop their own conceptual systems without explicit supervision. In this study, we propose methods for learning aligned vision-language conceptual systems inspired by infants' word learning mechanisms. The proposed model learns the associations of visual objects and words online and gradually constructs cross-modal relational graph networks. Additionally, we also propose an aligned cross-modal representation learning method that learns semantic representations of visual objects and words in a self-supervised manner based on the cross-modal relational graph networks. It allows entities of different modalities with conceptually the same meaning to have similar semantic representation vectors. We quantitatively and qualitatively evaluate our method, including object-to-word mapping and zero-shot learning tasks, showing that the proposed model significantly outperforms the baselines and that each conceptual system is topologically aligned.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0908102781,
        "newsscientist":0.138244361,
        "technologyreview":0.2129164116,
        "venturebeat":0.1814821728,
        "wired":0.1476446751,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01744v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1659256793000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00731v2",
        "predicted_newsworthiness":0.3910265234,
        "title":"Planar Modeling and Sim-to-Real of a Tethered Multimaterial Soft Swimmer Driven by Peano-HASELs",
        "summary":"Soft robotics has the potential to revolutionize robotic locomotion, in particular, soft robotic swimmers offer a minimally invasive and adaptive solution to explore and preserve our oceans. Unfortunately, current soft robotic swimmers are vastly inferior to evolved biological swimmers, especially in terms of controllability, efficiency, maneuverability, and longevity. Additionally, the tedious iterative fabrication and empirical testing required to design soft robots has hindered their optimization. In this work, we tackle this challenge by providing an efficient and straightforward pipeline for designing and fabricating soft robotic swimmers equipped with electrostatic actuation. We streamline the process to allow for rapid additive manufacturing, and show how a differentiable simulation can be used to match a simplified model to the real deformation of a robotic swimmer. We perform several experiments with the fabricated swimmer by varying the voltage and actuation frequency of the swimmer's antagonistic muscles. We show how the voltage and frequency vary the locomotion speed of the swimmer while moving in liquid oil and observe a clear optimum in forward swimming speed. The differentiable simulation model we propose has various downstream applications, such as control and shape optimization of the swimmer; optimization results can be directly mapped back to the real robot through our sim-to-real matching.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0865226265,
        "newsscientist":0.2055430311,
        "technologyreview":0.1913447633,
        "venturebeat":0.1520385583,
        "wired":0.1710167136,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00731v2",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659350025000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.14455v1",
        "predicted_newsworthiness":0.3910199976,
        "title":"Neural Density-Distance Fields",
        "summary":"The success of neural fields for 3D vision tasks is now indisputable. Following this trend, several methods aiming for visual localization (e.g., SLAM) have been proposed to estimate distance or density fields using neural fields. However, it is difficult to achieve high localization performance by only density fields-based methods such as Neural Radiance Field (NeRF) since they do not provide density gradient in most empty regions. On the other hand, distance field-based methods such as Neural Implicit Surface (NeuS) have limitations in objects' surface shapes. This paper proposes Neural Density-Distance Field (NeDDF), a novel 3D representation that reciprocally constrains the distance and density fields. We extend distance field formulation to shapes with no explicit boundary surface, such as fur or smoke, which enable explicit conversion from distance field to density field. Consistent distance and density fields realized by explicit conversion enable both robustness to initial values and high-quality registration. Furthermore, the consistency between fields allows fast convergence from sparse point clouds. Experiments show that NeDDF can achieve high localization performance while providing comparable results to NeRF on novel view synthesis. The code is available at https:\/\/github.com\/ueda0319\/neddf.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0907789336,
        "newsscientist":0.1523804844,
        "technologyreview":0.1975518315,
        "venturebeat":0.1828690119,
        "wired":0.1490099695,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14455v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659064405000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11542v1",
        "predicted_newsworthiness":0.390586377,
        "title":"Annealed Training for Combinatorial Optimization on Graphs",
        "summary":"The hardness of combinatorial optimization (CO) problems hinders collecting solutions for supervised learning. However, learning neural networks for CO problems is notoriously difficult in lack of the labeled data as the training is easily trapped at local optima. In this work, we propose a simple but effective annealed training framework for CO problems. In particular, we transform CO problems into unbiased energy-based models (EBMs). We carefully selected the penalties terms so as to make the EBMs as smooth as possible. Then we train graph neural networks to approximate the EBMs. To prevent the training from being stuck at local optima near the initialization, we introduce an annealed loss function. An experimental evaluation demonstrates that our annealed training framework obtains substantial improvements. In four types of CO problems, our method achieves performance substantially better than other unsupervised neural methods on both synthetic and real-world graphs.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0897440116,
        "newsscientist":0.1271840851,
        "technologyreview":0.2258535964,
        "venturebeat":0.2070552176,
        "wired":0.1335151445,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11542v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658591294000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14255v1",
        "predicted_newsworthiness":0.3905618637,
        "title":"Efficient Training of Language Models to Fill in the Middle",
        "summary":"We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0891320848,
        "newsscientist":0.1179190213,
        "technologyreview":0.211136703,
        "venturebeat":0.2153603679,
        "wired":0.1624541219,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14255v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659030047000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00483v1",
        "predicted_newsworthiness":0.3904007552,
        "title":"Building an Efficiency Pipeline: Commutativity and Cumulativeness of Efficiency Operators for Transformers",
        "summary":"There exists a wide variety of efficiency methods for natural language processing (NLP) tasks, such as pruning, distillation, dynamic inference, quantization, etc. We can consider an efficiency method as an operator applied on a model. Naturally, we may construct a pipeline of multiple efficiency methods, i.e., to apply multiple operators on the model sequentially. In this paper, we study the plausibility of this idea, and more importantly, the commutativity and cumulativeness of efficiency operators. We make two interesting observations: (1) Efficiency operators are commutative -- the order of efficiency methods within the pipeline has little impact on the final results; (2) Efficiency operators are also cumulative -- the final results of combining several efficiency methods can be estimated by combining the results of individual methods. These observations deepen our understanding of efficiency operators and provide useful guidelines for their real-world applications.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0842419195,
        "newsscientist":0.1012720738,
        "technologyreview":0.1725392308,
        "venturebeat":0.178255494,
        "wired":0.1284190472,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00483v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1659290466000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00109v1",
        "predicted_newsworthiness":0.3903311419,
        "title":"Traveler: Navigating Task Parallel Traces for Performance Analysis",
        "summary":"Understanding the behavior of software in execution is a key step in identifying and fixing performance issues. This is especially important in high performance computing contexts where even minor performance tweaks can translate into large savings in terms of computational resource use. To aid performance analysis, developers may collect an execution trace - a chronological log of program activity during execution. As traces represent the full history, developers can discover a wide array of possibly previously unknown performance issues, making them an important artifact for exploratory performance analysis. However, interactive trace visualization is difficult due to issues of data size and complexity of meaning. Traces represent nanosecond-level events across many parallel processes, meaning the collected data is often large and difficult to explore. The rise of asynchronous task parallel programming paradigms complicates the relation between events and their probable cause. To address these challenges, we conduct a continuing design study in collaboration with high performance computing researchers. We develop diverse and hierarchical ways to navigate and represent execution trace data in support of their trace analysis tasks. Through an iterative design process, we developed Traveler, an integrated visualization platform for task parallel traces. Traveler provides multiple linked interfaces to help navigate trace data from multiple contexts. We evaluate the utility of Traveler through feedback from users and a case study, finding that integrating multiple modes of navigation in our design supported performance analysis tasks and led to the discovery of previously unknown behavior in a distributed array library.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1182073994,
        "newsscientist":0.143906366,
        "technologyreview":0.1905145767,
        "venturebeat":0.2360756566,
        "wired":0.190956461,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00109v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1659142232000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.11441v2",
        "predicted_newsworthiness":0.3903116566,
        "title":"Meta Spatio-Temporal Debiasing for Video Scene Graph Generation",
        "summary":"Video scene graph generation (VidSGG) aims to parse the video content into scene graphs, which involves modeling the spatio-temporal contextual information in the video. However, due to the long-tailed training data in datasets, the generalization performance of existing VidSGG models can be affected by the spatio-temporal conditional bias problem. In this work, from the perspective of meta-learning, we propose a novel Meta Video Scene Graph Generation (MVSGG) framework to address such a bias problem. Specifically, to handle various types of spatio-temporal conditional biases, our framework first constructs a support set and a group of query sets from the training data, where the data distribution of each query set is different from that of the support set w.r.t. a type of conditional bias. Then, by performing a novel meta training and testing process to optimize the model to obtain good testing performance on these query sets after training on the support set, our framework can effectively guide the model to learn to well generalize against biases. Extensive experiments demonstrate the efficacy of our proposed framework.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.107641882,
        "newsscientist":0.1292881999,
        "technologyreview":0.2034814508,
        "venturebeat":0.1861599098,
        "wired":0.1779353864,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11441v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658559966000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14289v1",
        "predicted_newsworthiness":0.3901351695,
        "title":"Initialization and Alignment for Adversarial Texture Optimization",
        "summary":"While recovery of geometry from image and video data has received a lot of attention in computer vision, methods to capture the texture for a given geometry are less mature. Specifically, classical methods for texture generation often assume clean geometry and reasonably well-aligned image data. While very recent methods, e.g., adversarial texture optimization, better handle lower-quality data obtained from hand-held devices, we find them to still struggle frequently. To improve robustness, particularly of recent adversarial texture optimization, we develop an explicit initialization and an alignment procedure. It deals with complex geometry due to a robust mapping of the geometry to the texture map and a hard-assignment-based initialization. It deals with misalignment of geometry and images by integrating fast image-alignment into the texture refinement optimization. We demonstrate efficacy of our texture generation on a dataset of 11 scenes with a total of 2807 frames, observing 7.8% and 11.1% relative improvements regarding perceptual and sharpness measurements.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0990316084,
        "newsscientist":0.1441634554,
        "technologyreview":0.2229718625,
        "venturebeat":0.1938623709,
        "wired":0.1775990094,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14289v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659031195000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13224v1",
        "predicted_newsworthiness":0.3899270911,
        "title":"PI-ARS: Accelerating Evolution-Learned Visual-Locomotion with Predictive Information Representations",
        "summary":"Evolution Strategy (ES) algorithms have shown promising results in training complex robotic control policies due to their massive parallelism capability, simple implementation, effective parameter-space exploration, and fast training time. However, a key limitation of ES is its scalability to large capacity models, including modern neural network architectures. In this work, we develop Predictive Information Augmented Random Search (PI-ARS) to mitigate this limitation by leveraging recent advancements in representation learning to reduce the parameter search space for ES. Namely, PI-ARS combines a gradient-based representation learning technique, Predictive Information (PI), with a gradient-free ES algorithm, Augmented Random Search (ARS), to train policies that can process complex robot sensory inputs and handle highly nonlinear robot dynamics. We evaluate PI-ARS on a set of challenging visual-locomotion tasks where a quadruped robot needs to walk on uneven stepping stones, quincuncial piles, and moving platforms, as well as to complete an indoor navigation task. Across all tasks, PI-ARS demonstrates significantly better learning efficiency and performance compared to the ARS baseline. We further validate our algorithm by demonstrating that the learned policies can successfully transfer to a real quadruped robot, for example, achieving a 100% success rate on the real-world stepping stone environment, dramatically improving prior results achieving 40% success.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0876965277,
        "newsscientist":0.1812458976,
        "technologyreview":0.2819674352,
        "venturebeat":0.2389608647,
        "wired":0.1944233066,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13224v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai",
            "cs.lg"
        ],
        "published":1658881575000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.11817v1",
        "predicted_newsworthiness":0.3897207923,
        "title":"A Multiple-Entanglement Routing Framework for Quantum Networks",
        "summary":"Quantum networks are gaining momentum in finding applications in a wide range of domains. However, little research has investigated the potential of a quantum network framework to enable highly reliable communications. The goal of this work is to investigate and design the multiple-entanglement routing framework, namely k-entangled routing. In particular, the $k$-entangled routing will enable k paths connecting all demands (source-destination pairs) in the network. To design the $k$-entangled routing, we propose two algorithms that are called Sequential Multi-path Scheduling Algorithm and Min-Cut-based Multi-path Scheduling Algorithm. In addition, we evaluate the performance of the proposed algorithms and models through a realistic quantum network simulator, NetSquid, that models the stochastic processes underlying quantum communications. The results show that the proposed algorithms (SMPSA and MCSA) largely enhance the network's traffic flexibility. The proposed paradigms would lay the foundation for further research on the area of entanglement routing.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0630963493,
        "newsscientist":0.1171048719,
        "technologyreview":0.1282862716,
        "venturebeat":0.1040900788,
        "wired":0.1014381121,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11817v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658232543000,
        "published_hr":"Jul 19, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.12298v1",
        "predicted_newsworthiness":0.3897031601,
        "title":"Deforming Radiance Fields with Cages",
        "summary":"Recent advances in radiance fields enable photorealistic rendering of static or dynamic 3D scenes, but still do not support explicit deformation that is used for scene manipulation or animation. In this paper, we propose a method that enables a new type of deformation of the radiance field: free-form radiance field deformation. We use a triangular mesh that encloses the foreground object called cage as an interface, and by manipulating the cage vertices, our approach enables the free-form deformation of the radiance field. The core of our approach is cage-based deformation which is commonly used in mesh deformation. We propose a novel formulation to extend it to the radiance field, which maps the position and the view direction of the sampling points from the deformed space to the canonical space, thus enabling the rendering of the deformed scene. The deformation results of the synthetic datasets and the real-world datasets demonstrate the effectiveness of our approach.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0929218274,
        "newsscientist":0.1449694768,
        "technologyreview":0.1462400689,
        "venturebeat":0.1799398714,
        "wired":0.1567907294,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12298v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658765335000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13649v1",
        "predicted_newsworthiness":0.389585218,
        "title":"Explain My Surprise: Learning Efficient Long-Term Memory by Predicting Uncertain Outcomes",
        "summary":"In many sequential tasks, a model needs to remember relevant events from the distant past to make correct predictions. Unfortunately, a straightforward application of gradient based training requires intermediate computations to be stored for every element of a sequence. This requires prohibitively large computing memory if a sequence consists of thousands or even millions elements, and as a result, makes learning of very long-term dependencies infeasible. However, the majority of sequence elements can usually be predicted by taking into account only temporally local information. On the other hand, predictions affected by long-term dependencies are sparse and characterized by high uncertainty given only local information. We propose MemUP, a new training method that allows to learn long-term dependencies without backpropagating gradients through the whole sequence at a time. This method can be potentially applied to any gradient based sequence learning. MemUP implementation for recurrent architectures shows performances better or comparable to baselines while requiring significantly less computing memory.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1119137725,
        "newsscientist":0.1626726639,
        "technologyreview":0.2664849853,
        "venturebeat":0.242888318,
        "wired":0.1751013509,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13649v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658941967000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00383v1",
        "predicted_newsworthiness":0.3895697454,
        "title":"DRL-M4MR: An Intelligent Multicast Routing Approach Based on DQN Deep Reinforcement Learning in SDN",
        "summary":"Traditional multicast routing methods have some problems in constructing a multicast tree, such as limited access to network state information, poor adaptability to dynamic and complex changes in the network, and inflexible data forwarding. To address these defects, the optimal multicast routing problem in software-defined networking (SDN) is tailored as a multi-objective optimization problem, and an intelligent multicast routing algorithm DRL-M4MR based on the deep Q network (DQN) deep reinforcement learning (DRL) method is designed to construct a multicast tree in SDN. First, the multicast tree state matrix, link bandwidth matrix, link delay matrix, and link packet loss rate matrix are designed as the state space of the DRL agent by combining the global view and control of the SDN. Second, the action space of the agent is all the links in the network, and the action selection strategy is designed to add the links to the current multicast tree under four cases. Third, single-step and final reward function forms are designed to guide the intelligence to make decisions to construct the optimal multicast tree. The experimental results show that, compared with existing algorithms, the multicast tree construct by DRL-M4MR can obtain better bandwidth, delay, and packet loss rate performance after training, and it can make more intelligent multicast routing decisions in a dynamic network environment.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0746055999,
        "newsscientist":0.1089352643,
        "technologyreview":0.1949511236,
        "venturebeat":0.1998127093,
        "wired":0.1386104666,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00383v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni",
            "cs.ai"
        ],
        "published":1659252834000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.12744v1",
        "predicted_newsworthiness":0.3892794816,
        "title":"Distribution Learning Based on Evolutionary Algorithm Assisted Deep Neural Networks for Imbalanced Image Classification",
        "summary":"To address the trade-off problem of quality-diversity for the generated images in imbalanced classification tasks, we research on over-sampling based methods at the feature level instead of the data level and focus on searching the latent feature space for optimal distributions. On this basis, we propose an iMproved Estimation Distribution Algorithm based Latent featUre Distribution Evolution (MEDA_LUDE) algorithm, where a joint learning procedure is programmed to make the latent features both optimized and evolved by the deep neural networks and the evolutionary algorithm, respectively. We explore the effect of the Large-margin Gaussian Mixture (L-GM) loss function on distribution learning and design a specialized fitness function based on the similarities among samples to increase diversity. Extensive experiments on benchmark based imbalanced datasets validate the effectiveness of our proposed algorithm, which can generate images with both quality and diversity. Furthermore, the MEDA_LUDE algorithm is also applied to the industrial field and successfully alleviates the imbalanced issue in fabric defect classification.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0755463539,
        "newsscientist":0.113475353,
        "technologyreview":0.1948038783,
        "venturebeat":0.1704953112,
        "wired":0.1241572867,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12744v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658825507000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01874v1",
        "predicted_newsworthiness":0.3890062243,
        "title":"Exploring Generative Neural Temporal Point Process",
        "summary":"Temporal point process (TPP) is commonly used to model the asynchronous event sequence featuring occurrence timestamps and revealed by probabilistic models conditioned on historical impacts. While lots of previous works have focused on `goodness-of-fit' of TPP models by maximizing the likelihood, their predictive performance is unsatisfactory, which means the timestamps generated by models are far apart from true observations. Recently, deep generative models such as denoising diffusion and score matching models have achieved great progress in image generating tasks by demonstrating their capability of generating samples of high quality. However, there are no complete and unified works exploring and studying the potential of generative models in the context of event occurence modeling for TPP. In this work, we try to fill the gap by designing a unified \\textbf{g}enerative framework for \\textbf{n}eural \\textbf{t}emporal \\textbf{p}oint \\textbf{p}rocess (\\textsc{GNTPP}) model to explore their feasibility and effectiveness, and further improve models' predictive performance. Besides, in terms of measuring the historical impacts, we revise the attentive models which summarize influence from historical events with an adaptive reweighting term considering events' type relation and time intervals. Extensive experiments have been conducted to illustrate the improved predictive capability of \\textsc{GNTPP} with a line of generative probabilistic decoders, and performance gain from the revised attention. To the best of our knowledge, this is the first work that adapts generative models in a complete unified framework and studies their effectiveness in the context of TPP. Our codebase including all the methods given in Section.5.1.1 is open in \\url{https:\/\/github.com\/BIRD-TAO\/GNTPP}. We hope the code framework can facilitate future research in Neural TPPs.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1097165145,
        "newsscientist":0.1526839858,
        "technologyreview":0.2080530384,
        "venturebeat":0.1949016244,
        "wired":0.1757508828,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01874v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659509788000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00050v1",
        "predicted_newsworthiness":0.3889775389,
        "title":"Generating Complex 4D Expression Transitions by Learning Face Landmark Trajectories",
        "summary":"In this work, we address the problem of 4D facial expressions generation. This is usually addressed by animating a neutral 3D face to reach an expression peak, and then get back to the neutral state. In the real world though, people show more complex expressions, and switch from one expression to another. We thus propose a new model that generates transitions between different expressions, and synthesizes long and composed 4D expressions. This involves three sub-problems: (i) modeling the temporal dynamics of expressions, (ii) learning transitions between them, and (iii) deforming a generic mesh. We propose to encode the temporal evolution of expressions using the motion of a set of 3D landmarks, that we learn to generate by training a manifold-valued GAN (Motion3DGAN). To allow the generation of composed expressions, this model accepts two labels encoding the starting and the ending expressions. The final sequence of meshes is generated by a Sparse2Dense mesh Decoder (S2D-Dec) that maps the landmark displacements to a dense, per-vertex displacement of a known mesh topology. By explicitly working with motion trajectories, the model is totally independent from the identity. Extensive experiments on five public datasets show that our proposed approach brings significant improvements with respect to previous solutions, while retaining good generalization to unseen data.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0972277913,
        "newsscientist":0.1327907487,
        "technologyreview":0.2016222182,
        "venturebeat":0.1968454263,
        "wired":0.1759446473,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00050v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659123236000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12995v1",
        "predicted_newsworthiness":0.3885255367,
        "title":"Robust and Efficient Segmentation of Cross-domain Medical Images",
        "summary":"Efficient medical image segmentation aims to provide accurate pixel-wise prediction for the medical images with the lightweight implementation framework. However, lightweight frameworks generally fail to achieve high performance, and suffer from the poor generalizable ability on cross-domain tasks.In this paper, we propose a generalizable knowledge distillation method for robust and efficient segmentation of cross-domain medical images. Primarily, we propose the Model-Specific Alignment Networks (MSAN) to provide the domain-invariant representations which are regularized by a Pre-trained Semantic AutoEncoder (P-SAE). Meanwhile, a customized Alignment Consistency Training (ACT) strategy is designed to promote the MSAN training. With the domain-invariant representative vectors in MSAN, we propose two generalizable knowledge distillation schemes, Dual Contrastive Graph Distillation (DCGD) and Domain-Invariant Cross Distillation (DICD). Specifically, in DCGD, two types of implicit contrastive graphs are designed to represent the intra-coupling and inter-coupling semantic correlations from the perspective of data distribution. In DICD, the domain-invariant semantic vectors from the two models (i.e., teacher and student) are leveraged to cross-reconstruct features by the header exchange of MSAN, which achieves generalizable improvement for both the encoder and decoder in the student model. Furthermore, a metric named Fr\\'echet Semantic Distance (FSD) is tailored to verify the effectiveness of the regularized domain-invariant features. Extensive experiments conducted on the Liver and Retinal Vessel Segmentation datasets demonstrate the priority of our method, in terms of performance and generalization on lightweight frameworks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0685910482,
        "newsscientist":0.0972197563,
        "technologyreview":0.1844010532,
        "venturebeat":0.1608707784,
        "wired":0.1013486154,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12995v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658850936000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13374v1",
        "predicted_newsworthiness":0.3885082808,
        "title":"Efficient Video Deblurring Guided by Motion Magnitude",
        "summary":"Video deblurring is a highly under-constrained problem due to the spatially and temporally varying blur. An intuitive approach for video deblurring includes two steps: a) detecting the blurry region in the current frame; b) utilizing the information from clear regions in adjacent frames for current frame deblurring. To realize this process, our idea is to detect the pixel-wise blur level of each frame and combine it with video deblurring. To this end, we propose a novel framework that utilizes the motion magnitude prior (MMP) as guidance for efficient deep video deblurring. Specifically, as the pixel movement along its trajectory during the exposure time is positively correlated to the level of motion blur, we first use the average magnitude of optical flow from the high-frequency sharp frames to generate the synthetic blurry frames and their corresponding pixel-wise motion magnitude maps. We then build a dataset including the blurry frame and MMP pairs. The MMP is then learned by a compact CNN by regression. The MMP consists of both spatial and temporal blur level information, which can be further integrated into an efficient recurrent neural network (RNN) for video deblurring. We conduct intensive experiments to validate the effectiveness of the proposed methods on the public datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0864182679,
        "newsscientist":0.1080995252,
        "technologyreview":0.1512457298,
        "venturebeat":0.1324920925,
        "wired":0.125076268,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13374v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658912268000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12716v1",
        "predicted_newsworthiness":0.3883415992,
        "title":"MV-FCOS3D++: Multi-View Camera-Only 4D Object Detection with Pretrained Monocular Backbones",
        "summary":"In this technical report, we present our solution, dubbed MV-FCOS3D++, for the Camera-Only 3D Detection track in Waymo Open Dataset Challenge 2022. For multi-view camera-only 3D detection, methods based on bird-eye-view or 3D geometric representations can leverage the stereo cues from overlapped regions between adjacent views and directly perform 3D detection without hand-crafted post-processing. However, it lacks direct semantic supervision for 2D backbones, which can be complemented by pretraining simple monocular-based detectors. Our solution is a multi-view framework for 4D detection following this paradigm. It is built upon a simple monocular detector FCOS3D++, pretrained only with object annotations of Waymo, and converts multi-view features to a 3D grid space to detect 3D objects thereon. A dual-path neck for single-frame understanding and temporal stereo matching is devised to incorporate multi-frame information. Our method finally achieves 49.75% mAPL with a single model and wins 2nd place in the WOD challenge, without any LiDAR-based depth supervision during training. The code will be released at https:\/\/github.com\/Tai-Wang\/Depth-from-Motion.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0875357757,
        "newsscientist":0.1506814626,
        "technologyreview":0.2260949679,
        "venturebeat":0.240320652,
        "wired":0.2023836309,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12716v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ro"
        ],
        "published":1658823029000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14299v2",
        "predicted_newsworthiness":0.3882576773,
        "title":"Graph Inverse Reinforcement Learning from Diverse Videos",
        "summary":"Research on Inverse Reinforcement Learning (IRL) from third-person videos has shown encouraging results on removing the need for manual reward design for robotic tasks. However, most prior works are still limited by training from a relatively restricted domain of videos. In this paper, we argue that the true potential of third-person IRL lies in increasing the diversity of videos for better scaling. To learn a reward function from diverse videos, we propose to perform graph abstraction on the videos followed by temporal matching in the graph space to measure the task progress. Our insight is that a task can be described by entity interactions that form a graph, and this graph abstraction can help remove irrelevant information such as textures, resulting in more robust reward functions. We evaluate our approach, GraphIRL, on cross-embodiment learning in X-MAGICAL and learning from human demonstrations for real-robot manipulation. We show significant improvements in robustness to diverse video demonstrations over previous approaches, and even achieve better results than manual reward design on a real robot pushing task. Videos are available at https:\/\/sateeshkumar21.github.io\/GraphIRL .",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1169858558,
        "newsscientist":0.1823528758,
        "technologyreview":0.2802933233,
        "venturebeat":0.2428365736,
        "wired":0.2183662733,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14299v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv",
            "cs.ro"
        ],
        "published":1659030691000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01648v1",
        "predicted_newsworthiness":0.3882521566,
        "title":"Maximal Independent Vertex Set applied to Graph Pooling",
        "summary":"Convolutional neural networks (CNN) have enabled major advances in image classification through convolution and pooling. In particular, image pooling transforms a connected discrete grid into a reduced grid with the same connectivity and allows reduction functions to take into account all the pixels of an image. However, a pooling satisfying such properties does not exist for graphs. Indeed, some methods are based on a vertex selection step which induces an important loss of information. Other methods learn a fuzzy clustering of vertex sets which induces almost complete reduced graphs. We propose to overcome both problems using a new pooling method, named MIVSPool. This method is based on a selection of vertices called surviving vertices using a Maximal Independent Vertex Set (MIVS) and an assignment of the remaining vertices to the survivors. Consequently, our method does not discard any vertex information nor artificially increase the density of the graph. Experimental results show an increase in accuracy for graph classification on various standard datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0935539882,
        "newsscientist":0.1315927391,
        "technologyreview":0.2048774681,
        "venturebeat":0.1786790513,
        "wired":0.1404784262,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01648v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cv"
        ],
        "published":1659451378000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12000v1",
        "predicted_newsworthiness":0.3882119386,
        "title":"GNN Transformation Framework for Improving Efficiency and Scalability",
        "summary":"We propose a framework that automatically transforms non-scalable GNNs into precomputation-based GNNs which are efficient and scalable for large-scale graphs. The advantages of our framework are two-fold; 1) it transforms various non-scalable GNNs to scale well to large-scale graphs by separating local feature aggregation from weight learning in their graph convolution, 2) it efficiently executes precomputation on GPU for large-scale graphs by decomposing their edges into small disjoint and balanced sets. Through extensive experiments with large-scale graphs, we demonstrate that the transformed GNNs run faster in training time than existing GNNs while achieving competitive accuracy to the state-of-the-art GNNs. Consequently, our transformation framework provides simple and efficient baselines for future research on scalable GNNs.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0941278567,
        "newsscientist":0.1425151734,
        "technologyreview":0.2549773415,
        "venturebeat":0.2438869763,
        "wired":0.168503212,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12000v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658740799000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.06953v2",
        "predicted_newsworthiness":0.3881930004,
        "title":"Tackling Background Distraction in Video Object Segmentation",
        "summary":"Semi-supervised video object segmentation (VOS) aims to densely track certain designated objects in videos. One of the main challenges in this task is the existence of background distractors that appear similar to the target objects. We propose three novel strategies to suppress such distractors: 1) a spatio-temporally diversified template construction scheme to obtain generalized properties of the target objects; 2) a learnable distance-scoring function to exclude spatially-distant distractors by exploiting the temporal consistency between two consecutive frames; 3) swap-and-attach augmentation to force each object to have unique features by providing training samples containing entangled objects. On all public benchmark datasets, our model achieves a comparable performance to contemporary state-of-the-art approaches, even with real-time performance. Qualitative results also demonstrate the superiority of our approach over existing methods. We believe our approach will be widely used for future VOS research.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.079442382,
        "newsscientist":0.1302141654,
        "technologyreview":0.1733659102,
        "venturebeat":0.1674822315,
        "wired":0.1521969655,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.06953v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1657808719000,
        "published_hr":"Jul 14, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12579v1",
        "predicted_newsworthiness":0.3879718712,
        "title":"RenderNet: Visual Relocalization Using Virtual Viewpoints in Large-Scale Indoor Environments",
        "summary":"Visual relocalization has been a widely discussed problem in 3D vision: given a pre-constructed 3D visual map, the 6 DoF (Degrees-of-Freedom) pose of a query image is estimated. Relocalization in large-scale indoor environments enables attractive applications such as augmented reality and robot navigation. However, appearance changes fast in such environments when the camera moves, which is challenging for the relocalization system. To address this problem, we propose a virtual view synthesis-based approach, RenderNet, to enrich the database and refine poses regarding this particular scenario. Instead of rendering real images which requires high-quality 3D models, we opt to directly render the needed global and local features of virtual viewpoints and apply them in the subsequent image retrieval and feature matching operations respectively. The proposed method can largely improve the performance in large-scale indoor environments, e.g., achieving an improvement of 7.1\\% and 12.2\\% on the Inloc dataset.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0876214239,
        "newsscientist":0.1364357474,
        "technologyreview":0.2193775213,
        "venturebeat":0.2688102062,
        "wired":0.213947858,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12579v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658794123000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02108v1",
        "predicted_newsworthiness":0.387600893,
        "title":"MTGFlow: Unsupervised Multivariate Time Series Anomaly Detection via Dynamic Graph and Entity-aware Normalizing Flow",
        "summary":"Multivariate time series anomaly detection has been extensively studied under the semi-supervised setting, where a training dataset with all normal instances is required. However, preparing such a dataset is very laborious since each single data instance should be fully guaranteed to be normal. It is, therefore, desired to explore multivariate time series anomaly detection methods based on the dataset without any label knowledge. In this paper, we propose MTGFlow, an unsupervised anomaly detection approach for Multivariate Time series anomaly detection via dynamic Graph and entity-aware normalizing Flow, leaning only on a widely accepted hypothesis that abnormal instances exhibit sparse densities than the normal. However, the complex interdependencies among entities and the diverse inherent characteristics of each entity pose significant challenges on the density estimation, let alone to detect anomalies based on the estimated possibility distribution. To tackle these problems, we propose to learn the mutual and dynamic relations among entities via a graph structure learning model, which helps to model accurate distribution of multivariate time series. Moreover, taking account of distinct characteristics of the individual entities, an entity-aware normalizing flow is developed to describe each entity into a parameterized normal distribution, thereby producing fine-grained density estimation. Incorporating these two strategies, MTGFlowachieves superior anomaly detection performance. Experiments on the real-world datasets are conducted, demonstrating that MTGFlow outperforms the state-of-the-art (SOTA) by 5.0% and 1.6% AUROC for SWaT and WADI datasets respectively. Also, through the anomaly scores contributed by individual entities, MTGFlow can provide explanation information for the detection results.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1171995435,
        "newsscientist":0.1450674458,
        "technologyreview":0.2081518493,
        "venturebeat":0.2211659331,
        "wired":0.1727295053,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02108v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659537499000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.10856v2",
        "predicted_newsworthiness":0.3873977245,
        "title":"Prototype-Guided Continual Adaptation for Class-Incremental Unsupervised Domain Adaptation",
        "summary":"This paper studies a new, practical but challenging problem, called Class-Incremental Unsupervised Domain Adaptation (CI-UDA), where the labeled source domain contains all classes, but the classes in the unlabeled target domain increase sequentially. This problem is challenging due to two difficulties. First, source and target label sets are inconsistent at each time step, which makes it difficult to conduct accurate domain alignment. Second, previous target classes are unavailable in the current step, resulting in the forgetting of previous knowledge. To address this problem, we propose a novel Prototype-guided Continual Adaptation (ProCA) method, consisting of two solution strategies. 1) Label prototype identification: we identify target label prototypes by detecting shared classes with cumulative prediction probabilities of target samples. 2) Prototype-based alignment and replay: based on the identified label prototypes, we align both domains and enforce the model to retain previous knowledge. With these two strategies, ProCA is able to adapt the source model to a class-incremental unlabeled target domain effectively. Extensive experiments demonstrate the effectiveness and superiority of ProCA in resolving CI-UDA. The source code is available at https:\/\/github.com\/Hongbin98\/ProCA.git",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0724939453,
        "newsscientist":0.115095279,
        "technologyreview":0.1804992639,
        "venturebeat":0.1715914374,
        "wired":0.1254129362,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.10856v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658460156000,
        "published_hr":"Jul 21, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12166v1",
        "predicted_newsworthiness":0.3869045749,
        "title":"Graph Querying for Semantic Annotations",
        "summary":"This paper presents how the online tool GREW-MATCH can be used to make queries and visualise data from existing semantically annotated corpora. A dedicated syntax is available to construct simple to complex queries and execute them against a corpus. Such queries give transverse views of the annotated data, these views can help for checking the consistency of annotations in one corpus or across several corpora. GREW-MATCH can then be seen as an error mining tool: when inconsistencies are detected, it helps finding the sentences which should be fixed. Finally, GREW-MATCH can also be used as a side tool to assist annotation tasks helping to find annotation examples in existing corpora to be compared to the data to be annotated.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1279151424,
        "newsscientist":0.133507551,
        "technologyreview":0.1934270743,
        "venturebeat":0.2075213226,
        "wired":0.15579267,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12166v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1658754495000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2208.01875v1",
        "predicted_newsworthiness":0.3867962738,
        "title":"Introducing BEREL: BERT Embeddings for Rabbinic-Encoded Language",
        "summary":"We present a new pre-trained language model (PLM) for Rabbinic Hebrew, termed Berel (BERT Embeddings for Rabbinic-Encoded Language). Whilst other PLMs exist for processing Hebrew texts (e.g., HeBERT, AlephBert), they are all trained on modern Hebrew texts, which diverges substantially from Rabbinic Hebrew in terms of its lexicographical, morphological, syntactic and orthographic norms. We demonstrate the superiority of Berel on Rabbinic texts via a challenge set of Hebrew homographs. We release the new model and homograph challenge set for unrestricted use.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0951381759,
        "newsscientist":0.0942586547,
        "technologyreview":0.1464265777,
        "venturebeat":0.1549537716,
        "wired":0.1321668554,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01875v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659509944000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.12650v1",
        "predicted_newsworthiness":0.386629673,
        "title":"Asymmetric Scalable Cross-modal Hashing",
        "summary":"Cross-modal hashing is a successful method to solve large-scale multimedia retrieval issue. A lot of matrix factorization-based hashing methods are proposed. However, the existing methods still struggle with a few problems, such as how to generate the binary codes efficiently rather than directly relax them to continuity. In addition, most of the existing methods choose to use an $n\\times n$ similarity matrix for optimization, which makes the memory and computation unaffordable. In this paper we propose a novel Asymmetric Scalable Cross-Modal Hashing (ASCMH) to address these issues. It firstly introduces a collective matrix factorization to learn a common latent space from the kernelized features of different modalities, and then transforms the similarity matrix optimization to a distance-distance difference problem minimization with the help of semantic labels and common latent space. Hence, the computational complexity of the $n\\times n$ asymmetric optimization is relieved. In the generation of hash codes we also employ an orthogonal constraint of label information, which is indispensable for search accuracy. So the redundancy of computation can be much reduced. For efficient optimization and scalable to large-scale datasets, we adopt the two-step approach rather than optimizing simultaneously. Extensive experiments on three benchmark datasets: Wiki, MIRFlickr-25K, and NUS-WIDE, demonstrate that our ASCMH outperforms the state-of-the-art cross-modal hashing methods in terms of accuracy and efficiency.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0847685028,
        "newsscientist":0.1172766945,
        "technologyreview":0.1574931893,
        "venturebeat":0.1506102183,
        "wired":0.1467362077,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12650v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ir"
        ],
        "published":1658810327000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01909v1",
        "predicted_newsworthiness":0.3864222461,
        "title":"Rethinking the Evaluation of Unbiased Scene Graph Generation",
        "summary":"Since the severe imbalanced predicate distributions in common subject-object relations, current Scene Graph Generation (SGG) methods tend to predict frequent predicate categories and fail to recognize rare ones. To improve the robustness of SGG models on different predicate categories, recent research has focused on unbiased SGG and adopted mean Recall@K (mR@K) as the main evaluation metric. However, we discovered two overlooked issues about this de facto standard metric mR@K, which makes current unbiased SGG evaluation vulnerable and unfair: 1) mR@K neglects the correlations among predicates and unintentionally breaks category independence when ranking all the triplet predictions together regardless of the predicate categories, leading to the performance of some predicates being underestimated. 2) mR@K neglects the compositional diversity of different predicates and assigns excessively high weights to some oversimple category samples with limited composable relation triplet types. It totally conflicts with the goal of SGG task which encourages models to detect more types of visual relationship triplets. In addition, we investigate the under-explored correlation between objects and predicates, which can serve as a simple but strong baseline for unbiased SGG. In this paper, we refine mR@K and propose two complementary evaluation metrics for unbiased SGG: Independent Mean Recall (IMR) and weighted IMR (wIMR). These two metrics are designed by considering the category independence and diversity of composable relation triplets, respectively. We compare the proposed metrics with the de facto standard metrics through extensive experiments and discuss the solutions to evaluate unbiased SGG in a more trustworthy way.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1096793626,
        "newsscientist":0.143787328,
        "technologyreview":0.2285235916,
        "venturebeat":0.2057436592,
        "wired":0.177562904,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01909v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659515031000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01299v1",
        "predicted_newsworthiness":0.3863602374,
        "title":"To Answer or Not to Answer? Improving Machine Reading Comprehension Model with Span-based Contrastive Learning",
        "summary":"Machine Reading Comprehension with Unanswerable Questions is a difficult NLP task, challenged by the questions which can not be answered from passages. It is observed that subtle literal changes often make an answerable question unanswerable, however, most MRC models fail to recognize such changes. To address this problem, in this paper, we propose a span-based method of Contrastive Learning (spanCL) which explicitly contrast answerable questions with their answerable and unanswerable counterparts at the answer span level. With spanCL, MRC models are forced to perceive crucial semantic changes from slight literal differences. Experiments on SQuAD 2.0 dataset show that spanCL can improve baselines significantly, yielding 0.86-2.14 absolute EM improvements. Additional experiments also show that spanCL is an effective way to utilize generated questions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0957273146,
        "newsscientist":0.1181523224,
        "technologyreview":0.1806553619,
        "venturebeat":0.176126067,
        "wired":0.1306101251,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01299v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659427745000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11839v1",
        "predicted_newsworthiness":0.38602012,
        "title":"A Deep Dive into Deep Cluster",
        "summary":"Deep Learning has demonstrated a significant improvement against traditional machine learning approaches in different domains such as image and speech recognition. Their success on benchmark datasets is transferred to the real-world through pretrained models by practitioners. Pretraining visual models using supervised learning requires a significant amount of expensive data annotation. To tackle this limitation, DeepCluster - a simple and scalable unsupervised pretraining of visual representations - has been proposed. However, the underlying work of the model is not yet well understood. In this paper, we analyze DeepCluster internals and exhaustively evaluate the impact of various hyperparameters over a wide range of values on three different datasets. Accordingly, we propose an explanation of why the algorithm works in practice. We also show that DeepCluster convergence and performance highly depend on the interplay between the quality of the randomly initialized filters of the convolutional layer and the selected number of clusters. Furthermore, we demonstrate that continuous clustering is not critical for DeepCluster convergence. Therefore, early stopping of the clustering phase will reduce the training time and allow the algorithm to scale to large datasets. Finally, we derive plausible hyperparameter selection criteria in a semi-supervised setting.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0913270875,
        "newsscientist":0.1398700609,
        "technologyreview":0.2729802752,
        "venturebeat":0.2550987965,
        "wired":0.1769874067,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11839v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658703309000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12174v1",
        "predicted_newsworthiness":0.3858712303,
        "title":"How much of UCCA can be predicted from AMR?",
        "summary":"In this paper, we consider two of the currently popular semantic frameworks: Abstract Meaning Representation (AMR)a more abstract framework, and Universal Conceptual Cognitive Annotation (UCCA)-an anchored framework. We use a corpus-based approach to build two graph rewriting systems, a deterministic and a non-deterministic one, from the former to the latter framework. We present their evaluation and a number of ambiguities that we discovered while building our rules. Finally, we provide a discussion and some future work directions in relation to comparing semantic frameworks of different flavors.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1119863494,
        "newsscientist":0.1305212226,
        "technologyreview":0.2115551412,
        "venturebeat":0.2088345577,
        "wired":0.1665177873,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12174v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1658754814000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.07885v2",
        "predicted_newsworthiness":0.3858041169,
        "title":"Clover: Towards A Unified Video-Language Alignment and Fusion Model",
        "summary":"Building a universal video-language model for solving various video understanding tasks (e.g., text-video retrieval, video question answering) is an open challenge to the machine learning field. Towards this goal, most recent attempts train the models, usually consisting of uni-modal and cross-modal feature encoders, with supervised or pair-wise contrastive pre-text tasks. Though offering attractive generality, the resulted models have to compromise between efficiency and performance. We argue the flaws are caused by their pre-training strategies\\textemdash they cannot well align and fuse features from different modalities simultaneously. We then introduce Clover -- a Correlated Video-Language pre-training method -- towards a universal video-language model for solving multiple video understanding tasks with neither performance nor efficiency compromise. It improves cross-modal feature alignment and fusion via a novel tri-modal alignment pre-training task. Additionally, we propose to enhance the tri-modal alignment via incorporating learning from masked samples and a novel pair-wise ranking loss. It establishes new state-of-the-arts on multiple downstream tasks, including three retrieval tasks for both zero-shot and fine-tuning settings, and eight video question answering tasks. Codes and pre-trained models will be released at https:\/\/github.com\/LeeYN-43\/Clover.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0916329214,
        "newsscientist":0.1191784072,
        "technologyreview":0.217617109,
        "venturebeat":0.2093044841,
        "wired":0.1722509838,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.07885v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1657964332000,
        "published_hr":"Jul 16, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11814v1",
        "predicted_newsworthiness":0.3857302756,
        "title":"Object State Change Classification in Egocentric Videos using the Divided Space-Time Attention Mechanism",
        "summary":"This report describes our submission called \"TarHeels\" for the Ego4D: Object State Change Classification Challenge. We use a transformer-based video recognition model and leverage the Divided Space-Time Attention mechanism for classifying object state change in egocentric videos. Our submission achieves the second-best performance in the challenge. Furthermore, we perform an ablation study to show that identifying object state change in egocentric videos requires temporal modeling ability. Lastly, we present several positive and negative examples to visualize our model's predictions. The code is publicly available at: https:\/\/github.com\/md-mohaiminul\/ObjectStateChange",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1038475926,
        "newsscientist":0.1724982287,
        "technologyreview":0.2072773897,
        "venturebeat":0.2058322525,
        "wired":0.1917751655,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11814v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658696016000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13226v1",
        "predicted_newsworthiness":0.385511788,
        "title":"Point-McBert: A Multi-choice Self-supervised Framework for Point Cloud Pre-training",
        "summary":"Masked language modeling (MLM) has become one of the most successful self-supervised pre-training task. Inspired by its success, Point-Bert, as a pioneer work in point cloud, proposed masked point modeling (MPM) to pre-train point transformer on large scale unanotated dataset. Despite its great performance, we find inherent difference between language and point cloud tends to cause ambiguous tokenization for point cloud. For point cloud, there doesn't exist a gold standard for point cloud tokenization. Although Point-Bert introduce a discrete Variational AutoEncoder (dVAE) as tokenizer to allocate token ids to local patches, it tends to generate ambigious token ids for local patches. We find this imperfect tokenizer might generate different token ids for semantically-similar patches and same token ids for semantically-dissimilar patches. To tackle above problem, we propose our Point-McBert, a pre-training framework with eased and refined supervision signals. Specifically, we ease the previous single-choice constraint on patches, and provide multi-choice token ids for each patch as supervision. Moreover, we utilitze the high-level semantics learned by transformer to further refine our supervision signals. Extensive experiments on point cloud classification, few-shot classification and part segmentation tasks demonstrate the superiority of our method, e.g., the pre-trained transformer achieves 94.1% accuracy on ModelNet40, 84.28% accuracy on the hardest setting of ScanObjectNN and new state-of-the-art performance on few-shot learning. We also demonstrate that our method not only improves the performance of Point-Bert on all downstream tasks, but also incurs almost no extra computational overhead.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1011269792,
        "newsscientist":0.1200904579,
        "technologyreview":0.2128345137,
        "venturebeat":0.2090996088,
        "wired":0.1555530103,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13226v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658882073000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00160v1",
        "predicted_newsworthiness":0.3853362055,
        "title":"Learning Feature Decomposition for Domain Adaptive Monocular Depth Estimation",
        "summary":"Monocular depth estimation (MDE) has attracted intense study due to its low cost and critical functions for robotic tasks such as localization, mapping and obstacle detection. Supervised approaches have led to great success with the advance of deep learning, but they rely on large quantities of ground-truth depth annotations that are expensive to acquire. Unsupervised domain adaptation (UDA) transfers knowledge from labeled source data to unlabeled target data, so as to relax the constraint of supervised learning. However, existing UDA approaches may not completely align the domain gap across different datasets because of the domain shift problem. We believe better domain alignment can be achieved via well-designed feature decomposition. In this paper, we propose a novel UDA method for MDE, referred to as Learning Feature Decomposition for Adaptation (LFDA), which learns to decompose the feature space into content and style components. LFDA only attempts to align the content component since it has a smaller domain gap. Meanwhile, it excludes the style component which is specific to the source domain from training the primary task. Furthermore, LFDA uses separate feature distribution estimations to further bridge the domain gap. Extensive experiments on three domain adaptative MDE scenarios show that the proposed method achieves superior accuracy and lower computational cost compared to the state-of-the-art approaches.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.062850411,
        "newsscientist":0.1114264684,
        "technologyreview":0.1804469034,
        "venturebeat":0.1862906315,
        "wired":0.1502713645,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00160v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659168335000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01837v1",
        "predicted_newsworthiness":0.3851682313,
        "title":"Learning Prior Feature and Attention Enhanced Image Inpainting",
        "summary":"Many recent inpainting works have achieved impressive results by leveraging Deep Neural Networks (DNNs) to model various prior information for image restoration. Unfortunately, the performance of these methods is largely limited by the representation ability of vanilla Convolutional Neural Networks (CNNs) backbones.On the other hand, Vision Transformers (ViT) with self-supervised pre-training have shown great potential for many visual recognition and object detection tasks. A natural question is whether the inpainting task can be greatly benefited from the ViT backbone? However, it is nontrivial to directly replace the new backbones in inpainting networks, as the inpainting is an inverse problem fundamentally different from the recognition tasks. To this end, this paper incorporates the pre-training based Masked AutoEncoder (MAE) into the inpainting model, which enjoys richer informative priors to enhance the inpainting process. Moreover, we propose to use attention priors from MAE to make the inpainting model learn more long-distance dependencies between masked and unmasked regions. Sufficient ablations have been discussed about the inpainting and the self-supervised pre-training models in this paper. Besides, experiments on both Places2 and FFHQ demonstrate the effectiveness of our proposed model. Codes and pre-trained models are released in https:\/\/github.com\/ewrfcas\/MAE-FAR.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0830885656,
        "newsscientist":0.1149590214,
        "technologyreview":0.2030327016,
        "venturebeat":0.1640828768,
        "wired":0.1341667487,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01837v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659501173000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11673v1",
        "predicted_newsworthiness":0.3850688938,
        "title":"AutoWeird: Weird Translational Scoring Function Identified by Random Search",
        "summary":"Scoring function (SF) measures the plausibility of triplets in knowledge graphs. Different scoring functions can lead to huge differences in link prediction performances on different knowledge graphs. In this report, we describe a weird scoring function found by random search on the open graph benchmark (OGB). This scoring function, called AutoWeird, only uses tail entity and relation in a triplet to compute its plausibility score. Experimental results show that AutoWeird achieves top-1 performance on ogbl-wikikg2 data set, but has much worse performance than other methods on ogbl-biokg data set. By analyzing the tail entity distribution and evaluation protocol of these two data sets, we attribute the unexpected success of AutoWeird on ogbl-wikikg2 to inappropriate evaluation and concentrated tail entity distribution. Such results may motivate further research on how to accurately evaluate the performance of different link prediction methods for knowledge graphs.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1075752728,
        "newsscientist":0.1633978153,
        "technologyreview":0.2209940443,
        "venturebeat":0.2186653575,
        "wired":0.1665421797,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11673v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.cl"
        ],
        "published":1658644986000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2208.02005v1",
        "predicted_newsworthiness":0.3849112818,
        "title":"Gradient-based Uncertainty for Monocular Depth Estimation",
        "summary":"In monocular depth estimation, disturbances in the image context, like moving objects or reflecting materials, can easily lead to erroneous predictions. For that reason, uncertainty estimates for each pixel are necessary, in particular for safety-critical applications such as automated driving. We propose a post hoc uncertainty estimation approach for an already trained and thus fixed depth estimation model, represented by a deep neural network. The uncertainty is estimated with the gradients which are extracted with an auxiliary loss function. To avoid relying on ground-truth information for the loss definition, we present an auxiliary loss function based on the correspondence of the depth prediction for an image and its horizontally flipped counterpart. Our approach achieves state-of-the-art uncertainty estimation results on the KITTI and NYU Depth V2 benchmarks without the need to retrain the neural network. Models and code are publicly available at https:\/\/github.com\/jhornauer\/GrUMoDepth.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0815080513,
        "newsscientist":0.1209017819,
        "technologyreview":0.1974544424,
        "venturebeat":0.2009589062,
        "wired":0.1597865117,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02005v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659529262000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11876v1",
        "predicted_newsworthiness":0.3844974324,
        "title":"nLMVS-Net: Deep Non-Lambertian Multi-View Stereo",
        "summary":"We introduce a novel multi-view stereo (MVS) method that can simultaneously recover not just per-pixel depth but also surface normals, together with the reflectance of textureless, complex non-Lambertian surfaces captured under known but natural illumination. Our key idea is to formulate MVS as an end-to-end learnable network, which we refer to as nLMVS-Net, that seamlessly integrates radiometric cues to leverage surface normals as view-independent surface features for learned cost volume construction and filtering. It first estimates surface normals as pixel-wise probability densities for each view with a novel shape-from-shading network. These per-pixel surface normal densities and the input multi-view images are then input to a novel cost volume filtering network that learns to recover per-pixel depth and surface normal. The reflectance is also explicitly estimated by alternating with geometry reconstruction. Extensive quantitative evaluations on newly established synthetic and real-world datasets show that nLMVS-Net can robustly and accurately recover the shape and reflectance of complex objects in natural settings.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0746641604,
        "newsscientist":0.1203411698,
        "technologyreview":0.1731393127,
        "venturebeat":0.1518763123,
        "wired":0.1390783173,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11876v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658715621000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11598v1",
        "predicted_newsworthiness":0.3844054575,
        "title":"Generative Artisan: A Semantic-Aware and Controllable CLIPstyler",
        "summary":"Recall that most of the current image style transfer methods require the user to give an image of a particular style and then extract that styling feature and texture to generate the style of an image, but there are still some problems: the user may not have a reference style image, or it may be difficult to summarise the desired style in mind with just one image. The recently proposed CLIPstyler has solved this problem, which is able to perform style transfer based only on the provided description of the style image. Although CLIPstyler can achieve good performance when landscapes or portraits appear alone, it can blur the people and lose the original semantics when people and landscapes coexist. Based on these issues, we demonstrate a novel framework that uses a pre-trained CLIP text-image embedding model and guides image style transfer through an FCN semantic segmentation network. Specifically, we solve the portrait over-styling problem for both selfies and real-world landscape with human subjects photos, enhance the contrast between the effect of style transfer in portrait and landscape, and make the degree of image style transfer in different semantic parts fully controllable. Our Generative Artisan resolve the failure case of CLIPstyler and yield both qualitative and quantitative methods to prove ours have much better results than CLIPstyler in both selfies and real-world landscape with human subjects photos. This improvement makes it possible to commercialize our framework for business scenarios such as retouching graphics software.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1020407187,
        "newsscientist":0.13461361,
        "technologyreview":0.210703758,
        "venturebeat":0.2097889767,
        "wired":0.1947934901,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11598v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658608007000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01014v1",
        "predicted_newsworthiness":0.3841043785,
        "title":"Robust Change Detection Based on Neural Descriptor Fields",
        "summary":"The ability to reason about changes in the environment is crucial for robots operating over extended periods of time. Agents are expected to capture changes during operation so that actions can be followed to ensure a smooth progression of the working session. However, varying viewing angles and accumulated localization errors make it easy for robots to falsely detect changes in the surrounding world due to low observation overlap and drifted object associations. In this paper, based on the recently proposed category-level Neural Descriptor Fields (NDFs), we develop an object-level online change detection approach that is robust to partially overlapping observations and noisy localization results. Utilizing the shape completion capability and SE(3)-equivariance of NDFs, we represent objects with compact shape codes encoding full object shapes from partial observations. The objects are then organized in a spatial tree structure based on object centers recovered from NDFs for fast queries of object neighborhoods. By associating objects via shape code similarity and comparing local object-neighbor spatial layout, our proposed approach demonstrates robustness to low observation overlap and localization noises. We conduct experiments on both synthetic and real-world sequences and achieve improved change detection results compared to multiple baseline methods. Project webpage: https:\/\/yilundu.github.io\/ndf_change",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1077637191,
        "newsscientist":0.1732898602,
        "technologyreview":0.246261618,
        "venturebeat":0.2138423404,
        "wired":0.1903704342,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01014v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv",
            "cs.lg"
        ],
        "published":1659375936000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13274v1",
        "predicted_newsworthiness":0.383867627,
        "title":"Learning from Positive and Unlabeled Data with Augmented Classes",
        "summary":"Positive Unlabeled (PU) learning aims to learn a binary classifier from only positive and unlabeled data, which is utilized in many real-world scenarios. However, existing PU learning algorithms cannot deal with the real-world challenge in an open and changing scenario, where examples from unobserved augmented classes may emerge in the testing phase. In this paper, we propose an unbiased risk estimator for PU learning with Augmented Classes (PUAC) by utilizing unlabeled data from the augmented classes distribution, which can be easily collected in many real-world scenarios. Besides, we derive the estimation error bound for the proposed estimator, which provides a theoretical guarantee for its convergence to the optimal solution. Experiments on multiple realistic datasets demonstrate the effectiveness of proposed approach.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.101839574,
        "newsscientist":0.1337811637,
        "technologyreview":0.2090977882,
        "venturebeat":0.2021286767,
        "wired":0.1409596714,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13274v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658893250000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00183v1",
        "predicted_newsworthiness":0.3836812038,
        "title":"Few-shot Single-view 3D Reconstruction with Memory Prior Contrastive Network",
        "summary":"3D reconstruction of novel categories based on few-shot learning is appealing in real-world applications and attracts increasing research interests. Previous approaches mainly focus on how to design shape prior models for different categories. Their performance on unseen categories is not very competitive. In this paper, we present a Memory Prior Contrastive Network (MPCN) that can store shape prior knowledge in a few-shot learning based 3D reconstruction framework. With the shape memory, a multi-head attention module is proposed to capture different parts of a candidate shape prior and fuse these parts together to guide 3D reconstruction of novel categories. Besides, we introduce a 3D-aware contrastive learning method, which can not only complement the retrieval accuracy of memory network, but also better organize image features for downstream tasks. Compared with previous few-shot 3D reconstruction methods, MPCN can handle the inter-class variability without category annotations. Experimental results on a benchmark synthetic dataset and the Pascal3D+ real-world dataset show that our model outperforms the current state-of-the-art methods significantly.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0752814842,
        "newsscientist":0.1260112545,
        "technologyreview":0.1775329281,
        "venturebeat":0.1550880518,
        "wired":0.1312620642,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00183v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659178179000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13979v2",
        "predicted_newsworthiness":0.3835829317,
        "title":"Knowing Where and What: Unified Word Block Pretraining for Document Understanding",
        "summary":"Due to the complex layouts of documents, it is challenging to extract information for documents. Most previous studies develop multimodal pre-trained models in a self-supervised way. In this paper, we focus on the embedding learning of word blocks containing text and layout information, and propose UTel, a language model with Unified TExt and Layout pre-training. Specifically, we propose two pre-training tasks: Surrounding Word Prediction (SWP) for the layout learning, and Contrastive learning of Word Embeddings (CWE) for identifying different word blocks. Moreover, we replace the commonly used 1D position embedding with a 1D clipped relative position embedding. In this way, the joint training of Masked Layout-Language Modeling (MLLM) and two newly proposed tasks enables the interaction between semantic and spatial features in a unified way. Additionally, the proposed UTel can process arbitrary-length sequences by removing the 1D position embedding, while maintaining competitive performance. Extensive experimental results show UTel learns better joint representations and achieves superior performance than previous methods on various downstream tasks, though requiring no image modality. Code is available at \\url{https:\/\/github.com\/taosong2019\/UTel}.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.098255589,
        "newsscientist":0.1212948743,
        "technologyreview":0.1962769824,
        "venturebeat":0.1939169591,
        "wired":0.1593912022,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13979v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1659001386000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11795v1",
        "predicted_newsworthiness":0.38341017,
        "title":"Cross-Modal 3D Shape Generation and Manipulation",
        "summary":"Creating and editing the shape and color of 3D objects require tremendous human effort and expertise. Compared to direct manipulation in 3D interfaces, 2D interactions such as sketches and scribbles are usually much more natural and intuitive for the users. In this paper, we propose a generic multi-modal generative model that couples the 2D modalities and implicit 3D representations through shared latent spaces. With the proposed model, versatile 3D generation and manipulation are enabled by simply propagating the editing from a specific 2D controlling modality through the latent spaces. For example, editing the 3D shape by drawing a sketch, re-colorizing the 3D surface via painting color scribbles on the 2D rendering, or generating 3D shapes of a certain category given one or a few reference images. Unlike prior works, our model does not require re-training or fine-tuning per editing task and is also conceptually simple, easy to implement, robust to input domain shifts, and flexible to diverse reconstruction on partial 2D inputs. We evaluate our framework on two representative 2D modalities of grayscale line sketches and rendered color images, and demonstrate that our method enables various shape manipulation and generation tasks with these 2D modalities.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0798349426,
        "newsscientist":0.1268175224,
        "technologyreview":0.1889064687,
        "venturebeat":0.1933062123,
        "wired":0.16515636,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11795v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658690577000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00346v1",
        "predicted_newsworthiness":0.3831804926,
        "title":"Improving Distantly Supervised Relation Extraction by Natural Language Inference",
        "summary":"To reduce human annotations for relation extraction (RE) tasks, distantly supervised approaches have been proposed, while struggling with low performance. In this work, we propose a novel DSRE-NLI framework, which considers both distant supervision from existing knowledge bases and indirect supervision from pretrained language models for other tasks. DSRE-NLI energizes an off-the-shelf natural language inference (NLI) engine with a semi-automatic relation verbalization (SARV) mechanism to provide indirect supervision and further consolidates the distant annotations to benefit multi-classification RE models. The NLI-based indirect supervision acquires only one relation verbalization template from humans as a semantically general template for each relationship, and then the template set is enriched by high-quality textual patterns automatically mined from the distantly annotated corpus. With two simple and effective data consolidation strategies, the quality of training data is substantially improved. Extensive experiments demonstrate that the proposed framework significantly improves the SOTA performance (up to 7.73\\% of F1) on distantly supervised RE benchmark datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0892676271,
        "newsscientist":0.1001517461,
        "technologyreview":0.178219227,
        "venturebeat":0.1756708469,
        "wired":0.1221977015,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00346v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1659235714000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00524v1",
        "predicted_newsworthiness":0.3829924461,
        "title":"CloudAttention: Efficient Multi-Scale Attention Scheme For 3D Point Cloud Learning",
        "summary":"Processing 3D data efficiently has always been a challenge. Spatial operations on large-scale point clouds, stored as sparse data, require extra cost. Attracted by the success of transformers, researchers are using multi-head attention for vision tasks. However, attention calculations in transformers come with quadratic complexity in the number of inputs and miss spatial intuition on sets like point clouds. We redesign set transformers in this work and incorporate them into a hierarchical framework for shape classification and part and scene segmentation. We propose our local attention unit, which captures features in a spatial neighborhood. We also compute efficient and dynamic global cross attentions by leveraging sampling and grouping at each iteration. Finally, to mitigate the non-heterogeneity of point clouds, we propose an efficient Multi-Scale Tokenization (MST), which extracts scale-invariant tokens for attention operations. The proposed hierarchical model achieves state-of-the-art shape classification in mean accuracy and yields results on par with the previous segmentation methods while requiring significantly fewer computations. Our proposed architecture predicts segmentation labels with around half the latency and parameter count of the previous most efficient method with comparable performance. The code is available at https:\/\/github.com\/YigeWang-WHU\/CloudAttention.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1016419738,
        "newsscientist":0.1483697346,
        "technologyreview":0.2103417867,
        "venturebeat":0.2150158546,
        "wired":0.1784443804,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00524v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659303555000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13813v1",
        "predicted_newsworthiness":0.3827686323,
        "title":"Structural Similarity for Improved Transfer in Reinforcement Learning",
        "summary":"Transfer learning is an increasingly common approach for developing performant RL agents. However, it is not well understood how to define the relationship between the source and target tasks, and how this relationship contributes to successful transfer. We present an algorithm called Structural Similarity for Two MDPS, or SS2, that calculates a state similarity measure for states in two finite MDPs based on previously developed bisimulation metrics, and show that the measure satisfies properties of a distance metric. Then, through empirical results with GridWorld navigation tasks, we provide evidence that the distance measure can be used to improve transfer performance for Q-Learning agents over previous implementations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0917732579,
        "newsscientist":0.1426323481,
        "technologyreview":0.2247454047,
        "venturebeat":0.1949032946,
        "wired":0.1541080065,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13813v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658960498000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13259v1",
        "predicted_newsworthiness":0.3823073255,
        "title":"Spatiotemporal Self-attention Modeling with Temporal Patch Shift for Action Recognition",
        "summary":"Transformer-based methods have recently achieved great advancement on 2D image-based vision tasks. For 3D video-based tasks such as action recognition, however, directly applying spatiotemporal transformers on video data will bring heavy computation and memory burdens due to the largely increased number of patches and the quadratic complexity of self-attention computation. How to efficiently and effectively model the 3D self-attention of video data has been a great challenge for transformers. In this paper, we propose a Temporal Patch Shift (TPS) method for efficient 3D self-attention modeling in transformers for video-based action recognition. TPS shifts part of patches with a specific mosaic pattern in the temporal dimension, thus converting a vanilla spatial self-attention operation to a spatiotemporal one with little additional cost. As a result, we can compute 3D self-attention using nearly the same computation and memory cost as 2D self-attention. TPS is a plug-and-play module and can be inserted into existing 2D transformer models to enhance spatiotemporal feature learning. The proposed method achieves competitive performance with state-of-the-arts on Something-something V1 & V2, Diving-48, and Kinetics400 while being much more efficient on computation and memory cost. The source code of TPS can be found at https:\/\/github.com\/MartinXM\/TPS.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0862141235,
        "newsscientist":0.1284414459,
        "technologyreview":0.1782871747,
        "venturebeat":0.180195603,
        "wired":0.1613907835,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13259v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1658890027000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12032v1",
        "predicted_newsworthiness":0.3823021468,
        "title":"Cost Volume Pyramid Network with Multi-strategies Range Searching for Multi-view Stereo",
        "summary":"Multi-view stereo is an important research task in computer vision while still keeping challenging. In recent years, deep learning-based methods have shown superior performance on this task. Cost volume pyramid network-based methods which progressively refine depth map in coarse-to-fine manner, have yielded promising results while consuming less memory. However, these methods fail to take fully consideration of the characteristics of the cost volumes in each stage, leading to adopt similar range search strategies for each cost volume stage. In this work, we present a novel cost volume pyramid based network with different searching strategies for multi-view stereo. By choosing different depth range sampling strategies and applying adaptive unimodal filtering, we are able to obtain more accurate depth estimation in low resolution stages and iteratively upsample depth map to arbitrary resolution. We conducted extensive experiments on both DTU and BlendedMVS datasets, and results show that our method outperforms most state-of-the-art methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0651280022,
        "newsscientist":0.1093000622,
        "technologyreview":0.1661550131,
        "venturebeat":0.1682766123,
        "wired":0.1366091741,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12032v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658744093000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13367v1",
        "predicted_newsworthiness":0.3815927674,
        "title":"Optimizing transformations for contrastive learning in a differentiable framework",
        "summary":"Current contrastive learning methods use random transformations sampled from a large list of transformations, with fixed hyperparameters, to learn invariance from an unannotated database. Following previous works that introduce a small amount of supervision, we propose a framework to find optimal transformations for contrastive learning using a differentiable transformation network. Our method increases performances at low annotated data regime both in supervision accuracy and in convergence speed. In contrast to previous work, no generative model is needed for transformation optimization. Transformed images keep relevant information to solve the supervised task, here classification. Experiments were performed on 34000 2D slices of brain Magnetic Resonance Images and 11200 chest X-ray images. On both datasets, with 10% of labeled data, our model achieves better performances than a fully supervised model with 100% labels.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0885047467,
        "newsscientist":0.1300900287,
        "technologyreview":0.2118658847,
        "venturebeat":0.1732119721,
        "wired":0.1280558205,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13367v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1658911677000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11518v1",
        "predicted_newsworthiness":0.3815045992,
        "title":"Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition",
        "summary":"The teacher-free online Knowledge Distillation (KD) aims to train an ensemble of multiple student models collaboratively and distill knowledge from each other. Although existing online KD methods achieve desirable performance, they often focus on class probabilities as the core knowledge type, ignoring the valuable feature representational information. We present a Mutual Contrastive Learning (MCL) framework for online KD. The core idea of MCL is to perform mutual interaction and transfer of contrastive distributions among a cohort of networks in an online manner. Our MCL can aggregate cross-network embedding information and maximize the lower bound to the mutual information between two networks. This enables each network to learn extra contrastive knowledge from others, leading to better feature representations, thus improving the performance of visual recognition tasks. Beyond the final layer, we extend MCL to several intermediate layers assisted by auxiliary feature refinement modules. This further enhances the ability of representation learning for online KD. Experiments on image classification and transfer learning to visual recognition tasks show that MCL can lead to consistent performance gains against state-of-the-art online KD approaches. The superiority demonstrates that MCL can guide the network to generate better feature representations. Our code is publicly available at https:\/\/github.com\/winycg\/MCL.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0897282904,
        "newsscientist":0.1322451986,
        "technologyreview":0.231813282,
        "venturebeat":0.1982691071,
        "wired":0.1481873419,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11518v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658583541000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12964v1",
        "predicted_newsworthiness":0.3813433707,
        "title":"Incremental Few-Shot Semantic Segmentation via Embedding Adaptive-Update and Hyper-class Representation",
        "summary":"Incremental few-shot semantic segmentation (IFSS) targets at incrementally expanding model's capacity to segment new class of images supervised by only a few samples. However, features learned on old classes could significantly drift, causing catastrophic forgetting. Moreover, few samples for pixel-level segmentation on new classes lead to notorious overfitting issues in each learning session. In this paper, we explicitly represent class-based knowledge for semantic segmentation as a category embedding and a hyper-class embedding, where the former describes exclusive semantical properties, and the latter expresses hyper-class knowledge as class-shared semantic properties. Aiming to solve IFSS problems, we present EHNet, i.e., Embedding adaptive-update and Hyper-class representation Network from two aspects. First, we propose an embedding adaptive-update strategy to avoid feature drift, which maintains old knowledge by hyper-class representation, and adaptively update category embeddings with a class-attention scheme to involve new classes learned in individual sessions. Second, to resist overfitting issues caused by few training samples, a hyper-class embedding is learned by clustering all category embeddings for initialization and aligned with category embedding of the new class for enhancement, where learned knowledge assists to learn new knowledge, thus alleviating performance dependence on training data scale. Significantly, these two designs provide representation capability for classes with sufficient semantics and limited biases, enabling to perform segmentation tasks requiring high semantic dependence. Experiments on PASCAL-5i and COCO datasets show that EHNet achieves new state-of-the-art performance with remarkable advantages.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0745747903,
        "newsscientist":0.1129135545,
        "technologyreview":0.1874372521,
        "venturebeat":0.162795977,
        "wired":0.1247734842,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12964v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658848807000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12256v1",
        "predicted_newsworthiness":0.3809996764,
        "title":"Lifelong Machine Learning of Functionally Compositional Structures",
        "summary":"A hallmark of human intelligence is the ability to construct self-contained chunks of knowledge and reuse them in novel combinations for solving different problems. Learning such compositional structures has been a challenge for artificial systems, due to the underlying combinatorial search. To date, research into compositional learning has largely proceeded separately from work on lifelong or continual learning. This dissertation integrated these two lines of work to present a general-purpose framework for lifelong learning of functionally compositional structures. The framework separates the learning into two stages: learning how to combine existing components to assimilate a novel problem, and learning how to adapt the existing components to accommodate the new problem. This separation explicitly handles the trade-off between stability and flexibility. This dissertation instantiated the framework into various supervised and reinforcement learning (RL) algorithms. Supervised learning evaluations found that 1) compositional models improve lifelong learning of diverse tasks, 2) the multi-stage process permits lifelong learning of compositional knowledge, and 3) the components learned by the framework represent self-contained and reusable functions. Similar RL evaluations demonstrated that 1) algorithms under the framework accelerate the discovery of high-performing policies, and 2) these algorithms retain or improve performance on previously learned tasks. The dissertation extended one lifelong compositional RL algorithm to the nonstationary setting, where the task distribution varies over time, and found that modularity permits individually tracking changes to different elements in the environment. The final contribution of this dissertation was a new benchmark for compositional RL, which exposed that existing methods struggle to discover the compositional properties of the environment.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1121128189,
        "newsscientist":0.1702559407,
        "technologyreview":0.2794935101,
        "venturebeat":0.249988696,
        "wired":0.1853925832,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12256v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658762665000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14012v1",
        "predicted_newsworthiness":0.3808564465,
        "title":"Video Mask Transfiner for High-Quality Video Instance Segmentation",
        "summary":"While Video Instance Segmentation (VIS) has seen rapid progress, current approaches struggle to predict high-quality masks with accurate boundary details. Moreover, the predicted segmentations often fluctuate over time, suggesting that temporal consistency cues are neglected or not fully utilized. In this paper, we set out to tackle these issues, with the aim of achieving highly detailed and more temporally stable mask predictions for VIS. We first propose the Video Mask Transfiner (VMT) method, capable of leveraging fine-grained high-resolution features thanks to a highly efficient video transformer structure. Our VMT detects and groups sparse error-prone spatio-temporal regions of each tracklet in the video segment, which are then refined using both local and instance-level cues. Second, we identify that the coarse boundary annotations of the popular YouTube-VIS dataset constitute a major limiting factor. Based on our VMT architecture, we therefore design an automated annotation refinement approach by iterative training and self-correction. To benchmark high-quality mask predictions for VIS, we introduce the HQ-YTVIS dataset, consisting of a manually re-annotated test set and our automatically refined training data. We compare VMT with the most recent state-of-the-art methods on the HQ-YTVIS, as well as the Youtube-VIS, OVIS and BDD100K MOTS benchmarks. Experimental results clearly demonstrate the efficacy and effectiveness of our method on segmenting complex and dynamic objects, by capturing precise details.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0790400095,
        "newsscientist":0.1192163372,
        "technologyreview":0.1717226256,
        "venturebeat":0.1699168272,
        "wired":0.1582961133,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14012v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659006817000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01924v1",
        "predicted_newsworthiness":0.3808066748,
        "title":"Per-Clip Video Object Segmentation",
        "summary":"Recently, memory-based approaches show promising results on semi-supervised video object segmentation. These methods predict object masks frame-by-frame with the help of frequently updated memory of the previous mask. Different from this per-frame inference, we investigate an alternative perspective by treating video object segmentation as clip-wise mask propagation. In this per-clip inference scheme, we update the memory with an interval and simultaneously process a set of consecutive frames (i.e. clip) between the memory updates. The scheme provides two potential benefits: accuracy gain by clip-level optimization and efficiency gain by parallel computation of multiple frames. To this end, we propose a new method tailored for the per-clip inference. Specifically, we first introduce a clip-wise operation to refine the features based on intra-clip correlation. In addition, we employ a progressive matching mechanism for efficient information-passing within a clip. With the synergy of two modules and a newly proposed per-clip based training, our network achieves state-of-the-art performance on Youtube-VOS 2018\/2019 val (84.6% and 84.6%) and DAVIS 2016\/2017 val (91.9% and 86.1%). Furthermore, our model shows a great speed-accuracy trade-off with varying memory update intervals, which leads to huge flexibility.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0830657757,
        "newsscientist":0.1008181478,
        "technologyreview":0.1627405605,
        "venturebeat":0.1580972136,
        "wired":0.1369990372,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01924v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659517349000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12394v1",
        "predicted_newsworthiness":0.3805403846,
        "title":"Dynamic 3D Scene Analysis by Point Cloud Accumulation",
        "summary":"Multi-beam LiDAR sensors, as used on autonomous vehicles and mobile robots, acquire sequences of 3D range scans (\"frames\"). Each frame covers the scene sparsely, due to limited angular scanning resolution and occlusion. The sparsity restricts the performance of downstream processes like semantic segmentation or surface reconstruction. Luckily, when the sensor moves, frames are captured from a sequence of different viewpoints. This provides complementary information and, when accumulated in a common scene coordinate frame, yields a denser sampling and a more complete coverage of the underlying 3D scene. However, often the scanned scenes contain moving objects. Points on those objects are not correctly aligned by just undoing the scanner's ego-motion. In the present paper, we explore multi-frame point cloud accumulation as a mid-level representation of 3D scan sequences, and develop a method that exploits inductive biases of outdoor street scenes, including their geometric layout and object-level rigidity. Compared to state-of-the-art scene flow estimators, our proposed approach aims to align all 3D points in a common reference frame correctly accumulating the points on the individual objects. Our approach greatly reduces the alignment errors on several benchmark datasets. Moreover, the accumulated point clouds benefit high-level tasks like surface reconstruction.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1066181556,
        "newsscientist":0.1496594001,
        "technologyreview":0.2198878012,
        "venturebeat":0.2226904179,
        "wired":0.1950846859,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12394v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658771866000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13061v1",
        "predicted_newsworthiness":0.3802756411,
        "title":"NewsStories: Illustrating articles with visual summaries",
        "summary":"Recent self-supervised approaches have used large-scale image-text datasets to learn powerful representations that transfer to many tasks without finetuning. These methods often assume that there is one-to-one correspondence between its images and their (short) captions. However, many tasks require reasoning about multiple images and long text narratives, such as describing news articles with visual summaries. Thus, we explore a novel setting where the goal is to learn a self-supervised visual-language representation that is robust to varying text length and the number of images. In addition, unlike prior work which assumed captions have a literal relation to the image, we assume images only contain loose illustrative correspondence with the text. To explore this problem, we introduce a large-scale multimodal dataset containing over 31M articles, 22M images and 1M videos. We show that state-of-the-art image-text alignment methods are not robust to longer narratives with multiple images. Finally, we introduce an intuitive baseline that outperforms these methods on zero-shot image-set retrieval by 10% on the GoodNews dataset.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.131590575,
        "newsscientist":0.1341983198,
        "technologyreview":0.2247682752,
        "venturebeat":0.2099151587,
        "wired":0.1953680387,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13061v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.cl"
        ],
        "published":1658856851000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11836v1",
        "predicted_newsworthiness":0.3799752873,
        "title":"Federated Graph Contrastive Learning",
        "summary":"Graph learning models are critical tools for researchers to explore graph-structured data. To train a capable graph learning model, a conventional method uses sufficient training data to train a graph model on a single device. However, it is prohibitive to do so in real-world scenarios due to privacy concerns. Federated learning provides a feasible solution to address such limitations via introducing various privacy-preserving mechanisms, such as differential privacy on graph edges. Nevertheless, differential privacy in federated graph learning secures the classified information maintained in graphs. It degrades the performances of the graph learning models. In this paper, we investigate how to implement differential privacy on graph edges and observe the performances decreasing in the experiments. We also note that the differential privacy on graph edges introduces noises to perturb graph proximity, which is one of the graph augmentations in graph contrastive learning. Inspired by that, we propose to leverage the advantages of graph contrastive learning to alleviate the performance dropping caused by differential privacy. Extensive experiments are conducted with several representative graph models and widely-used datasets, showing that contrastive learning indeed alleviates the models' performance dropping caused by differential privacy.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.127254428,
        "newsscientist":0.1664525303,
        "technologyreview":0.2678796642,
        "venturebeat":0.252117393,
        "wired":0.2146093218,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11836v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658702931000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14696v1",
        "predicted_newsworthiness":0.3792947829,
        "title":"BiFeat: Supercharge GNN Training via Graph Feature Quantization",
        "summary":"Graph Neural Networks (GNNs) is a promising approach for applications with nonEuclidean data. However, training GNNs on large scale graphs with hundreds of millions nodes is both resource and time consuming. Different from DNNs, GNNs usually have larger memory footprints, and thus the GPU memory capacity and PCIe bandwidth are the main resource bottlenecks in GNN training. To address this problem, we present BiFeat: a graph feature quantization methodology to accelerate GNN training by significantly reducing the memory footprint and PCIe bandwidth requirement so that GNNs can take full advantage of GPU computing capabilities. Our key insight is that unlike DNN, GNN is less prone to the information loss of input features caused by quantization. We identify the main accuracy impact factors in graph feature quantization and theoretically prove that BiFeat training converges to a network where the loss is within $\\epsilon$ of the optimal loss of uncompressed network. We perform extensive evaluation of BiFeat using several popular GNN models and datasets, including GraphSAGE on MAG240M, the largest public graph dataset. The results demonstrate that BiFeat achieves a compression ratio of more than 30 and improves GNN training speed by 200%-320% with marginal accuracy loss. In particular, BiFeat achieves a record by training GraphSAGE on MAG240M within one hour using only four GPUs.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.110336079,
        "newsscientist":0.1517415719,
        "technologyreview":0.2527985169,
        "venturebeat":0.2435253273,
        "wired":0.1772724486,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14696v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659103655000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12988v1",
        "predicted_newsworthiness":0.3788750437,
        "title":"Monocular 3D Object Detection with Depth from Motion",
        "summary":"Perceiving 3D objects from monocular inputs is crucial for robotic systems, given its economy compared to multi-sensor settings. It is notably difficult as a single image can not provide any clues for predicting absolute depth values. Motivated by binocular methods for 3D object detection, we take advantage of the strong geometry structure provided by camera ego-motion for accurate object depth estimation and detection. We first make a theoretical analysis on this general two-view case and notice two challenges: 1) Cumulative errors from multiple estimations that make the direct prediction intractable; 2) Inherent dilemmas caused by static cameras and matching ambiguity. Accordingly, we establish the stereo correspondence with a geometry-aware cost volume as the alternative for depth estimation and further compensate it with monocular understanding to address the second problem. Our framework, named Depth from Motion (DfM), then uses the established geometry to lift 2D image features to the 3D space and detects 3D objects thereon. We also present a pose-free DfM to make it usable when the camera pose is unavailable. Our framework outperforms state-of-the-art methods by a large margin on the KITTI benchmark. Detailed quantitative and qualitative analyses also validate our theoretical conclusions. The code will be released at https:\/\/github.com\/Tai-Wang\/Depth-from-Motion.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0746864459,
        "newsscientist":0.1307205103,
        "technologyreview":0.1856287351,
        "venturebeat":0.194650902,
        "wired":0.1620679345,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12988v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ro"
        ],
        "published":1658850526000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11707v1",
        "predicted_newsworthiness":0.3786677123,
        "title":"Improving Test-Time Adaptation via Shift-agnostic Weight Regularization and Nearest Source Prototypes",
        "summary":"This paper proposes a novel test-time adaptation strategy that adjusts the model pre-trained on the source domain using only unlabeled online data from the target domain to alleviate the performance degradation due to the distribution shift between the source and target domains. Adapting the entire model parameters using the unlabeled online data may be detrimental due to the erroneous signals from an unsupervised objective. To mitigate this problem, we propose a shift-agnostic weight regularization that encourages largely updating the model parameters sensitive to distribution shift while slightly updating those insensitive to the shift, during test-time adaptation. This regularization enables the model to quickly adapt to the target domain without performance degradation by utilizing the benefit of a high learning rate. In addition, we present an auxiliary task based on nearest source prototypes to align the source and target features, which helps reduce the distribution shift and leads to further performance improvement. We show that our method exhibits state-of-the-art performance on various standard benchmarks and even outperforms its supervised counterpart.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0737799018,
        "newsscientist":0.1191428456,
        "technologyreview":0.2033025098,
        "venturebeat":0.1948790563,
        "wired":0.1440537555,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11707v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658657825000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12659v1",
        "predicted_newsworthiness":0.3786414938,
        "title":"Graph Neural Network and Spatiotemporal Transformer Attention for 3D Video Object Detection from Point Clouds",
        "summary":"Previous works for LiDAR-based 3D object detection mainly focus on the single-frame paradigm. In this paper, we propose to detect 3D objects by exploiting temporal information in multiple frames, i.e., the point cloud videos. We empirically categorize the temporal information into short-term and long-term patterns. To encode the short-term data, we present a Grid Message Passing Network (GMPNet), which considers each grid (i.e., the grouped points) as a node and constructs a k-NN graph with the neighbor grids. To update features for a grid, GMPNet iteratively collects information from its neighbors, thus mining the motion cues in grids from nearby frames. To further aggregate the long-term frames, we propose an Attentive Spatiotemporal Transformer GRU (AST-GRU), which contains a Spatial Transformer Attention (STA) module and a Temporal Transformer Attention (TTA) module. STA and TTA enhance the vanilla GRU to focus on small objects and better align the moving objects. Our overall framework supports both online and offline video object detection in point clouds. We implement our algorithm based on prevalent anchor-based and anchor-free detectors. The evaluation results on the challenging nuScenes benchmark show the superior performance of our method, achieving the 1st on the leaderboard without any bells and whistles, by the time the paper is submitted.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0993088705,
        "newsscientist":0.148971874,
        "technologyreview":0.2037188895,
        "venturebeat":0.2074880794,
        "wired":0.179475062,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12659v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658812588000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02035v1",
        "predicted_newsworthiness":0.3780631794,
        "title":"Template matching with white balance adjustment under multiple illuminants",
        "summary":"In this paper, we propose a novel template matching method with a white balancing adjustment, called N-white balancing, which was proposed for multi-illuminant scenes. To reduce the influence of lighting effects, N-white balancing is applied to images for multi-illumination color constancy, and then a template matching method is carried out by using adjusted images. In experiments, the effectiveness of the proposed method is demonstrated to be effective in object detection tasks under various illumination conditions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0576375712,
        "newsscientist":0.1089036176,
        "technologyreview":0.1346256487,
        "venturebeat":0.1314417435,
        "wired":0.1032464427,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02035v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659531438000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00767v1",
        "predicted_newsworthiness":0.3777728651,
        "title":"Multimodal Neural Machine Translation with Search Engine Based Image Retrieval",
        "summary":"Recently, numbers of works shows that the performance of neural machine translation (NMT) can be improved to a certain extent with using visual information. However, most of these conclusions are drawn from the analysis of experimental results based on a limited set of bilingual sentence-image pairs, such as Multi30K. In these kinds of datasets, the content of one bilingual parallel sentence pair must be well represented by a manually annotated image, which is different with the actual translation situation. Some previous works are proposed to addressed the problem by retrieving images from exiting sentence-image pairs with topic model. However, because of the limited collection of sentence-image pairs they used, their image retrieval method is difficult to deal with the out-of-vocabulary words, and can hardly prove that visual information enhance NMT rather than the co-occurrence of images and sentences. In this paper, we propose an open-vocabulary image retrieval methods to collect descriptive images for bilingual parallel corpus using image search engine. Next, we propose text-aware attentive visual encoder to filter incorrectly collected noise images. Experiment results on Multi30K and other two translation datasets show that our proposed method achieves significant improvements over strong baselines.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0959208938,
        "newsscientist":0.119307033,
        "technologyreview":0.1938700303,
        "venturebeat":0.1852978067,
        "wired":0.1390362544,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00767v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.cl",
            "cs.ir"
        ],
        "published":1658824926000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00776v1",
        "predicted_newsworthiness":0.3775509634,
        "title":"Deep 360$^\\circ$ Optical Flow Estimation Based on Multi-Projection Fusion",
        "summary":"Optical flow computation is essential in the early stages of the video processing pipeline. This paper focuses on a less explored problem in this area, the 360$^\\circ$ optical flow estimation using deep neural networks to support increasingly popular VR applications. To address the distortions of panoramic representations when applying convolutional neural networks, we propose a novel multi-projection fusion framework that fuses the optical flow predicted by the models trained using different projection methods. It learns to combine the complementary information in the optical flow results under different projections. We also build the first large-scale panoramic optical flow dataset to support the training of neural networks and the evaluation of panoramic optical flow estimation methods. The experimental results on our dataset demonstrate that our method outperforms the existing methods and other alternative deep networks that were developed for processing 360{\\deg} content.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0908582017,
        "newsscientist":0.1513491105,
        "technologyreview":0.2196447126,
        "venturebeat":0.2768733254,
        "wired":0.2080784117,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00776v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658940512000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12485v1",
        "predicted_newsworthiness":0.377471652,
        "title":"3D Shape Sequence of Human Comparison and Classification using Current and Varifolds",
        "summary":"In this paper we address the task of the comparison and the classification of 3D shape sequences of human. The non-linear dynamics of the human motion and the changing of the surface parametrization over the time make this task very challenging. To tackle this issue, we propose to embed the 3D shape sequences in an infinite dimensional space, the space of varifolds, endowed with an inner product that comes from a given positive definite kernel. More specifically, our approach involves two steps: 1) the surfaces are represented as varifolds, this representation induces metrics equivariant to rigid motions and invariant to parametrization; 2) the sequences of 3D shapes are represented by Gram matrices derived from their infinite dimensional Hankel matrices. The problem of comparison of two 3D sequences of human is formulated as a comparison of two Gram-Hankel matrices. Extensive experiments on CVSSP3D and Dyna datasets show that our method is competitive with state-of-the-art in 3D human sequence motion retrieval. Code for the experiments is available at https:\/\/github.com\/CRISTAL-3DSAM\/HumanComparisonVarifolds.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0860870497,
        "newsscientist":0.1409360826,
        "technologyreview":0.1723379718,
        "venturebeat":0.1588332678,
        "wired":0.1410623609,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12485v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658776963000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02245v1",
        "predicted_newsworthiness":0.3773772596,
        "title":"MinVIS: A Minimal Video Instance Segmentation Framework without Video-based Training",
        "summary":"We propose MinVIS, a minimal video instance segmentation (VIS) framework that achieves state-of-the-art VIS performance with neither video-based architectures nor training procedures. By only training a query-based image instance segmentation model, MinVIS outperforms the previous best result on the challenging Occluded VIS dataset by over 10% AP. Since MinVIS treats frames in training videos as independent images, we can drastically sub-sample the annotated frames in training videos without any modifications. With only 1% of labeled frames, MinVIS outperforms or is comparable to fully-supervised state-of-the-art approaches on YouTube-VIS 2019\/2021. Our key observation is that queries trained to be discriminative between intra-frame object instances are temporally consistent and can be used to track instances without any manually designed heuristics. MinVIS thus has the following inference pipeline: we first apply the trained query-based image instance segmentation to video frames independently. The segmented instances are then tracked by bipartite matching of the corresponding queries. This inference is done in an online fashion and does not need to process the whole video at once. MinVIS thus has the practical advantages of reducing both the labeling costs and the memory requirements, while not sacrificing the VIS performance. Code is available at: https:\/\/github.com\/NVlabs\/MinVIS",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0808601459,
        "newsscientist":0.1262410706,
        "technologyreview":0.1837086582,
        "venturebeat":0.1895720268,
        "wired":0.1657778674,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02245v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659549042000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11446v1",
        "predicted_newsworthiness":0.3764288365,
        "title":"BuyTheDips: PathLoss for improved topology-preserving deep learning-based image segmentation",
        "summary":"Capturing the global topology of an image is essential for proposing an accurate segmentation of its domain. However, most of existing segmentation methods do not preserve the initial topology of the given input, which is detrimental for numerous downstream object-based tasks. This is all the more true for deep learning models which most work at local scales. In this paper, we propose a new topology-preserving deep image segmentation method which relies on a new leakage loss: the Pathloss. Our method is an extension of the BALoss [1], in which we want to improve the leakage detection for better recovering the closeness property of the image segmentation. This loss allows us to correctly localize and fix the critical points (a leakage in the boundaries) that could occur in the predictions, and is based on a shortest-path search algorithm. This way, loss minimization enforces connectivity only where it is necessary and finally provides a good localization of the boundaries of the objects in the image. Moreover, according to our research, our Pathloss learns to preserve stronger elongated structure compared to methods without using topology-preserving loss. Training with our topological loss function, our method outperforms state-of-the-art topology-aware methods on two representative datasets of different natures: Electron Microscopy and Historical Map.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0800827952,
        "newsscientist":0.1357652909,
        "technologyreview":0.1929380667,
        "venturebeat":0.1524808917,
        "wired":0.1300523818,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11446v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658560770000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.07813v2",
        "predicted_newsworthiness":0.3761011021,
        "title":"Autonomously Untangling Long Cables",
        "summary":"Cables are ubiquitous in many settings and it is often useful to untangle them. However, cables are prone to self-occlusions and knots, making them difficult to perceive and manipulate. The challenge increases with cable length: long cables require more complex slack management to facilitate observability and reachability. In this paper, we focus on autonomously untangling cables up to 3 meters in length using a bilateral robot. We develop RGBD perception and motion primitives to efficiently untangle long cables and novel gripper jaws specialized for this task. We present Sliding and Grasping for Tangle Manipulation (SGTM), an algorithm that composes these primitives to iteratively untangle cables with success rates of 67% on isolated overhand and figure-eight knots and 50% on more complex configurations. Supplementary material, visualizations, and videos can be found at https:\/\/sites.google.com\/view\/rss-2022-untangling\/home.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0840743059,
        "newsscientist":0.1576606088,
        "technologyreview":0.1915275546,
        "venturebeat":0.1669457693,
        "wired":0.165094955,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.07813v2",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai"
        ],
        "published":1657938909000,
        "published_hr":"Jul 15, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13691v1",
        "predicted_newsworthiness":0.3759884422,
        "title":"ShAPO: Implicit Representations for Multi-Object Shape, Appearance, and Pose Optimization",
        "summary":"Our method studies the complex task of object-centric 3D understanding from a single RGB-D observation. As it is an ill-posed problem, existing methods suffer from low performance for both 3D shape and 6D pose and size estimation in complex multi-object scenarios with occlusions. We present ShAPO, a method for joint multi-object detection, 3D textured reconstruction, 6D object pose and size estimation. Key to ShAPO is a single-shot pipeline to regress shape, appearance and pose latent codes along with the masks of each object instance, which is then further refined in a sparse-to-dense fashion. A novel disentangled shape and appearance database of priors is first learned to embed objects in their respective shape and appearance space. We also propose a novel, octree-based differentiable optimization step, allowing us to further improve object shape, pose and appearance simultaneously under the learned latent space, in an analysis-by-synthesis fashion. Our novel joint implicit textured object representation allows us to accurately identify and reconstruct novel unseen objects without having access to their 3D meshes. Through extensive experiments, we show that our method, trained on simulated indoor scenes, accurately regresses the shape, appearance and pose of novel objects in the real-world with minimal fine-tuning. Our method significantly out-performs all baselines on the NOCS dataset with an 8% absolute improvement in mAP for 6D pose estimation. Project page: https:\/\/zubair-irshad.github.io\/projects\/ShAPO.html",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0907184095,
        "newsscientist":0.1436693862,
        "technologyreview":0.1920177943,
        "venturebeat":0.183656706,
        "wired":0.1531191739,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13691v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg",
            "cs.ro"
        ],
        "published":1658944771000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.10782v2",
        "predicted_newsworthiness":0.3758197326,
        "title":"Learning Deep SDF Maps Online for Robot Navigation and Exploration",
        "summary":"We propose an algorithm to (i) learn online a deep signed distance function (SDF) with a LiDAR-equipped robot to represent the 3D environment geometry, and (ii) plan collision-free trajectories given this deep learned map. Our algorithm takes a stream of incoming LiDAR scans and continually optimizes a neural network to represent the SDF of the environment around its current vicinity. When the SDF network quality saturates, we cache a copy of the network, along with a learned confidence metric, and initialize a new SDF network to continue mapping new regions of the environment. We then concatenate all the cached local SDFs through a confidence-weighted scheme to give a global SDF for planning. For planning, we make use of a sequential convex model predictive control (MPC) algorithm. The MPC planner optimizes a dynamically feasible trajectory for the robot while enforcing no collisions with obstacles mapped in the global SDF. We show that our online mapping algorithm produces higher-quality maps than existing methods for online SDF training. In the WeBots simulator, we further showcase the combined mapper and planner running online -- navigating autonomously and without collisions in an unknown environment.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.088270167,
        "newsscientist":0.1560978088,
        "technologyreview":0.2375007275,
        "venturebeat":0.2090711874,
        "wired":0.1855903955,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.10782v2",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658444631000,
        "published_hr":"Jul 21, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00385v1",
        "predicted_newsworthiness":0.3755974029,
        "title":"Evaluating Table Structure Recognition: A New Perspective",
        "summary":"Existing metrics used to evaluate table structure recognition algorithms have shortcomings with regard to capturing text and empty cells alignment. In this paper, we build on prior work and propose a new metric - TEDS based IOU similarity (TEDS (IOU)) for table structure recognition which uses bounding boxes instead of text while simultaneously being robust against the above disadvantages. We demonstrate the effectiveness of our metric against previous metrics through various examples.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0923827536,
        "newsscientist":0.1220570906,
        "technologyreview":0.1724430952,
        "venturebeat":0.180260283,
        "wired":0.138137049,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00385v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659253716000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02105v1",
        "predicted_newsworthiness":0.3755530782,
        "title":"Edge-Based Self-Supervision for Semi-Supervised Few-Shot Microscopy Image Cell Segmentation",
        "summary":"Deep neural networks currently deliver promising results for microscopy image cell segmentation, but they require large-scale labelled databases, which is a costly and time-consuming process. In this work, we relax the labelling requirement by combining self-supervised with semi-supervised learning. We propose the prediction of edge-based maps for self-supervising the training of the unlabelled images, which is combined with the supervised training of a small number of labelled images for learning the segmentation task. In our experiments, we evaluate on a few-shot microscopy image cell segmentation benchmark and show that only a small number of annotated images, e.g. 10% of the original training set, is enough for our approach to reach similar performance as with the fully annotated databases on 1- to 10-shots. Our code and trained models is made publicly available",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0757842894,
        "newsscientist":0.1476054042,
        "technologyreview":0.2167144187,
        "venturebeat":0.1840557115,
        "wired":0.1463289693,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02105v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659537300000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14284v1",
        "predicted_newsworthiness":0.3754728271,
        "title":"HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions",
        "summary":"Recent progress in vision Transformers exhibits great success in various tasks driven by the new spatial modeling mechanism based on dot-product self-attention. In this paper, we show that the key ingredients behind the vision Transformers, namely input-adaptive, long-range and high-order spatial interactions, can also be efficiently implemented with a convolution-based framework. We present the Recursive Gated Convolution ($\\textit{g}^\\textit{n}$Conv) that performs high-order spatial interactions with gated convolutions and recursive designs. The new operation is highly flexible and customizable, which is compatible with various variants of convolution and extends the two-order interactions in self-attention to arbitrary orders without introducing significant extra computation. $\\textit{g}^\\textit{n}$Conv can serve as a plug-and-play module to improve various vision Transformers and convolution-based models. Based on the operation, we construct a new family of generic vision backbones named HorNet. Extensive experiments on ImageNet classification, COCO object detection and ADE20K semantic segmentation show HorNet outperform Swin Transformers and ConvNeXt by a significant margin with similar overall architecture and training configurations. HorNet also shows favorable scalability to more training data and a larger model size. Apart from the effectiveness in visual encoders, we also show $\\textit{g}^\\textit{n}$Conv can be applied to task-specific decoders and consistently improve dense prediction performance with less computation. Our results demonstrate that $\\textit{g}^\\textit{n}$Conv can be a new basic module for visual modeling that effectively combines the merits of both vision Transformers and CNNs. Code is available at https:\/\/github.com\/raoyongming\/HorNet",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0889555226,
        "newsscientist":0.1450201476,
        "technologyreview":0.2366912935,
        "venturebeat":0.2182464663,
        "wired":0.1752119078,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14284v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659031142000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12020v1",
        "predicted_newsworthiness":0.375442241,
        "title":"Domain-invariant Feature Exploration for Domain Generalization",
        "summary":"Deep learning has achieved great success in the past few years. However, the performance of deep learning is likely to impede in face of non-IID situations. Domain generalization (DG) enables a model to generalize to an unseen test distribution, i.e., to learn domain-invariant representations. In this paper, we argue that domain-invariant features should be originating from both internal and mutual sides. Internal invariance means that the features can be learned with a single domain and the features capture intrinsic semantics of data, i.e., the property within a domain, which is agnostic to other domains. Mutual invariance means that the features can be learned with multiple domains (cross-domain) and the features contain common information, i.e., the transferable features w.r.t. other domains. We then propose DIFEX for Domain-Invariant Feature EXploration. DIFEX employs a knowledge distillation framework to capture the high-level Fourier phase as the internally-invariant features and learn cross-domain correlation alignment as the mutually-invariant features. We further design an exploration loss to increase the feature diversity for better generalization. Extensive experiments on both time-series and visual benchmarks demonstrate that the proposed DIFEX achieves state-of-the-art performance.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0900344864,
        "newsscientist":0.1394548405,
        "technologyreview":0.2410705862,
        "venturebeat":0.2240224299,
        "wired":0.1650530644,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12020v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1658742955000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11984v2",
        "predicted_newsworthiness":0.3743136575,
        "title":"RA-Depth: Resolution Adaptive Self-Supervised Monocular Depth Estimation",
        "summary":"Existing self-supervised monocular depth estimation methods can get rid of expensive annotations and achieve promising results. However, these methods suffer from severe performance degradation when directly adopting a model trained on a fixed resolution to evaluate at other different resolutions. In this paper, we propose a resolution adaptive self-supervised monocular depth estimation method (RA-Depth) by learning the scale invariance of the scene depth. Specifically, we propose a simple yet efficient data augmentation method to generate images with arbitrary scales for the same scene. Then, we develop a dual high-resolution network that uses the multi-path encoder and decoder with dense interactions to aggregate multi-scale features for accurate depth inference. Finally, to explicitly learn the scale invariance of the scene depth, we formulate a cross-scale depth consistency loss on depth predictions with different scales. Extensive experiments on the KITTI, Make3D and NYU-V2 datasets demonstrate that RA-Depth not only achieves state-of-the-art performance, but also exhibits a good ability of resolution adaptation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.063131733,
        "newsscientist":0.1020750562,
        "technologyreview":0.1609406708,
        "venturebeat":0.1858215501,
        "wired":0.1400725759,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11984v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658738999000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01188v1",
        "predicted_newsworthiness":0.3742417616,
        "title":"Curved Geometric Networks for Visual Anomaly Recognition",
        "summary":"Learning a latent embedding to understand the underlying nature of data distribution is often formulated in Euclidean spaces with zero curvature. However, the success of the geometry constraints, posed in the embedding space, indicates that curved spaces might encode more structural information, leading to better discriminative power and hence richer representations. In this work, we investigate benefits of the curved space for analyzing anomalies or out-of-distribution objects in data. This is achieved by considering embeddings via three geometry constraints, namely, spherical geometry (with positive curvature), hyperbolic geometry (with negative curvature) or mixed geometry (with both positive and negative curvatures). Three geometric constraints can be chosen interchangeably in a unified design given the task at hand. Tailored for the embeddings in the curved space, we also formulate functions to compute the anomaly score. Two types of geometric modules (i.e., Geometric-in-One and Geometric-in-Two models) are proposed to plug in the original Euclidean classifier, and anomaly scores are computed from the curved embeddings. We evaluate the resulting designs under a diverse set of visual recognition scenarios, including image detection (multi-class OOD detection and one-class anomaly detection) and segmentation (multi-class anomaly segmentation and one-class anomaly segmentation). The empirical results show the effectiveness of our proposal through the consistent improvement over various scenarios.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1001561685,
        "newsscientist":0.158975243,
        "technologyreview":0.2124631064,
        "venturebeat":0.178486845,
        "wired":0.1695922649,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01188v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659402939000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13268v1",
        "predicted_newsworthiness":0.374075057,
        "title":"End-to-end Graph-constrained Vectorized Floorplan Generation with Panoptic Refinement",
        "summary":"The automatic generation of floorplans given user inputs has great potential in architectural design and has recently been explored in the computer vision community. However, the majority of existing methods synthesize floorplans in the format of rasterized images, which are difficult to edit or customize. In this paper, we aim to synthesize floorplans as sequences of 1-D vectors, which eases user interaction and design customization. To generate high fidelity vectorized floorplans, we propose a novel two-stage framework, including a draft stage and a multi-round refining stage. In the first stage, we encode the room connectivity graph input by users with a graph convolutional network (GCN), then apply an autoregressive transformer network to generate an initial floorplan sequence. To polish the initial design and generate more visually appealing floorplans, we further propose a novel panoptic refinement network(PRN) composed of a GCN and a transformer network. The PRN takes the initial generated sequence as input and refines the floorplan design while encouraging the correct room connectivity with our proposed geometric loss. We have conducted extensive experiments on a real-world floorplan dataset, and the results show that our method achieves state-of-the-art performance under different settings and evaluation metrics.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1200035882,
        "newsscientist":0.1585281935,
        "technologyreview":0.2299576523,
        "venturebeat":0.2330520071,
        "wired":0.2317881473,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13268v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658891960000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12139v1",
        "predicted_newsworthiness":0.3739576605,
        "title":"Improving Pseudo Labels With Intra-Class Similarity for Unsupervised Domain Adaptation",
        "summary":"Unsupervised domain adaptation (UDA) transfers knowledge from a label-rich source domain to a different but related fully-unlabeled target domain. To address the problem of domain shift, more and more UDA methods adopt pseudo labels of the target samples to improve the generalization ability on the target domain. However, inaccurate pseudo labels of the target samples may yield suboptimal performance with error accumulation during the optimization process. Moreover, once the pseudo labels are generated, how to remedy the generated pseudo labels is far from explored. In this paper, we propose a novel approach to improve the accuracy of the pseudo labels in the target domain. It first generates coarse pseudo labels by a conventional UDA method. Then, it iteratively exploits the intra-class similarity of the target samples for improving the generated coarse pseudo labels, and aligns the source and target domains with the improved pseudo labels. The accuracy improvement of the pseudo labels is made by first deleting dissimilar samples, and then using spanning trees to eliminate the samples with the wrong pseudo labels in the intra-class samples. We have applied the proposed approach to several conventional UDA methods as an additional term. Experimental results demonstrate that the proposed method can boost the accuracy of the pseudo labels and further lead to more discriminative and domain invariant features than the conventional baselines.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0582365795,
        "newsscientist":0.0999302219,
        "technologyreview":0.1618344866,
        "venturebeat":0.1482638128,
        "wired":0.0987262776,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12139v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658752944000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14288v1",
        "predicted_newsworthiness":0.3731765692,
        "title":"Rewriting Geometric Rules of a GAN",
        "summary":"Deep generative models make visual content creation more accessible to novice users by automating the synthesis of diverse, realistic content based on a collected dataset. However, the current machine learning approaches miss a key element of the creative process -- the ability to synthesize things that go far beyond the data distribution and everyday experience. To begin to address this issue, we enable a user to \"warp\" a given model by editing just a handful of original model outputs with desired geometric changes. Our method applies a low-rank update to a single model layer to reconstruct edited examples. Furthermore, to combat overfitting, we propose a latent space augmentation method based on style-mixing. Our method allows a user to create a model that synthesizes endless objects with defined geometric changes, enabling the creation of a new generative model without the burden of curating a large-scale dataset. We also demonstrate that edited models can be composed to achieve aggregated effects, and we present an interactive interface to enable users to create new models through composition. Empirical measurements on multiple test cases suggest the advantage of our method against recent GAN fine-tuning methods. Finally, we showcase several applications using the edited models, including latent space interpolation and image editing.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1102420826,
        "newsscientist":0.1698731227,
        "technologyreview":0.251437923,
        "venturebeat":0.2491330615,
        "wired":0.2431168734,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14288v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659031176000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11699v1",
        "predicted_newsworthiness":0.3731678575,
        "title":"Semi-supervised Deep Multi-view Stereo",
        "summary":"Significant progress has been witnessed in learning-based Multi-view Stereo (MVS) of supervised and unsupervised settings. To combine their respective merits in accuracy and completeness, meantime reducing the demand for expensive labeled data, this paper explores a novel semi-supervised setting of learning-based MVS problem that only a tiny part of the MVS data is attached with dense depth ground truth. However, due to huge variation of scenarios and flexible setting in views, semi-supervised MVS problem (Semi-MVS) may break the basic assumption in classic semi-supervised learning, that unlabeled data and labeled data share the same label space and data distribution. To handle these issues, we propose a novel semi-supervised MVS framework, namely SE-MVS. For the simple case that the basic assumption works in MVS data, consistency regularization encourages the model predictions to be consistent between original sample and randomly augmented sample via constraints on KL divergence. For further troublesome case that the basic assumption is conflicted in MVS data, we propose a novel style consistency loss to alleviate the negative effect caused by the distribution gap. The visual style of unlabeled sample is transferred to labeled sample to shrink the gap, and the model prediction of generated sample is further supervised with the label in original labeled sample. The experimental results on DTU, BlendedMVS, GTA-SFM, and Tanks\\&Temples datasets show the superior performance of the proposed method. With the same settings in backbone network, our proposed SE-MVS outperforms its fully-supervised and unsupervised baselines.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0651580199,
        "newsscientist":0.1056885825,
        "technologyreview":0.1678256634,
        "venturebeat":0.1598974092,
        "wired":0.1378708884,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11699v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658655462000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01923v1",
        "predicted_newsworthiness":0.3727120763,
        "title":"Graph Regularized Nonnegative Latent Factor Analysis Model for Temporal Link Prediction in Cryptocurrency Transaction Networks",
        "summary":"With the development of blockchain technology, the cryptocurrency based on blockchain technology is becoming more and more popular. This gave birth to a huge cryptocurrency transaction network has received widespread attention. Link prediction learning structure of network is helpful to understand the mechanism of network, so it is also widely studied in cryptocurrency network. However, the dynamics of cryptocurrency transaction networks have been neglected in the past researches. We use graph regularized method to link past transaction records with future transactions. Based on this, we propose a single latent factor-dependent, non-negative, multiplicative and graph regularized-incorporated update (SLF-NMGRU) algorithm and further propose graph regularized nonnegative latent factor analysis (GrNLFA) model. Finally, experiments on a real cryptocurrency transaction network show that the proposed method improves both the accuracy and the computational efficiency",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1242390754,
        "newsscientist":0.1423343211,
        "technologyreview":0.233735674,
        "venturebeat":0.2226297178,
        "wired":0.1749942519,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01923v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.si"
        ],
        "published":1659517139000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12934v1",
        "predicted_newsworthiness":0.3725344543,
        "title":"A Reliable Online Method for Joint Estimation of Focal Length and Camera Rotation",
        "summary":"Linear perspectivecues deriving from regularities of the built environment can be used to recalibrate both intrinsic and extrinsic camera parameters online, but these estimates can be unreliable due to irregularities in the scene, uncertainties in line segment estimation and background clutter. Here we address this challenge through four initiatives. First, we use the PanoContext panoramic image dataset [27] to curate a novel and realistic dataset of planar projections over a broad range of scenes, focal lengths and camera poses. Second, we use this novel dataset and the YorkUrbanDB [4] to systematically evaluate the linear perspective deviation measures frequently found in the literature and show that the choice of deviation measure and likelihood model has a huge impact on reliability. Third, we use these findings to create a novel system for online camera calibration we call fR, and show that it outperforms the prior state of the art, substantially reducing error in estimated camera rotation and focal length. Our fourth contribution is a novel and efficient approach to estimating uncertainty that can dramatically improve online reliability for performance-critical applications by strategically selecting which frames to use for recalibration.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0902044628,
        "newsscientist":0.1382598329,
        "technologyreview":0.1818748272,
        "venturebeat":0.2385511246,
        "wired":0.2003386118,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12934v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658846382000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01762v1",
        "predicted_newsworthiness":0.3724983862,
        "title":"Robust RGB-D Fusion for Saliency Detection",
        "summary":"Efficiently exploiting multi-modal inputs for accurate RGB-D saliency detection is a topic of high interest. Most existing works leverage cross-modal interactions to fuse the two streams of RGB-D for intermediate features' enhancement. In this process, a practical aspect of the low quality of the available depths has not been fully considered yet. In this work, we aim for RGB-D saliency detection that is robust to the low-quality depths which primarily appear in two forms: inaccuracy due to noise and the misalignment to RGB. To this end, we propose a robust RGB-D fusion method that benefits from (1) layer-wise, and (2) trident spatial, attention mechanisms. On the one hand, layer-wise attention (LWA) learns the trade-off between early and late fusion of RGB and depth features, depending upon the depth accuracy. On the other hand, trident spatial attention (TSA) aggregates the features from a wider spatial context to address the depth misalignment problem. The proposed LWA and TSA mechanisms allow us to efficiently exploit the multi-modal inputs for saliency detection while being robust against low-quality depths. Our experiments on five benchmark datasets demonstrate that the proposed fusion method performs consistently better than the state-of-the-art fusion alternatives.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0708967087,
        "newsscientist":0.1159968205,
        "technologyreview":0.1643931082,
        "venturebeat":0.1790504751,
        "wired":0.1520772415,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01762v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659475380000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00635v1",
        "predicted_newsworthiness":0.3724266663,
        "title":"DictBERT: Dictionary Description Knowledge Enhanced Language Model Pre-training via Contrastive Learning",
        "summary":"Although pre-trained language models (PLMs) have achieved state-of-the-art performance on various natural language processing (NLP) tasks, they are shown to be lacking in knowledge when dealing with knowledge driven tasks. Despite the many efforts made for injecting knowledge into PLMs, this problem remains open. To address the challenge, we propose \\textbf{DictBERT}, a novel approach that enhances PLMs with dictionary knowledge which is easier to acquire than knowledge graph (KG). During pre-training, we present two novel pre-training tasks to inject dictionary knowledge into PLMs via contrastive learning: \\textit{dictionary entry prediction} and \\textit{entry description discrimination}. In fine-tuning, we use the pre-trained DictBERT as a plugin knowledge base (KB) to retrieve implicit knowledge for identified entries in an input sequence, and infuse the retrieved knowledge into the input to enhance its representation via a novel extra-hop attention mechanism. We evaluate our approach on a variety of knowledge driven and language understanding tasks, including NER, relation extraction, CommonsenseQA, OpenBookQA and GLUE. Experimental results demonstrate that our model can significantly improve typical PLMs: it gains a substantial improvement of 0.5\\%, 2.9\\%, 9.0\\%, 7.1\\% and 3.3\\% on BERT-large respectively, and is also effective on RoBERTa-large.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0908686955,
        "newsscientist":0.1170860895,
        "technologyreview":0.2192484432,
        "venturebeat":0.2246554088,
        "wired":0.1650784829,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00635v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659336199000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14660v1",
        "predicted_newsworthiness":0.3723903876,
        "title":"Matching with AffNet based rectifications",
        "summary":"We consider the problem of two-view matching under significant viewpoint changes with view synthesis. We propose two novel methods, minimizing the view synthesis overhead. The first one, named DenseAffNet, uses dense affine shapes estimates from AffNet, which allows it to partition the image, rectifying each partition with just a single affine map. The second one, named DepthAffNet, combines information from depth maps and affine shapes estimates to produce different sets of rectifying affine maps for different image partitions. DenseAffNet is faster than the state-of-the-art and more accurate on generic scenes. DepthAffNet is on par with the state of the art on scenes containing large planes. The evaluation is performed on 3 public datasets - EVD Dataset, Strong ViewPoint Changes Dataset and IMC Phototourism Dataset.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0605206734,
        "newsscientist":0.1039344075,
        "technologyreview":0.1652720987,
        "venturebeat":0.1740647697,
        "wired":0.1469984145,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14660v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659099978000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14772v1",
        "predicted_newsworthiness":0.3721068141,
        "title":"Combining Evolutionary Search with Behaviour Cloning for Procedurally Generated Content",
        "summary":"In this work, we consider the problem of procedural content generation for video game levels. Prior approaches have relied on evolutionary search (ES) methods capable of generating diverse levels, but this generation procedure is slow, which is problematic in real-time settings. Reinforcement learning (RL) has also been proposed to tackle the same problem, and while level generation is fast, training time can be prohibitively expensive. We propose a framework to tackle the procedural content generation problem that combines the best of ES and RL. In particular, our approach first uses ES to generate a sequence of levels evolved over time, and then uses behaviour cloning to distil these levels into a policy, which can then be queried to produce new levels quickly. We apply our approach to a maze game and Super Mario Bros, with our results indicating that our approach does in fact decrease the time required for level generation, especially when an increasing number of valid levels are required.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1085645447,
        "newsscientist":0.1778980994,
        "technologyreview":0.2467987657,
        "venturebeat":0.2673551526,
        "wired":0.2079697062,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14772v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1659111952000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.13286v1",
        "predicted_newsworthiness":0.3711927327,
        "title":"Vector Quantized Image-to-Image Translation",
        "summary":"Current image-to-image translation methods formulate the task with conditional generation models, leading to learning only the recolorization or regional changes as being constrained by the rich structural information provided by the conditional contexts. In this work, we propose introducing the vector quantization technique into the image-to-image translation framework. The vector quantized content representation can facilitate not only the translation, but also the unconditional distribution shared among different domains. Meanwhile, along with the disentangled style representation, the proposed method further enables the capability of image extension with flexibility in both intra- and inter-domains. Qualitative and quantitative experiments demonstrate that our framework achieves comparable performance to the state-of-the-art image-to-image translation and image extension methods. Compared to methods for individual tasks, the proposed method, as a unified framework, unleashes applications combining image-to-image translation, unconditional generation, and image extension altogether. For example, it provides style variability for image generation and extension, and equips image-to-image translation with further extension capabilities.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0911653122,
        "newsscientist":0.137071747,
        "technologyreview":0.1874703638,
        "venturebeat":0.1650669045,
        "wired":0.1458385454,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13286v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658895749000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01307v1",
        "predicted_newsworthiness":0.3704105168,
        "title":"Multilingual Coreference Resolution in Multiparty Dialogue",
        "summary":"Existing multiparty dialogue datasets for coreference resolution are nascent, and many challenges are still unaddressed. We create a large-scale dataset, Multilingual Multiparty Coref (MMC), for this task based on TV transcripts. Due to the availability of gold-quality subtitles in multiple languages, we propose reusing the annotations to create silver coreference data in other languages (Chinese and Farsi) via annotation projection. On the gold (English) data, off-the-shelf models perform relatively poorly on MMC, suggesting that MMC has broader coverage of multiparty coreference than prior datasets. On the silver data, we find success both using it for data augmentation and training from scratch, which effectively simulates the zero-shot cross-lingual setting.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1192848123,
        "newsscientist":0.1012377666,
        "technologyreview":0.1573703507,
        "venturebeat":0.1715626959,
        "wired":0.1707607013,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01307v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1659428820000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14273v1",
        "predicted_newsworthiness":0.3703051194,
        "title":"CuDi: Curve Distillation for Efficient and Controllable Exposure Adjustment",
        "summary":"We present Curve Distillation, CuDi, for efficient and controllable exposure adjustment without the requirement of paired or unpaired data during training. Our method inherits the zero-reference learning and curve-based framework from an effective low-light image enhancement method, Zero-DCE, with further speed up in its inference speed, reduction in its model size, and extension to controllable exposure adjustment. The improved inference speed and lightweight model are achieved through novel curve distillation that approximates the time-consuming iterative operation in the conventional curve-based framework by high-order curve's tangent line. The controllable exposure adjustment is made possible with a new self-supervised spatial exposure control loss that constrains the exposure levels of different spatial regions of the output to be close to the brightness distribution of an exposure map serving as an input condition. Different from most existing methods that can only correct either underexposed or overexposed photos, our approach corrects both underexposed and overexposed photos with a single model. Notably, our approach can additionally adjust the exposure levels of a photo globally or locally with the guidance of an input condition exposure map, which can be pre-defined or manually set in the inference stage. Through extensive experiments, we show that our method is appealing for its fast, robust, and flexible performance, outperforming state-of-the-art methods in real scenes. Project page: https:\/\/li-chongyi.github.io\/CuDi_files\/.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0845533912,
        "newsscientist":0.1261163206,
        "technologyreview":0.1712629187,
        "venturebeat":0.1499979033,
        "wired":0.1351863843,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14273v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659030826000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11889v1",
        "predicted_newsworthiness":0.3697325839,
        "title":"Salient Object Detection for Point Clouds",
        "summary":"This paper researches the unexplored task-point cloud salient object detection (SOD). Differing from SOD for images, we find the attention shift of point clouds may provoke saliency conflict, i.e., an object paradoxically belongs to salient and non-salient categories. To eschew this issue, we present a novel view-dependent perspective of salient objects, reasonably reflecting the most eye-catching objects in point cloud scenarios. Following this formulation, we introduce PCSOD, the first dataset proposed for point cloud SOD consisting of 2,872 in-\/out-door 3D views. The samples in our dataset are labeled with hierarchical annotations, e.g., super-\/sub-class, bounding box, and segmentation map, which endows the brilliant generalizability and broad applicability of our dataset verifying various conjectures. To evidence the feasibility of our solution, we further contribute a baseline model and benchmark five representative models for a comprehensive comparison. The proposed model can effectively analyze irregular and unordered points for detecting salient objects. Thanks to incorporating the task-tailored designs, our method shows visible superiority over other baselines, producing more satisfactory results. Extensive experiments and discussions reveal the promising potential of this research field, paving the way for further study.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1050433148,
        "newsscientist":0.1560458756,
        "technologyreview":0.2076142094,
        "venturebeat":0.2163088181,
        "wired":0.2010792739,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11889v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658720146000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12661v1",
        "predicted_newsworthiness":0.3697175095,
        "title":"Learning Visual Representation from Modality-Shared Contrastive Language-Image Pre-training",
        "summary":"Large-scale multi-modal contrastive pre-training has demonstrated great utility to learn transferable features for a range of downstream tasks by mapping multiple modalities into a shared embedding space. Typically, this has employed separate encoders for each modality. However, recent work suggests that transformers can support learning across multiple modalities and allow knowledge sharing. Inspired by this, we investigate a variety of Modality-Shared Contrastive Language-Image Pre-training (MS-CLIP) frameworks. More specifically, we question how many parameters of a transformer model can be shared across modalities during contrastive pre-training, and rigorously examine architectural design choices that position the proportion of parameters shared along a spectrum. In studied conditions, we observe that a mostly unified encoder for vision and language signals outperforms all other variations that separate more parameters. Additionally, we find that light-weight modality-specific parallel modules further improve performance. Experimental results show that the proposed MS-CLIP approach outperforms vanilla CLIP by up to 13\\% relative in zero-shot ImageNet classification (pre-trained on YFCC-100M), while simultaneously supporting a reduction of parameters. In addition, our approach outperforms vanilla CLIP by 1.6 points in linear probing on a collection of 24 downstream vision tasks. Furthermore, we discover that sharing parameters leads to semantic concepts from different modalities being encoded more closely in the embedding space, facilitating the transferring of common semantic structure (e.g., attention patterns) from language to vision. Code is available at \\href{https:\/\/github.com\/Hxyou\/MSCLIP}{URL}.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0923923651,
        "newsscientist":0.1293625043,
        "technologyreview":0.2313757294,
        "venturebeat":0.2125913988,
        "wired":0.1572012125,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12661v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.cl"
        ],
        "published":1658812756000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00368v1",
        "predicted_newsworthiness":0.369375346,
        "title":"Skeleton-Parted Graph Scattering Networks for 3D Human Motion Prediction",
        "summary":"Graph convolutional network based methods that model the body-joints' relations, have recently shown great promise in 3D skeleton-based human motion prediction. However, these methods have two critical issues: first, deep graph convolutions filter features within only limited graph spectrums, losing sufficient information in the full band; second, using a single graph to model the whole body underestimates the diverse patterns on various body-parts. To address the first issue, we propose adaptive graph scattering, which leverages multiple trainable band-pass graph filters to decompose pose features into richer graph spectrum bands. To address the second issue, body-parts are modeled separately to learn diverse dynamics, which enables finer feature extraction along the spatial dimensions. Integrating the above two designs, we propose a novel skeleton-parted graph scattering network (SPGSN). The cores of the model are cascaded multi-part graph scattering blocks (MPGSBs), building adaptive graph scattering on diverse body-parts, as well as fusing the decomposed features based on the inferred spectrum importance and body-part interactions. Extensive experiments have shown that SPGSN outperforms state-of-the-art methods by remarkable margins of 13.8%, 9.3% and 2.7% in terms of 3D mean per joint position error (MPJPE) on Human3.6M, CMU Mocap and 3DPW datasets, respectively.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0879879986,
        "newsscientist":0.1314700585,
        "technologyreview":0.1671295883,
        "venturebeat":0.1449291551,
        "wired":0.1338629272,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00368v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659246699000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00919v1",
        "predicted_newsworthiness":0.3693474322,
        "title":"Benchmarking Visual-Inertial Deep Multimodal Fusion for Relative Pose Regression and Odometry-aided Absolute Pose Regression",
        "summary":"Visual-inertial localization is a key problem in computer vision and robotics applications such as virtual reality, self-driving cars, and aerial vehicles. The goal is to estimate an accurate pose of an object when either the environment or the dynamics are known. Recent methods directly regress the pose using convolutional and spatio-temporal networks. Absolute pose regression (APR) techniques predict the absolute camera pose from an image input in a known scene. Odometry methods perform relative pose regression (RPR) that predicts the relative pose from a known object dynamic (visual or inertial inputs). The localization task can be improved by retrieving information of both data sources for a cross-modal setup, which is a challenging problem due to contradictory tasks. In this work, we conduct a benchmark to evaluate deep multimodal fusion based on PGO and attention networks. Auxiliary and Bayesian learning are integrated for the APR task. We show accuracy improvements for the RPR-aided APR task and for the RPR-RPR task for aerial vehicles and hand-held devices. We conduct experiments on the EuRoC MAV and PennCOSYVIO datasets, and record a novel industry dataset.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0817784013,
        "newsscientist":0.1332475677,
        "technologyreview":0.219249764,
        "venturebeat":0.2347778803,
        "wired":0.1931505461,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00919v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659366326000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11582v1",
        "predicted_newsworthiness":0.3693395983,
        "title":"Defining an action of SO(d)-rotations on images generated by projections of d-dimensional objects: Applications to pose inference with Geometric VAEs",
        "summary":"Recent advances in variational autoencoders (VAEs) have enabled learning latent manifolds as compact Lie groups, such as $SO(d)$. Since this approach assumes that data lies on a subspace that is homeomorphic to the Lie group itself, we here investigate how this assumption holds in the context of images that are generated by projecting a $d$-dimensional volume with unknown pose in $SO(d)$. Upon examining different theoretical candidates for the group and image space, we show that the attempt to define a group action on the data space generally fails, as it requires more specific geometric constraints on the volume. Using geometric VAEs, our experiments confirm that this constraint is key to proper pose inference, and we discuss the potential of these results for applications and future work.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0786627921,
        "newsscientist":0.1260090425,
        "technologyreview":0.1605401856,
        "venturebeat":0.1430040675,
        "wired":0.1277272206,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11582v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658604148000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00967v1",
        "predicted_newsworthiness":0.3692302332,
        "title":"Counterfactual Intervention Feature Transfer for Visible-Infrared Person Re-identification",
        "summary":"Graph-based models have achieved great success in person re-identification tasks recently, which compute the graph topology structure (affinities) among different people first and then pass the information across them to achieve stronger features. But we find existing graph-based methods in the visible-infrared person re-identification task (VI-ReID) suffer from bad generalization because of two issues: 1) train-test modality balance gap, which is a property of VI-ReID task. The number of two modalities data are balanced in the training stage, but extremely unbalanced in inference, causing the low generalization of graph-based VI-ReID methods. 2) sub-optimal topology structure caused by the end-to-end learning manner to the graph module. We analyze that the well-trained input features weaken the learning of graph topology, making it not generalized enough during the inference process. In this paper, we propose a Counterfactual Intervention Feature Transfer (CIFT) method to tackle these problems. Specifically, a Homogeneous and Heterogeneous Feature Transfer (H2FT) is designed to reduce the train-test modality balance gap by two independent types of well-designed graph modules and an unbalanced scenario simulation. Besides, a Counterfactual Relation Intervention (CRI) is proposed to utilize the counterfactual intervention and causal effect tools to highlight the role of topology structure in the whole training process, which makes the graph topology structure more reliable. Extensive experiments on standard VI-ReID benchmarks demonstrate that CIFT outperforms the state-of-the-art methods under various settings.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1025435137,
        "newsscientist":0.147967154,
        "technologyreview":0.2218361405,
        "venturebeat":0.1795656186,
        "wired":0.1424631618,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00967v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659370531000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00237v1",
        "predicted_newsworthiness":0.3687326915,
        "title":"RBP-Pose: Residual Bounding Box Projection for Category-Level Pose Estimation",
        "summary":"Category-level object pose estimation aims to predict the 6D pose as well as the 3D metric size of arbitrary objects from a known set of categories. Recent methods harness shape prior adaptation to map the observed point cloud into the canonical space and apply Umeyama algorithm to recover the pose and size. However, their shape prior integration strategy boosts pose estimation indirectly, which leads to insufficient pose-sensitive feature extraction and slow inference speed. To tackle this problem, in this paper, we propose a novel geometry-guided Residual Object Bounding Box Projection network RBP-Pose that jointly predicts object pose and residual vectors describing the displacements from the shape-prior-indicated object surface projections on the bounding box towards the real surface projections. Such definition of residual vectors is inherently zero-mean and relatively small, and explicitly encapsulates spatial cues of the 3D object for robust and accurate pose regression. We enforce geometry-aware consistency terms to align the predicted pose and residual vectors to further boost performance.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0787056585,
        "newsscientist":0.1163306644,
        "technologyreview":0.1660252044,
        "venturebeat":0.1690920343,
        "wired":0.1352367436,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00237v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659192320000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11972v1",
        "predicted_newsworthiness":0.3681603997,
        "title":"TransCL: Transformer Makes Strong and Flexible Compressive Learning",
        "summary":"Compressive learning (CL) is an emerging framework that integrates signal acquisition via compressed sensing (CS) and machine learning for inference tasks directly on a small number of measurements. It can be a promising alternative to classical image-domain methods and enjoys great advantages in memory saving and computational efficiency. However, previous attempts on CL are not only limited to a fixed CS ratio, which lacks flexibility, but also limited to MNIST\/CIFAR-like datasets and do not scale to complex real-world high-resolution (HR) data or vision tasks. In this paper, a novel transformer-based compressive learning framework on large-scale images with arbitrary CS ratios, dubbed TransCL, is proposed. Specifically, TransCL first utilizes the strategy of learnable block-based compressed sensing and proposes a flexible linear projection strategy to enable CL to be performed on large-scale images in an efficient block-by-block manner with arbitrary CS ratios. Then, regarding CS measurements from all blocks as a sequence, a pure transformer-based backbone is deployed to perform vision tasks with various task-oriented heads. Our sufficient analysis presents that TransCL exhibits strong resistance to interference and robust adaptability to arbitrary CS ratios. Extensive experiments for complex HR data demonstrate that the proposed TransCL can achieve state-of-the-art performance in image classification and semantic segmentation tasks. In particular, TransCL with a CS ratio of $10\\%$ can obtain almost the same performance as when operating directly on the original data and can still obtain satisfying performance even with an extremely low CS ratio of $1\\%$. The source codes of our proposed TransCL is available at \\url{https:\/\/github.com\/MC-E\/TransCL\/}.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0810255956,
        "newsscientist":0.1360923009,
        "technologyreview":0.2067613477,
        "venturebeat":0.1807626889,
        "wired":0.1421683854,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11972v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658737308000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14676v1",
        "predicted_newsworthiness":0.3681222894,
        "title":"Global-Local Self-Distillation for Visual Representation Learning",
        "summary":"The downstream accuracy of self-supervised methods is tightly linked to the proxy task solved during training and the quality of the gradients extracted from it. Richer and more meaningful gradients updates are key to allow self-supervised methods to learn better and in a more efficient manner. In a typical self-distillation framework, the representation of two augmented images are enforced to be coherent at the global level. Nonetheless, incorporating local cues in the proxy task can be beneficial and improve the model accuracy on downstream tasks. This leads to a dual objective in which, on the one hand, coherence between global-representations is enforced and on the other, coherence between local-representations is enforced. Unfortunately, an exact correspondence mapping between two sets of local-representations does not exist making the task of matching local-representations from one augmentation to another non-trivial. We propose to leverage the spatial information in the input images to obtain geometric matchings and compare this geometric approach against previous methods based on similarity matchings. Our study shows that not only 1) geometric matchings perform better than similarity based matchings in low-data regimes but also 2) that similarity based matchings are highly hurtful in low-data regimes compared to the vanilla baseline without local self-distillation. The code will be released upon acceptance.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.090407848,
        "newsscientist":0.1291589955,
        "technologyreview":0.2164899785,
        "venturebeat":0.1863514756,
        "wired":0.1505160151,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14676v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659102609000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12795v1",
        "predicted_newsworthiness":0.367703781,
        "title":"Static and Dynamic Concepts for Self-supervised Video Representation Learning",
        "summary":"In this paper, we propose a novel learning scheme for self-supervised video representation learning. Motivated by how humans understand videos, we propose to first learn general visual concepts then attend to discriminative local areas for video understanding. Specifically, we utilize static frame and frame difference to help decouple static and dynamic concepts, and respectively align the concept distributions in latent space. We add diversity and fidelity regularizations to guarantee that we learn a compact set of meaningful concepts. Then we employ a cross-attention mechanism to aggregate detailed local features of different concepts, and filter out redundant concepts with low activations to perform local concept contrast. Extensive experiments demonstrate that our method distills meaningful static and dynamic concepts to guide video understanding, and obtains state-of-the-art results on UCF-101, HMDB-51, and Diving-48.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0903234866,
        "newsscientist":0.1277635859,
        "technologyreview":0.1767835946,
        "venturebeat":0.153763258,
        "wired":0.1522145308,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12795v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658831324000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01502v1",
        "predicted_newsworthiness":0.3671755198,
        "title":"A Multi-body Tracking Framework -- From Rigid Objects to Kinematic Structures",
        "summary":"Kinematic structures are very common in the real world. They range from simple articulated objects to complex mechanical systems. However, despite their relevance, most model-based 3D tracking methods only consider rigid objects. To overcome this limitation, we propose a flexible framework that allows the extension of existing 6DoF algorithms to kinematic structures. Our approach focuses on methods that employ Newton-like optimization techniques, which are widely used in object tracking. The framework considers both tree-like and closed kinematic structures and allows a flexible configuration of joints and constraints. To project equations from individual rigid bodies to a multi-body system, Jacobians are used. For closed kinematic chains, a novel formulation that features Lagrange multipliers is developed. In a detailed mathematical proof, we show that our constraint formulation leads to an exact kinematic solution and converges in a single iteration. Based on the proposed framework, we extend ICG, which is a state-of-the-art rigid object tracking algorithm, to multi-body tracking. For the evaluation, we create a highly-realistic synthetic dataset that features a large number of sequences and various robots. Based on this dataset, we conduct a wide variety of experiments that demonstrate the excellent performance of the developed framework and our multi-body tracker.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0582765887,
        "newsscientist":0.1171305017,
        "technologyreview":0.1583450084,
        "venturebeat":0.1483094032,
        "wired":0.1283612688,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01502v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659451774000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01834v1",
        "predicted_newsworthiness":0.3671536285,
        "title":"Integrating Object-aware and Interaction-aware Knowledge for Weakly Supervised Scene Graph Generation",
        "summary":"Recently, increasing efforts have been focused on Weakly Supervised Scene Graph Generation (WSSGG). The mainstream solution for WSSGG typically follows the same pipeline: they first align text entities in the weak image-level supervisions (e.g., unlocalized relation triplets or captions) with image regions, and then train SGG models in a fully-supervised manner with aligned instance-level \"pseudo\" labels. However, we argue that most existing WSSGG works only focus on object-consistency, which means the grounded regions should have the same object category label as text entities. While they neglect another basic requirement for an ideal alignment: interaction-consistency, which means the grounded region pairs should have the same interactions (i.e., visual relations) as text entity pairs. Hence, in this paper, we propose to enhance a simple grounding module with both object-aware and interaction-aware knowledge to acquire more reliable pseudo labels. To better leverage these two types of knowledge, we regard them as two teachers and fuse their generated targets to guide the training process of our grounding module. Specifically, we design two different strategies to adaptively assign weights to different teachers by assessing their reliability on each training sample. Extensive experiments have demonstrated that our method consistently improves WSSGG performance on various kinds of weak supervision.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0774609847,
        "newsscientist":0.1095399943,
        "technologyreview":0.1931047657,
        "venturebeat":0.1702483169,
        "wired":0.1401975017,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01834v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659500417000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11605v1",
        "predicted_newsworthiness":0.3668739522,
        "title":"Event-based RGB-D sensing with structured light",
        "summary":"Event-based cameras (ECs) are bio-inspired sensors that asynchronously report brightness changes for each pixel. Due to their high dynamic range, pixel bandwidth, temporal resolution, low power consumption, and computational simplicity, they are beneficial for vision-based projects in challenging lighting conditions and they can detect fast movements with their microsecond response time. The first generation of ECs are monochrome, but color data is very useful and sometimes essential for certain vision-based applications. The latest technology enables manufacturers to build color ECs, trading off the size of the sensor and substantially reducing the resolution compared to monochrome models, despite having the same bandwidth. In addition, ECs only detect changes in light and do not show static or slowly moving objects. We introduce a method to detect full RGB events using a monochrome EC aided by a structured light projector. The projector emits rapidly changing RGB patterns of light beams on the scene, the reflection of which is captured by the EC. We combine the benefits of ECs and projection-based techniques and allow depth and color detection of static or moving objects with a commercial TI LightCrafter 4500 projector and a monocular monochrome EC, paving the way for frameless RGB-D sensing applications.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0865067753,
        "newsscientist":0.1566693437,
        "technologyreview":0.1762479404,
        "venturebeat":0.193243399,
        "wired":0.169777037,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11605v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658610601000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13702v1",
        "predicted_newsworthiness":0.3665883459,
        "title":"Physical Systems Modeled Without Physical Laws",
        "summary":"Physics-based simulations typically operate with a combination of complex differentiable equations and many scientific and geometric inputs. Our work involves gathering data from those simulations and seeing how well tree-based machine learning methods can emulate desired outputs without \"knowing\" the complex backing involved in the simulations. The selected physics-based simulations included Navier-Stokes, stress analysis, and electromagnetic field lines to benchmark performance as numerical and statistical algorithms. We specifically focus on predicting specific spatial-temporal data between two simulation outputs and increasing spatial resolution to generalize the physics predictions to finer test grids without the computational costs of repeating the numerical calculation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1036218616,
        "newsscientist":0.1687512681,
        "technologyreview":0.2281901052,
        "venturebeat":0.2006613537,
        "wired":0.1698909623,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13702v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658868680000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00511v1",
        "predicted_newsworthiness":0.3664823062,
        "title":"Aggretriever: A Simple Approach to Aggregate Textual Representation for Robust Dense Passage Retrieval",
        "summary":"Pre-trained transformers has declared its success in many NLP tasks. One thread of work focuses on training bi-encoder models (i.e., dense retrievers) to effectively encode sentences or passages into single-vector dense vectors for efficient approximate nearest neighbor (ANN) search. However, recent work has demonstrated that transformers pre-trained with mask language modeling (MLM) are not capable of effectively aggregating text information into a single dense vector due to task-mismatch between pre-training and fine-tuning. Therefore, computationally expensive techniques have been adopted to train dense retrievers, such as large batch size, knowledge distillation or post pre-training. In this work, we present a simple approach to effectively aggregate textual representation from the pre-trained transformer into a dense vector. Extensive experiments show that our approach improves the robustness of the single-vector approach under both in-domain and zero-shot evaluations without any computationally expensive training techniques. Our work demonstrates that MLM pre-trained transformers can be used to effectively encode text information into a single-vector for dense retrieval. Code are available at: https:\/\/github.com\/castorini\/dhr",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0937852183,
        "newsscientist":0.0975895457,
        "technologyreview":0.1806041111,
        "venturebeat":0.1954065071,
        "wired":0.1597402389,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00511v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1659299255000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.11685v1",
        "predicted_newsworthiness":0.3663474015,
        "title":"Kernel Relative-prototype Spectral Filtering for Few-shot Learning",
        "summary":"Few-shot learning performs classification tasks and regression tasks on scarce samples. As one of the most representative few-shot learning models, Prototypical Network represents each class as sample average, or a prototype, and measures the similarity of samples and prototypes by Euclidean distance. In this paper, we propose a framework of spectral filtering (shrinkage) for measuring the difference between query samples and prototypes, or namely the relative prototypes, in a reproducing kernel Hilbert space (RKHS). In this framework, we further propose a method utilizing Tikhonov regularization as the filter function for few-shot classification. We conduct several experiments to verify our method utilizing different kernels based on the miniImageNet dataset, tiered-ImageNet dataset and CIFAR-FS dataset. The experimental results show that the proposed model can perform the state-of-the-art. In addition, the experimental results show that the proposed shrinkage method can boost the performance. Source code is available at https:\/\/github.com\/zhangtao2022\/DSFN.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0898897596,
        "newsscientist":0.1315577369,
        "technologyreview":0.209591851,
        "venturebeat":0.1905711794,
        "wired":0.1545219008,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11685v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658649207000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01018v1",
        "predicted_newsworthiness":0.3660609045,
        "title":"BabelBERT: Massively Multilingual Transformers Meet a Massively Multilingual Lexical Resource",
        "summary":"While pretrained language models (PLMs) primarily serve as general purpose text encoders that can be fine-tuned for a wide variety of downstream tasks, recent work has shown that they can also be rewired to produce high-quality word representations (i.e., static word embeddings) and yield good performance in type-level lexical tasks. While existing work primarily focused on lexical specialization of PLMs in monolingual and bilingual settings, in this work we expose massively multilingual transformers (MMTs, e.g., mBERT or XLM-R) to multilingual lexical knowledge at scale, leveraging BabelNet as the readily available rich source of multilingual and cross-lingual type-level lexical knowledge. Concretely, we leverage BabelNet's multilingual synsets to create synonym pairs across $50$ languages and then subject the MMTs (mBERT and XLM-R) to a lexical specialization procedure guided by a contrastive objective. We show that such massively multilingual lexical specialization brings massive gains in two standard cross-lingual lexical tasks, bilingual lexicon induction and cross-lingual word similarity, as well as in cross-lingual sentence retrieval. Crucially, we observe gains for languages unseen in specialization, indicating that the multilingual lexical specialization enables generalization to languages with no lexical constraints. In a series of subsequent controlled experiments, we demonstrate that the pretraining quality of word representations in the MMT for languages involved in specialization has a much larger effect on performance than the linguistic diversity of the set of constraints. Encouragingly, this suggests that lexical tasks involving low-resource languages benefit the most from lexical knowledge of resource-rich languages, generally much more available.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0995943119,
        "newsscientist":0.0928656298,
        "technologyreview":0.1716427144,
        "venturebeat":0.1671925533,
        "wired":0.1293605225,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01018v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659376023000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14581v1",
        "predicted_newsworthiness":0.3653767884,
        "title":"Learning Prototype via Placeholder for Zero-shot Recognition",
        "summary":"Zero-shot learning (ZSL) aims to recognize unseen classes by exploiting semantic descriptions shared between seen classes and unseen classes. Current methods show that it is effective to learn visual-semantic alignment by projecting semantic embeddings into the visual space as class prototypes. However, such a projection function is only concerned with seen classes. When applied to unseen classes, the prototypes often perform suboptimally due to domain shift. In this paper, we propose to learn prototypes via placeholders, termed LPL, to eliminate the domain shift between seen and unseen classes. Specifically, we combine seen classes to hallucinate new classes which play as placeholders of the unseen classes in the visual and semantic space. Placed between seen classes, the placeholders encourage prototypes of seen classes to be highly dispersed. And more space is spared for the insertion of well-separated unseen ones. Empirically, well-separated prototypes help counteract visual-semantic misalignment caused by domain shift. Furthermore, we exploit a novel semantic-oriented fine-tuning to guarantee the semantic reliability of placeholders. Extensive experiments on five benchmark datasets demonstrate the significant performance gain of LPL over the state-of-the-art methods. Code is available at https:\/\/github.com\/zaiquanyang\/LPL.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0854848031,
        "newsscientist":0.1413994539,
        "technologyreview":0.2072444139,
        "venturebeat":0.1880134699,
        "wired":0.1607668697,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14581v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659088604000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14124v1",
        "predicted_newsworthiness":0.3652901356,
        "title":"Graph Neural Networks to Predict Sports Outcomes",
        "summary":"Predicting outcomes in sports is important for teams, leagues, bettors, media, and fans. Given the growing amount of player tracking data, sports analytics models are increasingly utilizing spatially-derived features built upon player tracking data. However, player-specific information, such as location, cannot readily be included as features themselves, since common modeling techniques rely on vector input. Accordingly, spatially-derived features are commonly constructed in relation to anchor objects, such as the distance to a ball or goal, through global feature aggregations, or via role-assignment schemes, where players are designated a distinct role in the game. In doing so, we sacrifice inter-player and local relationships in favor of global ones. To address this issue, we introduce a sport-agnostic graph-based representation of game states. We then use our proposed graph representation as input to graph neural networks to predict sports outcomes. Our approach preserves permutation invariance and allows for flexible player interaction weights. We demonstrate how our method provides statistically significant improvements over the state of the art for prediction tasks in both American football and esports, reducing test set loss by 9% and 20%, respectively. Additionally, we show how our model can be used to answer \"what if\" questions in sports and to visualize relationships between players.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1479935761,
        "newsscientist":0.1615249001,
        "technologyreview":0.2463835345,
        "venturebeat":0.2524658025,
        "wired":0.2015289613,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14124v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659019502000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13670v1",
        "predicted_newsworthiness":0.3651837914,
        "title":"Meta-Interpolation: Time-Arbitrary Frame Interpolation via Dual Meta-Learning",
        "summary":"Existing video frame interpolation methods can only interpolate the frame at a given intermediate time-step, e.g. 1\/2. In this paper, we aim to explore a more generalized kind of video frame interpolation, that at an arbitrary time-step. To this end, we consider processing different time-steps with adaptively generated convolutional kernels in a unified way with the help of meta-learning. Specifically, we develop a dual meta-learned frame interpolation framework to synthesize intermediate frames with the guidance of context information and optical flow as well as taking the time-step as side information. First, a content-aware meta-learned flow refinement module is built to improve the accuracy of the optical flow estimation based on the down-sampled version of the input frames. Second, with the refined optical flow and the time-step as the input, a motion-aware meta-learned frame interpolation module generates the convolutional kernels for every pixel used in the convolution operations on the feature map of the coarse warped version of the input frames to generate the predicted frame. Extensive qualitative and quantitative evaluations, as well as ablation studies, demonstrate that, via introducing meta-learning in our framework in such a well-designed way, our method not only achieves superior performance to state-of-the-art frame interpolation approaches but also owns an extended capacity to support the interpolation at an arbitrary time-step.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0705765938,
        "newsscientist":0.114366871,
        "technologyreview":0.16369694,
        "venturebeat":0.1679126538,
        "wired":0.1432020794,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13670v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658943383000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13629v2",
        "predicted_newsworthiness":0.3631425412,
        "title":"Proprioceptive Slip Detection for Planetary Rovers in Perceptually Degraded Extraterrestrial Environments",
        "summary":"Slip detection is of fundamental importance for the safety and efficiency of rovers driving on the surface of extraterrestrial bodies. Current planetary rover slip detection systems rely on visual perception on the assumption that sufficient visual features can be acquired in the environment. However, visual-based methods are prone to suffer in perceptually degraded planetary environments with dominant low terrain features such as regolith, glacial terrain, salt-evaporites, and poor lighting conditions such as dark caves and permanently shadowed regions. Relying only on visual sensors for slip detection also requires additional computational power and reduces the rover traversal rate. This paper answers the question of how to detect wheel slippage of a planetary rover without depending on visual perception. In this respect, we propose a slip detection system that obtains its information from a proprioceptive localization framework that is capable of providing reliable, continuous, and computationally efficient state estimation over hundreds of meters. This is accomplished by using zero velocity update, zero angular rate update, and non-holonomic constraints as pseudo-measurement updates on an inertial navigation system framework. The proposed method is evaluated on actual hardware and field-tested in a planetary-analog environment. The method achieves greater than 92% slip detection accuracy for distances around 150 m using only an IMU and wheel encoders.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0822811628,
        "newsscientist":0.1780911583,
        "technologyreview":0.1914499473,
        "venturebeat":0.1705223588,
        "wired":0.1684439915,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13629v2",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658940288000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00856v1",
        "predicted_newsworthiness":0.3629706788,
        "title":"ATCA: an Arc Trajectory Based Model with Curvature Attention for Video Frame Interpolation",
        "summary":"Video frame interpolation is a classic and challenging low-level computer vision task. Recently, deep learning based methods have achieved impressive results, and it has been proven that optical flow based methods can synthesize frames with higher quality. However, most flow-based methods assume a line trajectory with a constant velocity between two input frames. Only a little work enforces predictions with curvilinear trajectory, but this requires more than two frames as input to estimate the acceleration, which takes more time and memory to execute. To address this problem, we propose an arc trajectory based model (ATCA), which learns motion prior from only two consecutive frames and also is lightweight. Experiments show that our approach performs better than many SOTA methods with fewer parameters and faster inference speed.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0714973919,
        "newsscientist":0.1216362677,
        "technologyreview":0.1880407528,
        "venturebeat":0.1820474813,
        "wired":0.1617120565,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00856v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659361328000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12585v2",
        "predicted_newsworthiness":0.3629657357,
        "title":"PTGCF: Printing Texture Guided Color Fusion for Impressionism Oil Painting Style Rendering",
        "summary":"As a major branch of Non-Photorealistic Rendering (NPR), image stylization mainly uses the computer algorithms to render a photo into an artistic painting. Recent work has shown that the extraction of style information such as stroke texture and color of the target style image is the key to image stylization. Given its stroke texture and color characteristics, a new stroke rendering method is proposed, which fully considers the tonal characteristics and the representative color of the original oil painting, in order to fit the tone of the original oil painting image into the stylized image and make it close to the artist's creative effect. The experiments have validated the efficacy of the proposed model. This method would be more suitable for the works of pointillism painters with a relatively uniform sense of direction, especially for natural scenes. When the original painting brush strokes have a clearer sense of direction, using this method to simulate brushwork texture features can be less satisfactory.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0869219724,
        "newsscientist":0.1102921891,
        "technologyreview":0.1121637625,
        "venturebeat":0.1251887239,
        "wired":0.128832419,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12585v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658795483000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01171v1",
        "predicted_newsworthiness":0.3627133061,
        "title":"Patents Phrase to Phrase Semantic Matching Dataset",
        "summary":"There are many general purpose benchmark datasets for Semantic Textual Similarity but none of them are focused on technical concepts found in patents and scientific publications. This work aims to fill this gap by presenting a new human rated contextual phrase to phrase matching dataset. The entire dataset contains close to $50,000$ rated phrase pairs, each with a CPC (Cooperative Patent Classification) class as a context. This paper describes the dataset and some baseline models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1254790063,
        "newsscientist":0.1605740672,
        "technologyreview":0.2408576354,
        "venturebeat":0.2469385356,
        "wired":0.2022350471,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01171v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai",
            "cs.lg"
        ],
        "published":1659396810000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14094v2",
        "predicted_newsworthiness":0.3622973456,
        "title":"Entity Type Prediction Leveraging Graph Walks and Entity Descriptions",
        "summary":"The entity type information in Knowledge Graphs (KGs) such as DBpedia, Freebase, etc. is often incomplete due to automated generation or human curation. Entity typing is the task of assigning or inferring the semantic type of an entity in a KG. This paper presents \\textit{GRAND}, a novel approach for entity typing leveraging different graph walk strategies in RDF2vec together with textual entity descriptions. RDF2vec first generates graph walks and then uses a language model to obtain embeddings for each node in the graph. This study shows that the walk generation strategy and the embedding model have a significant effect on the performance of the entity typing task. The proposed approach outperforms the baseline approaches on the benchmark datasets DBpedia and FIGER for entity typing in KGs for both fine-grained and coarse-grained classes. The results show that the combination of order-aware RDF2vec variants together with the contextual embeddings of the textual entity descriptions achieve the best results.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0810832307,
        "newsscientist":0.1064387538,
        "technologyreview":0.1788989177,
        "venturebeat":0.190779025,
        "wired":0.1502371466,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14094v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1659016615000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00463v1",
        "predicted_newsworthiness":0.3617512227,
        "title":"Mismatching-Aware Unsupervised Translation Quality Estimation For Low-Resource Languages",
        "summary":"Translation Quality Estimation (QE) is the task of predicting the quality of machine translation (MT) output without any reference. This task has gained increasing attention as an important component in practical applications of MT. In this paper, we first propose XLMRScore, a simple unsupervised QE method based on the BERTScore computed using the XLM-RoBERTa (XLMR) model while discussing the issues that occur using this method. Next, we suggest two approaches to mitigate the issues: replacing untranslated words with the unknown token and the cross-lingual alignment of pre-trained model to represent aligned words closer to each other. We evaluate the proposed method on four low-resource language pairs of WMT21 QE shared task, as well as a new English-Farsi test dataset introduced in this paper. Experiments show that our method could get comparable results with the supervised baseline for two zero-shot scenarios, i.e., with less than 0.01 difference in Pearson correlation, while outperforming the unsupervised rivals in all the low-resource language pairs for above 8% in average.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0947229921,
        "newsscientist":0.094346202,
        "technologyreview":0.1423644265,
        "venturebeat":0.1476376515,
        "wired":0.111650416,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00463v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659284603000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00281v1",
        "predicted_newsworthiness":0.3607936987,
        "title":"Point Primitive Transformer for Long-Term 4D Point Cloud Video Understanding",
        "summary":"This paper proposes a 4D backbone for long-term point cloud video understanding. A typical way to capture spatial-temporal context is using 4Dconv or transformer without hierarchy. However, those methods are neither effective nor efficient enough due to camera motion, scene changes, sampling patterns, and the complexity of 4D data. To address those issues, we leverage the primitive plane as a mid-level representation to capture the long-term spatial-temporal context in 4D point cloud videos and propose a novel hierarchical backbone named Point Primitive Transformer(PPTr), which is mainly composed of intra-primitive point transformers and primitive transformers. Extensive experiments show that PPTr outperforms the previous state of the arts on different tasks",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0858323164,
        "newsscientist":0.1369927455,
        "technologyreview":0.1703926987,
        "venturebeat":0.1991724766,
        "wired":0.1773109379,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00281v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659202915000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12505v1",
        "predicted_newsworthiness":0.3607776246,
        "title":"On the benefits of non-linear weight updates",
        "summary":"Recent work has suggested that the generalisation performance of a DNN is related to the extent to which the Signal-to-Noise Ratio is optimised at each of the nodes. In contrast, Gradient Descent methods do not always lead to SNR-optimal weight configurations. One way to improve SNR performance is to suppress large weight updates and amplify small weight updates. Such balancing is already implicit in some common optimizers, but we propose an approach that makes this explicit. The method applies a non-linear function to gradients prior to making DNN parameter updates. We investigate the performance with such non-linear approaches. The result is an adaptation to existing optimizers that improves performance for many problem types.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0659829296,
        "newsscientist":0.1053502316,
        "technologyreview":0.1846082785,
        "venturebeat":0.1622675981,
        "wired":0.1236419064,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12505v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658779791000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14498v1",
        "predicted_newsworthiness":0.3603126915,
        "title":"Reference-Guided Texture and Structure Inference for Image Inpainting",
        "summary":"Existing learning-based image inpainting methods are still in challenge when facing complex semantic environments and diverse hole patterns. The prior information learned from the large scale training data is still insufficient for these situations. Reference images captured covering the same scenes share similar texture and structure priors with the corrupted images, which offers new prospects for the image inpainting tasks. Inspired by this, we first build a benchmark dataset containing 10K pairs of input and reference images for reference-guided inpainting. Then we adopt an encoder-decoder structure to separately infer the texture and structure features of the input image considering their pattern discrepancy of texture and structure during inpainting. A feature alignment module is further designed to refine these features of the input image with the guidance of a reference image. Both quantitative and qualitative evaluations demonstrate the superiority of our method over the state-of-the-art methods in terms of completing complex holes.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0749982376,
        "newsscientist":0.1084237782,
        "technologyreview":0.1600536181,
        "venturebeat":0.1310079673,
        "wired":0.1206261341,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14498v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659075963000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13235v1",
        "predicted_newsworthiness":0.3600877894,
        "title":"Mid-level Representation Enhancement and Graph Embedded Uncertainty Suppressing for Facial Expression Recognition",
        "summary":"Facial expression is an essential factor in conveying human emotional states and intentions. Although remarkable advancement has been made in facial expression recognition (FER) task, challenges due to large variations of expression patterns and unavoidable data uncertainties still remain. In this paper, we propose mid-level representation enhancement (MRE) and graph embedded uncertainty suppressing (GUS) addressing these issues. On one hand, MRE is introduced to avoid expression representation learning being dominated by a limited number of highly discriminative patterns. On the other hand, GUS is introduced to suppress the feature ambiguity in the representation space. The proposed method not only has stronger generalization capability to handle different variations of expression patterns but also more robustness to capture expression representations. Experimental evaluation on Aff-Wild2 have verified the effectiveness of the proposed method.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0898036901,
        "newsscientist":0.1162657567,
        "technologyreview":0.1930066386,
        "venturebeat":0.1679249261,
        "wired":0.1305162829,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13235v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658885127000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01903v1",
        "predicted_newsworthiness":0.3596469547,
        "title":"Neural Dynamic Movement Primitives -- a survey",
        "summary":"One of the most important challenges in robotics is producing accurate trajectories and controlling their dynamic parameters so that the robots can perform different tasks. The ability to provide such motion control is closely related to how such movements are encoded. Advances on deep learning have had a strong repercussion in the development of novel approaches for Dynamic Movement Primitives. In this work, we survey scientific literature related to Neural Dynamic Movement Primitives, to complement existing surveys on Dynamic Movement Primitives.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.075815489,
        "newsscientist":0.14790305,
        "technologyreview":0.2352466333,
        "venturebeat":0.1880206213,
        "wired":0.1595990567,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01903v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai"
        ],
        "published":1659514268000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.14338v1",
        "predicted_newsworthiness":0.3596220663,
        "title":"Self-Supervised Hypergraph Transformer for Recommender Systems",
        "summary":"Graph Neural Networks (GNNs) have been shown as promising solutions for collaborative filtering (CF) with the modeling of user-item interaction graphs. The key idea of existing GNN-based recommender systems is to recursively perform the message passing along the user-item interaction edge for refining the encoded embeddings. Despite their effectiveness, however, most of the current recommendation models rely on sufficient and high-quality training data, such that the learned representations can well capture accurate user preference. User behavior data in many practical recommendation scenarios is often noisy and exhibits skewed distribution, which may result in suboptimal representation performance in GNN-based models. In this paper, we propose SHT, a novel Self-Supervised Hypergraph Transformer framework (SHT) which augments user representations by exploring the global collaborative relationships in an explicit way. Specifically, we first empower the graph neural CF paradigm to maintain global collaborative effects among users and items with a hypergraph transformer network. With the distilled global context, a cross-view generative self-supervised learning component is proposed for data augmentation over the user-item interaction graph, so as to enhance the robustness of recommender systems. Extensive experiments demonstrate that SHT can significantly improve the performance over various state-of-the-art baselines. Further ablation studies show the superior representation ability of our SHT recommendation framework in alleviating the data sparsity and noise issues. The source code and evaluation datasets are available at: https:\/\/github.com\/akaxlh\/SHT.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1067578918,
        "newsscientist":0.1353606172,
        "technologyreview":0.2192702652,
        "venturebeat":0.2226587219,
        "wired":0.1906680864,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14338v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.ai"
        ],
        "published":1659033630000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.00119v1",
        "predicted_newsworthiness":0.3592563218,
        "title":"DAS: Densely-Anchored Sampling for Deep Metric Learning",
        "summary":"Deep Metric Learning (DML) serves to learn an embedding function to project semantically similar data into nearby embedding space and plays a vital role in many applications, such as image retrieval and face recognition. However, the performance of DML methods often highly depends on sampling methods to choose effective data from the embedding space in the training. In practice, the embeddings in the embedding space are obtained by some deep models, where the embedding space is often with barren area due to the absence of training points, resulting in so called \"missing embedding\" issue. This issue may impair the sample quality, which leads to degenerated DML performance. In this work, we investigate how to alleviate the \"missing embedding\" issue to improve the sampling quality and achieve effective DML. To this end, we propose a Densely-Anchored Sampling (DAS) scheme that considers the embedding with corresponding data point as \"anchor\" and exploits the anchor's nearby embedding space to densely produce embeddings without data points. Specifically, we propose to exploit the embedding space around single anchor with Discriminative Feature Scaling (DFS) and multiple anchors with Memorized Transformation Shifting (MTS). In this way, by combing the embeddings with and without data points, we are able to provide more embeddings to facilitate the sampling process thus boosting the performance of DML. Our method is effortlessly integrated into existing DML frameworks and improves them without bells and whistles. Extensive experiments on three benchmark datasets demonstrate the superiority of our method.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0762802397,
        "newsscientist":0.1009944576,
        "technologyreview":0.1783658634,
        "venturebeat":0.1676767159,
        "wired":0.1389044154,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00119v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659146866000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14422v1",
        "predicted_newsworthiness":0.3590077945,
        "title":"Neural-Guided RuntimePrediction of Planners for Improved Motion and Task Planning with Graph Neural Networks",
        "summary":"The past decade has amply demonstrated the remarkable functionality that can be realized by learning complex input\/output relationships. Algorithmically, one of the most important and opaque relationships is that between a problem's structure and an effective solution method. Here, we quantitatively connect the structure of a planning problem to the performance of a given sampling-based motion planning (SBMP) algorithm. We demonstrate that the geometric relationships of motion planning problems can be well captured by graph neural networks (GNNs) to predict SBMP runtime. By using an algorithm portfolio we show that GNN predictions of runtime on particular problems can be leveraged to accelerate online motion planning in both navigation and manipulation tasks. Moreover, the problem-to-runtime map can be inverted to identify subproblems easier to solve by particular SBMPs. We provide a motivating example of how this knowledge may be used to improve integrated task and motion planning on simulated examples. These successes rely on the relational structure of GNNs to capture scalable generalization from low-dimensional navigation tasks to high degree-of-freedom manipulation tasks in 3d environments.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.073656546,
        "newsscientist":0.1312776733,
        "technologyreview":0.2277133943,
        "venturebeat":0.1997111908,
        "wired":0.1747557338,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14422v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659056425000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13686v1",
        "predicted_newsworthiness":0.3586712995,
        "title":"Shift-tolerant Perceptual Similarity Metric",
        "summary":"Existing perceptual similarity metrics assume an image and its reference are well aligned. As a result, these metrics are often sensitive to a small alignment error that is imperceptible to the human eyes. This paper studies the effect of small misalignment, specifically a small shift between the input and reference image, on existing metrics, and accordingly develops a shift-tolerant similarity metric. This paper builds upon LPIPS, a widely used learned perceptual similarity metric, and explores architectural design considerations to make it robust against imperceptible misalignment. Specifically, we study a wide spectrum of neural network elements, such as anti-aliasing filtering, pooling, striding, padding, and skip connection, and discuss their roles in making a robust metric. Based on our studies, we develop a new deep neural network-based perceptual similarity metric. Our experiments show that our metric is tolerant to imperceptible shifts while being consistent with the human similarity judgment.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.088460001,
        "newsscientist":0.1450280437,
        "technologyreview":0.2311112558,
        "venturebeat":0.2158522325,
        "wired":0.1740025926,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13686v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658944504000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00898v1",
        "predicted_newsworthiness":0.3580600035,
        "title":"Joint covariate-alignment and concept-alignment: a framework for domain generalization",
        "summary":"In this paper, we propose a novel domain generalization (DG) framework based on a new upper bound to the risk on the unseen domain. Particularly, our framework proposes to jointly minimize both the covariate-shift as well as the concept-shift between the seen domains for a better performance on the unseen domain. While the proposed approach can be implemented via an arbitrary combination of covariate-alignment and concept-alignment modules, in this work we use well-established approaches for distributional alignment namely, Maximum Mean Discrepancy (MMD) and covariance Alignment (CORAL), and use an Invariant Risk Minimization (IRM)-based approach for concept alignment. Our numerical results show that the proposed methods perform as well as or better than the state-of-the-art for domain generalization on several data sets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0857884987,
        "newsscientist":0.1024814059,
        "technologyreview":0.166086046,
        "venturebeat":0.1660979767,
        "wired":0.1139361937,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00898v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cv"
        ],
        "published":1659364775000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.09759v2",
        "predicted_newsworthiness":0.3571478126,
        "title":"Task-adaptive Spatial-Temporal Video Sampler for Few-shot Action Recognition",
        "summary":"A primary challenge faced in few-shot action recognition is inadequate video data for training. To address this issue, current methods in this field mainly focus on devising algorithms at the feature level while little attention is paid to processing input video data. Moreover, existing frame sampling strategies may omit critical action information in temporal and spatial dimensions, which further impacts video utilization efficiency. In this paper, we propose a novel video frame sampler for few-shot action recognition to address this issue, where task-specific spatial-temporal frame sampling is achieved via a temporal selector (TS) and a spatial amplifier (SA). Specifically, our sampler first scans the whole video at a small computational cost to obtain a global perception of video frames. The TS plays its role in selecting top-T frames that contribute most significantly and subsequently. The SA emphasizes the discriminative information of each frame by amplifying critical regions with the guidance of saliency maps. We further adopt task-adaptive learning to dynamically adjust the sampling strategy according to the episode task at hand. Both the implementations of TS and SA are differentiable for end-to-end optimization, facilitating seamless integration of our proposed sampler with most few-shot action recognition methods. Extensive experiments show a significant boost in the performances on various benchmarks including long-term videos.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0841257539,
        "newsscientist":0.1064297462,
        "technologyreview":0.1427380864,
        "venturebeat":0.1482824224,
        "wired":0.1383841498,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.09759v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658307852000,
        "published_hr":"Jul 20, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14287v1",
        "predicted_newsworthiness":0.3568751304,
        "title":"Depth Field Networks for Generalizable Multi-view Scene Representation",
        "summary":"Modern 3D computer vision leverages learning to boost geometric reasoning, mapping image data to classical structures such as cost volumes or epipolar constraints to improve matching. These architectures are specialized according to the particular problem, and thus require significant task-specific tuning, often leading to poor domain generalization performance. Recently, generalist Transformer architectures have achieved impressive results in tasks such as optical flow and depth estimation by encoding geometric priors as inputs rather than as enforced constraints. In this paper, we extend this idea and propose to learn an implicit, multi-view consistent scene representation, introducing a series of 3D data augmentation techniques as a geometric inductive prior to increase view diversity. We also show that introducing view synthesis as an auxiliary task further improves depth estimation. Our Depth Field Networks (DeFiNe) achieve state-of-the-art results in stereo and video depth estimation without explicit geometric constraints, and improve on zero-shot domain generalization by a wide margin.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0777627538,
        "newsscientist":0.1225410285,
        "technologyreview":0.2000684304,
        "venturebeat":0.1908261844,
        "wired":0.1616340224,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14287v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659031171000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12931v1",
        "predicted_newsworthiness":0.3566586028,
        "title":"Demystifying Graph Convolution with a Simple Concatenation",
        "summary":"Graph convolution (GConv) is a widely used technique that has been demonstrated to be extremely effective for graph learning applications, most notably node categorization. On the other hand, many GConv-based models do not quantify the effect of graph topology and node features on performance, and are even surpassed by some models that do not consider graph structure or node properties. We quantify the information overlap between graph topology, node features, and labels in order to determine graph convolution's representation power in the node classification task. In this work, we first determine the linear separability of graph convoluted features using analysis of variance. Mutual information is used to acquire a better understanding of the possible non-linear relationship between graph topology, node features, and labels. Our theoretical analysis demonstrates that a simple and efficient graph operation that concatenates only graph topology and node properties consistently outperforms conventional graph convolution, especially in the heterophily case. Extensive empirical research utilizing a synthetic dataset and real-world benchmarks demonstrates that graph concatenation is a simple but more flexible alternative to graph convolution.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1052932906,
        "newsscientist":0.1377897179,
        "technologyreview":0.2184740016,
        "venturebeat":0.1998263782,
        "wired":0.1739687108,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12931v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658162373000,
        "published_hr":"Jul 18, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00150v1",
        "predicted_newsworthiness":0.3565947187,
        "title":"Learning Shadow Correspondence for Video Shadow Detection",
        "summary":"Video shadow detection aims to generate consistent shadow predictions among video frames. However, the current approaches suffer from inconsistent shadow predictions across frames, especially when the illumination and background textures change in a video. We make an observation that the inconsistent predictions are caused by the shadow feature inconsistency, i.e., the features of the same shadow regions show dissimilar proprieties among the nearby frames.In this paper, we present a novel Shadow-Consistent Correspondence method (SC-Cor) to enhance pixel-wise similarity of the specific shadow regions across frames for video shadow detection. Our proposed SC-Cor has three main advantages. Firstly, without requiring the dense pixel-to-pixel correspondence labels, SC-Cor can learn the pixel-wise correspondence across frames in a weakly-supervised manner. Secondly, SC-Cor considers intra-shadow separability, which is robust to the variant textures and illuminations in videos. Finally, SC-Cor is a plug-and-play module that can be easily integrated into existing shadow detectors with no extra computational cost. We further design a new evaluation metric to evaluate the temporal stability of the video shadow detection results. Experimental results show that SC-Cor outperforms the prior state-of-the-art method, by 6.51% on IoU and 3.35% on the newly introduced temporal stability metric.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0802166017,
        "newsscientist":0.1240670973,
        "technologyreview":0.1553292871,
        "venturebeat":0.1590379184,
        "wired":0.130292757,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00150v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659162642000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01957v1",
        "predicted_newsworthiness":0.3565579169,
        "title":"PolarMOT: How Far Can Geometric Relations Take Us in 3D Multi-Object Tracking?",
        "summary":"Most (3D) multi-object tracking methods rely on appearance-based cues for data association. By contrast, we investigate how far we can get by only encoding geometric relationships between objects in 3D space as cues for data-driven data association. We encode 3D detections as nodes in a graph, where spatial and temporal pairwise relations among objects are encoded via localized polar coordinates on graph edges. This representation makes our geometric relations invariant to global transformations and smooth trajectory changes, especially under non-holonomic motion. This allows our graph neural network to learn to effectively encode temporal and spatial interactions and fully leverage contextual and motion cues to obtain final scene interpretation by posing data association as edge classification. We establish a new state-of-the-art on nuScenes dataset and, more importantly, show that our method, PolarMOT, generalizes remarkably well across different locations (Boston, Singapore, Karlsruhe) and datasets (nuScenes and KITTI).",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0967687336,
        "newsscientist":0.1444727903,
        "technologyreview":0.2042216437,
        "venturebeat":0.1930561285,
        "wired":0.1682121754,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01957v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659521216000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01161v1",
        "predicted_newsworthiness":0.3565531413,
        "title":"Pose Uncertainty Aware Movement Synchrony Estimation via Spatial-Temporal Graph Transformer",
        "summary":"Movement synchrony reflects the coordination of body movements between interacting dyads. The estimation of movement synchrony has been automated by powerful deep learning models such as transformer networks. However, instead of designing a specialized network for movement synchrony estimation, previous transformer-based works broadly adopted architectures from other tasks such as human activity recognition. Therefore, this paper proposed a skeleton-based graph transformer for movement synchrony estimation. The proposed model applied ST-GCN, a spatial-temporal graph convolutional neural network for skeleton feature extraction, followed by a spatial transformer for spatial feature generation. The spatial transformer is guided by a uniquely designed joint position embedding shared between the same joints of interacting individuals. Besides, we incorporated a temporal similarity matrix in temporal attention computation considering the periodic intrinsic of body movements. In addition, the confidence score associated with each joint reflects the uncertainty of a pose, while previous works on movement synchrony estimation have not sufficiently emphasized this point. Since transformer networks demand a significant amount of data to train, we constructed a dataset for movement synchrony estimation using Human3.6M, a benchmark dataset for human activity recognition, and pretrained our model on it using contrastive learning. We further applied knowledge distillation to alleviate information loss introduced by pose detector failure in a privacy-preserving way. We compared our method with representative approaches on PT13, a dataset collected from autism therapy interventions. Our method achieved an overall accuracy of 88.98% and surpassed its counterparts by a wide margin while maintaining data privacy.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1176440266,
        "newsscientist":0.166499877,
        "technologyreview":0.224034454,
        "venturebeat":0.2014254961,
        "wired":0.1713075175,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01161v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.hc"
        ],
        "published":1659393332000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01820v1",
        "predicted_newsworthiness":0.3547447736,
        "title":"Link Prediction on Heterophilic Graphs via Disentangled Representation Learning",
        "summary":"Link prediction is an important task that has wide applications in various domains. However, the majority of existing link prediction approaches assume the given graph follows homophily assumption, and designs similarity-based heuristics or representation learning approaches to predict links. However, many real-world graphs are heterophilic graphs, where the homophily assumption does not hold, which challenges existing link prediction methods. Generally, in heterophilic graphs, there are many latent factors causing the link formation, and two linked nodes tend to be similar in one or two factors but might be dissimilar in other factors, leading to low overall similarity. Thus, one way is to learn disentangled representation for each node with each vector capturing the latent representation of a node on one factor, which paves a way to model the link formation in heterophilic graphs, resulting in better node representation learning and link prediction performance. However, the work on this is rather limited. Therefore, in this paper, we study a novel problem of exploring disentangled representation learning for link prediction on heterophilic graphs. We propose a novel framework DisenLink which can learn disentangled representations by modeling the link formation and perform factor-aware message-passing to facilitate link prediction. Extensive experiments on 13 real-world datasets demonstrate the effectiveness of DisenLink for link prediction on both heterophilic and hemophiliac graphs. Our codes are available at https:\/\/github.com\/sjz5202\/DisenLink",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1003642637,
        "newsscientist":0.1450890079,
        "technologyreview":0.203066942,
        "venturebeat":0.1806262166,
        "wired":0.1550257736,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01820v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.si"
        ],
        "published":1659494906000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01287v1",
        "predicted_newsworthiness":0.354645774,
        "title":"Multiview Regenerative Morphing with Dual Flows",
        "summary":"This paper aims to address a new task of image morphing under a multiview setting, which takes two sets of multiview images as the input and generates intermediate renderings that not only exhibit smooth transitions between the two input sets but also ensure visual consistency across different views at any transition state. To achieve this goal, we propose a novel approach called Multiview Regenerative Morphing that formulates the morphing process as an optimization to solve for rigid transformation and optimal-transport interpolation. Given the multiview input images of the source and target scenes, we first learn a volumetric representation that models the geometry and appearance for each scene to enable the rendering of novel views. Then, the morphing between the two scenes is obtained by solving optimal transport between the two volumetric representations in Wasserstein metrics. Our approach does not rely on user-specified correspondences or 2D\/3D input meshes, and we do not assume any predefined categories of the source and target scenes. The proposed view-consistent interpolation scheme directly works on multiview images to yield a novel and visually plausible effect of multiview free-form morphing.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0759940024,
        "newsscientist":0.1213534563,
        "technologyreview":0.1427737764,
        "venturebeat":0.1614786256,
        "wired":0.1435128441,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01287v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659424968000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11464v1",
        "predicted_newsworthiness":0.3529342512,
        "title":"Learning Object Placement via Dual-path Graph Completion",
        "summary":"Object placement aims to place a foreground object over a background image with a suitable location and size. In this work, we treat object placement as a graph completion problem and propose a novel graph completion module (GCM). The background scene is represented by a graph with multiple nodes at different spatial locations with various receptive fields. The foreground object is encoded as a special node that should be inserted at a reasonable place in this graph. We also design a dual-path framework upon the structure of GCM to fully exploit annotated composite images. With extensive experiments on OPA dataset, our method proves to significantly outperform existing methods in generating plausible object placement without loss of diversity.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0953540145,
        "newsscientist":0.1424413802,
        "technologyreview":0.2100797347,
        "venturebeat":0.1979295415,
        "wired":0.1777268524,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11464v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658565579000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00306v1",
        "predicted_newsworthiness":0.3524613349,
        "title":"Doubly Deformable Aggregation of Covariance Matrices for Few-shot Segmentation",
        "summary":"Training semantic segmentation models with few annotated samples has great potential in various real-world applications. For the few-shot segmentation task, the main challenge is how to accurately measure the semantic correspondence between the support and query samples with limited training data. To address this problem, we propose to aggregate the learnable covariance matrices with a deformable 4D Transformer to effectively predict the segmentation map. Specifically, in this work, we first devise a novel hard example mining mechanism to learn covariance kernels for the Gaussian process. The learned covariance kernel functions have great advantages over existing cosine similarity-based methods in correspondence measurement. Based on the learned covariance kernels, an efficient doubly deformable 4D Transformer module is designed to adaptively aggregate feature similarity maps into segmentation results. By combining these two designs, the proposed method can not only set new state-of-the-art performance on public benchmarks, but also converge extremely faster than existing methods. Experiments on three public datasets have demonstrated the effectiveness of our method.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0697212693,
        "newsscientist":0.130705812,
        "technologyreview":0.202897353,
        "venturebeat":0.1936029053,
        "wired":0.1478635895,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00306v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659213698000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01853v1",
        "predicted_newsworthiness":0.3524509227,
        "title":"Robust Graph Neural Networks using Weighted Graph Laplacian",
        "summary":"Graph neural network (GNN) is achieving remarkable performances in a variety of application domains. However, GNN is vulnerable to noise and adversarial attacks in input data. Making GNN robust against noises and adversarial attacks is an important problem. The existing defense methods for GNNs are computationally demanding and are not scalable. In this paper, we propose a generic framework for robustifying GNN known as Weighted Laplacian GNN (RWL-GNN). The method combines Weighted Graph Laplacian learning with the GNN implementation. The proposed method benefits from the positive semi-definiteness property of Laplacian matrix, feature smoothness, and latent features via formulating a unified optimization framework, which ensures the adversarial\/noisy edges are discarded and connections in the graph are appropriately weighted. For demonstration, the experiments are conducted with Graph convolutional neural network(GCNN) architecture, however, the proposed framework is easily amenable to any existing GNN architecture. The simulation results with benchmark dataset establish the efficacy of the proposed method, both in accuracy and computational efficiency. Code can be accessed at https:\/\/github.com\/Bharat-Runwal\/RWL-GNN.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1134920787,
        "newsscientist":0.1586123107,
        "technologyreview":0.2620418381,
        "venturebeat":0.226932091,
        "wired":0.1856513077,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01853v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659504995000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13607v1",
        "predicted_newsworthiness":0.3523152614,
        "title":"Neural Radiance Transfer Fields for Relightable Novel-view Synthesis with Global Illumination",
        "summary":"Given a set of images of a scene, the re-rendering of this scene from novel views and lighting conditions is an important and challenging problem in Computer Vision and Graphics. On the one hand, most existing works in Computer Vision usually impose many assumptions regarding the image formation process, e.g. direct illumination and predefined materials, to make scene parameter estimation tractable. On the other hand, mature Computer Graphics tools allow modeling of complex photo-realistic light transport given all the scene parameters. Combining these approaches, we propose a method for scene relighting under novel views by learning a neural precomputed radiance transfer function, which implicitly handles global illumination effects using novel environment maps. Our method can be solely supervised on a set of real images of the scene under a single unknown lighting condition. To disambiguate the task during training, we tightly integrate a differentiable path tracer in the training process and propose a combination of a synthesized OLAT and a real image loss. Results show that the recovered disentanglement of scene parameters improves significantly over the current state of the art and, thus, also our re-rendering results are more realistic and accurate.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0846911502,
        "newsscientist":0.1364573513,
        "technologyreview":0.1836297267,
        "venturebeat":0.1725596401,
        "wired":0.1514868586,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13607v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658938068000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14539v1",
        "predicted_newsworthiness":0.3518348548,
        "title":"Contrastive Pre-training of Spatial-Temporal Trajectory Embeddings",
        "summary":"Pre-training trajectory embeddings is a fundamental and critical procedure in spatial-temporal trajectory mining, and is beneficial for a wide range of downstream tasks. The key for generating effective trajectory embeddings is to extract high-level travel semantics from trajectories, including movement patterns and travel purposes, with consideration of the trajectories' long-term spatial-temporal correlations. Despite the existing efforts, there are still major challenges in pre-training trajectory embeddings. First, commonly used generative pretext tasks are not suitable for extracting high-level semantics from trajectories. Second, existing data augmentation methods fit badly on trajectory datasets. Third, current encoder designs fail to fully incorporate long-term spatial-temporal correlations hidden in trajectories. To tackle these challenges, we propose a novel Contrastive Spatial-Temporal Trajectory Embedding (CSTTE) model for learning comprehensive trajectory embeddings. CSTTE adopts the contrastive learning framework so that its pretext task is robust to noise. A specially designed data augmentation method for trajectories is coupled with the contrastive pretext task to preserve the high-level travel semantics. We also build an efficient spatial-temporal trajectory encoder to efficiently and comprehensively model the long-term spatial-temporal correlations in trajectories. Extensive experiments on two downstream tasks and three real-world datasets prove the superiority of our model compared with the existing trajectory embedding methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1283248456,
        "newsscientist":0.1456919566,
        "technologyreview":0.1847381802,
        "venturebeat":0.1851194895,
        "wired":0.1908111219,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14539v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659082580000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11761v1",
        "predicted_newsworthiness":0.3512932368,
        "title":"SGAT: Simplicial Graph Attention Network",
        "summary":"Heterogeneous graphs have multiple node and edge types and are semantically richer than homogeneous graphs. To learn such complex semantics, many graph neural network approaches for heterogeneous graphs use metapaths to capture multi-hop interactions between nodes. Typically, features from non-target nodes are not incorporated into the learning procedure. However, there can be nonlinear, high-order interactions involving multiple nodes or edges. In this paper, we present Simplicial Graph Attention Network (SGAT), a simplicial complex approach to represent such high-order interactions by placing features from non-target nodes on the simplices. We then use attention mechanisms and upper adjacencies to generate representations. We empirically demonstrate the efficacy of our approach with node classification tasks on heterogeneous graph datasets and further show SGAT's ability in extracting structural information by employing random node features. Numerical experiments indicate that SGAT performs better than other current state-of-the-art heterogeneous graph learning methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0947709524,
        "newsscientist":0.1214373207,
        "technologyreview":0.2071108923,
        "venturebeat":0.1942997405,
        "wired":0.1483848187,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11761v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658676041000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13807v1",
        "predicted_newsworthiness":0.3512468263,
        "title":"Pose-NDF: Modeling Human Pose Manifolds with Neural Distance Fields",
        "summary":"We present Pose-NDF, a continuous model for plausible human poses based on neural distance fields (NDFs). Pose or motion priors are important for generating realistic new poses and for reconstructing accurate poses from noisy or partial observations. Pose-NDF learns a manifold of plausible poses as the zero level set of a neural implicit function, extending the idea of modeling implicit surfaces in 3D to the high-dimensional domain SO(3)^K, where a human pose is defined by a single data point, represented by K quaternions. The resulting high-dimensional implicit function can be differentiated with respect to the input poses and thus can be used to project arbitrary poses onto the manifold by using gradient descent on the set of 3-dimensional hyperspheres. In contrast to previous VAE-based human pose priors, which transform the pose space into a Gaussian distribution, we model the actual pose manifold, preserving the distances between poses. We demonstrate that PoseNDF outperforms existing state-of-the-art methods as a prior in various downstream tasks, ranging from denoising real-world human mocap data, pose recovery from occluded data to 3D pose reconstruction from images. Furthermore, we show that it can be used to generate more diverse poses by random sampling and projection than VAE-based methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0955670548,
        "newsscientist":0.1370271028,
        "technologyreview":0.1859512849,
        "venturebeat":0.1829579368,
        "wired":0.1547063,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13807v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658958407000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12208v1",
        "predicted_newsworthiness":0.3510958142,
        "title":"Series2Graph: Graph-based Subsequence Anomaly Detection for Time Series",
        "summary":"Subsequence anomaly detection in long sequences is an important problem with applications in a wide range of domains. However, the approaches proposed so far in the literature have severe limitations: they either require prior domain knowledge used to design the anomaly discovery algorithms, or become cumbersome and expensive to use in situations with recurrent anomalies of the same type. In this work, we address these problems, and propose an unsupervised method suitable for domain agnostic subsequence anomaly detection. Our method, Series2Graph, is based on a graph representation of a novel low-dimensionality embedding of subsequences. Series2Graph needs neither labeled instances (like supervised techniques) nor anomaly-free data (like zero-positive learning techniques), and identifies anomalies of varying lengths. The experimental results, on the largest set of synthetic and real datasets used to date, demonstrate that the proposed approach correctly identifies single and recurrent anomalies without any prior knowledge of their characteristics, outperforming by a large margin several competing approaches in accuracy, while being up to orders of magnitude faster. This paper has appeared in VLDB 2020.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1096912619,
        "newsscientist":0.1580346273,
        "technologyreview":0.201809385,
        "venturebeat":0.2038601028,
        "wired":0.1635455856,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12208v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658757343000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13529v1",
        "predicted_newsworthiness":0.3508799595,
        "title":"A Variational AutoEncoder for Transformers with Nonparametric Variational Information Bottleneck",
        "summary":"We propose a VAE for Transformers by developing a variational information bottleneck regulariser for Transformer embeddings. We formalise the embedding space of Transformer encoders as mixture probability distributions, and use Bayesian nonparametrics to derive a nonparametric variational information bottleneck (NVIB) for such attention-based embeddings. The variable number of mixture components supported by nonparametric methods captures the variable number of vectors supported by attention, and the exchangeability of our nonparametric distributions captures the permutation invariance of attention. This allows NVIB to regularise the number of vectors accessible with attention, as well as the amount of information in individual vectors. By regularising the cross-attention of a Transformer encoder-decoder with NVIB, we propose a nonparametric variational autoencoder (NVAE). Initial experiments on training a NVAE on natural language text show that the induced embedding space has the desired properties of a VAE for Transformers.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0801944091,
        "newsscientist":0.1066490906,
        "technologyreview":0.1797000651,
        "venturebeat":0.1600291053,
        "wired":0.1306683083,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13529v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cl"
        ],
        "published":1658930363000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00993v1",
        "predicted_newsworthiness":0.3506656253,
        "title":"MULTIPAR: Supervised Irregular Tensor Factorization with Multi-task Learning",
        "summary":"Tensor factorization has received increasing interest due to its intrinsic ability to capture latent factors in multi-dimensional data with many applications such as recommender systems and Electronic Health Records (EHR) mining. PARAFAC2 and its variants have been proposed to address irregular tensors where one of the tensor modes is not aligned, e.g., different users in recommender systems or patients in EHRs may have different length of records. PARAFAC2 has been successfully applied on EHRs for extracting meaningful medical concepts (phenotypes). Despite recent advancements, current models' predictability and interpretability are not satisfactory, which limits its utility for downstream analysis. In this paper, we propose MULTIPAR: a supervised irregular tensor factorization with multi-task learning. MULTIPAR is flexible to incorporate both static (e.g. in-hospital mortality prediction) and continuous or dynamic (e.g. the need for ventilation) tasks. By supervising the tensor factorization with downstream prediction tasks and leveraging information from multiple related predictive tasks, MULTIPAR can yield not only more meaningful phenotypes but also better predictive performance for downstream tasks. We conduct extensive experiments on two real-world temporal EHR datasets to demonstrate that MULTIPAR is scalable and achieves better tensor fit with more meaningful subgroups and stronger predictive performance compared to existing state-of-the-art methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1304279954,
        "newsscientist":0.1551212894,
        "technologyreview":0.2319947431,
        "venturebeat":0.2462858988,
        "wired":0.1757978402,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00993v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659373643000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14466v1",
        "predicted_newsworthiness":0.3500904098,
        "title":"Towards Domain-agnostic Depth Completion",
        "summary":"Existing depth completion methods are often targeted at a specific sparse depth type, and generalize poorly across task domains. We present a method to complete sparse\/semi-dense, noisy, and potentially low-resolution depth maps obtained by various range sensors, including those in modern mobile phones, or by multi-view reconstruction algorithms. Our method leverages a data driven prior in the form of a single image depth prediction network trained on large-scale datasets, the output of which is used as an input to our model. We propose an effective training scheme where we simulate various sparsity patterns in typical task domains. In addition, we design two new benchmarks to evaluate the generalizability and the robustness of depth completion methods. Our simple method shows superior cross-domain generalization ability against state-of-the-art depth completion methods, introducing a practical solution to high quality depth capture on a mobile device. Code is available at: https:\/\/github.com\/YvanYin\/FillDepth.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0739189041,
        "newsscientist":0.1205949578,
        "technologyreview":0.179816051,
        "venturebeat":0.1913796088,
        "wired":0.169467345,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14466v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659067822000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12061v1",
        "predicted_newsworthiness":0.3496736101,
        "title":"Balancing Stability and Plasticity through Advanced Null Space in Continual Learning",
        "summary":"Continual learning is a learning paradigm that learns tasks sequentially with resources constraints, in which the key challenge is stability-plasticity dilemma, i.e., it is uneasy to simultaneously have the stability to prevent catastrophic forgetting of old tasks and the plasticity to learn new tasks well. In this paper, we propose a new continual learning approach, Advanced Null Space (AdNS), to balance the stability and plasticity without storing any old data of previous tasks. Specifically, to obtain better stability, AdNS makes use of low-rank approximation to obtain a novel null space and projects the gradient onto the null space to prevent the interference on the past tasks. To control the generation of the null space, we introduce a non-uniform constraint strength to further reduce forgetting. Furthermore, we present a simple but effective method, intra-task distillation, to improve the performance of the current task. Finally, we theoretically find that null space plays a key role in plasticity and stability, respectively. Experimental results show that the proposed method can achieve better performance compared to state-of-the-art continual learning approaches.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0709308643,
        "newsscientist":0.1217183688,
        "technologyreview":0.1993289193,
        "venturebeat":0.1619778388,
        "wired":0.129080123,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12061v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658747062000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00380v1",
        "predicted_newsworthiness":0.3482007737,
        "title":"Less is More: Consistent Video Depth Estimation with Masked Frames Modeling",
        "summary":"Temporal consistency is the key challenge of video depth estimation. Previous works are based on additional optical flow or camera poses, which is time-consuming. By contrast, we derive consistency with less information. Since videos inherently exist with heavy temporal redundancy, a missing frame could be recovered from neighboring ones. Inspired by this, we propose the frame masking network (FMNet), a spatial-temporal transformer network predicting the depth of masked frames based on their neighboring frames. By reconstructing masked temporal features, the FMNet can learn intrinsic inter-frame correlations, which leads to consistency. Compared with prior arts, experimental results demonstrate that our approach achieves comparable spatial accuracy and higher temporal consistency without any additional information. Our work provides a new perspective on consistent video depth estimation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0816684503,
        "newsscientist":0.1158949129,
        "technologyreview":0.161138434,
        "venturebeat":0.1681309265,
        "wired":0.145931088,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00380v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659251480000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.02207v1",
        "predicted_newsworthiness":0.3476554498,
        "title":"Robot Learning from Demonstration Using Elastic Maps",
        "summary":"Learning from Demonstration (LfD) is a popular method of reproducing and generalizing robot skills from human-provided demonstrations. In this paper, we propose a novel optimization-based LfD method that encodes demonstrations as elastic maps. An elastic map is a graph of nodes connected through a mesh of springs. We build a skill model by fitting an elastic map to the set of demonstrations. The formulated optimization problem in our approach includes three objectives with natural and physical interpretations. The main term rewards the mean squared error in the Cartesian coordinate. The second term penalizes the non-equidistant distribution of points resulting in the optimum total length of the trajectory. The third term rewards smoothness while penalizing nonlinearity. These quadratic objectives form a convex problem that can be solved efficiently with local optimizers. We examine nine methods for constructing and weighting the elastic maps and study their performance in robotic tasks. We also evaluate the proposed method in several simulated and real-world experiments using a UR5e manipulator arm, and compare it to other LfD approaches to demonstrate its benefits and flexibility across a variety of metrics.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0858014979,
        "newsscientist":0.1439052223,
        "technologyreview":0.2238564677,
        "venturebeat":0.1619443429,
        "wired":0.1712011947,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02207v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659544927000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.11467v1",
        "predicted_newsworthiness":0.3469704746,
        "title":"CompNVS: Novel View Synthesis with Scene Completion",
        "summary":"We introduce a scalable framework for novel view synthesis from RGB-D images with largely incomplete scene coverage. While generative neural approaches have demonstrated spectacular results on 2D images, they have not yet achieved similar photorealistic results in combination with scene completion where a spatial 3D scene understanding is essential. To this end, we propose a generative pipeline performing on a sparse grid-based neural scene representation to complete unobserved scene parts via a learned distribution of scenes in a 2.5D-3D-2.5D manner. We process encoded image features in 3D space with a geometry completion network and a subsequent texture inpainting network to extrapolate the missing area. Photorealistic image sequences can be finally obtained via consistency-relevant differentiable rendering. Comprehensive experiments show that the graphical outputs of our method outperform the state of the art, especially within unobserved scene parts.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0837182455,
        "newsscientist":0.1146495812,
        "technologyreview":0.1630825234,
        "venturebeat":0.1762972606,
        "wired":0.1589545821,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11467v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658566993000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00709v1",
        "predicted_newsworthiness":0.3464579975,
        "title":"Visual-Inertial SLAM with Tightly-Coupled Dropout-Tolerant GPS Fusion",
        "summary":"Robotic applications are continuously striving towards higher levels of autonomy. To achieve that goal, a highly robust and accurate state estimation is indispensable. Combining visual and inertial sensor modalities has proven to yield accurate and locally consistent results in short-term applications. Unfortunately, visual-inertial state estimators suffer from the accumulation of drift for long-term trajectories. To eliminate this drift, global measurements can be fused into the state estimation pipeline. The most known and widely available source of global measurements is the Global Positioning System (GPS). In this paper, we propose a novel approach that fully combines stereo Visual-Inertial Simultaneous Localisation and Mapping (SLAM), including visual loop closures, with the fusion of global sensor modalities in a tightly-coupled and optimisation-based framework. Incorporating measurement uncertainties, we provide a robust criterion to solve the global reference frame initialisation problem. Furthermore, we propose a loop-closure-like optimisation scheme to compensate drift accumulated during outages in receiving GPS signals. Experimental validation on datasets and in a real-world experiment demonstrates the robustness of our approach to GPS dropouts as well as its capability to estimate highly accurate and globally consistent trajectories compared to existing state-of-the-art methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0790795718,
        "newsscientist":0.1149859524,
        "technologyreview":0.1558060376,
        "venturebeat":0.1632863486,
        "wired":0.1637627338,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00709v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659346682000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.11484v1",
        "predicted_newsworthiness":0.3459697384,
        "title":"GraphFit: Learning Multi-scale Graph-Convolutional Representation for Point Cloud Normal Estimation",
        "summary":"We propose a precise and efficient normal estimation method that can deal with noise and nonuniform density for unstructured 3D point clouds. Unlike existing approaches that directly take patches and ignore the local neighborhood relationships, which make them susceptible to challenging regions such as sharp edges, we propose to learn graph convolutional feature representation for normal estimation, which emphasizes more local neighborhood geometry and effectively encodes intrinsic relationships. Additionally, we design a novel adaptive module based on the attention mechanism to integrate point features with their neighboring features, hence further enhancing the robustness of the proposed normal estimator against point density variations. To make it more distinguishable, we introduce a multi-scale architecture in the graph block to learn richer geometric features. Our method outperforms competitors with the state-of-the-art accuracy on various benchmark datasets, and is quite robust against noise, outliers, as well as the density variations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0958553398,
        "newsscientist":0.1327975243,
        "technologyreview":0.1918773103,
        "venturebeat":0.184732801,
        "wired":0.1585695131,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11484v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658572166000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13440v1",
        "predicted_newsworthiness":0.3457892069,
        "title":"Iterative Scene Graph Generation",
        "summary":"The task of scene graph generation entails identifying object entities and their corresponding interaction predicates in a given image (or video). Due to the combinatorially large solution space, existing approaches to scene graph generation assume certain factorization of the joint distribution to make the estimation feasible (e.g., assuming that objects are conditionally independent of predicate predictions). However, this fixed factorization is not ideal under all scenarios (e.g., for images where an object entailed in interaction is small and not discernible on its own). In this work, we propose a novel framework for scene graph generation that addresses this limitation, as well as introduces dynamic conditioning on the image, using message passing in a Markov Random Field. This is implemented as an iterative refinement procedure wherein each modification is conditioned on the graph generated in the previous iteration. This conditioning across refinement steps allows joint reasoning over entities and relations. This framework is realized via a novel and end-to-end trainable transformer-based architecture. In addition, the proposed framework can improve existing approach performance. Through extensive experiments on Visual Genome and Action Genome benchmark datasets we show improved performance on the scene graph generation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0926321911,
        "newsscientist":0.1459426051,
        "technologyreview":0.2445496542,
        "venturebeat":0.2131129425,
        "wired":0.1688420922,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13440v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658918249000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11850v1",
        "predicted_newsworthiness":0.3456853849,
        "title":"Visual Perturbation-aware Collaborative Learning for Overcoming the Language Prior Problem",
        "summary":"Several studies have recently pointed that existing Visual Question Answering (VQA) models heavily suffer from the language prior problem, which refers to capturing superficial statistical correlations between the question type and the answer whereas ignoring the image contents. Numerous efforts have been dedicated to strengthen the image dependency by creating the delicate models or introducing the extra visual annotations. However, these methods cannot sufficiently explore how the visual cues explicitly affect the learned answer representation, which is vital for language reliance alleviation. Moreover, they generally emphasize the class-level discrimination of the learned answer representation, which overlooks the more fine-grained instance-level patterns and demands further optimization. In this paper, we propose a novel collaborative learning scheme from the viewpoint of visual perturbation calibration, which can better investigate the fine-grained visual effects and mitigate the language prior problem by learning the instance-level characteristics. Specifically, we devise a visual controller to construct two sorts of curated images with different perturbation extents, based on which the collaborative learning of intra-instance invariance and inter-instance discrimination is implemented by two well-designed discriminators. Besides, we implement the information bottleneck modulator on latent space for further bias alleviation and representation calibration. We impose our visual perturbation-aware framework to three orthodox baselines and the experimental results on two diagnostic VQA-CP benchmark datasets evidently demonstrate its effectiveness. In addition, we also justify its robustness on the balanced VQA benchmark.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0942060931,
        "newsscientist":0.1302286303,
        "technologyreview":0.220986407,
        "venturebeat":0.1988541424,
        "wired":0.1396076438,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11850v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658706652000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11790v1",
        "predicted_newsworthiness":0.3451071071,
        "title":"PatchRD: Detail-Preserving Shape Completion by Learning Patch Retrieval and Deformation",
        "summary":"This paper introduces a data-driven shape completion approach that focuses on completing geometric details of missing regions of 3D shapes. We observe that existing generative methods lack the training data and representation capacity to synthesize plausible, fine-grained details with complex geometry and topology. Our key insight is to copy and deform patches from the partial input to complete missing regions. This enables us to preserve the style of local geometric features, even if it drastically differs from the training data. Our fully automatic approach proceeds in two stages. First, we learn to retrieve candidate patches from the input shape. Second, we select and deform some of the retrieved candidates to seamlessly blend them into the complete shape. This method combines the advantages of the two most common completion methods: similarity-based single-instance completion, and completion by learning a shape space. We leverage repeating patterns by retrieving patches from the partial input, and learn global structural priors by using a neural network to guide the retrieval and deformation steps. Experimental results show our approach considerably outperforms baselines across multiple datasets and shape categories. Code and data are available at https:\/\/github.com\/GitBoSun\/PatchRD.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0734680559,
        "newsscientist":0.1211650199,
        "technologyreview":0.1642530541,
        "venturebeat":0.1327501578,
        "wired":0.1379622561,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11790v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658689149000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.09686v2",
        "predicted_newsworthiness":0.3450752409,
        "title":"Object-Compositional Neural Implicit Surfaces",
        "summary":"The neural implicit representation has shown its effectiveness in novel view synthesis and high-quality 3D reconstruction from multi-view images. However, most approaches focus on holistic scene representation yet ignore individual objects inside it, thus limiting potential downstream applications. In order to learn object-compositional representation, a few works incorporate the 2D semantic map as a cue in training to grasp the difference between objects. But they neglect the strong connections between object geometry and instance semantic information, which leads to inaccurate modeling of individual instance. This paper proposes a novel framework, ObjectSDF, to build an object-compositional neural implicit representation with high fidelity in 3D reconstruction and object representation. Observing the ambiguity of conventional volume rendering pipelines, we model the scene by combining the Signed Distance Functions (SDF) of individual object to exert explicit surface constraint. The key in distinguishing different instances is to revisit the strong association between an individual object's SDF and semantic label. Particularly, we convert the semantic information to a function of object SDF and develop a unified and compact representation for scene and objects. Experimental results show the superiority of ObjectSDF framework in representing both the holistic object-compositional scene and the individual instances. Code can be found at https:\/\/qianyiwu.github.io\/objectsdf\/",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0814944789,
        "newsscientist":0.1412188242,
        "technologyreview":0.1933783809,
        "venturebeat":0.1732907801,
        "wired":0.1543789744,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.09686v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658299084000,
        "published_hr":"Jul 20, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12163v1",
        "predicted_newsworthiness":0.3447934848,
        "title":"Multi-Scale RAFT: Combining Hierarchical Concepts for Learning-based Optical FLow Estimation",
        "summary":"Many classical and learning-based optical flow methods rely on hierarchical concepts to improve both accuracy and robustness. However, one of the currently most successful approaches -- RAFT -- hardly exploits such concepts. In this work, we show that multi-scale ideas are still valuable. More precisely, using RAFT as a baseline, we propose a novel multi-scale neural network that combines several hierarchical concepts within a single estimation framework. These concepts include (i) a partially shared coarse-to-fine architecture, (ii) multi-scale features, (iii) a hierarchical cost volume and (iv) a multi-scale multi-iteration loss. Experiments on MPI Sintel and KITTI clearly demonstrate the benefits of our approach. They show not only substantial improvements compared to RAFT, but also state-of-the-art results -- in particular in non-occluded regions. Code will be available at https:\/\/github.com\/cv-stuttgart\/MS_RAFT.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0680105479,
        "newsscientist":0.1366791114,
        "technologyreview":0.1873930452,
        "venturebeat":0.1713573805,
        "wired":0.1414896999,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12163v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658754210000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00789v1",
        "predicted_newsworthiness":0.3446531097,
        "title":"Self-supervised learning with rotation-invariant kernels",
        "summary":"A major paradigm for learning image representations in a self-supervised manner is to learn a model that is invariant to some predefined image transformations (cropping, blurring, color jittering, etc.), while regularizing the embedding distribution to avoid learning a degenerate solution. Our first contribution is to propose a general kernel framework to design a generic regularization loss that promotes the embedding distribution to be close to the uniform distribution on the hypersphere, with respect to the maximum mean discrepancy pseudometric. Our framework uses rotation-invariant kernels defined on the hypersphere, also known as dot-product kernels. Our second contribution is to show that this flexible kernel approach encompasses several existing self-supervised learning methods, including uniformity-based and information-maximization methods. Finally, by exploring empirically several kernel choices, our experiments demonstrate that using a truncated rotation-invariant kernel provides competitive results compared to state-of-the-art methods, and we show practical situations where our method benefits from the kernel trick to reduce computational complexity.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0782017875,
        "newsscientist":0.1315139672,
        "technologyreview":0.2129362503,
        "venturebeat":0.179310646,
        "wired":0.1556525555,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00789v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658995584000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14525v1",
        "predicted_newsworthiness":0.3444049977,
        "title":"Curriculum Learning for Data-Efficient Vision-Language Alignment",
        "summary":"Aligning image and text encoders from scratch using contrastive learning requires large amounts of paired image-text data. We alleviate this need by aligning individually pre-trained language and vision representation models using a much smaller amount of paired data, augmented with a curriculum learning algorithm to learn fine-grained vision-language alignments. TOnICS (Training with Ontology-Informed Contrastive Sampling) initially samples minibatches whose image-text pairs contain a wide variety of objects to learn object-level alignment, and progressively samples minibatches where all image-text pairs contain the same object to learn finer-grained contextual alignment. Aligning pre-trained BERT and VinVL models to each other using TOnICS outperforms CLIP on downstream zero-shot image retrieval while using less than 1% as much training data.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0746249054,
        "newsscientist":0.1009914018,
        "technologyreview":0.1893979819,
        "venturebeat":0.188655299,
        "wired":0.1296335703,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14525v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.cl",
            "cs.lg"
        ],
        "published":1659080756000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13262v1",
        "predicted_newsworthiness":0.3441290029,
        "title":"Factorial User Modeling with Hierarchical Graph Neural Network for Enhanced Sequential Recommendation",
        "summary":"Most sequential recommendation (SR) systems employing graph neural networks (GNNs) only model a user's interaction sequence as a flat graph without hierarchy, overlooking diverse factors in the user's preference. Moreover, the timespan between interacted items is not sufficiently utilized by previous models, restricting SR performance gains. To address these problems, we propose a novel SR system employing a hierarchical graph neural network (HGNN) to model factorial user preferences. Specifically, a timespan-aware sequence graph (TSG) for the target user is first constructed with the timespan among interacted items. Next, all original nodes in TSG are softly clustered into factor nodes, each of which represents a certain factor of the user's preference. At last, all factor nodes' representations are used together to predict SR results. Our extensive experiments upon two datasets justify that our HGNN-based factorial user modeling obtains better SR performance than the state-of-the-art SR models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0808800695,
        "newsscientist":0.1210508609,
        "technologyreview":0.1956890911,
        "venturebeat":0.2173190501,
        "wired":0.1854317678,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13262v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1658890570000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.12620v1",
        "predicted_newsworthiness":0.3438620959,
        "title":"Large-displacement 3D Object Tracking with Hybrid Non-local Optimization",
        "summary":"Optimization-based 3D object tracking is known to be precise and fast, but sensitive to large inter-frame displacements. In this paper we propose a fast and effective non-local 3D tracking method. Based on the observation that erroneous local minimum are mostly due to the out-of-plane rotation, we propose a hybrid approach combining non-local and local optimizations for different parameters, resulting in efficient non-local search in the 6D pose space. In addition, a precomputed robust contour-based tracking method is proposed for the pose optimization. By using long search lines with multiple candidate correspondences, it can adapt to different frame displacements without the need of coarse-to-fine search. After the pre-computation, pose updates can be conducted very fast, enabling the non-local optimization to run in real time. Our method outperforms all previous methods for both small and large displacements. For large displacements, the accuracy is greatly improved ($81.7\\% \\;\\text{v.s.}\\; 19.4\\%$). At the same time, real-time speed ($>$50fps) can be achieved with only CPU. The source code is available at \\url{https:\/\/github.com\/cvbubbles\/nonlocal-3dtracking}.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0459765738,
        "newsscientist":0.1034857809,
        "technologyreview":0.1472286934,
        "venturebeat":0.1585791516,
        "wired":0.1230405842,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12620v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658803871000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01006v1",
        "predicted_newsworthiness":0.3435725384,
        "title":"Multi-Document Summarization with Centroid-Based Pretraining",
        "summary":"In multi-document summarization (MDS), the input is a cluster of documents, and the output is the cluster summary. In this paper, we focus on pretraining objectives for MDS. Specifically, we introduce a simple pretraining objective of choosing the ROUGE-based centroid of each document cluster as a proxy for its summary. Our objective thus does not require human written summaries and can be used for pretraining on a dataset containing only clusters of documents. Through zero-shot and fully supervised experiments on multiple MDS datasets, we show that our model Centrum is better or comparable to a state-of-the-art model. We release our pretrained and finetuned models at https:\/\/github.com\/ratishsp\/centrum.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.081971569,
        "newsscientist":0.0903690822,
        "technologyreview":0.1349936459,
        "venturebeat":0.142397806,
        "wired":0.1168862764,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01006v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659374882000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00751v1",
        "predicted_newsworthiness":0.3433261967,
        "title":"CSDN: Cross-modal Shape-transfer Dual-refinement Network for Point Cloud Completion",
        "summary":"How will you repair a physical object with some missings? You may imagine its original shape from previously captured images, recover its overall (global) but coarse shape first, and then refine its local details. We are motivated to imitate the physical repair procedure to address point cloud completion. To this end, we propose a cross-modal shape-transfer dual-refinement network (termed CSDN), a coarse-to-fine paradigm with images of full-cycle participation, for quality point cloud completion. CSDN mainly consists of \"shape fusion\" and \"dual-refinement\" modules to tackle the cross-modal challenge. The first module transfers the intrinsic shape characteristics from single images to guide the geometry generation of the missing regions of point clouds, in which we propose IPAdaIN to embed the global features of both the image and the partial point cloud into completion. The second module refines the coarse output by adjusting the positions of the generated points, where the local refinement unit exploits the geometric relation between the novel and the input points by graph convolution, and the global constraint unit utilizes the input image to fine-tune the generated offset. Different from most existing approaches, CSDN not only explores the complementary information from images but also effectively exploits cross-modal data in the whole coarse-to-fine completion procedure. Experimental results indicate that CSDN performs favorably against ten competitors on the cross-modal benchmark.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0706843799,
        "newsscientist":0.1108142686,
        "technologyreview":0.1636106908,
        "venturebeat":0.1427369662,
        "wired":0.1270562713,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00751v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659352856000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00945v1",
        "predicted_newsworthiness":0.3432273064,
        "title":"DoF-NeRF: Depth-of-Field Meets Neural Radiance Fields",
        "summary":"Neural Radiance Field (NeRF) and its variants have exhibited great success on representing 3D scenes and synthesizing photo-realistic novel views. However, they are generally based on the pinhole camera model and assume all-in-focus inputs. This limits their applicability as images captured from the real world often have finite depth-of-field (DoF). To mitigate this issue, we introduce DoF-NeRF, a novel neural rendering approach that can deal with shallow DoF inputs and can simulate DoF effect. In particular, it extends NeRF to simulate the aperture of lens following the principles of geometric optics. Such a physical guarantee allows DoF-NeRF to operate views with different focus configurations. Benefiting from explicit aperture modeling, DoF-NeRF also enables direct manipulation of DoF effect by adjusting virtual aperture and focus parameters. It is plug-and-play and can be inserted into NeRF-based frameworks. Experiments on synthetic and real-world datasets show that, DoF-NeRF not only performs comparably with NeRF in the all-in-focus setting, but also can synthesize all-in-focus novel views conditioned on shallow DoF inputs. An interesting application of DoF-NeRF to DoF rendering is also demonstrated. The source code will be made available at https:\/\/github.com\/zijinwuzijin\/DoF-NeRF.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0848948465,
        "newsscientist":0.1347766875,
        "technologyreview":0.1943565127,
        "venturebeat":0.2087041964,
        "wired":0.1714666951,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00945v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659369194000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13320v1",
        "predicted_newsworthiness":0.3422164561,
        "title":"Generator Knows What Discriminator Should Learn in Unconditional GANs",
        "summary":"Recent methods for conditional image generation benefit from dense supervision such as segmentation label maps to achieve high-fidelity. However, it is rarely explored to employ dense supervision for unconditional image generation. Here we explore the efficacy of dense supervision in unconditional generation and find generator feature maps can be an alternative of cost-expensive semantic label maps. From our empirical evidences, we propose a new generator-guided discriminator regularization(GGDR) in which the generator feature maps supervise the discriminator to have rich semantic representations in unconditional generation. In specific, we employ an U-Net architecture for discriminator, which is trained to predict the generator feature maps given fake images as inputs. Extensive experiments on mulitple datasets show that our GGDR consistently improves the performance of baseline methods in terms of quantitative and qualitative aspects. Code is available at https:\/\/github.com\/naver-ai\/GGDR",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0759541106,
        "newsscientist":0.1165882286,
        "technologyreview":0.2098482416,
        "venturebeat":0.1763710357,
        "wired":0.1331585408,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13320v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658904566000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12045v1",
        "predicted_newsworthiness":0.3420468604,
        "title":"Online Reinforcement Learning for Periodic MDP",
        "summary":"We study learning in periodic Markov Decision Process(MDP), a special type of non-stationary MDP where both the state transition probabilities and reward functions vary periodically, under the average reward maximization setting. We formulate the problem as a stationary MDP by augmenting the state space with the period index, and propose a periodic upper confidence bound reinforcement learning-2 (PUCRL2) algorithm. We show that the regret of PUCRL2 varies linearly with the period and as sub-linear with the horizon length. Numerical results demonstrate the efficacy of PUCRL2.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0759920279,
        "newsscientist":0.1022670202,
        "technologyreview":0.1332473513,
        "venturebeat":0.141644151,
        "wired":0.1095772338,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12045v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658745429000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01817v1",
        "predicted_newsworthiness":0.3417662641,
        "title":"Neural Contourlet Network for Monocular 360 Depth Estimation",
        "summary":"For a monocular 360 image, depth estimation is a challenging because the distortion increases along the latitude. To perceive the distortion, existing methods devote to designing a deep and complex network architecture. In this paper, we provide a new perspective that constructs an interpretable and sparse representation for a 360 image. Considering the importance of the geometric structure in depth estimation, we utilize the contourlet transform to capture an explicit geometric cue in the spectral domain and integrate it with an implicit cue in the spatial domain. Specifically, we propose a neural contourlet network consisting of a convolutional neural network and a contourlet transform branch. In the encoder stage, we design a spatial-spectral fusion module to effectively fuse two types of cues. Contrary to the encoder, we employ the inverse contourlet transform with learned low-pass subbands and band-pass directional subbands to compose the depth in the decoder. Experiments on the three popular panoramic image datasets demonstrate that the proposed approach outperforms the state-of-the-art schemes with faster convergence. Code is available at https:\/\/github.com\/zhijieshen-bjtu\/Neural-Contourlet-Network-for-MODE.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0825958708,
        "newsscientist":0.1364793189,
        "technologyreview":0.1863104174,
        "venturebeat":0.2053120364,
        "wired":0.1693333894,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01817v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659493555000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01421v1",
        "predicted_newsworthiness":0.3406115363,
        "title":"T4DT: Tensorizing Time for Learning Temporal 3D Visual Data",
        "summary":"Unlike 2D raster images, there is no single dominant representation for 3D visual data processing. Different formats like point clouds, meshes, or implicit functions each have their strengths and weaknesses. Still, grid representations such as signed distance functions have attractive properties also in 3D. In particular, they offer constant-time random access and are eminently suitable for modern machine learning. Unfortunately, the storage size of a grid grows exponentially with its dimension. Hence they often exceed memory limits even at moderate resolution. This work explores various low-rank tensor formats, including the Tucker, tensor train, and quantics tensor train decompositions, to compress time-varying 3D data. Our method iteratively computes, voxelizes, and compresses each frame's truncated signed distance function and applies tensor rank truncation to condense all frames into a single, compressed tensor that represents the entire 4D scene. We show that low-rank tensor compression is extremely compact to store and query time-varying signed distance functions. It significantly reduces the memory footprint of 4D scenes while surprisingly preserving their geometric quality. Unlike existing iterative learning-based approaches like DeepSDF and NeRF, our method uses a closed-form algorithm with theoretical guarantees.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0929445774,
        "newsscientist":0.1456103516,
        "technologyreview":0.2086720407,
        "venturebeat":0.2302099359,
        "wired":0.1865601022,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01421v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659445028000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.09644v2",
        "predicted_newsworthiness":0.3383085719,
        "title":"Hierarchically Self-Supervised Transformer for Human Skeleton Representation Learning",
        "summary":"Despite the success of fully-supervised human skeleton sequence modeling, utilizing self-supervised pre-training for skeleton sequence representation learning has been an active field because acquiring task-specific skeleton annotations at large scales is difficult. Recent studies focus on learning video-level temporal and discriminative information using contrastive learning, but overlook the hierarchical spatial-temporal nature of human skeletons. Different from such superficial supervision at the video level, we propose a self-supervised hierarchical pre-training scheme incorporated into a hierarchical Transformer-based skeleton sequence encoder (Hi-TRS), to explicitly capture spatial, short-term, and long-term temporal dependencies at frame, clip, and video levels, respectively. To evaluate the proposed self-supervised pre-training scheme with Hi-TRS, we conduct extensive experiments covering three skeleton-based downstream tasks including action recognition, action detection, and motion prediction. Under both supervised and semi-supervised evaluation protocols, our method achieves the state-of-the-art performance. Additionally, we demonstrate that the prior knowledge learned by our model in the pre-training stage has strong transfer capability for different downstream tasks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0878635027,
        "newsscientist":0.1383668554,
        "technologyreview":0.1699533446,
        "venturebeat":0.1571978184,
        "wired":0.1382249703,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.09644v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658290865000,
        "published_hr":"Jul 19, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00955v1",
        "predicted_newsworthiness":0.3378229288,
        "title":"Large-Scale Product Retrieval with Weakly Supervised Representation Learning",
        "summary":"Large-scale weakly supervised product retrieval is a practically useful yet computationally challenging problem. This paper introduces a novel solution for the eBay Visual Search Challenge (eProduct) held at the Ninth Workshop on Fine-Grained Visual Categorisation workshop (FGVC9) of CVPR 2022. This competition presents two challenges: (a) E-commerce is a drastically fine-grained domain including many products with subtle visual differences; (b) A lacking of target instance-level labels for model training, with only coarse category labels and product titles available. To overcome these obstacles, we formulate a strong solution by a set of dedicated designs: (a) Instead of using text training data directly, we mine thousands of pseudo-attributes from product titles and use them as the ground truths for multi-label classification. (b) We incorporate several strong backbones with advanced training recipes for more discriminative representation learning. (c) We further introduce a number of post-processing techniques including whitening, re-ranking and model ensemble for retrieval enhancement. By achieving 71.53% MAR, our solution \"Involution King\" achieves the second position on the leaderboard.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0909871433,
        "newsscientist":0.1203989333,
        "technologyreview":0.2170995611,
        "venturebeat":0.2220538132,
        "wired":0.1718181882,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00955v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659369945000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01587v1",
        "predicted_newsworthiness":0.3375917321,
        "title":"Learning to Incorporate Texture Saliency Adaptive Attention to Image Cartoonization",
        "summary":"Image cartoonization is recently dominated by generative adversarial networks (GANs) from the perspective of unsupervised image-to-image translation, in which an inherent challenge is to precisely capture and sufficiently transfer characteristic cartoon styles (e.g., clear edges, smooth color shading, abstract fine structures, etc.). Existing advanced models try to enhance cartoonization effect by learning to promote edges adversarially, introducing style transfer loss, or learning to align style from multiple representation space. This paper demonstrates that more distinct and vivid cartoonization effect could be easily achieved with only basic adversarial loss. Observing that cartoon style is more evident in cartoon-texture-salient local image regions, we build a region-level adversarial learning branch in parallel with the normal image-level one, which constrains adversarial learning on cartoon-texture-salient local patches for better perceiving and transferring cartoon texture features. To this end, a novel cartoon-texture-saliency-sampler (CTSS) module is proposed to dynamically sample cartoon-texture-salient patches from training data. With extensive experiments, we demonstrate that texture saliency adaptive attention in adversarial learning, as a missing ingredient of related methods in image cartoonization, is of significant importance in facilitating and enhancing image cartoon stylization, especially for high-resolution input pictures.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0933811489,
        "newsscientist":0.1287413519,
        "technologyreview":0.1825529133,
        "venturebeat":0.1686499451,
        "wired":0.1674938066,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01587v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659458755000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11517v1",
        "predicted_newsworthiness":0.3359318291,
        "title":"Contrastive Monotonic Pixel-Level Modulation",
        "summary":"Continuous one-to-many mapping is a less investigated yet important task in both low-level visions and neural image translation. In this paper, we present a new formulation called MonoPix, an unsupervised and contrastive continuous modulation model, and take a step further to enable a pixel-level spatial control which is critical but can not be properly handled previously. The key feature of this work is to model the monotonicity between controlling signals and the domain discriminator with a novel contrastive modulation framework and corresponding monotonicity constraints. We have also introduced a selective inference strategy with logarithmic approximation complexity and support fast domain adaptations. The state-of-the-art performance is validated on a variety of continuous mapping tasks, including AFHQ cat-dog and Yosemite summer-winter translation. The introduced approach also helps to provide a new solution for many low-level tasks like low-light enhancement and natural noise generation, which is beyond the long-established practice of one-to-one training and inference. Code is available at https:\/\/github.com\/lukun199\/MonoPix.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0730237401,
        "newsscientist":0.1332185962,
        "technologyreview":0.1928612887,
        "venturebeat":0.1660571729,
        "wired":0.1399871155,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11517v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658582484000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00916v1",
        "predicted_newsworthiness":0.3342936432,
        "title":"Locally Optimal Estimation and Control of Cable Driven Parallel Robots using Time Varying Linear Quadratic Gaussian Control",
        "summary":"We present a locally optimal tracking controller for Cable Driven Parallel Robot (CDPR) control based on a time-varying Linear Quadratic Gaussian (TV-LQG) controller. In contrast to many methods which use fixed feedback gains, our time-varying controller computes the optimal gains depending on the location in the workspace and the future trajectory. Meanwhile, we rely heavily on offline computation to reduce the burden of online implementation and feasibility checking. Following the growing popularity of probabilistic graphical models for optimal control, we use factor graphs as a tool to formulate our controller for their efficiency, intuitiveness, and modularity. The topology of a factor graph encodes the relevant structural properties of equations in a way that facilitates insight and efficient computation using sparse linear algebra solvers. We first use factor graph optimization to compute a nominal trajectory, then linearize the graph and apply variable elimination to compute the locally optimal, time varying linear feedback gains. Next, we leverage the factor graph formulation to compute the locally optimal, time-varying Kalman Filter gains, and finally combine the locally optimal linear control and estimation laws to form a TV-LQG controller. We compare the tracking accuracy of our TV-LQG controller to a state-of-the-art dual-space feed-forward controller on a 2.9m x 2.3m, 4-cable planar robot and demonstrate improved tracking accuracies of 0.8{\\deg} and 11.6mm root mean square error in rotation and translation respectively.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0436185143,
        "newsscientist":0.1005551209,
        "technologyreview":0.1463030683,
        "venturebeat":0.1398317476,
        "wired":0.1291277273,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00916v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659366015000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00318v1",
        "predicted_newsworthiness":0.3339517806,
        "title":"Smoothing Entailment Graphs with Language Models",
        "summary":"The diversity and Zipfian frequency distribution of natural language predicates in corpora leads to sparsity when learning Entailment Graphs. As symbolic models for natural language inference, an EG cannot recover if missing a novel premise or hypothesis at test-time. In this paper we approach the problem of vertex sparsity by introducing a new method of graph smoothing, using a Language Model to find the nearest approximations of missing predicates. We improve recall by 25.1 and 16.3 absolute percentage points on two difficult directional entailment datasets while exceeding average precision, and show a complementarity with other improvements to edge sparsity. We further analyze language model embeddings and discuss why they are naturally suitable for premise-smoothing, but not hypothesis-smoothing. Finally, we formalize a theory for smoothing a symbolic inference method by constructing transitive chains to smooth both the premise and hypothesis.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0864437007,
        "newsscientist":0.0975550006,
        "technologyreview":0.1496803572,
        "venturebeat":0.1398495526,
        "wired":0.1158170031,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00318v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659219322000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11887v1",
        "predicted_newsworthiness":0.333630972,
        "title":"HIRE: Distilling High-order Relational Knowledge From Heterogeneous Graph Neural Networks",
        "summary":"Researchers have recently proposed plenty of heterogeneous graph neural networks (HGNNs) due to the ubiquity of heterogeneous graphs in both academic and industrial areas. Instead of pursuing a more powerful HGNN model, in this paper, we are interested in devising a versatile plug-and-play module, which accounts for distilling relational knowledge from pre-trained HGNNs. To the best of our knowledge, we are the first to propose a HIgh-order RElational (HIRE) knowledge distillation framework on heterogeneous graphs, which can significantly boost the prediction performance regardless of model architectures of HGNNs. Concretely, our HIRE framework initially performs first-order node-level knowledge distillation, which encodes the semantics of the teacher HGNN with its prediction logits. Meanwhile, the second-order relation-level knowledge distillation imitates the relational correlation between node embeddings of different types generated by the teacher HGNN. Extensive experiments on various popular HGNNs models and three real-world heterogeneous graphs demonstrate that our method obtains consistent and considerable performance enhancement, proving its effectiveness and generalization ability.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1057627857,
        "newsscientist":0.1245015592,
        "technologyreview":0.2288817191,
        "venturebeat":0.2131735687,
        "wired":0.1580221791,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11887v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658719515000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01250v1",
        "predicted_newsworthiness":0.3328395874,
        "title":"Geometric Interaction Augmented Graph Collaborative Filtering",
        "summary":"Graph-based collaborative filtering is capable of capturing the essential and abundant collaborative signals from the high-order interactions, and thus received increasingly research interests. Conventionally, the embeddings of users and items are defined in the Euclidean spaces, along with the propagation on the interaction graphs. Meanwhile, recent works point out that the high-order interactions naturally form up the tree-likeness structures, which the hyperbolic models thrive on. However, the interaction graphs inherently exhibit the hybrid and nested geometric characteristics, while the existing single geometry-based models are inadequate to fully capture such sophisticated topological patterns. In this paper, we propose to model the user-item interactions in a hybrid geometric space, in which the merits of Euclidean and hyperbolic spaces are simultaneously enjoyed to learn expressive representations. Experimental results on public datasets validate the effectiveness of our proposal.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1009350842,
        "newsscientist":0.1310430436,
        "technologyreview":0.1810049805,
        "venturebeat":0.1815080363,
        "wired":0.186671249,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01250v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1659415997000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.11757v1",
        "predicted_newsworthiness":0.3311621432,
        "title":"Learning Generalizable Light Field Networks from Few Images",
        "summary":"We explore a new strategy for few-shot novel view synthesis based on a neural light field representation. Given a target camera pose, an implicit neural network maps each ray to its target pixel's color directly. The network is conditioned on local ray features generated by coarse volumetric rendering from an explicit 3D feature volume. This volume is built from the input images using a 3D ConvNet. Our method achieves competitive performances on synthetic and real MVS data with respect to state-of-the-art neural radiance field based competition, while offering a 100 times faster rendering.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0807860759,
        "newsscientist":0.1331811735,
        "technologyreview":0.2077536177,
        "venturebeat":0.2073778761,
        "wired":0.1713448873,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11757v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658674031000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13779v1",
        "predicted_newsworthiness":0.3311565078,
        "title":"Physical Pooling Functions in Graph Neural Networks for Molecular Property Prediction",
        "summary":"Graph neural networks (GNNs) are emerging in chemical engineering for the end-to-end learning of physicochemical properties based on molecular graphs. A key element of GNNs is the pooling function which combines atom feature vectors into molecular fingerprints. Most previous works use a standard pooling function to predict a variety of properties. However, unsuitable pooling functions can lead to unphysical GNNs that poorly generalize. We compare and select meaningful GNN pooling methods based on physical knowledge about the learned properties. The impact of physical pooling functions is demonstrated with molecular properties calculated from quantum mechanical computations. We also compare our results to the recent set2set pooling approach. We recommend using sum pooling for the prediction of properties that depend on molecular size and compare pooling functions for properties that are molecular size-independent. Overall, we show that the use of physical pooling functions significantly enhances generalization.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.09717367,
        "newsscientist":0.1732265407,
        "technologyreview":0.2173347334,
        "venturebeat":0.1744178106,
        "wired":0.133510868,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13779v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658953459000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11996v2",
        "predicted_newsworthiness":0.328314101,
        "title":"Generative Subgraph Contrast for Self-Supervised Graph Representation Learning",
        "summary":"Contrastive learning has shown great promise in the field of graph representation learning. By manually constructing positive\/negative samples, most graph contrastive learning methods rely on the vector inner product based similarity metric to distinguish the samples for graph representation. However, the handcrafted sample construction (e.g., the perturbation on the nodes or edges of the graph) may not effectively capture the intrinsic local structures of the graph. Also, the vector inner product based similarity metric cannot fully exploit the local structures of the graph to characterize the graph difference well. To this end, in this paper, we propose a novel adaptive subgraph generation based contrastive learning framework for efficient and robust self-supervised graph representation learning, and the optimal transport distance is utilized as the similarity metric between the subgraphs. It aims to generate contrastive samples by capturing the intrinsic structures of the graph and distinguish the samples based on the features and structures of subgraphs simultaneously. Specifically, for each center node, by adaptively learning relation weights to the nodes of the corresponding neighborhood, we first develop a network to generate the interpolated subgraph. We then construct the positive and negative pairs of subgraphs from the same and different nodes, respectively. Finally, we employ two types of optimal transport distances (i.e., Wasserstein distance and Gromov-Wasserstein distance) to construct the structured contrastive loss. Extensive node classification experiments on benchmark datasets verify the effectiveness of our graph contrastive learning method.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0886561253,
        "newsscientist":0.1148998406,
        "technologyreview":0.1763639973,
        "venturebeat":0.1497289626,
        "wired":0.1161927817,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11996v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658740126000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01905v1",
        "predicted_newsworthiness":0.3283014519,
        "title":"Graph Signal Processing for Heterogeneous Change Detection Part II: Spectral Domain Analysis",
        "summary":"This is the second part of the paper that provides a new strategy for the heterogeneous change detection (HCD) problem, that is, solving HCD from the perspective of graph signal processing (GSP). We construct a graph to represent the structure of each image, and treat each image as a graph signal defined on the graph. In this way, we can convert the HCD problem into a comparison of responses of signals on systems defined on the graphs. In the part I, the changes are measured by comparing the structure difference between the graphs from the vertex domain. In this part II, we analyze the GSP for HCD from the spectral domain. We first analyze the spectral properties of the different images on the same graph, and show that their spectra exhibit commonalities and dissimilarities. Specially, it is the change that leads to the dissimilarities of their spectra. Then, we propose a regression model for the HCD, which decomposes the source signal into the regressed signal and changed signal, and requires the regressed signal have the same spectral property as the target signal on the same graph. With the help of graph spectral analysis, the proposed regression model is flexible and scalable. Experiments conducted on seven real data sets show the effectiveness of the proposed method.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0785999461,
        "newsscientist":0.1270750412,
        "technologyreview":0.144470767,
        "venturebeat":0.1224674014,
        "wired":0.1086575168,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01905v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659514284000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11681v1",
        "predicted_newsworthiness":0.3277784786,
        "title":"Learning Graph Neural Networks for Image Style Transfer",
        "summary":"State-of-the-art parametric and non-parametric style transfer approaches are prone to either distorted local style patterns due to global statistics alignment, or unpleasing artifacts resulting from patch mismatching. In this paper, we study a novel semi-parametric neural style transfer framework that alleviates the deficiency of both parametric and non-parametric stylization. The core idea of our approach is to establish accurate and fine-grained content-style correspondences using graph neural networks (GNNs). To this end, we develop an elaborated GNN model with content and style local patches as the graph vertices. The style transfer procedure is then modeled as the attention-based heterogeneous message passing between the style and content nodes in a learnable manner, leading to adaptive many-to-one style-content correlations at the local patch level. In addition, an elaborated deformable graph convolutional operation is introduced for cross-scale style-content matching. Experimental results demonstrate that the proposed semi-parametric image stylization approach yields encouraging results on the challenging style patterns, preserving both global appearance and exquisite details. Furthermore, by controlling the number of edges at the inference stage, the proposed method also triggers novel functionalities like diversified patch-based stylization with a single model.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0823204506,
        "newsscientist":0.1030454448,
        "technologyreview":0.1514724761,
        "venturebeat":0.1323699183,
        "wired":0.1384264145,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11681v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658648491000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13298v1",
        "predicted_newsworthiness":0.3273297832,
        "title":"Is Attention All NeRF Needs?",
        "summary":"We present Generalizable NeRF Transformer (GNT), a pure, unified transformer-based architecture that efficiently reconstructs Neural Radiance Fields (NeRFs) on the fly from source views. Unlike prior works on NeRF that optimize a per-scene implicit representation by inverting a handcrafted rendering equation, GNT achieves generalizable neural scene representation and rendering, by encapsulating two transformer-based stages. The first stage of GNT, called view transformer, leverages multi-view geometry as an inductive bias for attention-based scene representation, and predicts coordinate-aligned features by aggregating information from epipolar lines on the neighboring views. The second stage of GNT, named ray transformer, renders novel views by ray marching and directly decodes the sequence of sampled point features using the attention mechanism. Our experiments demonstrate that when optimized on a single scene, GNT can successfully reconstruct NeRF without explicit rendering formula, and even improve the PSNR by ~1.3dB on complex scenes due to the learnable ray renderer. When trained across various scenes, GNT consistently achieves the state-of-the-art performance when transferring to forward-facing LLFF dataset (LPIPS ~20%, SSIM ~25%$) and synthetic blender dataset (LPIPS ~20%, SSIM ~4%). In addition, we show that depth and occlusion can be inferred from the learned attention maps, which implies that the pure attention mechanism is capable of learning a physically-grounded rendering process. All these results bring us one step closer to the tantalizing hope of utilizing transformers as the \"universal modeling tool\" even for graphics. Please refer to our project page for video results: https:\/\/vita-group.github.io\/GNT\/.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1111111696,
        "newsscientist":0.1654514263,
        "technologyreview":0.2447162992,
        "venturebeat":0.2458590016,
        "wired":0.2021747458,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13298v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658898594000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01795v1",
        "predicted_newsworthiness":0.3270048895,
        "title":"Dynamic Modeling of Branched Robots using Modular Composition",
        "summary":"This letter proposes a systematic modular procedure for the dynamic modeling of branched robots comprising several subsystems, each of which being composed of multiple rigid bodies. Furthermore, the proposed strategy is applicable even if some subsystems are regarded as black boxes, requiring only the twists and wrenches at the connection points between different subsystems. To help in the model composition, we also propose a graph representation that encodes the propagation of twists and wrenches between the subsystems. Numerical results show that the proposed formalism is as accurate as a state-of-the-art library for robotic dynamic modeling.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0536828939,
        "newsscientist":0.1160279682,
        "technologyreview":0.1259473068,
        "venturebeat":0.091397667,
        "wired":0.1098663747,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01795v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659486847000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13008v1",
        "predicted_newsworthiness":0.3259113162,
        "title":"Efficient Algorithms for Sparse Moment Problems without Separation",
        "summary":"We consider the sparse moment problem of learning a $k$-spike mixture in high dimensional space from its noisy moment information in any dimension. We measure the accuracy of the learned mixtures using transportation distance. Previous algorithms either assume certain separation assumptions, use more recovery moments, or run in (super) exponential time. Our algorithm for the 1-dimension problem (also called the sparse Hausdorff moment problem) is a robust version of the classic Prony's method, and our contribution mainly lies in the analysis. We adopt a global and much tighter analysis than previous work (which analyzes the perturbation of the intermediate results of Prony's method). A useful technical ingredient is a connection between the linear system defined by the Vandermonde matrix and the Schur polynomial, which allows us to provide tight perturbation bound independent of the separation and may be useful in other contexts. To tackle the high dimensional problem, we first solve the 2-dimensional problem by extending the 1-dimension algorithm and analysis to complex numbers. Our algorithm for the high dimensional case determines the coordinates of each spike by aligning a 1-d projection of the mixture to a random vector and a set of 2d-projections of the mixture. Our results have applications to learning topic models and Gaussian mixtures, implying improved sample complexity results or running time over prior work.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0723346895,
        "newsscientist":0.1037132856,
        "technologyreview":0.1390767881,
        "venturebeat":0.1294276687,
        "wired":0.1225037852,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13008v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658852252000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14741v2",
        "predicted_newsworthiness":0.3245139253,
        "title":"End-to-end View Synthesis via NeRF Attention",
        "summary":"In this paper, we present a simple seq2seq formulation for view synthesis where we take a set of ray points as input and output colors corresponding to the rays. Directly applying a standard transformer on this seq2seq formulation has two limitations. First, the standard attention cannot successfully fit the volumetric rendering procedure, and therefore high-frequency components are missing in the synthesized views. Second, applying global attention to all rays and pixels is extremely inefficient. Inspired by the neural radiance field (NeRF), we propose the NeRF attention (NeRFA) to address the above problems. On the one hand, NeRFA considers the volumetric rendering equation as a soft feature modulation procedure. In this way, the feature modulation enhances the transformers with the NeRF-like inductive bias. On the other hand, NeRFA performs multi-stage attention to reduce the computational overhead. Furthermore, the NeRFA model adopts the ray and pixel transformers to learn the interactions between rays and pixels. NeRFA demonstrates superior performance over NeRF and NerFormer on four datasets: DeepVoxels, Blender, LLFF, and CO3D. Besides, NeRFA establishes a new state-of-the-art under two settings: the single-scene view synthesis and the category-centric novel view synthesis. The code will be made publicly available.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1017995208,
        "newsscientist":0.1561161211,
        "technologyreview":0.2165624935,
        "venturebeat":0.2412498779,
        "wired":0.2123117055,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14741v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659108376000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11938v1",
        "predicted_newsworthiness":0.3219189248,
        "title":"Reference-based Image Super-Resolution with Deformable Attention Transformer",
        "summary":"Reference-based image super-resolution (RefSR) aims to exploit auxiliary reference (Ref) images to super-resolve low-resolution (LR) images. Recently, RefSR has been attracting great attention as it provides an alternative way to surpass single image SR. However, addressing the RefSR problem has two critical challenges: (i) It is difficult to match the correspondence between LR and Ref images when they are significantly different; (ii) How to transfer the relevant texture from Ref images to compensate the details for LR images is very challenging. To address these issues of RefSR, this paper proposes a deformable attention Transformer, namely DATSR, with multiple scales, each of which consists of a texture feature encoder (TFE) module, a reference-based deformable attention (RDA) module and a residual feature aggregation (RFA) module. Specifically, TFE first extracts image transformation (e.g., brightness) insensitive features for LR and Ref images, RDA then can exploit multiple relevant textures to compensate more information for LR features, and RFA lastly aggregates LR features and relevant textures to get a more visually pleasant result. Extensive experiments demonstrate that our DATSR achieves state-of-the-art performance on benchmark datasets quantitatively and qualitatively.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0670017146,
        "newsscientist":0.106113157,
        "technologyreview":0.1550813572,
        "venturebeat":0.1503459131,
        "wired":0.1187752426,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11938v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658732820000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11559v1",
        "predicted_newsworthiness":0.3196545267,
        "title":"Tensor-based Multi-view Spectral Clustering via Shared Latent Space",
        "summary":"Multi-view Spectral Clustering (MvSC) attracts increasing attention due to diverse data sources. However, most existing works are prohibited in out-of-sample predictions and overlook model interpretability and exploration of clustering results. In this paper, a new method for MvSC is proposed via a shared latent space from the Restricted Kernel Machine framework. Through the lens of conjugate feature duality, we cast the weighted kernel principal component analysis problem for MvSC and develop a modified weighted conjugate feature duality to formulate dual variables. In our method, the dual variables, playing the role of hidden features, are shared by all views to construct a common latent space, coupling the views by learning projections from view-specific spaces. Such single latent space promotes well-separated clusters and provides straightforward data exploration, facilitating visualization and interpretation. Our method requires only a single eigendecomposition, whose dimension is independent of the number of views. To boost higher-order correlations, tensor-based modelling is introduced without increasing computational complexity. Our method can be flexibly applied with out-of-sample extensions, enabling greatly improved efficiency for large-scale data with fixed-size kernel schemes. Numerical experiments verify that our method is effective regarding accuracy, efficiency, and interpretability, showing a sharp eigenvalue decay and distinct latent variable distributions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0749241113,
        "newsscientist":0.1101198935,
        "technologyreview":0.1460459363,
        "venturebeat":0.1436255756,
        "wired":0.1196977609,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11559v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658597454000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11789v1",
        "predicted_newsworthiness":0.3188846934,
        "title":"Hierarchical Semi-Supervised Contrastive Learning for Contamination-Resistant Anomaly Detection",
        "summary":"Anomaly detection aims at identifying deviant samples from the normal data distribution. Contrastive learning has provided a successful way to sample representation that enables effective discrimination on anomalies. However, when contaminated with unlabeled abnormal samples in training set under semi-supervised settings, current contrastive-based methods generally 1) ignore the comprehensive relation between training data, leading to suboptimal performance, and 2) require fine-tuning, resulting in low efficiency. To address the above two issues, in this paper, we propose a novel hierarchical semi-supervised contrastive learning (HSCL) framework, for contamination-resistant anomaly detection. Specifically, HSCL hierarchically regulates three complementary relations: sample-to-sample, sample-to-prototype, and normal-to-abnormal relations, enlarging the discrimination between normal and abnormal samples with a comprehensive exploration of the contaminated data. Besides, HSCL is an end-to-end learning approach that can efficiently learn discriminative representations without fine-tuning. HSCL achieves state-of-the-art performance in multiple scenarios, such as one-class classification and cross-dataset detection. Extensive ablation studies further verify the effectiveness of each considered relation. The code is available at https:\/\/github.com\/GaoangW\/HSCL.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.109809895,
        "newsscientist":0.1395579416,
        "technologyreview":0.2149915593,
        "venturebeat":0.1974440579,
        "wired":0.1543632858,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11789v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658688566000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00821v1",
        "predicted_newsworthiness":0.3156494084,
        "title":"Locally Supervised Learning with Periodic Global Guidance",
        "summary":"Locally supervised learning aims to train a neural network based on a local estimation of the global loss function at each decoupled module of the network. Auxiliary networks are typically appended to the modules to approximate the gradient updates based on the greedy local losses. Despite being advantageous in terms of parallelism and reduced memory consumption, this paradigm of training severely degrades the generalization performance of neural networks. In this paper, we propose Periodically Guided local Learning (PGL), which reinstates the global objective repetitively into the local-loss based training of neural networks primarily to enhance the model's generalization capability. We show that a simple periodic guidance scheme begets significant performance gains while having a low memory footprint. We conduct extensive experiments on various datasets and networks to demonstrate the effectiveness of PGL, especially in the configuration with numerous decoupled modules.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0876764438,
        "newsscientist":0.1399848824,
        "technologyreview":0.2164983458,
        "venturebeat":0.1910768691,
        "wired":0.1420889987,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00821v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659359186000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13667v1",
        "predicted_newsworthiness":0.3140970966,
        "title":"Unsupervised Training for Neural TSP Solver",
        "summary":"There has been a growing number of machine learning methods for approximately solving the travelling salesman problem. However, these methods often require solved instances for training or use complex reinforcement learning approaches that need a large amount of tuning. To avoid these problems, we introduce a novel unsupervised learning approach. We use a relaxation of an integer linear program for TSP to construct a loss function that does not require correct instance labels. With variable discretization, its minimum coincides with the optimal or near-optimal solution. Furthermore, this loss function is differentiable and thus can be used to train neural networks directly. We use our loss function with a Graph Neural Network and design controlled experiments on both Euclidean and asymmetric TSP. Our approach has the advantage over supervised learning of not requiring large labelled datasets. In addition, the performance of our approach surpasses reinforcement learning for asymmetric TSP and is comparable to reinforcement learning for Euclidean instances. Our approach is also more stable and easier to train than reinforcement learning.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0594903727,
        "newsscientist":0.116825734,
        "technologyreview":0.1898078201,
        "venturebeat":0.1770690242,
        "wired":0.1268920619,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13667v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658943209000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14811v1",
        "predicted_newsworthiness":0.3135156329,
        "title":"StyleLight: HDR Panorama Generation for Lighting Estimation and Editing",
        "summary":"We present a new lighting estimation and editing framework to generate high-dynamic-range (HDR) indoor panorama lighting from a single limited field-of-view (LFOV) image captured by low-dynamic-range (LDR) cameras. Existing lighting estimation methods either directly regress lighting representation parameters or decompose this problem into LFOV-to-panorama and LDR-to-HDR lighting generation sub-tasks. However, due to the partial observation, the high-dynamic-range lighting, and the intrinsic ambiguity of a scene, lighting estimation remains a challenging task. To tackle this problem, we propose a coupled dual-StyleGAN panorama synthesis network (StyleLight) that integrates LDR and HDR panorama synthesis into a unified framework. The LDR and HDR panorama synthesis share a similar generator but have separate discriminators. During inference, given an LDR LFOV image, we propose a focal-masked GAN inversion method to find its latent code by the LDR panorama synthesis branch and then synthesize the HDR panorama by the HDR panorama synthesis branch. StyleLight takes LFOV-to-panorama and LDR-to-HDR lighting generation into a unified framework and thus greatly improves lighting estimation. Extensive experiments demonstrate that our framework achieves superior performance over state-of-the-art methods on indoor lighting estimation. Notably, StyleLight also enables intuitive lighting editing on indoor HDR panoramas, which is suitable for real-world applications. Code is available at https:\/\/style-light.github.io.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0894484498,
        "newsscientist":0.1226710346,
        "technologyreview":0.1693927522,
        "venturebeat":0.1880717391,
        "wired":0.1738231249,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14811v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659117538000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14617v1",
        "predicted_newsworthiness":0.3131843002,
        "title":"KG-NSF: Knowledge Graph Completion with a Negative-Sample-Free Approach",
        "summary":"Knowledge Graph (KG) completion is an important task that greatly benefits knowledge discovery in many fields (e.g. biomedical research). In recent years, learning KG embeddings to perform this task has received considerable attention. Despite the success of KG embedding methods, they predominantly use negative sampling, resulting in increased computational complexity as well as biased predictions due to the closed world assumption. To overcome these limitations, we propose \\textbf{KG-NSF}, a negative sampling-free framework for learning KG embeddings based on the cross-correlation matrices of embedding vectors. It is shown that the proposed method achieves comparable link prediction performance to negative sampling-based methods while converging much faster.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0887877139,
        "newsscientist":0.1205778301,
        "technologyreview":0.1693733287,
        "venturebeat":0.153518013,
        "wired":0.1260951569,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14617v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cl"
        ],
        "published":1659094744000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01198v1",
        "predicted_newsworthiness":0.309085914,
        "title":"Late Fusion Multi-view Clustering via Global and Local Alignment Maximization",
        "summary":"Multi-view clustering (MVC) optimally integrates complementary information from different views to improve clustering performance. Although demonstrating promising performance in various applications, most of existing approaches directly fuse multiple pre-specified similarities to learn an optimal similarity matrix for clustering, which could cause over-complicated optimization and intensive computational cost. In this paper, we propose late fusion MVC via alignment maximization to address these issues. To do so, we first reveal the theoretical connection of existing k-means clustering and the alignment between base partitions and the consensus one. Based on this observation, we propose a simple but effective multi-view algorithm termed LF-MVC-GAM. It optimally fuses multiple source information in partition level from each individual view, and maximally aligns the consensus partition with these weighted base ones. Such an alignment is beneficial to integrate partition level information and significantly reduce the computational complexity by sufficiently simplifying the optimization procedure. We then design another variant, LF-MVC-LAM to further improve the clustering performance by preserving the local intrinsic structure among multiple partition spaces. After that, we develop two three-step iterative algorithms to solve the resultant optimization problems with theoretically guaranteed convergence. Further, we provide the generalization error bound analysis of the proposed algorithms. Extensive experiments on eighteen multi-view benchmark datasets demonstrate the effectiveness and efficiency of the proposed LF-MVC-GAM and LF-MVC-LAM, ranging from small to large-scale data items. The codes of the proposed algorithms are publicly available at https:\/\/github.com\/wangsiwei2010\/latefusionalignment.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0584996144,
        "newsscientist":0.0896561207,
        "technologyreview":0.1325168172,
        "venturebeat":0.1524976318,
        "wired":0.1006750738,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01198v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659404971000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12305v1",
        "predicted_newsworthiness":0.3078502716,
        "title":"Error-Aware Spatial Ensembles for Video Frame Interpolation",
        "summary":"Video frame interpolation~(VFI) algorithms have improved considerably in recent years due to unprecedented progress in both data-driven algorithms and their implementations. Recent research has introduced advanced motion estimation or novel warping methods as the means to address challenging VFI scenarios. However, none of the published VFI works considers the spatially non-uniform characteristics of the interpolation error (IE). This work introduces such a solution. By closely examining the correlation between optical flow and IE, the paper proposes novel error prediction metrics that partition the middle frame into distinct regions corresponding to different IE levels. Building upon this IE-driven segmentation, and through the use of novel error-controlled loss functions, it introduces an ensemble of spatially adaptive interpolation units that progressively processes and integrates the segmented regions. This spatial ensemble results in an effective and computationally attractive VFI solution. Extensive experimentation on popular video interpolation benchmarks indicates that the proposed solution outperforms the current state-of-the-art (SOTA) in applications of current interest.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0603072139,
        "newsscientist":0.101605584,
        "technologyreview":0.1239895227,
        "venturebeat":0.1519017349,
        "wired":0.1231322754,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12305v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658765738000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14431v1",
        "predicted_newsworthiness":0.3027215088,
        "title":"Learning idempotent representation for subspace clustering",
        "summary":"The critical point for the successes of spectral-type subspace clustering algorithms is to seek reconstruction coefficient matrices which can faithfully reveal the subspace structures of data sets. An ideal reconstruction coefficient matrix should have two properties: 1) it is block diagonal with each block indicating a subspace; 2) each block is fully connected. Though there are various spectral-type subspace clustering algorithms have been proposed, some defects still exist in the reconstruction coefficient matrices constructed by these algorithms. We find that a normalized membership matrix naturally satisfies the above two conditions. Therefore, in this paper, we devise an idempotent representation (IDR) algorithm to pursue reconstruction coefficient matrices approximating normalized membership matrices. IDR designs a new idempotent constraint for reconstruction coefficient matrices. And by combining the doubly stochastic constraints, the coefficient matrices which are closed to normalized membership matrices could be directly achieved. We present the optimization algorithm for solving IDR problem and analyze its computation burden as well as convergence. The comparisons between IDR and related algorithms show the superiority of IDR. Plentiful experiments conducted on both synthetic and real world datasets prove that IDR is an effective and efficient subspace clustering algorithm.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0587191453,
        "newsscientist":0.0925660259,
        "technologyreview":0.1208620355,
        "venturebeat":0.1133061292,
        "wired":0.0792148355,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14431v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659058765000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00850v1",
        "predicted_newsworthiness":0.3013318629,
        "title":"Subgraph Neighboring Relations Infomax for Inductive Link Prediction on Knowledge Graphs",
        "summary":"Inductive link prediction for knowledge graph aims at predicting missing links between unseen entities, those not shown in training stage. Most previous works learn entity-specific embeddings of entities, which cannot handle unseen entities. Recent several methods utilize enclosing subgraph to obtain inductive ability. However, all these works only consider the enclosing part of subgraph without complete neighboring relations, which leads to the issue that partial neighboring relations are neglected, and sparse subgraphs are hard to be handled. To address that, we propose Subgraph Neighboring Relations Infomax, SNRI, which sufficiently exploits complete neighboring relations from two aspects: neighboring relational feature for node feature and neighboring relational path for sparse subgraph. To further model neighboring relations in a global way, we innovatively apply mutual information (MI) maximization for knowledge graph. Experiments show that SNRI outperforms existing state-of-art methods by a large margin on inductive link prediction task, and verify the effectiveness of exploring complete neighboring relations in a global way to characterize node features and reason on sparse subgraphs.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0716167375,
        "newsscientist":0.1003326838,
        "technologyreview":0.1533719773,
        "venturebeat":0.1446655814,
        "wired":0.1063501842,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00850v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.lg"
        ],
        "published":1658973159000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.12214v1",
        "predicted_newsworthiness":0.3003159394,
        "title":"Laplacian-based Cluster-Contractive t-SNE for High Dimensional Data Visualization",
        "summary":"Dimensionality reduction techniques aim at representing high-dimensional data in low-dimensional spaces to extract hidden and useful information or facilitate visual understanding and interpretation of the data. However, few of them take into consideration the potential cluster information contained implicitly in the high-dimensional data. In this paper, we propose LaptSNE, a new graph-layout nonlinear dimensionality reduction method based on t-SNE, one of the best techniques for visualizing high-dimensional data as 2D scatter plots. Specifically, LaptSNE leverages the eigenvalue information of the graph Laplacian to shrink the potential clusters in the low-dimensional embedding when learning to preserve the local and global structure from high-dimensional space to low-dimensional space. It is nontrivial to solve the proposed model because the eigenvalues of normalized symmetric Laplacian are functions of the decision variable. We provide a majorization-minimization algorithm with convergence guarantee to solve the optimization problem of LaptSNE and show how to calculate the gradient analytically, which may be of broad interest when considering optimization with Laplacian-composited objective. We evaluate our method by a formal comparison with state-of-the-art methods, both visually and via established quantitative measurements. The results demonstrate the superiority of our method over baselines such as t-SNE and UMAP. We also extend our method to spectral clustering and establish an accurate and parameter-free clustering algorithm, which provides us high reliability and convenience in real applications.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0820176337,
        "newsscientist":0.1295527031,
        "technologyreview":0.1717270377,
        "venturebeat":0.15195512,
        "wired":0.136293755,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12214v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.hc"
        ],
        "published":1658758224000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02068v1",
        "predicted_newsworthiness":0.2984509217,
        "title":"HybridGNN: Learning Hybrid Representation in Multiplex Heterogeneous Networks",
        "summary":"Recently, graph neural networks have shown the superiority of modeling the complex topological structures in heterogeneous network-based recommender systems. Due to the diverse interactions among nodes and abundant semantics emerging from diverse types of nodes and edges, there is a bursting research interest in learning expressive node representations in multiplex heterogeneous networks. One of the most important tasks in recommender systems is to predict the potential connection between two nodes under a specific edge type (i.e., relationship). Although existing studies utilize explicit metapaths to aggregate neighbors, practically they only consider intra-relationship metapaths and thus fail to leverage the potential uplift by inter-relationship information. Moreover, it is not always straightforward to exploit inter-relationship metapaths comprehensively under diverse relationships, especially with the increasing number of node and edge types. In addition, contributions of different relationships between two nodes are difficult to measure. To address the challenges, we propose HybridGNN, an end-to-end GNN model with hybrid aggregation flows and hierarchical attentions to fully utilize the heterogeneity in the multiplex scenarios. Specifically, HybridGNN applies a randomized inter-relationship exploration module to exploit the multiplexity property among different relationships. Then, our model leverages hybrid aggregation flows under intra-relationship metapaths and randomized exploration to learn the rich semantics. To explore the importance of different aggregation flow and take advantage of the multiplexity property, we bring forward a novel hierarchical attention module which leverages both metapath-level attention and relationship-level attention. Extensive experimental results suggest that HybridGNN achieves the best performance compared to several state-of-the-art baselines.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0892372963,
        "newsscientist":0.1103041614,
        "technologyreview":0.1859152504,
        "venturebeat":0.1918329769,
        "wired":0.1511510168,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02068v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659533987000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00152v1",
        "predicted_newsworthiness":0.2847490109,
        "title":"Local Graph Embeddings Based on Neighbors Degree Frequency of Nodes",
        "summary":"We propose a local-to-global strategy for graph machine learning and network analysis by defining certain local features and vector representations of nodes and then using them to learn globally defined metrics and properties of the nodes by means of deep neural networks. By extending the notion of the degree of a node via Breath-First Search, a general family of {\\bf parametric centrality functions} is defined which are able to reveal the importance of nodes. We introduce the {\\bf neighbors degree frequency (NDF)}, as a locally defined embedding of nodes of undirected graphs into euclidean spaces. This gives rise to a vectorized labeling of nodes which encodes the structure of local neighborhoods of nodes and can be used for graph isomorphism testing. We add flexibility to our construction so that it can handle dynamic graphs as well. Afterwards, the Breadth-First Search is used to extend NDF vector representations into two different matrix representations of nodes which contain higher order information about the neighborhoods of nodes. Our matrix representations of nodes provide us with a new way of visualizing the shape of the neighborhood of a node. Furthermore, we use these matrix representations to obtain feature vectors, which are suitable for typical deep learning algorithms. To demonstrate these node embeddings actually contain some information about the nodes, in a series of examples, we show that PageRank and closeness centrality can be learned by applying deep learning to these local features. Our constructions are flexible enough to handle evolving graphs. Finally, we explain how to adapt our constructions for directed graphs.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1166748903,
        "newsscientist":0.1640947386,
        "technologyreview":0.2451682777,
        "venturebeat":0.2205793408,
        "wired":0.1848081657,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00152v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.lg"
        ],
        "published":1659164850000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    }
]