[
    {
        "arxiv_id":"2201.04292v2",
        "predicted_newsworthiness":0.6810475591,
        "title":"Predicting Terrorist Attacks in the United States using Localized News Data",
        "summary":"Terrorism is a major problem worldwide, causing thousands of fatalities and billions of dollars in damage every year. Toward the end of better understanding and mitigating these attacks, we present a set of machine learning models that learn from localized news data in order to predict whether a terrorist attack will occur on a given calendar date and in a given state. The best model--a Random Forest that learns from a novel variable-length moving average representation of the feature space--achieves area under the receiver operating characteristic scores $> .667$ on four of the five states that were impacted most by terrorism between 2015 and 2018. Our key findings include that modeling terrorism as a set of independent events, rather than as a continuous process, is a fruitful approach--especially when the events are sparse and dissimilar. Additionally, our results highlight the need for localized models that account for differences between locations. From a machine learning perspective, we found that the Random Forest model outperformed several deep models on our multimodal, noisy, and imbalanced data set, thus demonstrating the efficacy of our novel feature representation method in such a context. We also show that its predictions are relatively robust to time gaps between attacks and observed characteristics of the attacks. Finally, we analyze factors that limit model performance, which include a noisy feature space and small amount of available data. These contributions provide an important foundation for the use of machine learning in efforts against terrorism in the United States and beyond.",
        "completion1":" Terrorism: Localized News Data May Help Predict Attacks, Says Study",
        "completion2":"Can Localized News Data Really Help Predict Terrorist Attacks?",
        "completion3":"Study Shows That Machine Learning Can Improve Prediction of Terrorist Attacks in the United States",
        "technologyreview":0.3441602178,
        "venturebeat":0.3068780912,
        "wired":0.3112029398,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.04292v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1641859200000,
        "code_mentioned":1,
        "readability":0.92
    },
    {
        "arxiv_id":"2201.04796v1",
        "predicted_newsworthiness":0.4424047451,
        "title":"CFNet: Learning Correlation Functions for One-Stage Panoptic Segmentation",
        "summary":"Recently, there is growing attention on one-stage panoptic segmentation methods which aim to segment instances and stuff jointly within a fully convolutional pipeline efficiently. However, most of the existing works directly feed the backbone features to various segmentation heads ignoring the demands for semantic and instance segmentation are different: The former needs semantic-level discriminative features, while the latter requires features to be distinguishable across instances. To alleviate this, we propose to first predict semantic-level and instance-level correlations among different locations that are utilized to enhance the backbone features, and then feed the improved discriminative features into the corresponding segmentation heads, respectively. Specifically, we organize the correlations between a given location and all locations as a continuous sequence and predict it as a whole. Considering that such a sequence can be extremely complicated, we adopt Discrete Fourier Transform (DFT), a tool that can approximate an arbitrary sequence parameterized by amplitudes and phrases. For different tasks, we generate these parameters from the backbone features in a fully convolutional way which is optimized implicitly by corresponding tasks. As a result, these accurate and consistent correlations contribute to producing plausible discriminative features which meet the requirements of the complicated panoptic segmentation task. To verify the effectiveness of our methods, we conduct experiments on several challenging panoptic segmentation datasets and achieve state-of-the-art performance on MS COCO with $45.1$\\% PQ and ADE20k with $32.6$\\% PQ.",
        "completion1":"CFNet: A new method for panoptic segmentation",
        "completion2":"CFNet: Learning Correlation Functions for improved one-stage panoptic segmentation",
        "completion3":"FNet: Discrete Fourier Transform used to approximate complex sequence for panoptic segmentation",
        "technologyreview":0.1928303358,
        "venturebeat":0.1843661975,
        "wired":0.1533243699,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.04796v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1641945600000,
        "code_mentioned":0,
        "readability":0.79
    },
    {
        "arxiv_id":"2201.07604v1",
        "predicted_newsworthiness":0.3920024519,
        "title":"Semi-Supervised Clustering with Contrastive Learning for Discovering New Intents",
        "summary":"Most dialogue systems in real world rely on predefined intents and answers for QA service, so discovering potential intents from large corpus previously is really important for building such dialogue services. Considering that most scenarios have few intents known already and most intents waiting to be discovered, we focus on semi-supervised text clustering and try to make the proposed method benefit from labeled samples for better overall clustering performance. In this paper, we propose Deep Contrastive Semi-supervised Clustering (DCSC), which aims to cluster text samples in a semi-supervised way and provide grouped intents to operation staff. To make DCSC fully utilize the limited known intents, we propose a two-stage training procedure for DCSC, in which DCSC will be trained on both labeled samples and unlabeled samples, and achieve better text representation and clustering performance. We conduct experiments on two public datasets to compare our model with several popular methods, and the results show DCSC achieve best performance across all datasets and circumstances, indicating the effect of the improvements in our work.",
        "completion1":"Deep Contrastive Semi-supervised Clustering: Achieving Best Performance Across All Datasets and Circumstances",
        "completion2":"New Intent Discovery Method Achieves Best Results Yet",
        "completion3":"Semi-Supervised Clustering with Contrastive Learning Yields New Insights",
        "technologyreview":0.2199865449,
        "venturebeat":0.2527258284,
        "wired":0.1819539657,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.07604v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cl"
        ],
        "published":1641513600000,
        "code_mentioned":0,
        "readability":0.83
    },
    {
        "arxiv_id":"2201.11278v1",
        "predicted_newsworthiness":0.4355658432,
        "title":"Hierarchical Visual Interface for Lecture Video Retrieval and Summarization",
        "summary":"With the emergence of large-scale open online courses and online academic conferences, it has become increasingly feasible and convenient to access online educational resources. However, it is time consuming and challenging to effectively retrieve and present numerous lecture videos for common users. In this work, we propose a hierarchical visual interface for retrieving and summarizing lecture videos. Users can utilize the proposed interface to effectively explore the required video information through the results of the video summary generation in different layers. We retrieve the input keywords with the corresponding video layer with timestamps, a frame layer with slides, and the poster layer with summarization of the lecture videos. We verified the proposed interface with our user study by comparing it with other conventional interfaces. The results from our user study confirmed that the proposed interface can achieve high retrieval accuracy and good user experience.see video here https:\/\/www.youtube.com\/watch?v=zrnejwsOVpc .",
        "completion1":"Hierarchical Visual Interface for Lecture Video Retrieval and Summarization.",
        "completion2":"A new way to effectively explore and find lecture videos.",
        "completion3":"The proposed interface achieves high retrieval accuracy and good user experience.",
        "technologyreview":0.2053882338,
        "venturebeat":0.2244239233,
        "wired":0.2239625772,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2201.11278v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1643155200000,
        "code_mentioned":0,
        "readability":0.81
    },
    {
        "arxiv_id":"2202.01365v1",
        "predicted_newsworthiness":0.5219397587,
        "title":"Feasibility of Interactive 3D Map for Remote Sighted Assistance",
        "summary":"Remote sighted assistance (RSA) has emerged as a conversational assistive technology, where remote sighted workers, i.e., agents, provide real-time assistance to users with vision impairments via video-chat-like communication. Researchers found that agents' lack of environmental knowledge, the difficulty of orienting users in their surroundings, and the inability to estimate distances from users' camera feeds are key challenges to sighted agents. To address these challenges, researchers have suggested assisting agents with computer vision technologies, especially 3D reconstruction. This paper presents a high-fidelity prototype of such an RSA, where agents use interactive 3D maps with localization capability. We conducted a walkthrough study with thirteen agents and one user with simulated vision impairment using this prototype. The study revealed that, compared to baseline RSA, the agents were significantly faster in providing navigational assistance to users, and their mental workload was significantly reduced -- all indicate the feasibility and prospect of 3D maps in RSA.",
        "completion1":"Interactive 3D Map Helps Remote Sighted Agents Provide Better Assistance to Those With Vision Impairments",
        "completion2":"Study Shows That 3D Maps Significantly Reduce Mental Workload for Remote Sighted Agents",
        "completion3":"3D Maps May Be the Key to Better Remote Sighted Assistance for Those With Vision Impairments",
        "technologyreview":0.2864226282,
        "venturebeat":0.3257897526,
        "wired":0.2727364139,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.01365v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1643760000000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2202.02278v1",
        "predicted_newsworthiness":0.5011711681,
        "title":"LTU Attacker for Membership Inference",
        "summary":"We address the problem of defending predictive models, such as machine learning classifiers (Defender models), against membership inference attacks, in both the black-box and white-box setting, when the trainer and the trained model are publicly released. The Defender aims at optimizing a dual objective: utility and privacy. Both utility and privacy are evaluated with an external apparatus including an Attacker and an Evaluator. On one hand, Reserved data, distributed similarly to the Defender training data, is used to evaluate Utility; on the other hand, Reserved data, mixed with Defender training data, is used to evaluate membership inference attack robustness. In both cases classification accuracy or error rate are used as the metric: Utility is evaluated with the classification accuracy of the Defender model; Privacy is evaluated with the membership prediction error of a so-called \"Leave-Two-Unlabeled\" LTU Attacker, having access to all of the Defender and Reserved data, except for the membership label of one sample from each. We prove that, under certain conditions, even a \"na\\\"ive\" LTU Attacker can achieve lower bounds on privacy loss with simple attack strategies, leading to concrete necessary conditions to protect privacy, including: preventing over-fitting and adding some amount of randomness. However, we also show that such a na\\\"ive LTU Attacker can fail to attack the privacy of models known to be vulnerable in the literature, demonstrating that knowledge must be complemented with strong attack strategies to turn the LTU Attacker into a powerful means of evaluating privacy. Our experiments on the QMNIST and CIFAR-10 datasets validate our theoretical results and confirm the roles of over-fitting prevention and randomness in the algorithms to protect against privacy attacks.",
        "completion1":"na\\\"ive\" LTU Attacker can achieve lower bounds on privacy loss with simple attack strategies",
        "completion2":"na\\\"ive\" LTU Attacker can fail to attack the privacy of models known to be vulnerable in the literature",
        "completion3":"over-fitting prevention and randomness in the algorithms to protect against privacy attacks",
        "technologyreview":0.2910279871,
        "venturebeat":0.2679426263,
        "wired":0.2357450285,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.02278v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1643932800000,
        "code_mentioned":1,
        "readability":0.87
    },
    {
        "arxiv_id":"2202.03684v1",
        "predicted_newsworthiness":0.3593761731,
        "title":"Efficiently Escaping Saddle Points in Bilevel Optimization",
        "summary":"Bilevel optimization is one of the fundamental problems in machine learning and optimization. Recent theoretical developments in bilevel optimization focus on finding the first-order stationary points for nonconvex-strongly-convex cases. In this paper, we analyze algorithms that can escape saddle points in nonconvex-strongly-convex bilevel optimization. Specifically, we show that the perturbed approximate implicit differentiation (AID) with a warm start strategy finds $\\epsilon$-approximate local minimum of bilevel optimization in $\\tilde{O}(\\epsilon^{-2})$ iterations with high probability. Moreover, we propose an inexact NEgative-curvature-Originated-from-Noise Algorithm (iNEON), a pure first-order algorithm that can escape saddle point and find local minimum of stochastic bilevel optimization. As a by-product, we provide the first nonasymptotic analysis of perturbed multi-step gradient descent ascent (GDmax) algorithm that converges to local minimax point for minimax problems.",
        "completion1":"New algorithm can efficiently escape saddle points in bilevel optimization.",
        "completion2":"iNEON: new algorithm for stochastic bilevel optimization that converges to local minimum.",
        "completion3":"Perturbed multi-step gradient descent ascent (GDmax) algorithm converges to local minimax point.",
        "technologyreview":0.1622305919,
        "venturebeat":0.1622399308,
        "wired":0.1197461472,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.03684v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1644278400000,
        "code_mentioned":0,
        "readability":0.7
    },
    {
        "arxiv_id":"2202.07832v1",
        "predicted_newsworthiness":0.3929932787,
        "title":"Heterogeneous Graph Learning for Explainable Recommendation over Academic Networks",
        "summary":"With the explosive growth of new graduates with research degrees every year, unprecedented challenges arise for early-career researchers to find a job at a suitable institution. This study aims to understand the behavior of academic job transition and hence recommend suitable institutions for PhD graduates. Specifically, we design a deep learning model to predict the career move of early-career researchers and provide suggestions. The design is built on top of scholarly\/academic networks, which contains abundant information about scientific collaboration among scholars and institutions. We construct a heterogeneous scholarly network to facilitate the exploring of the behavior of career moves and the recommendation of institutions for scholars. We devise an unsupervised learning model called HAI (Heterogeneous graph Attention InfoMax) which aggregates attention mechanism and mutual information for institution recommendation. Moreover, we propose scholar attention and meta-path attention to discover the hidden relationships between several meta-paths. With these mechanisms, HAI provides ordered recommendations with explainability. We evaluate HAI upon a real-world dataset against baseline methods. Experimental results verify the effectiveness and efficiency of our approach.",
        "completion1":"Early-career researchers have trouble finding a job at a suitable institution, study finds",
        "completion2":"Deep learning model may help early-career researchers find the right job",
        "completion3":"Heterogeneous graph learning can explain recommendations for academic networks",
        "technologyreview":0.2659757458,
        "venturebeat":0.256423437,
        "wired":0.2192662629,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.07832v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.ai",
            "cs.ir",
            "cs.lg"
        ],
        "published":1644883200000,
        "code_mentioned":0,
        "readability":0.83
    },
    {
        "arxiv_id":"2202.07861v1",
        "predicted_newsworthiness":0.43680082,
        "title":"Practical Network Acceleration with Tiny Sets",
        "summary":"Network compression is effective in accelerating the inference of deep neural networks, but often requires finetuning with all the training data to recover from the accuracy loss. It is impractical in some applications, however, due to data privacy issues or constraints in compression time budget. To deal with the above issues, we propose a method named PRACTISE to accelerate the network with tiny sets of training images. By considering both the pruned part and the unpruned part of a compressed model, PRACTISE alleviates layer-wise error accumulation, which is the main drawback of previous methods. Furthermore, existing methods are confined to few compression schemes, have limited speedup in terms of latency, and are unstable. In contrast, PRACTISE is stable, fast to train, versatile to handle various compression schemes, and achieves low latency. We also propose that dropping entire blocks is a better way than existing compression schemes when only tiny sets of training data are available. Extensive experiments demonstrate that PRACTISE achieves much higher accuracy and more stable models than state-of-the-art methods.",
        "completion1":"Practical Network Acceleration with Tiny Sets",
        "completion2":"Network Compression is Effective in Accelerating the Inference of Deep Neural Networks",
        "completion3":"PRACTISE Achieves Much Higher Accuracy and More Stable Models Than State-Of-The-Art Methods",
        "technologyreview":0.2683560313,
        "venturebeat":0.2482650086,
        "wired":0.1862411729,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.07861v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1644883200000,
        "code_mentioned":1,
        "readability":0.87
    },
    {
        "arxiv_id":"2202.10560v1",
        "predicted_newsworthiness":0.3571317879,
        "title":"Moment Matching Deep Contrastive Latent Variable Models",
        "summary":"In the contrastive analysis (CA) setting, machine learning practitioners are specifically interested in discovering patterns that are enriched in a target dataset as compared to a background dataset generated from sources of variation irrelevant to the task at hand. For example, a biomedical data analyst may seek to understand variations in genomic data only present among patients with a given disease as opposed to those also present in healthy control subjects. Such scenarios have motivated the development of contrastive latent variable models to isolate variations unique to these target datasets from those shared across the target and background datasets, with current state of the art models based on the variational autoencoder (VAE) framework. However, previously proposed models do not explicitly enforce the constraints on latent variables underlying CA, potentially leading to the undesirable leakage of information between the two sets of latent variables. Here we propose the moment matching contrastive VAE (MM-cVAE), a reformulation of the VAE for CA that uses the maximum mean discrepancy to explicitly enforce two crucial latent variable constraints underlying CA. On three challenging CA tasks we find that our method outperforms the previous state-of-the-art both qualitatively and on a set of quantitative metrics.",
        "completion1":"Machine learning practitioners develop new contrastive latent variable models to isolate variations unique to target datasets.",
        "completion2":"The new model, MM-cVAE, outperforms previous state-of-the-art models both qualitatively and quantitatively.",
        "completion3":"MM-cVAE uses the maximum mean discrepancy to explicitly enforce two crucial latent variable constraints underlying CA.",
        "technologyreview":0.1967226617,
        "venturebeat":0.1781009829,
        "wired":0.1230392381,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.10560v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1645401600000,
        "code_mentioned":1,
        "readability":0.8
    },
    {
        "arxiv_id":"2202.10704v1",
        "predicted_newsworthiness":0.4377188646,
        "title":"Privacy-Preserving In-Bed Pose Monitoring: A Fusion and Reconstruction Study",
        "summary":"Recently, in-bed human pose estimation has attracted the interest of researchers due to its relevance to a wide range of healthcare applications. Compared to the general problem of human pose estimation, in-bed pose estimation has several inherent challenges, the most prominent being frequent and severe occlusions caused by bedding. In this paper we explore the effective use of images from multiple non-visual and privacy-preserving modalities such as depth, long-wave infrared (LWIR) and pressure maps for the task of in-bed pose estimation in two settings. First, we explore the effective fusion of information from different imaging modalities for better pose estimation. Secondly, we propose a framework that can estimate in-bed pose estimation when visible images are unavailable, and demonstrate the applicability of fusion methods to scenarios where only LWIR images are available. We analyze and demonstrate the effect of fusing features from multiple modalities. For this purpose, we consider four different techniques: 1) Addition, 2) Concatenation, 3) Fusion via learned modal weights, and 4) End-to-end fully trainable approach; with a state-of-the-art pose estimation model. We also evaluate the effect of reconstructing a data-rich modality (i.e., visible modality) from a privacy-preserving modality with data scarcity (i.e., long-wavelength infrared) for in-bed human pose estimation. For reconstruction, we use a conditional generative adversarial network. We conduct ablative studies across different design decisions of our framework. This includes selecting features with different levels of granularity, using different fusion techniques, and varying model parameters. Through extensive evaluations, we demonstrate that our method produces on par or better results compared to the state-of-the-art.",
        "completion1":"New Technique for In-Bed Pose Estimation Could Improve Healthcare Applications",
        "completion2":"Study Shows how to Preserve Privacy while Monitoring In-Bed Poses",
        "completion3":"Researchers Develop Framework for Estimating Poses in the Absence of Visible Images",
        "technologyreview":0.2115857794,
        "venturebeat":0.2020666314,
        "wired":0.1757075087,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.10704v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1645488000000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2202.11532v2",
        "predicted_newsworthiness":0.5086910464,
        "title":"Inference of Affordances and Active Motor Control in Simulated Agents",
        "summary":"Flexible, goal-directed behavior is a fundamental aspect of human life. Based on the free energy minimization principle, the theory of active inference formalizes the generation of such behavior from a computational neuroscience perspective. Based on the theory, we introduce an output-probabilistic, temporally predictive, modular artificial neural network architecture, which processes sensorimotor information, infers behavior-relevant aspects of its world, and invokes highly flexible, goal-directed behavior. We show that our architecture, which is trained end-to-end to minimize an approximation of free energy, develops latent states that can be interpreted as affordance maps. That is, the emerging latent states signal which actions lead to which effects dependent on the local context. In combination with active inference, we show that flexible, goal-directed behavior can be invoked, incorporating the emerging affordance maps. As a result, our simulated agent flexibly steers through continuous spaces, avoids collisions with obstacles, and prefers pathways that lead to the goal with high certainty. Additionally, we show that the learned agent is highly suitable for zero-shot generalization across environments: After training the agent in a handful of fixed environments with obstacles and other terrains affecting its behavior, it performs similarly well in procedurally generated environments containing different amounts of obstacles and terrains of various sizes at different locations. To improve and focus model learning further, we plan to invoke active inference-based, information-gain-oriented behavior also while learning the temporally predictive model itself in the near future. Moreover, we intend to foster the development of both deeper event-predictive abstractions and compact, habitual behavioral primitives.",
        "completion1":"Introduction of a new artificial neural network architecture that simulates human behavior.",
        "completion2":"This architecture is based on the theory of active inference and is able to learn and interpret latent states as affordance maps.",
        "completion3":"The simulated agent is able to flexibly steer through continuous spaces, avoid collisions with obstacles, and prefer pathways that lead to the goal with high certainty.",
        "technologyreview":0.2966073014,
        "venturebeat":0.2369822009,
        "wired":0.2074857886,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.11532v2",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1645574400000,
        "code_mentioned":0,
        "readability":0.8
    },
    {
        "arxiv_id":"2202.11911v2",
        "predicted_newsworthiness":0.4887494802,
        "title":"When Transformer Meets Robotic Grasping: Exploits Context for Efficient Grasp Detection",
        "summary":"In this paper, we present a transformer-based architecture, namely TF-Grasp, for robotic grasp detection. The developed TF-Grasp framework has two elaborate designs making it well suitable for visual grasping tasks. The first key design is that we adopt the local window attention to capture local contextual information and detailed features of graspable objects. Then, we apply the cross window attention to model the long-term dependencies between distant pixels. Object knowledge, environmental configuration, and relationships between different visual entities are aggregated for subsequent grasp detection. The second key design is that we build a hierarchical encoder-decoder architecture with skip-connections, delivering shallow features from encoder to decoder to enable a multi-scale feature fusion. Due to the powerful attention mechanism, the TF-Grasp can simultaneously obtain the local information (i.e., the contours of objects), and model long-term connections such as the relationships between distinct visual concepts in clutter. Extensive computational experiments demonstrate that the TF-Grasp achieves superior results versus state-of-art grasping convolutional models and attain a higher accuracy of 97.99% and 94.6% on Cornell and Jacquard grasping datasets, respectively. Real-world experiments using a 7DoF Franka Emika Panda robot also demonstrate its capability of grasping unseen objects in a variety of scenarios. The code and pre-trained models will be available at https:\/\/github.com\/WangShaoSUN\/grasp-transformer",
        "completion1":"New transformer-based architecture for robotic grasp detection presents two key designs.",
        "completion2":"Transformer-based architecture for robotic grasp detection outperforms state-of-art models.",
        "completion3":"Transformer-based architecture for robotic grasp detection demonstrated in real-world scenarios.",
        "technologyreview":0.2264210836,
        "venturebeat":0.1901413503,
        "wired":0.1558954895,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2202.11911v2",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai",
            "cs.cv"
        ],
        "published":1645574400000,
        "code_mentioned":1,
        "readability":0.78
    },
    {
        "arxiv_id":"2203.00762v1",
        "predicted_newsworthiness":0.4156294257,
        "title":"Topic Analysis for Text with Side Data",
        "summary":"Although latent factor models (e.g., matrix factorization) obtain good performance in predictions, they suffer from several problems including cold-start, non-transparency, and suboptimal recommendations. In this paper, we employ text with side data to tackle these limitations. We introduce a hybrid generative probabilistic model that combines a neural network with a latent topic model, which is a four-level hierarchical Bayesian model. In the model, each document is modeled as a finite mixture over an underlying set of topics and each topic is modeled as an infinite mixture over an underlying set of topic probabilities. Furthermore, each topic probability is modeled as a finite mixture over side data. In the context of text, the neural network provides an overview distribution about side data for the corresponding text, which is the prior distribution in LDA to help perform topic grouping. The approach is evaluated on several different datasets, where the model is shown to outperform standard LDA and Dirichlet-multinomial regression (DMR) in terms of topic grouping, model perplexity, classification and comment generation.",
        "completion1":"A new hybrid probabilistic model tackles the limitations of latent factor models.",
        "completion2":"The four-level hierarchical Bayesian model outperforms standard LDA in topic grouping, perplexity, classification and comment generation.",
        "completion3":"The neural network provides an overview distribution of side data for the corresponding text.",
        "technologyreview":0.1965659222,
        "venturebeat":0.2023778501,
        "wired":0.1782010338,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.00762v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cl",
            "cs.ir"
        ],
        "published":1646092800000,
        "code_mentioned":1,
        "readability":0.79
    },
    {
        "arxiv_id":"2203.01784v2",
        "predicted_newsworthiness":0.5023470912,
        "title":"Revisiting Click-based Interactive Video Object Segmentation",
        "summary":"While current methods for interactive Video Object Segmentation (iVOS) rely on scribble-based interactions to generate precise object masks, we propose a Click-based interactive Video Object Segmentation (CiVOS) framework to simplify the required user workload as much as possible. CiVOS builds on de-coupled modules reflecting user interaction and mask propagation. The interaction module converts click-based interactions into an object mask, which is then inferred to the remaining frames by the propagation module. Additional user interactions allow for a refinement of the object mask. The approach is extensively evaluated on the popular interactive~DAVIS dataset, but with an inevitable adaptation of scribble-based interactions with click-based counterparts. We consider several strategies for generating clicks during our evaluation to reflect various user inputs and adjust the DAVIS performance metric to perform a hardware-independent comparison. The presented CiVOS pipeline achieves competitive results, although requiring a lower user workload.",
        "completion1":"A new, simpler method for interactive Video Object Segmentation.",
        "completion2":"CiVOS: Click-based interactive Video Object Segmentation.",
        "completion3":"Evaluating the effectiveness of click-based interactions for object segmentation in video.",
        "technologyreview":0.1974562993,
        "venturebeat":0.207855711,
        "wired":0.1795805482,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.01784v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1646265600000,
        "code_mentioned":1,
        "readability":0.75
    },
    {
        "arxiv_id":"2203.09053v2",
        "predicted_newsworthiness":0.3974051084,
        "title":"Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework",
        "summary":"Simultaneous machine translation (SiMT) starts translating while receiving the streaming source inputs, and hence the source sentence is always incomplete during translating. Different from the full-sentence MT using the conventional seq-to-seq architecture, SiMT often applies prefix-to-prefix architecture, which forces each target word to only align with a partial source prefix to adapt to the incomplete source in streaming inputs. However, the source words in the front positions are always illusoryly considered more important since they appear in more prefixes, resulting in position bias, which makes the model pay more attention on the front source positions in testing. In this paper, we first analyze the phenomenon of position bias in SiMT, and develop a Length-Aware Framework to reduce the position bias by bridging the structural gap between SiMT and full-sentence MT. Specifically, given the streaming inputs, we first predict the full-sentence length and then fill the future source position with positional encoding, thereby turning the streaming inputs into a pseudo full-sentence. The proposed framework can be integrated into most existing SiMT methods to further improve performance. Experiments on two representative SiMT methods, including the state-of-the-art adaptive policy, show that our method successfully reduces the position bias and thereby achieves better SiMT performance.",
        "completion1":"Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework",
        "completion2":"New Framework Aims to Improve Simultaneous Machine Translation",
        "completion3":"Researchers Develop Method to Reduce Position Bias in Simultaneous Machine Translation",
        "technologyreview":0.1703364267,
        "venturebeat":0.1877461987,
        "wired":0.1505687101,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.09053v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1647388800000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2203.09210v3",
        "predicted_newsworthiness":0.3969105746,
        "title":"Universal Conditional Masked Language Pre-training for Neural Machine Translation",
        "summary":"Pre-trained sequence-to-sequence models have significantly improved Neural Machine Translation (NMT). Different from prior works where pre-trained models usually adopt an unidirectional decoder, this paper demonstrates that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. We also introduce two simple but effective methods to enhance the CeMAT, aligned code-switching & masking and dynamic dual-masking. We conduct extensive experiments and show that our CeMAT can achieve significant performance improvement for all scenarios from low- to extremely high-resource languages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it can also produce consistent performance gains, i.e., up to +5.3 BLEU. To the best of our knowledge, this is the first work to pre-train a unified model for fine-tuning on both NMT tasks. Code, data, and pre-trained models are available at https:\/\/github.com\/huawei-noah\/Pretrained-Language-Model\/tree\/master\/CeMAT.",
        "completion1":"CeMAT: Universal Conditional Masked Language Pre-training for Neural Machine Translation",
        "completion2":"New pre-trained sequence-to-sequence model outperforms prior works",
        "completion3":"CeMAT achieves significant performance improvement for all scenarios from low- to extremely high-resource languages",
        "technologyreview":0.201473651,
        "venturebeat":0.1963250979,
        "wired":0.1393395878,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2203.09210v3",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1647475200000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2204.02739v1",
        "predicted_newsworthiness":0.5490375301,
        "title":"P4RROT: Generating P4 Code for the Application Layer",
        "summary":"Throughput and latency critical applications could often benefit of performing computations close to the client. To enable this, distributed computing paradigms such as edge computing have recently emerged. However, with the advent of programmable data planes, computations cannot only be performed by servers but they can be offloaded to network switches. Languages like P4 enable to flexibly reprogram the entire packet processing pipeline. Though these devices promise high throughput and ultra-low response times, implementing application-layer tasks in the data plane programming language P4 is still challenging for an application developer who is not familiar with networking domain. In this paper, we first identify and examine obstacles and pain points one can experience when offloading server-based computations to the network. Then we present P4RROT, a code generator (in form of a library) which allows to overcome these limitations by providing a user-friendly API to describe computations to be offloaded. After discussing the design choices behind P4RROT, we introduce our proof-of-concept implementation for two P4 targets: Netronome SmartNIC and BMv2.",
        "completion1":"P4RROT: Generating P4 Code for the Application Layer",
        "completion2":"Overcoming Obstacles to Offload Server-Based Computations to Network Switches",
        "completion3":"Introducing P4RROT: A Code Generator for P4",
        "technologyreview":0.2227612598,
        "venturebeat":0.2897508561,
        "wired":0.209395735,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.02739v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1649203200000,
        "code_mentioned":1,
        "readability":0.83
    },
    {
        "arxiv_id":"2204.05409v1",
        "predicted_newsworthiness":0.4747674905,
        "title":"Unified Speech-Text Pre-training for Speech Translation and Recognition",
        "summary":"We describe a method to jointly pre-train speech and text in an encoder-decoder modeling framework for speech translation and recognition. The proposed method incorporates four self-supervised and supervised subtasks for cross modality learning. A self-supervised speech subtask leverages unlabelled speech data, and a (self-)supervised text to text subtask makes use of abundant text training data. Two auxiliary supervised speech tasks are included to unify speech and text modeling space. Our contribution lies in integrating linguistic information from the text corpus into the speech pre-training. Detailed analysis reveals learning interference among subtasks. Two pre-training configurations for speech translation and recognition, respectively, are presented to alleviate subtask interference. Our experiments show the proposed method can effectively fuse speech and text information into one model. It achieves between 1.7 and 2.3 BLEU improvement above the state of the art on the MuST-C speech translation dataset and comparable WERs to wav2vec 2.0 on the Librispeech speech recognition task.",
        "completion1":"Unified Speech-Text Pre-training for Speech Translation and Recognition",
        "completion2":"We describe a method to jointly pre-train speech and text in an encoder-decoder modeling framework for speech translation and recognition",
        "completion3":"The proposed method incorporates four self-supervised and supervised subtasks for cross modality learning",
        "technologyreview":0.1982126711,
        "venturebeat":0.2105034513,
        "wired":0.1605977779,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.05409v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1649635200000,
        "code_mentioned":1,
        "readability":0.81
    },
    {
        "arxiv_id":"2204.08039v1",
        "predicted_newsworthiness":0.5055880307,
        "title":"Pathologies of Pre-trained Language Models in Few-shot Fine-tuning",
        "summary":"Although adapting pre-trained language models with few examples has shown promising performance on text classification, there is a lack of understanding of where the performance gain comes from. In this work, we propose to answer this question by interpreting the adaptation behavior using post-hoc explanations from model predictions. By modeling feature statistics of explanations, we discover that (1) without fine-tuning, pre-trained models (e.g. BERT and RoBERTa) show strong prediction bias across labels; (2) although few-shot fine-tuning can mitigate the prediction bias and demonstrate promising prediction performance, our analysis shows models gain performance improvement by capturing non-task-related features (e.g. stop words) or shallow data patterns (e.g. lexical overlaps). These observations alert that pursuing model performance with fewer examples may incur pathological prediction behavior, which requires further sanity check on model predictions and careful design in model evaluations in few-shot fine-tuning.",
        "completion1":"Pathologies of Pre-trained Language Models in Few-shot Fine-tuning",
        "completion2":"Pre-trained Language Models Show Promise in Few-shot Fine-tuning",
        "completion3":"Pathologies Found in Pre-trained Language Models Used for Few-shot Fine-tuning",
        "technologyreview":0.2588556908,
        "venturebeat":0.2484837559,
        "wired":0.1857014916,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.08039v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1650153600000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2204.09186v2",
        "predicted_newsworthiness":0.3515754491,
        "title":"Reconstruction-Aware Prior Distillation for Semi-supervised Point Cloud Completion",
        "summary":"Point clouds scanned by real-world sensors are always incomplete, irregular, and noisy, making the point cloud completion task become increasingly more important. Though many point cloud completion methods have been proposed, most of them require a large number of paired complete-incomplete point clouds for training, which is labor exhausted. In contrast, this paper proposes a novel Reconstruction-Aware Prior Distillation semi-supervised point cloud completion method named RaPD, which takes advantage of a two-stage training scheme to reduce the dependence on a large-scale paired dataset. In training stage 1, the so-called deep semantic prior is learned from both unpaired complete and unpaired incomplete point clouds using a reconstruction-aware pretraining process. While in training stage 2, we introduce a semi-supervised prior distillation process, where an encoder-decoder-based completion network is trained by distilling the prior into the network utilizing only a small number of paired training samples. A self-supervised completion module is further introduced, excavating the value of a large number of unpaired incomplete point clouds, leading to an increase in the network's performance. Extensive experiments on several widely used datasets demonstrate that RaPD, the first semi-supervised point cloud completion method, achieves superior performance to previous methods on both homologous and heterologous scenarios.",
        "completion1":"The first semi-supervised point cloud completion method is proposed",
        "completion2":"The method is named RaPD and utilizes a two-stage training scheme",
        "completion3":"The method has been shown to superior to previous methods in extensive experiments",
        "technologyreview":0.1908130771,
        "venturebeat":0.1789888362,
        "wired":0.1428593409,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.09186v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1650326400000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2204.09524v1",
        "predicted_newsworthiness":0.4689648564,
        "title":"An Empirical Study on the Relationship Between the Number of Coordinated Views and Visual Analysis",
        "summary":"Coordinated Multiple views (CMVs) are a visualization technique that simultaneously presents multiple visualizations in separate but linked views. There are many studies that report the advantages (e.g., usefulness for finding hidden relationships) and disadvantages (e.g., cognitive load) of CMVs. But little empirical work exists on the impact of the number of views on visual anlaysis results and processes, which results in uncertainty in the relationship between the view number and visual anlaysis. In this work, we aim at investigating the relationship between the number of coordinated views and users analytic processes and results. To achieve the goal, we implemented a CMV tool for visual anlaysis. We also provided visualization duplication in the tool to help users easily create a desired number of visualization views on-the-fly. We conducted a between-subject study with 44 participants, where we asked participants to solve five analytic problems using the visual tool. Through quantitative and qualitative analysis, we discovered the positive correlation between the number of views and analytic results. We also found that visualization duplication encourages users to create more views and to take various analysis strategies. Based on the results, we provide implications and limitations of our study.",
        "completion1":"CMVs: The More the Merrier?",
        "completion2":"Coordinated Multiple Views: Pros and Cons",
        "completion3":"tudy Finds Relationship Between Number of Coordinated Views and Visual Analysis",
        "technologyreview":0.2328386751,
        "venturebeat":0.2532642516,
        "wired":0.2131700249,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.09524v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1650412800000,
        "code_mentioned":1,
        "readability":0.84
    },
    {
        "arxiv_id":"2204.13921v1",
        "predicted_newsworthiness":0.4831230469,
        "title":"QRelScore: Better Evaluating Generated Questions with Deeper Understanding of Context-aware Relevance",
        "summary":"Existing metrics for assessing question generation not only require costly human reference but also fail to take into account the input context of generation, rendering the lack of deep understanding of the relevance between the generated questions and input contexts. As a result, they may wrongly penalize a legitimate and reasonable candidate question when it (i) involves complicated reasoning with the context or (ii) can be grounded by multiple evidences in the context. In this paper, we propose $\\textbf{QRelScore}$, a context-aware $\\underline{\\textbf{Rel}}$evance evaluation metric for $\\underline{\\textbf{Q}}$uestion Generation. Based on off-the-shelf language models such as BERT and GPT2, QRelScore employs both word-level hierarchical matching and sentence-level prompt-based generation to cope with the complicated reasoning and diverse generation from multiple evidences, respectively. Compared with existing metrics, our experiments demonstrate that QRelScore is able to achieve a higher correlation with human judgments while being much more robust to adversarial samples.",
        "completion1":"QRelScore: A Better Way to Evaluate Generated Questions",
        "completion2":"New Metric for Assessing Question Generation Takes Into Account Context",
        "completion3":"QRelScore Improves Accuracy of Question Generation Metrics",
        "technologyreview":0.2378975252,
        "venturebeat":0.2346336033,
        "wired":0.1899112192,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2204.13921v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1651190400000,
        "code_mentioned":0,
        "readability":0.86
    },
    {
        "arxiv_id":"2205.00809v1",
        "predicted_newsworthiness":0.6550556109,
        "title":"Automated Driving Systems: Impact of Haptic Guidance on Driving Performance after a Take Over Request",
        "summary":"In conditional automation, a response from the driver is expected when a take over request is issued due to unexpected events, emergencies, or reaching the operational design domain boundaries. Cooperation between the automated driving system and the driver can help to guarantee a safe and pleasant transfer if the driver is guided through a haptic guidance system that applies a slight counter-steering force to the steering wheel. We examine in this work the impact of haptic guidance systems on driving performance after a take over request was triggered to avoid sudden obstacles on the road. We studied different driver conditions that involved Non-Driving Related Tasks (NRDT). Results showed that haptic guidance systems increased road safety by reducing the lateral error, the distance and reaction time to a sudden obstacle and the number of collisions.",
        "completion1":"Haptic Guidance Systems Improve Driving Performance After Take Over Requests",
        "completion2":"Study: Haptic Guidance Systems Increase Road Safety",
        "completion3":"New Technology Improves DriverSafety and Reaction Time",
        "technologyreview":0.274304535,
        "venturebeat":0.2449465123,
        "wired":0.233952932,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.00809v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1651449600000,
        "code_mentioned":0,
        "readability":0.92
    },
    {
        "arxiv_id":"2205.08210v2",
        "predicted_newsworthiness":0.5523108055,
        "title":"Towards Robotic Laboratory Automation Plug & Play: Teaching-free Robot Integration with the LAPP Digital Twin",
        "summary":"The Laboratory Automation Plug & Play (LAPP) framework is a high-level abstraction layer that makes the autonomous operation of life science laboratory robots possible. The plug & play nature lies in the fact that the manual teaching and configuration of robots is not required. A digital twin (DT) based concept is proposed that outlines the types of information that has to be provided for each relevant component of the system. In particular, for the devices that the robot interfaces with, the robot positions have to be defined beforehand in a device-attached coordinate system (CS) by the vendor. This CS has to be detectable by the vision system of the robot by means of optical markers placed on the front side of the device. With that, the robot is capable of tending the machine by performing the pick-and-place type transportation of standard sample carriers. This basic use case is the primary scope of the LAPP-DT framework. The hardware scope is limited to simple benchtop and mobile manipulators with parallel grippers at this stage. This paper first provides an overview of relevant literature and state-of-the-art solutions, after which it outlines the framework on the conceptual level, followed by the specification of the relevant DT parameters for the robot, for the devices and for the facility. Finally, appropriate technologies and strategies are identified for the implementation.",
        "completion1":"New framework makes autonomous operation of life science laboratory robots possible.",
        "completion2":"Digital twin based concept proposed for robotic laboratory automation.",
        "completion3":"LAPP-DT framework outlines the types of information that has to be provided for each relevant component of the system.",
        "technologyreview":0.2517570541,
        "venturebeat":0.205299972,
        "wired":0.1985752338,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.08210v2",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1652745600000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2205.11664v4",
        "predicted_newsworthiness":0.4216611373,
        "title":"Towards Model Generalization for Monocular 3D Object Detection",
        "summary":"Monocular 3D object detection (Mono3D) has achieved tremendous improvements with emerging large-scale autonomous driving datasets and the rapid development of deep learning techniques. However, caused by severe domain gaps (e.g., the field of view (FOV), pixel size, and object size among datasets), Mono3D detectors have difficulty in generalization, leading to drastic performance degradation on unseen domains. To solve these issues, we combine the position-invariant transform and multi-scale training with the pixel-size depth strategy to construct an effective unified camera-generalized paradigm (CGP). It fully considers discrepancies in the FOV and pixel size of images captured by different cameras. Moreover, we further investigate the obstacle in quantitative metrics when cross-dataset inference through an exhaustive systematic study. We discern that the size bias of prediction leads to a colossal failure. Hence, we propose the 2D-3D geometry-consistent object scaling strategy (GCOS) to bridge the gap via an instance-level augment. Our method called DGMono3D achieves remarkable performance on all evaluated datasets and surpasses the SoTA unsupervised domain adaptation scheme even without utilizing data on the target domain.",
        "completion1":"New breakthrough in 3D object detection allows for cross-dataset inference.",
        "completion2":"DGMono3D outperforms current state-of-the-art unsupervised domain adaptation scheme.",
        "completion3":"2D-3D geometry-consistent object scaling strategy bridges the gap for cross-dataset inference.",
        "technologyreview":0.2354457143,
        "venturebeat":0.2215207332,
        "wired":0.184922125,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2205.11664v4",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1653264000000,
        "code_mentioned":0,
        "readability":0.76
    },
    {
        "arxiv_id":"2206.00335v1",
        "predicted_newsworthiness":0.8175500921,
        "title":"Putting AI Ethics into Practice: The Hourglass Model of Organizational AI Governance",
        "summary":"The organizational use of artificial intelligence (AI) has rapidly spread across various sectors. Alongside the awareness of the benefits brought by AI, there is a growing consensus on the necessity of tackling the risks and potential harms, such as bias and discrimination, brought about by advanced AI technologies. A multitude of AI ethics principles have been proposed to tackle these risks, but the outlines of organizational processes and practices for ensuring socially responsible AI development are in a nascent state. To address the paucity of comprehensive governance models, we present an AI governance framework, the hourglass model of organizational AI governance, which targets organizations that develop and use AI systems. The framework is designed to help organizations deploying AI systems translate ethical AI principles into practice and align their AI systems and processes with the forthcoming European AI Act. The hourglass framework includes governance requirements at the environmental, organizational, and AI system levels. At the AI system level, we connect governance requirements to AI system life cycles to ensure governance throughout the system's life span. The governance model highlights the systemic nature of AI governance and opens new research avenues into its practical implementation, the mechanisms that connect different AI governance layers, and the dynamics between the AI governance actors. The model also offers a starting point for organizational decision-makers to consider the governance components needed to ensure social acceptability, mitigate risks, and realize the potential of AI.",
        "completion1":"new AI governance framework offers organizations a way to ensure ethical development and use of AI systems",
        "completion2":"European AI Act sparks conversation on best practices for global AI system governance",
        "completion3":"Hourglass model of organizational AI governance offers a comprehensive look at how to put ethics into practice",
        "technologyreview":0.5044258979,
        "venturebeat":0.4538775053,
        "wired":0.3943554187,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.00335v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.cy"
        ],
        "published":1654041600000,
        "code_mentioned":0,
        "readability":0.87
    },
    {
        "arxiv_id":"2206.01585v1",
        "predicted_newsworthiness":0.4202256759,
        "title":"Extracting Similar Questions From Naturally-occurring Business Conversations",
        "summary":"Pre-trained contextualized embedding models such as BERT are a standard building block in many natural language processing systems. We demonstrate that the sentence-level representations produced by some off-the-shelf contextualized embedding models have a narrow distribution in the embedding space, and thus perform poorly for the task of identifying semantically similar questions in real-world English business conversations. We describe a method that uses appropriately tuned representations and a small set of exemplars to group questions of interest to business users in a visualization that can be used for data exploration or employee coaching.",
        "completion1":"Pre-trained contextualized embedding models such as BERT are a standard building block in many natural language processing systems.",
        "completion2":"We demonstrate that the sentence-level representations produced by some off-the-shelf contextualized embedding models have a narrow distribution in the embedding space, and thus perform poorly for the task of identifying semantically similar questions in real-world English business conversations.",
        "completion3":"We describe a method that uses appropriately tuned representations and a small set of exemplars to group questions of interest to business users in a visualization that can be used for data exploration or employee coaching.",
        "technologyreview":0.2147117891,
        "venturebeat":0.2517879928,
        "wired":0.1804085553,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.01585v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1654214400000,
        "code_mentioned":0,
        "readability":0.87
    },
    {
        "arxiv_id":"2206.01714v2",
        "predicted_newsworthiness":0.405902806,
        "title":"Compositional Visual Generation with Composable Diffusion Models",
        "summary":"Large text-guided diffusion models, such as DALLE-2, are able to generate stunning photorealistic images given natural language descriptions. While such models are highly flexible, they struggle to understand the composition of certain concepts, such as confusing the attributes of different objects or relations between objects. In this paper, we propose an alternative structured approach for compositional generation using diffusion models. An image is generated by composing a set of diffusion models, with each of them modeling a certain component of the image. To do this, we interpret diffusion models as energy-based models in which the data distributions defined by the energy functions may be explicitly combined. The proposed method can generate scenes at test time that are substantially more complex than those seen in training, composing sentence descriptions, object relations, human facial attributes, and even generalizing to new combinations that are rarely seen in the real world. We further illustrate how our approach may be used to compose pre-trained text-guided diffusion models and generate photorealistic images containing all the details described in the input descriptions, including the binding of certain object attributes that have been shown difficult for DALLE-2. These results point to the effectiveness of the proposed method in promoting structured generalization for visual generation. Project page: https:\/\/energy-based-model.github.io\/Compositional-Visual-Generation-with-Composable-Diffusion-Models\/",
        "completion1":"New method for compositional visual generation produces stunning photorealistic images",
        "completion2":" Diffusion models interpret energy-based data distributions to generate complex scenes",
        "completion3":"Pre-trained text-guided diffusion models compose photorealistic images with binding of object attributes",
        "technologyreview":0.2378355978,
        "venturebeat":0.2032286372,
        "wired":0.1992442316,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.01714v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1654214400000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2206.02559v1",
        "predicted_newsworthiness":0.4902134191,
        "title":"Conversation Group Detection With Spatio-Temporal Context",
        "summary":"In this work, we propose an approach for detecting conversation groups in social scenarios like cocktail parties and networking events, from overhead camera recordings. We posit the detection of conversation groups as a learning problem that could benefit from leveraging the spatial context of the surroundings, and the inherent temporal context in interpersonal dynamics which is reflected in the temporal dynamics in human behavior signals, an aspect that has not been addressed in recent prior works. This motivates our approach which consists of a dynamic LSTM-based deep learning model that predicts continuous pairwise affinity values indicating how likely two people are in the same conversation group. These affinity values are also continuous in time, since relationships and group membership do not occur instantaneously, even though the ground truths of group membership are binary. Using the predicted affinity values, we apply a graph clustering method based on Dominant Set extraction to identify the conversation groups. We benchmark the proposed method against established methods on multiple social interaction datasets. Our results showed that the proposed method improves group detection performance in data that has more temporal granularity in conversation group labels. Additionally, we provide an analysis in the predicted affinity values in relation to the conversation group detection. Finally, we demonstrate the usability of the predicted affinity values in a forecasting framework to predict group membership for a given forecast horizon.",
        "completion1":"A new approach for detecting conversation groups could improve performance in data that has more temporal granularity in group labels.",
        "completion2":"The proposed method for detecting conversation groups uses a dynamic LSTM-based deep learning model to predict continuous pairwise affinity values.",
        "completion3":"The predicted affinity values are used to apply a graph clustering method based on Dominant Set extraction to identify the conversation groups.",
        "technologyreview":0.2451762427,
        "venturebeat":0.2417727896,
        "wired":0.2267648356,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.02559v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1654128000000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2206.04846v1",
        "predicted_newsworthiness":0.4074724118,
        "title":"Masked Autoencoders are Robust Data Augmentors",
        "summary":"Deep neural networks are capable of learning powerful representations to tackle complex vision tasks but expose undesirable properties like the over-fitting issue. To this end, regularization techniques like image augmentation are necessary for deep neural networks to generalize well. Nevertheless, most prevalent image augmentation recipes confine themselves to off-the-shelf linear transformations like scale, flip, and colorjitter. Due to their hand-crafted property, these augmentations are insufficient to generate truly hard augmented examples. In this paper, we propose a novel perspective of augmentation to regularize the training process. Inspired by the recent success of applying masked image modeling to self-supervised learning, we adopt the self-supervised masked autoencoder to generate the distorted view of the input images. We show that utilizing such model-based nonlinear transformation as data augmentation can improve high-level recognition tasks. We term the proposed method as \\textbf{M}ask-\\textbf{R}econstruct \\textbf{A}ugmentation (MRA). The extensive experiments on various image classification benchmarks verify the effectiveness of the proposed augmentation. Specifically, MRA consistently enhances the performance on supervised, semi-supervised as well as few-shot classification. The code will be available at \\url{https:\/\/github.com\/haohang96\/MRA}.",
        "completion1":" Masked autoencoders found to be robust data augmentors.",
        "completion2":"Deep neural networks learn powerful representations to tackle complex vision tasks.",
        "completion3":"Regularization techniques like image augmentation are necessary for deep neural networks to generalize well.",
        "technologyreview":0.287560437,
        "venturebeat":0.2494727178,
        "wired":0.1922040115,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.04846v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1654732800000,
        "code_mentioned":1,
        "readability":0.79
    },
    {
        "arxiv_id":"2206.05519v1",
        "predicted_newsworthiness":0.5394578095,
        "title":"Bridging the Gap Between Training and Inference of Bayesian Controllable Language Models",
        "summary":"Large-scale pre-trained language models have achieved great success on natural language generation tasks. However, it is difficult to control the pre-trained language models to generate sentences with the desired attribute such as topic and sentiment, etc. Recently, Bayesian Controllable Language Models (BCLMs) have been shown to be efficient in controllable language generation. Rather than fine-tuning the parameters of pre-trained language models, BCLMs use external discriminators to guide the generation of pre-trained language models. However, the mismatch between training and inference of BCLMs limits the performance of the models. To address the problem, in this work we propose a \"Gemini Discriminator\" for controllable language generation which alleviates the mismatch problem with a small computational cost. We tested our method on two controllable language generation tasks: sentiment control and topic control. On both tasks, our method reached achieved new state-of-the-art results in automatic and human evaluations.",
        "completion1":"New state-of-the-art results in controllable language generation.",
        "completion2":"Gemini Discriminator alleviates mismatch problem with small computational cost.",
        "completion3":"BCLMs use external discriminators to guide pre-trained language model generation.",
        "technologyreview":0.2324013109,
        "venturebeat":0.2372110539,
        "wired":0.1877670841,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.05519v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1654905600000,
        "code_mentioned":0,
        "readability":0.85
    },
    {
        "arxiv_id":"2206.06588v1",
        "predicted_newsworthiness":0.4718490067,
        "title":"Shopping Queries Dataset: A Large-Scale ESCI Benchmark for Improving Product Search",
        "summary":"Improving the quality of search results can significantly enhance users experience and engagement with search engines. In spite of several recent advancements in the fields of machine learning and data mining, correctly classifying items for a particular user search query has been a long-standing challenge, which still has a large room for improvement. This paper introduces the \"Shopping Queries Dataset\", a large dataset of difficult Amazon search queries and results, publicly released with the aim of fostering research in improving the quality of search results. The dataset contains around 130 thousand unique queries and 2.6 million manually labeled (query,product) relevance judgements. The dataset is multilingual with queries in English, Japanese, and Spanish. The Shopping Queries Dataset is being used in one of the KDDCup'22 challenges. In this paper, we describe the dataset and present three evaluation tasks along with baseline results: (i) ranking the results list, (ii) classifying product results into relevance categories, and (iii) identifying substitute products for a given query. We anticipate that this data will become the gold standard for future research in the topic of product search.",
        "completion1":"Shopping Queries Dataset\": A Large-Scale ESCI Benchmark for Improving Product Search",
        "completion2":"The Shopping Queries Dataset: A New Resource for Enhancing Product Search Quality",
        "completion3":"Introducing the Shopping Queries Dataset: A Benchmark for Improved Product Search",
        "technologyreview":0.269690892,
        "venturebeat":0.3016354135,
        "wired":0.2419125421,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.06588v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.lg"
        ],
        "published":1655078400000,
        "code_mentioned":1,
        "readability":0.85
    },
    {
        "arxiv_id":"2206.13559v2",
        "predicted_newsworthiness":0.4339017207,
        "title":"ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning for Action Recognition",
        "summary":"Capitalizing on large pre-trained models for various downstream tasks of interest have recently emerged with promising performance. Due to the ever-growing model size, the standard full fine-tuning based task adaptation strategy becomes prohibitively costly in terms of model training and storage. This has led to a new research direction in parameter-efficient transfer learning. However, existing attempts typically focus on downstream tasks from the same modality (e.g., image understanding) of the pre-trained model. This creates a limit because in some specific modalities, (e.g., video understanding) such a strong pre-trained model with sufficient knowledge is less or not available. In this work, we investigate such a novel cross-modality transfer learning setting, namely parameter-efficient image-to-video transfer learning. To solve this problem, we propose a new Spatio-Temporal Adapter (ST-Adapter) for parameter-efficient fine-tuning per video task. With a built-in spatio-temporal reasoning capability in a compact design, ST-Adapter enables a pre-trained image model without temporal knowledge to reason about dynamic video content at a small (~8%) per-task parameter cost, requiring approximately 20 times fewer updated parameters compared to previous work. Extensive experiments on video action recognition tasks show that our ST-Adapter can match or even outperform the strong full fine-tuning strategy and state-of-the-art video models, whilst enjoying the advantage of parameter efficiency.",
        "completion1":"ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning for Action Recognition",
        "completion2":"Capitalizing on large pre-trained models for various downstream tasks of interest have recently emerged with promising performance",
        "completion3":"Due to the ever-growing model size, the standard full fine-tuning based task adaptation strategy becomes prohibitively costly in terms of model training and storage",
        "technologyreview":0.1844122142,
        "venturebeat":0.1708604598,
        "wired":0.1353762716,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.13559v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1656288000000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2206.14998v1",
        "predicted_newsworthiness":0.4427145331,
        "title":"Understanding Physical Effects for Effective Tool-use",
        "summary":"We present a robot learning and planning framework that produces an effective tool-use strategy with the least joint efforts, capable of handling objects different from training. Leveraging a Finite Element Method (FEM)-based simulator that reproduces fine-grained, continuous visual and physical effects given observed tool-use events, the essential physical properties contributing to the effects are identified through the proposed Iterative Deepening Symbolic Regression (IDSR) algorithm. We further devise an optimal control-based motion planning scheme to integrate robot- and tool-specific kinematics and dynamics to produce an effective trajectory that enacts the learned properties. In simulation, we demonstrate that the proposed framework can produce more effective tool-use strategies, drastically different from the observed ones in two exemplar tasks.",
        "completion1":"A new robot learning and planning framework produces effective tool-use strategies with the least joint efforts.",
        "completion2":"The proposed framework can produce more effective tool-use strategies, drastically different from the observed ones in two exemplar tasks.",
        "completion3":"The essential physical properties contributing to the effects are identified through the proposed Iterative Deepening Symbolic Regression algorithm.",
        "technologyreview":0.248264788,
        "venturebeat":0.1895977944,
        "wired":0.1879499089,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2206.14998v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai"
        ],
        "published":1656460800000,
        "code_mentioned":0,
        "readability":0.83
    },
    {
        "arxiv_id":"2207.01600v1",
        "predicted_newsworthiness":0.4279044414,
        "title":"CRFormer: A Cross-Region Transformer for Shadow Removal",
        "summary":"Aiming to restore the original intensity of shadow regions in an image and make them compatible with the remaining non-shadow regions without a trace, shadow removal is a very challenging problem that benefits many downstream image\/video-related tasks. Recently, transformers have shown their strong capability in various applications by capturing global pixel interactions and this capability is highly desirable in shadow removal. However, applying transformers to promote shadow removal is non-trivial for the following two reasons: 1) The patchify operation is not suitable for shadow removal due to irregular shadow shapes; 2) shadow removal only needs one-way interaction from the non-shadow region to the shadow region instead of the common two-way interactions among all pixels in the image. In this paper, we propose a novel cross-region transformer, namely CRFormer, for shadow removal which differs from existing transformers by only considering the pixel interactions from the non-shadow region to the shadow region without splitting images into patches. This is achieved by a carefully designed region-aware cross-attention operation that can aggregate the recovered shadow region features conditioned on the non-shadow region features. Extensive experiments on ISTD, AISTD, SRD, and Video Shadow Removal datasets demonstrate the superiority of our method compared to other state-of-the-art methods.",
        "completion1":"A new transformer for shadow removal promises better results than current methods.",
        "completion2":"The CRFormer overcomes several challenges facing other transformers for shadow removal.",
        "completion3":"Extensive experiments demonstrate the superiority of the CRFormer for shadow removal.",
        "technologyreview":0.154843122,
        "venturebeat":0.1439890808,
        "wired":0.1214869018,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.01600v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1656892800000,
        "code_mentioned":0,
        "readability":0.88
    },
    {
        "arxiv_id":"2207.01955v1",
        "predicted_newsworthiness":0.5128726488,
        "title":"Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework",
        "summary":"Despite the promising results achieved, state-of-the-art interactive reinforcement learning schemes rely on passively receiving supervision signals from advisor experts, in the form of either continuous monitoring or pre-defined rules, which inevitably result in a cumbersome and expensive learning process. In this paper, we introduce a novel initiative advisor-in-the-loop actor-critic framework, termed as Ask-AC, that replaces the unilateral advisor-guidance mechanism with a bidirectional learner-initiative one, and thereby enables a customized and efficacious message exchange between learner and advisor. At the heart of Ask-AC are two complementary components, namely action requester and adaptive state selector, that can be readily incorporated into various discrete actor-critic architectures. The former component allows the agent to initiatively seek advisor intervention in the presence of uncertain states, while the latter identifies the unstable states potentially missed by the former especially when environment changes, and then learns to promote the ask action on such states. Experimental results on both stationary and non-stationary environments and across different actor-critic backbones demonstrate that the proposed framework significantly improves the learning efficiency of the agent, and achieves the performances on par with those obtained by continuous advisor monitoring.",
        "completion1":"New Initiative Advisor-in-the-Loop Actor-Critic Framework, termed as Ask-AC, enables a customized and efficacious message exchange between learner and advisor.",
        "completion2":"At the heart of Ask-AC are two complementary components, namely action requester and adaptive state selector, that can be readily incorporated into various discrete actor-critic architectures.",
        "completion3":"The proposed framework significantly improves the learning efficiency of the agent, and achieves the performances on par with those obtained by continuous advisor monitoring.",
        "technologyreview":0.288306119,
        "venturebeat":0.2640979845,
        "wired":0.2046477054,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.01955v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1656979200000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2207.02088v1",
        "predicted_newsworthiness":0.5338385438,
        "title":"SiamMask: A Framework for Fast Online Object Tracking and Segmentation",
        "summary":"In this paper we introduce SiamMask, a framework to perform both visual object tracking and video object segmentation, in real-time, with the same simple method. We improve the offline training procedure of popular fully-convolutional Siamese approaches by augmenting their losses with a binary segmentation task. Once the offline training is completed, SiamMask only requires a single bounding box for initialization and can simultaneously carry out visual object tracking and segmentation at high frame-rates. Moreover, we show that it is possible to extend the framework to handle multiple object tracking and segmentation by simply re-using the multi-task model in a cascaded fashion. Experimental results show that our approach has high processing efficiency, at around 55 frames per second. It yields real-time state-of-the-art results on visual-object tracking benchmarks, while at the same time demonstrating competitive performance at a high speed for video object segmentation benchmarks.",
        "completion1":"SiamMask: A Framework for Fast Online Object Tracking and Segmentation.",
        "completion2":"SiamMask improves the offline training procedure of popular fully-convolutional Siamese approaches by augmenting their losses with a binary segmentation task.",
        "completion3":"SiamMask only requires a single bounding box for initialization and can simultaneously carry out visual object tracking and segmentation at high frame-rates.",
        "technologyreview":0.1833025741,
        "venturebeat":0.1758295227,
        "wired":0.1429211257,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.02088v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1656979200000,
        "code_mentioned":0,
        "readability":0.82
    },
    {
        "arxiv_id":"2207.02619v1",
        "predicted_newsworthiness":0.5257960561,
        "title":"Multimodal Hydrostatic Actuators for Wearable Robots: A Preliminary Assessment of Mass-Saving and Energy-Efficiency Opportunities",
        "summary":"Wearable robots are limited by their actuators performances because they must bear the weight of their own power system and energy source. This paper explores the idea of leveraging hybrid modes to meet multiple operating points with a lightweight and efficient system by using hydraulic valves to dynamically reconfigure the connections of a hydrostatic actuator. The analyzed opportunities consist in 1) switching between a highly geared power source or a fast power source, 2) dynamically connecting an energy accumulator and 3) using a locking mechanism for holding. Based on a knee exoskeleton case study analysis, results show that switching between gearing ratio can lead to a lighter and more efficient actuator. Also, results show that using an accumulator to provide a preload continuous force has great mass-saving potential, but does not reduce mass significantly if used as a power booster for short transients. Finally, using a locking valve can slightly reduce battery mass if the work cycle includes frequent stops. The operating principles of the proposed multimodal schemes are demonstrated with a one-DOF prototype.",
        "completion1":"Robots are about to get a lot more wearable, thanks to new actuator technology",
        "completion2":"Scientists develop 'multimodal' actuators for efficient, lightweight wearable robots",
        "completion3":"Wearable robot actuators just got a whole lot better - here's why",
        "technologyreview":0.1696713727,
        "venturebeat":0.1464669676,
        "wired":0.167513404,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.02619v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1657065600000,
        "code_mentioned":0,
        "readability":0.83
    },
    {
        "arxiv_id":"2207.04306v1",
        "predicted_newsworthiness":0.4903946087,
        "title":"Out-of-Distribution Detection in Time-Series Domain: A Novel Seasonal Ratio Scoring Approach",
        "summary":"Safe deployment of time-series classifiers for real-world applications relies on the ability to detect the data which is not generated from the same distribution as training data. This task is referred to as out-of-distribution (OOD) detection. We consider the novel problem of OOD detection for the time-series domain. We discuss the unique challenges posed by time-series data and explain why prior methods from the image domain will perform poorly. Motivated by these challenges, this paper proposes a novel {\\em Seasonal Ratio Scoring (SRS)} approach. SRS consists of three key algorithmic steps. First, each input is decomposed into class-wise semantic component and remainder. Second, this decomposition is employed to estimate the class-wise conditional likelihoods of the input and remainder using deep generative models. The seasonal ratio score is computed from these estimates. Third, a threshold interval is identified from the in-distribution data to detect OOD examples. Experiments on diverse real-world benchmarks demonstrate that the SRS method is well-suited for time-series OOD detection when compared to baseline methods. Open-source code for SRS method is provided at https:\/\/github.com\/tahabelkhouja\/SRS",
        "completion1":"Out-of-Distribution Detection in Time-Series Domain: A Novel Seasonal Ratio Scoring Approach",
        "completion2":"Safe deployment of time-series classifiers for real-world applications relies on the ability to detect the data which is not generated from the same distribution as training data",
        "completion3":"We consider the novel problem of OOD detection for the time-series domain",
        "technologyreview":0.2042024699,
        "venturebeat":0.2094486981,
        "wired":0.1687613499,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.04306v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1657324800000,
        "code_mentioned":1,
        "readability":0.86
    },
    {
        "arxiv_id":"2207.05796v1",
        "predicted_newsworthiness":0.5585406093,
        "title":"Estimating Test Performance for AI Medical Devices under Distribution Shift with Conformal Prediction",
        "summary":"Estimating the test performance of software AI-based medical devices under distribution shifts is crucial for evaluating the safety, efficiency, and usability prior to clinical deployment. Due to the nature of regulated medical device software and the difficulty in acquiring large amounts of labeled medical datasets, we consider the task of predicting the test accuracy of an arbitrary black-box model on an unlabeled target domain without modification to the original training process or any distributional assumptions of the original source data (i.e. we treat the model as a \"black-box\" and only use the predicted output responses). We propose a \"black-box\" test estimation technique based on conformal prediction and evaluate it against other methods on three medical imaging datasets (mammography, dermatology, and histopathology) under several clinically relevant types of distribution shift (institution, hardware scanner, atlas, hospital). We hope that by promoting practical and effective estimation techniques for black-box models, manufacturers of medical devices will develop more standardized and realistic evaluation procedures to improve the robustness and trustworthiness of clinical AI tools.",
        "completion1":"Estimating Test Performance of AI Medical Devices",
        "completion2":"Estimating the Test Performance of Software AI-based Medical Devices",
        "completion3":"Estimating the Test Performance of AI Medical Devices under Distribution Shift",
        "technologyreview":0.3070727949,
        "venturebeat":0.2820182495,
        "wired":0.2223362537,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.05796v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1657584000000,
        "code_mentioned":0,
        "readability":0.85
    },
    {
        "arxiv_id":"2207.07826v1",
        "predicted_newsworthiness":0.4220329867,
        "title":"Cross-Domain Cross-Set Few-Shot Learning via Learning Compact and Aligned Representations",
        "summary":"Few-shot learning (FSL) aims to recognize novel queries with only a few support samples through leveraging prior knowledge from a base dataset. In this paper, we consider the domain shift problem in FSL and aim to address the domain gap between the support set and the query set. Different from previous cross-domain FSL work (CD-FSL) that considers the domain shift between base and novel classes, the new problem, termed cross-domain cross-set FSL (CDSC-FSL), requires few-shot learners not only to adapt to the new domain, but also to be consistent between different domains within each novel class. To this end, we propose a novel approach, namely stabPA, to learn prototypical compact and cross-domain aligned representations, so that the domain shift and few-shot learning can be addressed simultaneously. We evaluate our approach on two new CDCS-FSL benchmarks built from the DomainNet and Office-Home datasets respectively. Remarkably, our approach outperforms multiple elaborated baselines by a large margin, e.g., improving 5-shot accuracy by 6.0 points on average on DomainNet. Code is available at https:\/\/github.com\/WentaoChen0813\/CDCS-FSL",
        "completion1":"Cross-Domain Cross-Set Few-Shot Learning: A New Approach",
        "completion2":"NewFew-Shot Learning Approach Outperforms Baselines",
        "completion3":"Study Shows That New Learning Method Is Highly Effective",
        "technologyreview":0.2034521435,
        "venturebeat":0.2225511328,
        "wired":0.1563419038,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.07826v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1657843200000,
        "code_mentioned":1,
        "readability":0.82
    },
    {
        "arxiv_id":"2207.07922v1",
        "predicted_newsworthiness":0.4185893932,
        "title":"Learning Quality-aware Dynamic Memory for Video Object Segmentation",
        "summary":"Recently, several spatial-temporal memory-based methods have verified that storing intermediate frames and their masks as memory are helpful to segment target objects in videos. However, they mainly focus on better matching between the current frame and the memory frames without explicitly paying attention to the quality of the memory. Therefore, frames with poor segmentation masks are prone to be memorized, which leads to a segmentation mask error accumulation problem and further affect the segmentation performance. In addition, the linear increase of memory frames with the growth of frame number also limits the ability of the models to handle long videos. To this end, we propose a Quality-aware Dynamic Memory Network (QDMN) to evaluate the segmentation quality of each frame, allowing the memory bank to selectively store accurately segmented frames to prevent the error accumulation problem. Then, we combine the segmentation quality with temporal consistency to dynamically update the memory bank to improve the practicability of the models. Without any bells and whistles, our QDMN achieves new state-of-the-art performance on both DAVIS and YouTube-VOS benchmarks. Moreover, extensive experiments demonstrate that the proposed Quality Assessment Module (QAM) can be applied to memory-based methods as generic plugins and significantly improves performance. Our source code is available at https:\/\/github.com\/workforai\/QDMN.",
        "completion1":"Learning Quality-aware Dynamic Memory for Video Object Segmentation",
        "completion2":"New state-of-the-art performance on both DAVIS and YouTube-VOS benchmarks",
        "completion3":"Quality Assessment Module as generic plugin significantly improves performance",
        "technologyreview":0.1634882651,
        "venturebeat":0.1464814635,
        "wired":0.123155972,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.07922v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1657929600000,
        "code_mentioned":1,
        "readability":0.85
    }
]