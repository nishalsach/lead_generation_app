[
    {
        "arxiv_id":"2207.14086v1",
        "predicted_newsworthiness":0.8860131419,
        "title":"Ever heard of ethical AI? Investigating the salience of ethical AI issues among the German population",
        "summary":"Building and implementing ethical AI systems that benefit the whole society is cost-intensive and a multi-faceted task fraught with potential problems. While computer science focuses mostly on the technical questions to mitigate social issues, social science addresses citizens' perceptions to elucidate social and political demands that influence the societal implementation of AI systems. Thus, in this study, we explore the salience of AI issues in the public with an emphasis on ethical criteria to investigate whether it is likely that ethical AI is actively requested by the population. Between May 2020 and April 2021, we conducted 15 surveys asking the German population about the most important AI-related issues (total of N=14,988 respondents). Our results show that the majority of respondents were not concerned with AI at all. However, it can be seen that general interest in AI and a higher educational level are predictive of some engagement with AI. Among those, who reported having thought about AI, specific applications (e.g., autonomous driving) were by far the most mentioned topics. Ethical issues are voiced only by a small subset of citizens with fairness, accountability, and transparency being the least mentioned ones. These have been identified in several ethical guidelines (including the EU Commission's proposal) as key elements for the development of ethical AI. The salience of ethical issues affects the behavioral intentions of citizens in the way that they 1) tend to avoid AI technology and 2) engage in public discussions about AI. We conclude that the low level of ethical implications may pose a serious problem for the actual implementation of ethical AI for the Common Good and emphasize that those who are presumably most affected by ethical issues of AI are especially unaware of ethical risks. Yet, once ethical AI is top of the mind, there is some potential for activism.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.3196319641,
        "newsscientist":0.3196175518,
        "technologyreview":0.4894837359,
        "venturebeat":0.4248078926,
        "wired":0.4004314725,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14086v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659015973000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.01305v1",
        "predicted_newsworthiness":0.8625239629,
        "title":"Humble Machines: Attending to the Underappreciated Costs of Misplaced Distrust",
        "summary":"It is curious that AI increasingly outperforms human decision makers, yet much of the public distrusts AI to make decisions affecting their lives. In this paper we explore a novel theory that may explain one reason for this. We propose that public distrust of AI is a moral consequence of designing systems that prioritize reduction of costs of false positives over less tangible costs of false negatives. We show that such systems, which we characterize as 'distrustful', are more likely to miscategorize trustworthy individuals, with cascading consequences to both those individuals and the overall human-AI trust relationship. Ultimately, we argue that public distrust of AI stems from well-founded concern about the potential of being miscategorized. We propose that restoring public trust in AI will require that systems are designed to embody a stance of 'humble trust', whereby the moral costs of the misplaced distrust associated with false negatives is weighted appropriately during development and use.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2930865734,
        "newsscientist":0.3178910533,
        "technologyreview":0.4846068623,
        "venturebeat":0.4255554812,
        "wired":0.3997664167,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01305v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659428669000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.11521v1",
        "predicted_newsworthiness":0.8402257629,
        "title":"Vaccine Discourse on Twitter During the COVID-19 Pandemic",
        "summary":"Since the onset of the COVID-19 pandemic, vaccines have been an important topic in public discourse. The discussions around vaccines are polarized as some see them as an important measure to end the pandemic, and others are hesitant or find them harmful. This study investigates posts related to COVID-19 vaccines on Twitter and focuses on those which have a negative stance toward vaccines. A dataset of 16,713,238 English tweets related to COVID-19 vaccines was collected covering the period from March 1, 2020, to July 31, 2021. We used the Scikit-learn Python library to apply a support vector machine (SVM) classifier to identify the tweets with a negative stance toward the COVID-19 vaccines. A total of 5,163 tweets were used to train the classifier, out of which a subset of 2,484 tweets were manually annotated by us and made publicly available. We used the BERTtopic model to extract and investigate the topics discussed within the negative tweets and how they changed over time. We show that the negativity with respect to COVID-19 vaccines has decreased over time along with the vaccine roll-outs. We identify 37 topics of discussion and present their respective importance over time. We show that popular topics consist of conspiratorial discussions such as 5G towers and microchips, but also contain legitimate concerns around vaccination safety and side effects as well as concerns about policies. Our study shows that even unpopular opinions or conspiracy theories can become widespread when paired with a widely popular discussion topic such as COVID-19 vaccines. Understanding the concerns and the discussed topics and how they change over time is essential for policymakers and public health authorities to provide better and in-time information and policies, to facilitate vaccination of the population in future similar crises.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.3844258275,
        "newsscientist":0.3448199254,
        "technologyreview":0.4124848021,
        "venturebeat":0.3181777582,
        "wired":0.3744791755,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11521v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy",
            "cs.cl",
            "cs.si"
        ],
        "published":1658584251000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.00681v1",
        "predicted_newsworthiness":0.8233200001,
        "title":"The Many Facets of Trust in AI: Formalizing the Relation Between Trust and Fairness, Accountability, and Transparency",
        "summary":"Efforts to promote fairness, accountability, and transparency are assumed to be critical in fostering Trust in AI (TAI), but extant literature is frustratingly vague regarding this 'trust'. The lack of exposition on trust itself suggests that trust is commonly understood, uncomplicated, or even uninteresting. But is it? Our analysis of TAI publications reveals numerous orientations which differ in terms of who is doing the trusting (agent), in what (object), on the basis of what (basis), in order to what (objective), and why (impact). We develop an ontology that encapsulates these key axes of difference to a) illuminate seeming inconsistencies across the literature and b) more effectively manage a dizzying number of TAI considerations. We then reflect this ontology through a corpus of publications exploring fairness, accountability, and transparency to examine the variety of ways that TAI is considered within and between these approaches to promoting trust.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2252426563,
        "newsscientist":0.2432175128,
        "technologyreview":0.418128042,
        "venturebeat":0.3744825468,
        "wired":0.3257343687,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00681v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659342417000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.12555v1",
        "predicted_newsworthiness":0.8124202163,
        "title":"Ethics for social robotics: A critical analysis",
        "summary":"Social robotics development for the practice of care and European prospects to incorporate these AI-based systems in institutional healthcare contexts call for an urgent ethical reflection to (re)configurate our practical life according to human values and rights. Despite the growing attention to the ethical implications of social robotics, the current debate on one of its central branches, social assistive robotics (SAR), rests upon an impoverished ethical approach. This paper presents and examines some tendencies of this prevailing approach, which have been identified as a result of a critical literature review. Based on this analysis of a representative case of how ethical reflection is being led towards social robotics, some future research lines are outlined, which may help reframe and deepen in its ethical implications.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2697229953,
        "newsscientist":0.274276975,
        "technologyreview":0.3750523503,
        "venturebeat":0.3043923991,
        "wired":0.3041642676,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12555v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy",
            "cs.ro"
        ],
        "published":1658787480000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.00280v1",
        "predicted_newsworthiness":0.8063317058,
        "title":"How to Make Users Adopt More Sustainable Cryptocurrencies: Evidence from Nigeria",
        "summary":"Some of the most popular decentralised cryptocurrency networks have drawn widespread criticism for consuming vast amounts of electricity and have thus become targets of regulatory interest. Attempts to influence cryptocurrency network operations via policy in the pursuit of sustainability in the past, however, have been widely unsuccessful. Some were abandoned out of fear of jeopardising innovation while others failed due to the highly globalised nature of decentralised systems. Considering Bitcoin as an archetype for cryptocurrencies with high energy demand, this study takes a bottom-up approach by analysing statements made by Nigerian cryptocurrency users ($N = 158$) concerning their perception of sustainability issues. Three main findings emerged: 1) Despite self-reporting as highly knowledgeable, most participants significantly underestimate the energy demand of Bitcoin. 2) Those who accurately assess the energy demand of Bitcoin are more likely to support measures targeting its energy demand than those who misestimate it. 3) Those who support measures predominantly hold private actors responsible. In light of these findings, it is concluded that the primary task of policy makers in the context of cryptocurrency sustainability is to enforce consumer education.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.3041697502,
        "newsscientist":0.278828296,
        "technologyreview":0.3551121535,
        "venturebeat":0.2747786999,
        "wired":0.2836655742,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00280v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659202245000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.02056v1",
        "predicted_newsworthiness":0.8006555235,
        "title":"Fast or Accurate? Governing Conflicting Goals in Highly Autonomous Vehicles",
        "summary":"The tremendous excitement around the deployment of autonomous vehicles (AVs) comes from their purported promise. In addition to decreasing accidents, AVs are projected to usher in a new era of equity in human autonomy by providing affordable, accessible, and widespread mobility for disabled, elderly, and low-income populations. However, to realize this promise, it is necessary to ensure that AVs are safe for deployment, and to contend with the risks AV technology poses, which threaten to eclipse its benefits. In this Article, we focus on an aspect of AV engineering currently unexamined in the legal literature, but with critical implications for safety, accountability, liability, and power. Specifically, we explain how understanding the fundamental engineering trade-off between accuracy and speed in AVs is critical for policymakers to regulate the uncertainty and risk inherent in AV systems. We discuss how understanding the trade-off will help create tools that will enable policymakers to assess how the trade-off is being implemented. Such tools will facilitate opportunities for developing concrete, ex ante AV safety standards and conclusive mechanisms for ex post determination of accountability after accidents occur. This will shift the balance of power from manufacturers to the public by facilitating effective regulation, reducing barriers to tort recovery, and ensuring that public values like safety and accountability are appropriately balanced.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.252143492,
        "newsscientist":0.2494928565,
        "technologyreview":0.4119373094,
        "venturebeat":0.3531646938,
        "wired":0.3730778011,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02056v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659533065000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.01350v1",
        "predicted_newsworthiness":0.7819377184,
        "title":"Application of Blockchain Smart Contracts in E-Commerce and Government",
        "summary":"With technological advances and the establishment of e-commerce models, business challenges have shifted to online platforms. The promise of embedding self-executing and autonomous programs into blockchain technologies has attracted increased interest and its use in niche solutions. Using qualitative interviews, this paper sought the opinions of the eleven industry leaders regarding smart contracts. Findings reveal that the technology is gaining momentum in e-commerce, particularly in financial transfer, record-keeping, real estate, and property management, insurance, mortgage, supply chain management, data storage, authorization of credit, denaturalized intelligence, aviation sector, shipping of products, invoice financing and other domains. The significant benefits of widespread adoption and deployment of smart contracts include their capability to deliver decentralization, efficacy, cost-effectiveness, transparency, speed, autonomy, transparency, privacy, and security, encouraging the emergence of novel business models. Albeit these benefits that revolutionize online transactions, the technology faced multifaceted challenges. Smart technologies are only a decade old and are not advanced in security, transparency, cost-effectiveness, and regulatory framework. Furthermore, organizational, and technical challenges limit their deployment: incompatibility with legacy systems, scalability, bugs, speed, and lack of talent and understanding regarding smart contracts. Consequently, policymakers, developers, researchers, practitioners, and other stakeholders need to invest effort and time to foster the technologies and address pertinent issues to enable the global adoption of smart contracts by small and big businesses.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2569041657,
        "newsscientist":0.2023218446,
        "technologyreview":0.4087943456,
        "venturebeat":0.3932755114,
        "wired":0.3312630576,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01350v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659436441000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.11474v1",
        "predicted_newsworthiness":0.7758885165,
        "title":"Investigating the Validity of Botometer-based Social Bot Studies",
        "summary":"The idea that social media platforms like Twitter are inhabited by vast numbers of social bots has become widely accepted in recent years. Social bots are assumed to be automated social media accounts operated by malicious actors with the goal of manipulating public opinion. They are credited with the ability to produce content autonomously and to interact with human users. Social bot activity has been reported in many different political contexts, including the U.S. presidential elections, discussions about migration, climate change, and COVID-19. However, the relevant publications either use crude and questionable heuristics to discriminate between supposed social bots and humans or -- in the vast majority of the cases -- fully rely on the output of automatic bot detection tools, most commonly Botometer. In this paper, we point out a fundamental theoretical flaw in the widely-used study design for estimating the prevalence of social bots. Furthermore, we empirically investigate the validity of peer-reviewed Botometer-based studies by closely and systematically inspecting hundreds of accounts that had been counted as social bots. We were unable to find a single social bot. Instead, we found mostly accounts undoubtedly operated by human users, the vast majority of them using Twitter in an inconspicuous and unremarkable fashion without the slightest traces of automation. We conclude that studies claiming to investigate the prevalence, properties, or influence of social bots based on Botometer have, in reality, just investigated false positives and artifacts of this approach.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2931209385,
        "newsscientist":0.2629624584,
        "technologyreview":0.4318042705,
        "venturebeat":0.3666187813,
        "wired":0.4049302556,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11474v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.cy",
            "cs.hc"
        ],
        "published":1658568690000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.14677v1",
        "predicted_newsworthiness":0.7655344929,
        "title":"Big Data and Analytics Implementation in Tertiary Institutions to Predict Students Performance in Nigeria",
        "summary":"The term Big Data has been coined to refer to the gargantuan bulk of data that cannot be dealt with by traditional data-handling techniques. Big Data is still a novel concept, and in the following literature, we intend to elaborate on it in a palpable fashion. It commences with the concept of the subject in itself, along with its properties and the two general approaches to dealing with it. Big Data provides an opportunity for educational Institutions to use their Information Technology resources strategically to improve educational quality, guide students to higher completion rates and improve student persistence and outcomes. This paper explores the attributes of big data that are relevant to educational institutions, investigates the factors influencing the adoption of big data and analytics in learning institutions, and seeks to establish the limiting factors hindering the use of big data in Institutions of higher learning. A survey research design was adopted in conducting this research, and Questionnaires were the instrument employed for data collection.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.227607648,
        "newsscientist":0.1863571064,
        "technologyreview":0.2939845726,
        "venturebeat":0.2825912849,
        "wired":0.2206322776,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14677v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy",
            "cs.ai",
            "cs.lg"
        ],
        "published":1659102744000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.01509v1",
        "predicted_newsworthiness":0.7646605236,
        "title":"Characterizing Vaccination Movements on YouTube in the United States and Brazil",
        "summary":"In the context of COVID-19 pandemic, social networks such as Twitter and YouTube stand out as important sources of information. YouTube, as the largest and most engaging online media consumption platform, has a large influence in the spread of information and misinformation, which makes it important to study how it deals with the problems that arise from disinformation, as well as how its users interact with different types of content. Considering that United States (USA) and Brazil (BR) are two countries with the highest COVID-19 death tolls, we asked the following question: What are the nuances of vaccination campaigns in the two countries? With that in mind, we engage in a comparative analysis of pro and anti-vaccine movements on YouTube. We also investigate the role of YouTube in countering online vaccine misinformation in USA and BR. For this means, we monitored the removal of vaccine related content on the platform and also applied various techniques to analyze the differences in discourse and engagement in pro and anti-vaccine \"comment sections\". We found that American anti-vaccine content tend to lead to considerably more toxic and negative discussion than their pro-vaccine counterparts while also leading to 18% higher user-user engagement, while Brazilian anti-vaccine content was significantly less engaging. We also found that pro-vaccine and anti-vaccine discourses are considerably different as the former is associated with conspiracy theories (e.g. ccp), misinformation and alternative medicine (e.g. hydroxychloroquine), while the latter is associated with protective measures. Finally, it was observed that YouTube content removals are still insufficient, with only approximately 16% of the anti-vaccine content being removed by the end of the studied period, with the USA registering the highest percentage of removed anti-vaccine content(34%) and BR registering the lowest(9.8%).",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.3838318719,
        "newsscientist":0.3288438803,
        "technologyreview":0.4125312808,
        "venturebeat":0.3112443754,
        "wired":0.3988916917,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01509v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1659452061000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.13913v1",
        "predicted_newsworthiness":0.7580802271,
        "title":"A health telemonitoring platform based on data integration from different sources",
        "summary":"The management of people with long-term or chronic illness is one of the biggest challenges for national health systems. In fact, these diseases are among the leading causes of hospitalization, especially for the elderly, and huge amount of resources required to monitor them leads to problems with sustainability of the healthcare systems. The increasing diffusion of portable devices and new connectivity technologies allows the implementation of telemonitoring system capable of providing support to health care providers and lighten the burden on hospitals and clinics. In this paper, we present the implementation of a telemonitoring platform for healthcare, designed to capture several types of physiological health parameters from different consumer mobile and custom devices. Consumer medical devices can be integrated into the platform via the Google Fit ecosystem that supports hundreds of devices, while custom devices can directly interact with the platform with standard communication protocols. The platform is designed to process the acquired data using machine learning algorithms, and to provide patients and physicians the physiological health parameters with a user-friendly, comprehensive, and easy to understand dashboard which monitors the parameters through time. Preliminary usability tests show a good user satisfaction in terms of functionality and usefulness.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2091557729,
        "newsscientist":0.2464104139,
        "technologyreview":0.3597847185,
        "venturebeat":0.3782604527,
        "wired":0.3159808618,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13913v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy",
            "cs.ai",
            "cs.hc"
        ],
        "published":1658992384000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.11569v1",
        "predicted_newsworthiness":0.7527614891,
        "title":"Robots Enact Malignant Stereotypes",
        "summary":"Stereotypes, bias, and discrimination have been extensively documented in Machine Learning (ML) methods such as Computer Vision (CV) [18, 80], Natural Language Processing (NLP) [6], or both, in the case of large image and caption models such as OpenAI CLIP [14]. In this paper, we evaluate how ML bias manifests in robots that physically and autonomously act within the world. We audit one of several recently published CLIP-powered robotic manipulation methods, presenting it with objects that have pictures of human faces on the surface which vary across race and gender, alongside task descriptions that contain terms associated with common stereotypes. Our experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy, at scale. Furthermore, the audited methods are less likely to recognize Women and People of Color. Our interdisciplinary sociotechnical analysis synthesizes across fields and applications such as Science Technology and Society (STS), Critical Studies, History, Safety, Robotics, and AI. We find that robots powered by large datasets and Dissolution Models (sometimes called \"foundation models\", e.g. CLIP) that contain humans risk physically amplifying malignant stereotypes in general; and that merely correcting disparities will be insufficient for the complexity and scale of the problem. Instead, we recommend that robot learning methods that physically manifest stereotypes or other harmful outcomes be paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just. Finally, we discuss comprehensive policy changes and the potential of new interdisciplinary research on topics like Identity Safety Assessment Frameworks and Design Justice to better understand and address these harms.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2938802498,
        "newsscientist":0.3127555077,
        "technologyreview":0.467283079,
        "venturebeat":0.3986650812,
        "wired":0.3901454749,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11569v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai",
            "cs.cv",
            "cs.cy",
            "cs.lg"
        ],
        "published":1658599692000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.11602v1",
        "predicted_newsworthiness":0.7505031382,
        "title":"Challenges Faced by Teaching Assistants in Computer Science Education Across Europe",
        "summary":"Teaching assistants (TAs) are heavily used in computer science courses as a way to handle high enrollment and still being able to offer students individual tutoring and detailed assessments. TAs are themselves students who take on this additional role in parallel with their own studies at the same institution. Previous research has shown that being a TA can be challenging but has mainly been conducted on TAs from a single institution or within a single course. This paper offers a multi-institutional, multi-national perspective of challenges that TAs in computer science face. This has been done by conducting a thematic analysis of 180 reflective essays written by TAs from three institutions across Europe. The thematic analysis resulted in five main challenges: becoming a professional TA, student focused challenges, assessment, defining and using best practice, and threats to best practice. In addition, these challenges were all identified within the essays from all three institutions, indicating that the identified challenges are not particularly context-dependent. Based on these findings, we also outline implications for educators involved in TA training and coordinators of computer science courses with TAs.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.219857159,
        "newsscientist":0.1635914758,
        "technologyreview":0.2398843092,
        "venturebeat":0.2019276759,
        "wired":0.1889864019,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11602v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1658610269000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.00249v1",
        "predicted_newsworthiness":0.7467260496,
        "title":"Cause-and-Effect Analysis of ADAS: A Comparison Study between Literature Review and Complaint Data",
        "summary":"Advanced driver assistance systems (ADAS) are designed to improve vehicle safety. However, it is difficult to achieve such benefits without understanding the causes and limitations of the current ADAS and their possible solutions. This study 1) investigated the limitations and solutions of ADAS through a literature review, 2) identified the causes and effects of ADAS through consumer complaints using natural language processing models, and 3) compared the major differences between the two. These two lines of research identified similar categories of ADAS causes, including human factors, environmental factors, and vehicle factors. However, academic research focused more on human factors of ADAS issues and proposed advanced algorithms to mitigate such issues while drivers complained more of vehicle factors of ADAS failures, which led to associated top consequences. The findings from these two sources tend to complement each other and provide important implications for the improvement of ADAS in the future.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2027812493,
        "newsscientist":0.2270131969,
        "technologyreview":0.3739325604,
        "venturebeat":0.347231182,
        "wired":0.3165305471,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00249v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659194158000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00721v1",
        "predicted_newsworthiness":0.7326938567,
        "title":"Domain Analysis of Ethical, Social and Environmental Accounting Methods",
        "summary":"Ethical, social and environmental accounting is the practice of assessing and reporting organisations' performance on environmental, social and governance topics. There are ample methods that describe how to perform such sustainability assessments. This report presents a domain analysis of ethical, social and environmental accounting methods. Our analysis contains 21 methods. Each method is modelled as a process deliverable diagram. The diagrams have been validated by experts in the methods. The diagrams lay the foundation for further analysis and software development. In this report, we touch upon the ethical, social and environmental accounting method ontology that has been created based on the domain analysis.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2303333567,
        "newsscientist":0.1771144121,
        "technologyreview":0.2011265489,
        "venturebeat":0.1848841107,
        "wired":0.144423744,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00721v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659348721000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.01112v1",
        "predicted_newsworthiness":0.7233488516,
        "title":"VacciNet: Towards a Smart Framework for Learning the Distribution Chain Optimization of Vaccines for a Pandemic",
        "summary":"Vaccinations against viruses have always been the need of the hour since long past. However, it is hard to efficiently distribute the vaccines (on time) to all the corners of a country, especially during a pandemic. Considering the vastness of the population, diversified communities, and demands of a smart society, it is an important task to optimize the vaccine distribution strategy in any country\/state effectively. Although there is a profusion of data (Big Data) from various vaccine administration sites that can be mined to gain valuable insights about mass vaccination drives, very few attempts has been made towards revolutionizing the traditional mass vaccination campaigns to mitigate the socio-economic crises of pandemic afflicted countries. In this paper, we bridge this gap in studies and experimentation. We collect daily vaccination data which is publicly available and carefully analyze it to generate meaning-full insights and predictions. We put forward a novel framework leveraging Supervised Learning and Reinforcement Learning (RL) which we call VacciNet, that is capable of learning to predict the demand of vaccination in a state of a country as well as suggest optimal vaccine allocation in the state for minimum cost of procurement and supply. At the present, our framework is trained and tested with vaccination data of the USA.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.3392498895,
        "newsscientist":0.3202406679,
        "technologyreview":0.3893165145,
        "venturebeat":0.3417112838,
        "wired":0.274602864,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01112v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cy"
        ],
        "published":1659382653000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01355v1",
        "predicted_newsworthiness":0.7232059955,
        "title":"A Comparative Study on COVID-19 Fake News Detection Using Different Transformer Based Models",
        "summary":"The rapid advancement of social networks and the convenience of internet availability have accelerated the rampant spread of false news and rumors on social media sites. Amid the COVID 19 epidemic, this misleading information has aggravated the situation by putting peoples mental and physical lives in danger. To limit the spread of such inaccuracies, identifying the fake news from online platforms could be the first and foremost step. In this research, the authors have conducted a comparative analysis by implementing five transformer based models such as BERT, BERT without LSTM, ALBERT, RoBERTa, and a Hybrid of BERT & ALBERT in order to detect the fraudulent news of COVID 19 from the internet. COVID 19 Fake News Dataset has been used for training and testing the models. Among all these models, the RoBERTa model has performed better than other models by obtaining an F1 score of 0.98 in both real and fake classes.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2382013794,
        "newsscientist":0.2409579808,
        "technologyreview":0.3279640589,
        "venturebeat":0.2880117414,
        "wired":0.282955269,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01355v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1659437416000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.13644v1",
        "predicted_newsworthiness":0.7214489131,
        "title":"Using Deep Learning to Detecting Deepfakes",
        "summary":"In the recent years, social media has grown to become a major source of information for many online users. This has given rise to the spread of misinformation through deepfakes. Deepfakes are videos or images that replace one persons face with another computer-generated face, often a more recognizable person in society. With the recent advances in technology, a person with little technological experience can generate these videos. This enables them to mimic a power figure in society, such as a president or celebrity, creating the potential danger of spreading misinformation and other nefarious uses of deepfakes. To combat this online threat, researchers have developed models that are designed to detect deepfakes. This study looks at various deepfake detection models that use deep learning algorithms to combat this looming threat. This survey focuses on providing a comprehensive overview of the current state of deepfake detection models and the unique approaches many researchers take to solving this problem. The benefits, limitations, and suggestions for future work will be thoroughly discussed throughout this paper.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2247113403,
        "newsscientist":0.2476348717,
        "technologyreview":0.4010318635,
        "venturebeat":0.3384334261,
        "wired":0.3426181491,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13644v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1658941516000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11713v1",
        "predicted_newsworthiness":0.7101547768,
        "title":"Discovering adoption barriers of Clinical Decision Support Systems in primary health care sector",
        "summary":"Adopting a good health information system (HIS) is essential for providing high-quality healthcare. With rapid advances in technology in the healthcare industry in recent years, healthcare providers seek effective options to deal with numerous diseases and a growing number of patients, adopting advanced HIS such as for clinical decision support. While the clinical decision support systems (CDSS) can help medical personnel make better decisions, they may bring negative results due to a lack of understanding of the elements that influence GP's adoption of CDSS. This paper focuses on discovering obstacles that may contribute to the problems surrounding CDSS adoption. Thirty general practitioners were interviewed from different primary health centers in Saudi Arabia in order to determine the challenges and obstacles in the sector. While the outcome confirms that there are obstacles that affect the aspects, such as time risk, quality of the system used, slow Internet speed, user interface, lack of training, high costs, patient satisfaction, multiple systems used, technical support, computer skills, lack of flexibility, system update, professional skills and knowledge, computer efficiency and quality and accuracy of data.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2127504143,
        "newsscientist":0.1885409565,
        "technologyreview":0.2691177098,
        "venturebeat":0.2697390451,
        "wired":0.187423126,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11713v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1658659775000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.02187v1",
        "predicted_newsworthiness":0.7067764891,
        "title":"On the independence between phenomenal consciousness and computational intelligence",
        "summary":"Consciousness and intelligence are properties commonly understood as dependent by folk psychology and society in general. The term artificial intelligence and the kind of problems that it managed to solve in the recent years has been shown as an argument to establish that machines experience some sort of consciousness. Following the analogy of Russell, if a machine is able to do what a conscious human being does, the likelihood that the machine is conscious increases. However, the social implications of this analogy are catastrophic. Concretely, if rights are given to entities that can solve the kind of problems that a neurotypical person can, does the machine have potentially more rights that a person that has a disability? For example, the autistic syndrome disorder spectrum can make a person unable to solve the kind of problems that a machine solves. We believe that the obvious answer is no, as problem solving does not imply consciousness. Consequently, we will argue in this paper how phenomenal consciousness and, at least, computational intelligence are independent and why machines do not possess phenomenal consciousness, although they can potentially develop a higher computational intelligence that human beings. In order to do so, we try to formulate an objective measure of computational intelligence and study how it presents in human beings, animals and machines. Analogously, we study phenomenal consciousness as a dichotomous variable and how it is distributed in humans, animals and machines. As phenomenal consciousness and computational intelligence are independent, this fact has critical implications for society that we also analyze in this work.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2595174929,
        "newsscientist":0.3119430525,
        "technologyreview":0.4074786229,
        "venturebeat":0.3362680605,
        "wired":0.3267331472,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02187v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1659543431000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.11490v1",
        "predicted_newsworthiness":0.7066856645,
        "title":"Towards Smart Fake News Detection Through Explainable AI",
        "summary":"People now see social media sites as their sole source of information due to their popularity. The Majority of people get their news through social media. At the same time, fake news has grown exponentially on social media platforms in recent years. Several artificial intelligence-based solutions for detecting fake news have shown promising results. On the other hand, these detection systems lack explanation capabilities, i.e., the ability to explain why they made a prediction. This paper highlights the current state of the art in explainable fake news detection. We discuss the pitfalls in the current explainable AI-based fake news detection models and present our ongoing research on multi-modal explainable fake news detection model.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1533141012,
        "newsscientist":0.1751418184,
        "technologyreview":0.3154371545,
        "venturebeat":0.2840607785,
        "wired":0.2500151834,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11490v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.ir",
            "cs.si"
        ],
        "published":1658573325000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.14740v1",
        "predicted_newsworthiness":0.705135514,
        "title":"Rating the Crisis of Online Public Opinion Using a Multi-Level Index System",
        "summary":"Online public opinion usually spreads rapidly and widely, thus a small incident probably evolves into a large social crisis in a very short time, and results in a heavy loss in credit or economic aspects. We propose a method to rate the crisis of online public opinion based on a multi-level index system to evaluate the impact of events objectively. Firstly, the dissemination mechanism of online public opinion is explained from the perspective of information ecology. According to the mechanism, some evaluation indexes are selected through correlation analysis and principal component analysis. Then, a classification model of text emotion is created via the training by deep learning to achieve the accurate quantification of the emotional indexes in the index system. Finally, based on the multi-level evaluation index system and grey correlation analysis, we propose a method to rate the crisis of online public opinion. The experiment with the real-time incident show that this method can objectively evaluate the emotional tendency of Internet users and rate the crisis in different dissemination stages of online public opinion. It is helpful to realizing the crisis warning of online public opinion and timely blocking the further spread of the crisis.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2672835477,
        "newsscientist":0.2340631085,
        "technologyreview":0.3356827374,
        "venturebeat":0.2990269235,
        "wired":0.3111443545,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14740v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.ai",
            "cs.cl"
        ],
        "published":1659108336000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.14394v2",
        "predicted_newsworthiness":0.7018481872,
        "title":"Logic and Accuracy Testing: A Fifty-State Review",
        "summary":"Pre-election logic and accuracy (L&A) testing is a process in which election officials validate the behavior of voting equipment by casting a known set of test ballots and confirming the expected results. Ideally, such testing can serve to detect certain forms of human error or fraud and help bolster voter confidence. We present the first detailed analysis of L&A testing practices across the United States. We find that while all states require L&A testing before every election, their implementations vary dramatically in scope, transparency, and rigorousness. We summarize each state's requirements and score them according to uniform criteria. We also highlight best practices and flag opportunities for improvement, in hopes of encouraging broader adoption of more effective L&A processes.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2443560787,
        "newsscientist":0.2053425207,
        "technologyreview":0.2978911318,
        "venturebeat":0.2435656952,
        "wired":0.2599704942,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14394v2",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659046897000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.11603v1",
        "predicted_newsworthiness":0.6969148219,
        "title":"Experience with Abrupt Transition to Remote Teaching of Embedded Systems",
        "summary":"Due to the pandemic of COVID-19, many university courses had to abruptly transform to enable remote teaching. Adjusting courses on embedded systems and micro-controllers was extra challenging since interaction with real hardware is their integral part. We start by comparing our experience with four basic alternatives of teaching embedded systems: 1) interacting with hardware at school, 2) having remote access to hardware, 3) lending hardware to students for at-home work and 4) virtualizing hardware. Afterward, we evaluate in detail our experience of the fast transition from traditional, offline at-school hardware programming course to using remote access to real hardware present in the lab. The somewhat unusual remote hardware access approach turned out to be a fully viable alternative for teaching embedded systems, enabling a relatively low-effort transition. Our setup is based on existing solutions and stable open technologies without the need for custom-developed applications that require high maintenance. We evaluate the experience of both the students and teachers and condense takeaways for future courses. The specific environment setup is available online as an inspiration for others.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1737747892,
        "newsscientist":0.1954015854,
        "technologyreview":0.2884420841,
        "venturebeat":0.2950198368,
        "wired":0.3006420304,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11603v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy",
            "cs.ro"
        ],
        "published":1658610306000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.13941v1",
        "predicted_newsworthiness":0.6963390132,
        "title":"A Civil Protection Early Warning System to Improve the Resilience of Adriatic-Ionian Territories to Natural and Man-made Risk",
        "summary":"We are currently witnessing an increased occurrence of extreme weather events, causing a great deal of disruption and distress across the globe. In this setting, the importance and utility of Early Warning Systems is becoming increasingly obvious. In this work, we present the design of an early warning system called TransCPEarlyWarning, aimed at seven countries in the Adriatic-Ionian area in Europe. The overall objective is to increase the level of cooperation among national civil protection institutions in these countries, addressing natural and man-made risks from the early warning stage and improving the intervention capabilities of civil protection mechanisms. The system utilizes an innovative approach with a lever effect, while also aiming to support the whole system of Civil Protection.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2814792266,
        "newsscientist":0.2601614791,
        "technologyreview":0.2732509101,
        "venturebeat":0.2391736776,
        "wired":0.2397302445,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13941v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1658995537000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.11897v1",
        "predicted_newsworthiness":0.6811131876,
        "title":"AI Powered Anti-Cyber Bullying System using Machine Learning Algorithm of Multinomial Naive Bayes and Optimized Linear Support Vector Machine",
        "summary":"\"Unless and until our society recognizes cyber bullying for what it is, the suffering of thousands of silent victims will continue.\" ~ Anna Maria Chavez. There had been series of research on cyber bullying which are unable to provide reliable solution to cyber bullying. In this research work, we were able to provide a permanent solution to this by developing a model capable of detecting and intercepting bullying incoming and outgoing messages with 92% accuracy. We also developed a chatbot automation messaging system to test our model leading to the development of Artificial Intelligence powered anti-cyber bullying system using machine learning algorithm of Multinomial Naive Bayes (MNB) and optimized linear Support Vector Machine (SVM). Our model is able to detect and intercept bullying outgoing and incoming bullying messages and take immediate action.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1855827184,
        "newsscientist":0.2180865804,
        "technologyreview":0.3274025914,
        "venturebeat":0.3166124584,
        "wired":0.2662303096,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11897v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1658721722000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.11500v1",
        "predicted_newsworthiness":0.6778673303,
        "title":"Catch Me If You Can: Deceiving Stance Detection and Geotagging Models to Protect Privacy of Individuals on Twitter",
        "summary":"The recent advances in natural language processing have yielded many exciting developments in text analysis and language understanding models; however, these models can also be used to track people, bringing severe privacy concerns. In this work, we investigate what individuals can do to avoid being detected by those models while using social media platforms. We ground our investigation in two exposure-risky tasks, stance detection and geotagging. We explore a variety of simple techniques for modifying text, such as inserting typos in salient words, paraphrasing, and adding dummy social media posts. Our experiments show that the performance of BERT-based models fined tuned for stance detection decreases significantly due to typos, but it is not affected by paraphrasing. Moreover, we find that typos have minimal impact on state-of-the-art geotagging models due to their increased reliance on social networks; however, we show that users can deceive those models by interacting with different users, reducing their performance by almost 50%.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2179474553,
        "newsscientist":0.2123886857,
        "technologyreview":0.3427253041,
        "venturebeat":0.30195669,
        "wired":0.3271118943,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11500v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.cy"
        ],
        "published":1658577318000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.12589v1",
        "predicted_newsworthiness":0.6734360902,
        "title":"Folk Models of Misinformation on Social Media",
        "summary":"In this paper we investigate what folk models of misinformation exist through semi-structured interviews with a sample of 235 social media users. Work on social media misinformation does not investigate how ordinary users - the target of misinformation - deal with it; rather, the focus is mostly on the anxiety, tensions, or divisions misinformation creates. Studying the aspects of creation, diffusion and amplification also overlooks how misinformation is internalized by users on social media and thus is quick to prescribe \"inoculation\" strategies for the presumed lack of immunity to misinformation. How users grapple with social media content to develop \"natural immunity\" as a precursor to misinformation resilience remains an open question. We have identified at least five folk models that conceptualize misinformation as either: political (counter)argumentation, out-of-context narratives, inherently fallacious information, external propaganda, or simply entertainment. We use the rich conceptualizations embodied in these folk models to uncover how social media users minimize adverse reactions to misinformation encounters in their everyday lives.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.3260661933,
        "newsscientist":0.2571649636,
        "technologyreview":0.3530517288,
        "venturebeat":0.2598362848,
        "wired":0.3809045008,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12589v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.cy"
        ],
        "published":1658796026000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.14681v1",
        "predicted_newsworthiness":0.6732919625,
        "title":"Unfolding Values through Systematic Guidance: Conducting a Value-Centered Participatory Workshop for a Patient-Oriented Data Donation",
        "summary":"Routinely collected clinical patient data posits a valuable resource for data-driven medical innovation. Such secondary data use for medical research purposes is dependent on the patient's consent. To gain an understanding of the patients' values and needs regarding medical data donations, we developed a participatory workshop method, integrating approaches from value-sensitive and reflective design to explore patients' values and translate them into hypothetical, ideal design solutions. The data gathered in the workshop are used to derive practicable design requirements for patient-oriented data donation technologies. In this paper, we introduce the workshop process and evaluate its application.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2703233935,
        "newsscientist":0.2666623907,
        "technologyreview":0.3541892508,
        "venturebeat":0.3251561215,
        "wired":0.3137170128,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14681v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1659102975000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.13394v2",
        "predicted_newsworthiness":0.6713335466,
        "title":"Statistical Keystroke Synthesis for Improved Bot Detection",
        "summary":"This work proposes two statistical approaches for the synthesis of keystroke biometric data based on Universal and User-dependent Models. Both approaches are validated on the bot detection task, using the keystroke synthetic data to better train the systems. Our experiments include a dataset with 136 million keystroke events from 168,000 subjects. We have analyzed the performance of the two synthesis approaches through qualitative and quantitative experiments. Different bot detectors are considered based on two supervised classifiers (Support Vector Machine and Long Short-Term Memory network) and a learning framework including human and generated samples. Our results prove that the proposed statistical approaches are able to generate realistic human-like synthetic keystroke samples. Also, the classification results suggest that in scenarios with large labeled data, these synthetic samples can be detected with high accuracy. However, in few-shot learning scenarios it represents an important challenge.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1363732192,
        "newsscientist":0.2268406563,
        "technologyreview":0.3277191741,
        "venturebeat":0.3088736276,
        "wired":0.2639149127,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13394v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1658913975000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12829v1",
        "predicted_newsworthiness":0.6697050947,
        "title":"Visualization Design Practices in a Crisis: Behind the Scenes with COVID-19 Dashboard Creators",
        "summary":"During the COVID-19 pandemic, a number of data visualizations were created to inform the public about the rapidly evolving crisis. Data dashboards, a form of information dissemination used during the pandemic, have facilitated this process by visualizing statistics regarding the number of COVID-19 cases over time. In this research, we conducted a qualitative interview study among dashboard creators from federal agencies, state health departments, mainstream news media outlets, and other organizations that created (often widely-used) COVID-19 dashboards to answer the following questions: how did visualization creators engage in COVID-19 dashboard design, and what tensions, conflicts, and challenges arose during this process? Our findings detail the trajectory of design practices -- from creation to expansion, maintenance, and termination -- that are shaped by the complex interplay between design goals, tools and technologies, labor, emerging crisis contexts, and public engagement. We particularly examined the tensions between designers and the general public involved in these processes. These conflicts, which often materialized due to a divergence between public demands and standing policies, centered around the type and amount of information to be visualized, how public perceptions shape and are shaped by visualization design, and the strategies utilized to deal with (potential) misinterpretations and misuse of visualizations. Our findings and lessons learned shed light on new ways of thinking in visualization design, focusing on the bundled activities that are invariably involved in human and nonhuman participation throughout the entire trajectory of design practice.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.3630463624,
        "newsscientist":0.3089610604,
        "technologyreview":0.3839500459,
        "venturebeat":0.3354065959,
        "wired":0.3655656891,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12829v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.cy"
        ],
        "published":1658836355000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.13834v1",
        "predicted_newsworthiness":0.6684354556,
        "title":"Toward Supporting Perceptual Complementarity in Human-AI Collaboration via Reflection on Unobservables",
        "summary":"In many real world contexts, successful human-AI collaboration requires humans to productively integrate complementary sources of information into AI-informed decisions. However, in practice human decision-makers often lack understanding of what information an AI model has access to in relation to themselves. There are few available guidelines regarding how to effectively communicate about unobservables: features that may influence the outcome, but which are unavailable to the model. In this work, we conducted an online experiment to understand whether and how explicitly communicating potentially relevant unobservables influences how people integrate model outputs and unobservables when making predictions. Our findings indicate that presenting prompts about unobservables can change how humans integrate model outputs and unobservables, but do not necessarily lead to improved performance. Furthermore, the impacts of these prompts can vary depending on decision-makers' prior domain expertise. We conclude by discussing implications for future research and design of AI-based decision support tools.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2124801589,
        "newsscientist":0.2595795922,
        "technologyreview":0.3993887907,
        "venturebeat":0.3793583619,
        "wired":0.2947500751,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13834v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ai"
        ],
        "published":1658966714000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.12196v1",
        "predicted_newsworthiness":0.6605923557,
        "title":"On the Relation Between Opinion Change and Information Consumption on Reddit",
        "summary":"While much attention has been devoted to the causes of opinion change, little is known about its consequences. Our study sheds a light on the relationship between one user's opinion change episode and subsequent behavioral change on an online social media, Reddit. In particular, we look at r\/ChangeMyView, an online community dedicated to debating one's own opinions. Interestingly, this forum adopts a well-codified schema for explicitly self-reporting opinion change. Starting from this ground truth, we analyze changes in future online information consumption behavior that arise after a self-reported opinion change on sociopolitical topics; and in particular, operationalized in this work as the participation to sociopolitical subreddits. Such participation profile is important as it represents one's information diet, and is a reliable proxy for, e.g., political affiliation or health choices. We find that people who report an opinion change are significantly more likely to change their future participation in a specific subset of online communities. We characterize which communities are more likely to be abandoned after opinion change, and find a significant association (r=0.46) between propaganda-like language used in a community and the increase in chances of leaving it. We find comparable results (r=0.39) for the opposite direction, i.e., joining a community. This finding suggests how propagandistic communities act as a first gateway to internalize a shift in one's sociopolitical opinion. Finally, we show that the textual content of the discussion associated with opinion change is indicative of which communities are going to be subject to a participation change. In fact, a predictive model based only on the opinion change post is able to pinpoint these communities with an AP@5 of 0.20, similar to what can be reached by using all the past history of participation in communities.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2885333961,
        "newsscientist":0.2379209511,
        "technologyreview":0.3404313654,
        "venturebeat":0.2758260009,
        "wired":0.3524482464,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12196v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.cy"
        ],
        "published":1658756056000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.11528v1",
        "predicted_newsworthiness":0.6593762865,
        "title":"Supporting peace negotiations in the Yemen war through machine learning",
        "summary":"Today's conflicts are becoming increasingly complex, fluid and fragmented, often involving a host of national and international actors with multiple and often divergent interests. This development poses significant challenges for conflict mediation, as mediators struggle to make sense of conflict dynamics, such as the range of conflict parties and the evolution of their political positions, the distinction between relevant and less relevant actors in peace-making, or the identification of key conflict issues and their interdependence. International peace efforts appear ill-equipped to successfully address these challenges. While technology is already being experimented with and used in a range of conflict related fields, such as conflict predicting or information gathering, less attention has been given to how technology can contribute to conflict mediation. This case study contributes to emerging research on the use of state-of-the-art machine learning technologies and techniques in conflict mediation processes. Using dialogue transcripts from peace negotiations in Yemen, this study shows how machine-learning can effectively support mediating teams by providing them with tools for knowledge management, extraction and conflict analysis. Apart from illustrating the potential of machine learning tools in conflict mediation, the paper also emphasises the importance of interdisciplinary and participatory, co-creation methodology for the development of context-sensitive and targeted tools and to ensure meaningful and responsible implementation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2770948453,
        "newsscientist":0.225041465,
        "technologyreview":0.3638059283,
        "venturebeat":0.3311332626,
        "wired":0.3044240478,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11528v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.cy",
            "cs.lg"
        ],
        "published":1658586278000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11893v1",
        "predicted_newsworthiness":0.657495982,
        "title":"Overview of the Shared Task on Fake News Detection in Urdu at FIRE 2020",
        "summary":"This overview paper describes the first shared task on fake news detection in Urdu language. The task was posed as a binary classification task, in which the goal is to differentiate between real and fake news. We provided a dataset divided into 900 annotated news articles for training and 400 news articles for testing. The dataset contained news in five domains: (i) Health, (ii) Sports, (iii) Showbiz, (iv) Technology, and (v) Business. 42 teams from 6 different countries (India, China, Egypt, Germany, Pakistan, and the UK) registered for the task. 9 teams submitted their experimental results. The participants used various machine learning methods ranging from feature-based traditional machine learning to neural networks techniques. The best performing system achieved an F-score value of 0.90, showing that the BERT-based approach outperforms other machine learning techniques",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2147536585,
        "newsscientist":0.195985429,
        "technologyreview":0.3117521784,
        "venturebeat":0.2742314685,
        "wired":0.2732099562,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11893v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658720492000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00433v1",
        "predicted_newsworthiness":0.6540955563,
        "title":"The Who in Code-Switching: A Case Study for Predicting Egyptian Arabic-English Code-Switching Levels based on Character Profiles",
        "summary":"Code-switching (CS) is a common linguistic phenomenon exhibited by multilingual individuals, where they tend to alternate between languages within one single conversation. CS is a complex phenomenon that not only encompasses linguistic challenges, but also contains a great deal of complexity in terms of its dynamic behaviour across speakers. Given that the factors giving rise to CS vary from one country to the other, as well as from one person to the other, CS is found to be a speaker-dependant behaviour, where the frequency by which the foreign language is embedded differs across speakers. While several researchers have looked into predicting CS behaviour from a linguistic point of view, research is still lacking in the task of predicting user CS behaviour from sociological and psychological perspectives. We provide an empirical user study, where we investigate the correlations between users' CS levels and character traits. We conduct interviews with bilinguals and gather information on their profiles, including their demographics, personality traits, and traveling experiences. We then use machine learning (ML) to predict users' CS levels based on their profiles, where we identify the main influential factors in the modeling process. We experiment with both classification as well as regression tasks. Our results show that the CS behaviour is affected by the relation between speakers, travel experiences as well as Neuroticism and Extraversion personality traits.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2121034445,
        "newsscientist":0.2109356285,
        "technologyreview":0.2861730907,
        "venturebeat":0.2849080632,
        "wired":0.2579108808,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00433v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659275255000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.12406v1",
        "predicted_newsworthiness":0.6539820246,
        "title":"UrduFake@FIRE2020: Shared Track on Fake News Identification in Urdu",
        "summary":"This paper gives the overview of the first shared task at FIRE 2020 on fake news detection in the Urdu language. This is a binary classification task in which the goal is to identify fake news using a dataset composed of 900 annotated news articles for training and 400 news articles for testing. The dataset contains news in five domains: (i) Health, (ii) Sports, (iii) Showbiz, (iv) Technology, and (v) Business. 42 teams from 6 different countries (India, China, Egypt, Germany, Pakistan, and the UK) registered for the task. 9 teams submitted their experimental results. The participants used various machine learning methods ranging from feature-based traditional machine learning to neural network techniques. The best performing system achieved an F-score value of 0.90, showing that the BERT-based approach outperforms other machine learning classifiers.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2218470732,
        "newsscientist":0.2013635417,
        "technologyreview":0.3133568942,
        "venturebeat":0.2773478417,
        "wired":0.2826276421,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12406v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658720811000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.13749v1",
        "predicted_newsworthiness":0.6515948139,
        "title":"Nutzungsverhalten und Funktionsanforderungen digitaler Trainingsanwendungen w\u00e4hrend der Pandemie",
        "summary":"Due to contact restrictions, closure of fitness centers and quarantine measures, the SARS-CoV-2 pandemic led to a considerable decline of sporting activities. The first relaxation of these restrictions allowed German citizens to mostly return to their normal training and exercise behavior, yet the long-term impact of the recurring measures (i.e. the \"Lockdown\", \"Lockdown light\" as well as the \"Corona Emergency Break\" in the case of Germany) remain rather under-investigated. Using a survey of (n=108) German sportspersons, we measured a significant decline of sporting activities even within the intermediary phases without major pandemic constraints. To evaluate the capabilities of digital training applications in countering these effects, we additionally recorded the usage of, among others, apps, trackers, videos and conferencing systems and identified the most important as well as missing and\/or essential features with regards to their capabilities of facilitating individual sport and training in times without access to facilities or social contacts. Effectively, the usage of smart watches, online videos and conferences increased significantly when compared to before the pandemic; and especially online videos and conferences contributed to higher training frequencies. Data-driven or individual feedback, motivation and collaboration revealed to be the most important or even necessary functions for users of digital training applications to counter the decline of social components of training.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.3429847692,
        "newsscientist":0.2899181469,
        "technologyreview":0.354094497,
        "venturebeat":0.3356411816,
        "wired":0.3301646766,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13749v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1658948376000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.00479v1",
        "predicted_newsworthiness":0.650269421,
        "title":"The impact of Twitter on political influence on the choice of a running mate: Social Network Analysis and Semantic Analysis -- A Review",
        "summary":"In this new era of social media, social networks are becoming increasingly important sources of user-generated content on the internet. These kinds of information resources, which include a lot of people's feelings, opinions, feedback, and reviews, are very useful for big businesses, markets, politics, journalism, and many other fields. Politics is one of the most talked-about and popular topics on social media networks right now. Many politicians use micro-blogging services like Twitter because they have a large number of followers and supporters on those networks. Politicians, political parties, political organizations, and foundations use social media networks to communicate with citizens ahead of time. Today, social media is used by hundreds of thousands of political groups and politicians. On these social media networks, every politician and political party has millions of followers, and politicians find new and innovative ways to urge individuals to participate in politics. Furthermore, social media assists politicians in various decision-making processes by providing recommendations, such as developing policies and strategies based on previous experiences, recommending and selecting suitable candidates for a particular constituency, recommending a suitable person for a particular position in the party, and launching a political campaign based on citizen sentiments on various issues and controversies, among other things. This research is a review on the use of social network analysis (SNA) and semantic analysis (SA) on the Twitter platform to study the supporters networks of political leaders because it can help in decision-making when predicting their political futures.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2716114038,
        "newsscientist":0.1937511297,
        "technologyreview":0.3324583585,
        "venturebeat":0.2843187941,
        "wired":0.3461680868,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00479v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.si"
        ],
        "published":1659289497000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00176v2",
        "predicted_newsworthiness":0.6487458938,
        "title":"ELF22: A Context-based Counter Trolling Dataset to Combat Internet Trolls",
        "summary":"Online trolls increase social costs and cause psychological damage to individuals. With the proliferation of automated accounts making use of bots for trolling, it is difficult for targeted individual users to handle the situation both quantitatively and qualitatively. To address this issue, we focus on automating the method to counter trolls, as counter responses to combat trolls encourage community users to maintain ongoing discussion without compromising freedom of expression. For this purpose, we propose a novel dataset for automatic counter response generation. In particular, we constructed a pair-wise dataset that includes troll comments and counter responses with labeled response strategies, which enables models fine-tuned on our dataset to generate responses by varying counter responses according to the specified strategy. We conducted three tasks to assess the effectiveness of our dataset and evaluated the results through both automatic and human evaluation. In human evaluation, we demonstrate that the model fine-tuned on our dataset shows a significantly improved performance in strategy-controlled sentence generation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1991123947,
        "newsscientist":0.1937065253,
        "technologyreview":0.3179598239,
        "venturebeat":0.2945075391,
        "wired":0.2876244283,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00176v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659176081000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.13658v1",
        "predicted_newsworthiness":0.6438311438,
        "title":"BotBuster: Multi-platform Bot Detection Using A Mixture of Experts",
        "summary":"Despite rapid development, current bot detection models still face challenges in dealing with incomplete data and cross-platform applications. In this paper, we propose BotBuster, a social bot detector built with the concept of a mixture of experts approach. Each expert is trained to analyze a portion of account information, e.g. username, and are combined to estimate the probability that the account is a bot. Experiments on 10 Twitter datasets show that BotBuster outperforms popular bot-detection baselines (avg F1=73.54 vs avg F1=45.12). This is accompanied with F1=60.04 on a Reddit dataset and F1=60.92 on an external evaluation set. Further analysis shows that only 36 posts is required for a stable bot classification. Investigation shows that bot post features have changed across the years and can be difficult to differentiate from human features, making bot detection a difficult and ongoing problem.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.167184137,
        "newsscientist":0.1964428491,
        "technologyreview":0.3449257349,
        "venturebeat":0.3333170075,
        "wired":0.2735056731,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13658v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1658942769000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.13222v1",
        "predicted_newsworthiness":0.6434960127,
        "title":"Information We Can Extract About a User From 'One Minute Mobile Application Usage'",
        "summary":"Understanding human behavior is an important task and has applications in many domains such as targeted advertisement, health analytics, security, and entertainment, etc. For this purpose, designing a system for activity recognition (AR) is important. However, since every human can have different behaviors, understanding and analyzing common patterns become a challenging task. Since smartphones are easily available to every human being in the modern world, using them to track the human activities becomes possible. In this paper, we extracted different human activities using accelerometer, magnetometer, and gyroscope sensors of android smartphones by building an android mobile applications. Using different social media applications, such as Facebook, Instagram, Whatsapp, and Twitter, we extracted the raw sensor values along with the attributes of $29$ subjects along with their attributes (class labels) such as age, gender, and left\/right\/both hands application usage. We extract features from the raw signals and use them to perform classification using different machine learning (ML) algorithms. Using statistical analysis, we show the importance of different features towards the prediction of class labels. In the end, we use the trained ML model on our data to extract unknown features from a well known activity recognition data from UCI repository, which highlights the potential of privacy breach using ML models. This security analysis could help researchers in future to take appropriate steps to preserve the privacy of human subjects.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1835834782,
        "newsscientist":0.2319685795,
        "technologyreview":0.3349760017,
        "venturebeat":0.3300883502,
        "wired":0.3190461539,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13222v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cy"
        ],
        "published":1658881391000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01157v1",
        "predicted_newsworthiness":0.6432820404,
        "title":"Performance Disparities Between Accents in Automatic Speech Recognition",
        "summary":"Automatic speech recognition (ASR) services are ubiquitous, transforming speech into text for systems like Amazon's Alexa, Google's Assistant, and Microsoft's Cortana. However, researchers have identified biases in ASR performance between particular English language accents by racial group and by nationality. In this paper, we expand this discussion both qualitatively by relating it to historical precedent and quantitatively through a large-scale audit. Standardization of language and the use of language to maintain global and political power have played an important role in history, which we explain to show the parallels in the ways in which ASR services act on English language speakers today. Then, using a large and global data set of speech from The Speech Accent Archive which includes over 2,700 speakers of English born in 171 different countries, we perform an international audit of some of the most popular English ASR services. We show that performance disparities exist as a function of whether or not a speaker's first language is English and, even when controlling for multiple linguistic covariates, that these disparities have a statistically significant relationship to the political alignment of the speaker's birth country with respect to the United States' geopolitical power.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.235050667,
        "newsscientist":0.2168448898,
        "technologyreview":0.3455641025,
        "venturebeat":0.3186473076,
        "wired":0.2867880365,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01157v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.cy"
        ],
        "published":1659391821000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11562v1",
        "predicted_newsworthiness":0.6422687289,
        "title":"Better Reasoning Behind Classification Predictions with BERT for Fake News Detection",
        "summary":"Fake news detection has become a major task to solve as there has been an increasing number of fake news on the internet in recent years. Although many classification models have been proposed based on statistical learning methods showing good results, reasoning behind the classification performances may not be enough. In the self-supervised learning studies, it has been highlighted that a quality of representation (embedding) space matters and directly affects a downstream task performance. In this study, a quality of the representation space is analyzed visually and analytically in terms of linear separability for different classes on a real and fake news dataset. To further add interpretability to a classification model, a modification of Class Activation Mapping (CAM) is proposed. The modified CAM provides a CAM score for each word token, where the CAM score on a word token denotes a level of focus on that word token to make the prediction. Finally, it is shown that the naive BERT model topped with a learnable linear layer is enough to achieve robust performance while being compatible with CAM.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1835238711,
        "newsscientist":0.2024831467,
        "technologyreview":0.3311862468,
        "venturebeat":0.2909554145,
        "wired":0.2707858785,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11562v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658598888000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14526v1",
        "predicted_newsworthiness":0.6417106397,
        "title":"Leveraging Explanations in Interactive Machine Learning: An Overview",
        "summary":"Explanations have gained an increasing level of interest in the AI and Machine Learning (ML) communities in order to improve model transparency and allow users to form a mental model of a trained ML model. However, explanations can go beyond this one way communication as a mechanism to elicit user control, because once users understand, they can then provide feedback. The goal of this paper is to present an overview of research where explanations are combined with interactive capabilities as a mean to learn new models from scratch and to edit and debug existing ones. To this end, we draw a conceptual map of the state-of-the-art, grouping relevant approaches based on their intended purpose and on how they structure the interaction, highlighting similarities and differences between them. We also discuss open research issues and outline possible directions forward, with the hope of spurring further research on this blooming research topic.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.15830719,
        "newsscientist":0.201991534,
        "technologyreview":0.3462702371,
        "venturebeat":0.3264676858,
        "wired":0.2466518568,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14526v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659080771000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01273v1",
        "predicted_newsworthiness":0.6404476538,
        "title":"Industry 4.0 Asset Administration Shell (AAS): Interoperable Skill-Based Service-Robots",
        "summary":"This paper describes our use of Industry 4.0 Asset Administration Shells (AASs) in the context of service robots. We use AASs with software components of service robots and with complete service robot systems. The AAS for a software component serves as a standardized digital data sheet. It helps sysem builders at design time in finding and selecting software components that match system-level requirements of the systems to be built. The AAS for a system comprises a data sheet for the system and furthermore collects at runtime operational data and it allows for skill-level commanding of the service robot. AASs are generated and filled as part of our model-driven development and composition workflow for service robotics. AASs can serve as a key enabler for a standardized integration and interaction with service robots.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1046392223,
        "newsscientist":0.1606648869,
        "technologyreview":0.3110231635,
        "venturebeat":0.3066105144,
        "wired":0.2473367472,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01273v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659422663000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.12761v1",
        "predicted_newsworthiness":0.6397705074,
        "title":"The Human in the Infinite Loop: A Case Study on Revealing and Explaining Human-AI Interaction Loop Failures",
        "summary":"Interactive AI systems increasingly employ a human-in-the-loop strategy. This creates new challenges for the HCI community when designing such systems. We reveal and investigate some of these challenges in a case study with an industry partner, and developed a prototype human-in-the-loop system for preference-guided 3D model processing. Two 3D artists used it in their daily work for 3 months. We found that the human-AI loop often did not converge towards a satisfactory result and designed a lab study (N=20) to investigate this further. We analyze interaction data and user feedback through the lens of theories of human judgment to explain the observed human-in-the-loop failures with two key insights: 1) optimization using preferential choices lacks mechanisms to deal with inconsistent and contradictory human judgments; 2) machine outcomes, in turn, influence future user inputs via heuristic biases and loss aversion. To mitigate these problems, we propose descriptive UI design guidelines. Our case study draws attention to challenging and practically relevant imperfections in human-AI loops that need to be considered when designing human-in-the-loop systems.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1940931621,
        "newsscientist":0.2495430646,
        "technologyreview":0.3762120154,
        "venturebeat":0.3535348298,
        "wired":0.3281805353,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12761v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1658826674000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.12551v1",
        "predicted_newsworthiness":0.6397453176,
        "title":"DialCrowd 2.0: A Quality-Focused Dialog System Crowdsourcing Toolkit",
        "summary":"Dialog system developers need high-quality data to train, fine-tune and assess their systems. They often use crowdsourcing for this since it provides large quantities of data from many workers. However, the data may not be of sufficiently good quality. This can be due to the way that the requester presents a task and how they interact with the workers. This paper introduces DialCrowd 2.0 to help requesters obtain higher quality data by, for example, presenting tasks more clearly and facilitating effective communication with workers. DialCrowd 2.0 guides developers in creating improved Human Intelligence Tasks (HITs) and is directly applicable to the workflows used currently by developers and researchers.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1772571285,
        "newsscientist":0.2143828074,
        "technologyreview":0.3642648231,
        "venturebeat":0.3807594572,
        "wired":0.3076354796,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12551v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658786780000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.02007v1",
        "predicted_newsworthiness":0.6389625458,
        "title":"Maintaining Performance with Less Data",
        "summary":"We propose a novel method for training a neural network for image classification to reduce input data dynamically, in order to reduce the costs of training a neural network model. As Deep Learning tasks become more popular, their computational complexity increases, leading to more intricate algorithms and models which have longer runtimes and require more input data. The result is a greater cost on time, hardware, and environmental resources. By using data reduction techniques, we reduce the amount of work performed, and therefore the environmental impact of AI techniques, and with dynamic data reduction we show that accuracy may be maintained while reducing runtime by up to 50%, and reducing carbon emission proportionally.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1685615701,
        "newsscientist":0.2288991013,
        "technologyreview":0.3480001276,
        "venturebeat":0.3227379419,
        "wired":0.2328671249,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02007v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1659529338000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01696v1",
        "predicted_newsworthiness":0.6375634717,
        "title":"Measuring Commonality in Recommendation of Cultural Content: Recommender Systems to Enhance Cultural Citizenship",
        "summary":"Recommender systems have become the dominant means of curating cultural content, significantly influencing the nature of individual cultural experience. While the majority of research on recommender systems optimizes for personalized user experience, this paradigm does not capture the ways that recommender systems impact cultural experience in the aggregate, across populations of users. Although existing novelty, diversity, and fairness studies probe how systems relate to the broader social role of cultural content, they do not adequately center culture as a core concept and challenge. In this work, we introduce commonality as a new measure that reflects the degree to which recommendations familiarize a given user population with specified categories of cultural content. Our proposed commonality metric responds to a set of arguments developed through an interdisciplinary dialogue between researchers in computer science and the social sciences and humanities. With reference to principles underpinning non-profit, public service media systems in democratic societies, we identify universality of address and content diversity in the service of strengthening cultural citizenship as particularly relevant goals for recommender systems delivering cultural content. Taking diversity in movie recommendation as a case study in enhancing pluralistic cultural experience, we empirically compare systems' performance using commonality and existing utility, diversity, and fairness metrics. Our results demonstrate that commonality captures a property of system behavior complementary to existing metrics and suggest the need for alternative, non-personalized interventions in recommender systems oriented to strengthening cultural citizenship across populations of users. In this way, commonality contributes to a growing body of scholarship developing 'public good' rationales for digital media and ML systems.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2678207729,
        "newsscientist":0.1993060673,
        "technologyreview":0.3201509678,
        "venturebeat":0.3060444426,
        "wired":0.3100962501,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01696v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy",
            "cs.ir"
        ],
        "published":1659467689000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.11953v1",
        "predicted_newsworthiness":0.6364616646,
        "title":"Deep Learning for Forecasting the Energy Consumption in Public Buildings",
        "summary":"In this paper we propose a Long Short-Term Memory Network based method to forecast the energy consumption in public buildings, based on past measurements. Our approach consists of three main steps: data processing step, training and validation step, and finally the forecasting step. We tested our method on a data set consisting of measurements taken every half an hour from the main building of the National Archives of the United Kingdom, in Kew and as evaluation metrics we have used Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE).",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1608783183,
        "newsscientist":0.1926354725,
        "technologyreview":0.2450218039,
        "venturebeat":0.2335310574,
        "wired":0.1830661309,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11953v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658734975000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12054v1",
        "predicted_newsworthiness":0.6342318599,
        "title":"A Reference Data Model for Process-Related User Interaction Logs",
        "summary":"User interaction (UI) logs are high-resolution event logs that record low-level activities performed by a user during the execution of a task in an information system. Each event in a UI log corresponds to a single interaction between the user and the interface, such as clicking a button or entering a string into a text field. UI logs are used for purposes like task mining or robotic process automation (RPA), but each study and tool relies on a different conceptualization and implementation of the elements and attributes that constitute user interactions. This lack of standardization makes it difficult to integrate UI logs from different sources and to combine tools for UI data collection with downstream analytics or automation solutions. To address this, we propose a universally applicable reference data model for process-related UI logs. Based on a review of scientific literature and industry solutions, this model includes the core attributes of UI logs, but remains flexible with regard to the scope, level of abstraction, and case notion. We provide an implementation of the model as an extension to the XES interchange standard for event logs and demonstrate its practical applicability in a real-life RPA scenario.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.112064822,
        "newsscientist":0.1386772244,
        "technologyreview":0.2325377081,
        "venturebeat":0.2761316306,
        "wired":0.2057988504,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12054v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1658746067000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2208.01802v1",
        "predicted_newsworthiness":0.6327826423,
        "title":"Mutual Information Scoring: Increasing Interpretability in Categorical Clustering Tasks with Applications to Child Welfare Data",
        "summary":"Youth in the American foster care system are significantly more likely than their peers to face a number of negative life outcomes, from homelessness to incarceration. Administrative data on these youth have the potential to provide insights that can help identify ways to improve their path towards a better life. However, such data also suffer from a variety of biases, from missing data to reflections of systemic inequality. The present work proposes a novel, prescriptive approach to using these data to provide insights about both data biases and the systems and youth they track. Specifically, we develop a novel categorical clustering and cluster summarization methodology that allows us to gain insights into subtle biases in existing data on foster youth, and to provide insight into where further (often qualitative) research is needed to identify potential ways of assisting youth.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2680982662,
        "newsscientist":0.1808820486,
        "technologyreview":0.2547669494,
        "venturebeat":0.2246162433,
        "wired":0.2137680507,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01802v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659489069000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2208.00780v2",
        "predicted_newsworthiness":0.6313206373,
        "title":"Visual correspondence-based explanations improve AI robustness and human-AI team accuracy",
        "summary":"Explaining artificial intelligence (AI) predictions is increasingly important and even imperative in many high-stakes applications where humans are the ultimate decision-makers. In this work, we propose two novel architectures of self-interpretable image classifiers that first explain, and then predict (as opposed to post-hoc explanations) by harnessing the visual correspondences between a query image and exemplars. Our models consistently improve (by 1 to 4 points) on out-of-distribution (OOD) datasets while performing marginally worse (by 1 to 2 points) on in-distribution tests than ResNet-50 and a $k$-nearest neighbor classifier (kNN). Via a large-scale, human study on ImageNet and CUB, our correspondence-based explanations are found to be more useful to users than kNN explanations. Our explanations help users more accurately reject AI's wrong decisions than all other tested methods. Interestingly, for the first time, we show that it is possible to achieve complementary human-AI team accuracy (i.e., that is higher than either AI-alone or human-alone), in ImageNet and CUB image classification tasks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1408994668,
        "newsscientist":0.2178582052,
        "technologyreview":0.389216765,
        "venturebeat":0.362375405,
        "wired":0.2561048009,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00780v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.hc",
            "cs.lg"
        ],
        "published":1658833182000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14382v2",
        "predicted_newsworthiness":0.629950362,
        "title":"Large Language Models and the Reverse Turing Test",
        "summary":"Large Language Models (LLMs) have been transformative. They are pre-trained foundational models that can be adapted with fine tuning to many different natural language tasks, each of which previously would have required a separate network model. This is one step closer to the extraordinary versatility of human language. GPT-3 and more recently LaMDA can carry on dialogs with humans on many topics after minimal priming with a few examples. However, there has been a wide range of reactions on whether these LLMs understand what they are saying or exhibit signs of intelligence. This high variance is exhibited in three interviews with LLMs reaching wildly different conclusions. A new possibility was uncovered that could explain this divergence. What appears to be intelligence in LLMs may in fact be a mirror that reflects the intelligence of the interviewer, a remarkable twist that could be considered a Reverse Turing Test. If so, then by studying interviews we may be learning more about the intelligence and beliefs of the interviewer than the intelligence of the LLMs. As LLMs become more capable they may transform the way we access and use information.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2012542224,
        "newsscientist":0.2558554005,
        "technologyreview":0.4039185237,
        "venturebeat":0.3798393361,
        "wired":0.2983199451,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14382v2",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai",
            "cs.lg"
        ],
        "published":1659043367000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14636v1",
        "predicted_newsworthiness":0.6263294377,
        "title":"Detecting Spam Reviews on Vietnamese E-commerce Websites",
        "summary":"The reviews of customers play an essential role in online shopping. People often refer to reviews or comments of previous customers to decide whether to buy a new product. Catching up with this behavior, some people create untruths and illegitimate reviews to hoax customers about the fake quality of products. These reviews are called spam reviews, which confuse consumers on online shopping platforms and negatively affect online shopping behaviors. We propose the dataset called ViSpamReviews, which has a strict annotation procedure for detecting spam reviews on e-commerce platforms. Our dataset consists of two tasks: the binary classification task for detecting whether a review is a spam or not and the multi-class classification task for identifying the type of spam. The PhoBERT obtained the highest results on both tasks, 88.93% and 72.17%, respectively, by macro average F1 score.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1473920418,
        "newsscientist":0.1536726668,
        "technologyreview":0.2632584047,
        "venturebeat":0.265843473,
        "wired":0.2203753241,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14636v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1658918234000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.01312v1",
        "predicted_newsworthiness":0.6257730767,
        "title":"BEIKE NLP at SemEval-2022 Task 4: Prompt-Based Paragraph Classification for Patronizing and Condescending Language Detection",
        "summary":"PCL detection task is aimed at identifying and categorizing language that is patronizing or condescending towards vulnerable communities in the general media.Compared to other NLP tasks of paragraph classification, the negative language presented in the PCL detection task is usually more implicit and subtle to be recognized, making the performance of common text-classification approaches disappointed. Targeting the PCL detection problem in SemEval-2022 Task 4, in this paper, we give an introduction to our team's solution, which exploits the power of prompt-based learning on paragraph classification. We reformulate the task as an appropriate cloze prompt and use pre-trained Masked Language Models to fill the cloze slot. For the two subtasks, binary classification and multi-label classification, DeBERTa model is adopted and fine-tuned to predict masked label words of task-specific prompts. On the evaluation dataset, for binary classification, our approach achieves an F1-score of 0.6406; for multi-label classification, our approach achieves an macro-F1-score of 0.4689 and ranks first in the leaderboard.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2057628977,
        "newsscientist":0.1734312238,
        "technologyreview":0.2734468165,
        "venturebeat":0.2549071726,
        "wired":0.2565036979,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01312v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659429527000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00516v1",
        "predicted_newsworthiness":0.6238324532,
        "title":"Learning an Interpretable Model for Driver Behavior Prediction with Inductive Biases",
        "summary":"To plan safe maneuvers and act with foresight, autonomous vehicles must be capable of accurately predicting the uncertain future. In the context of autonomous driving, deep neural networks have been successfully applied to learning predictive models of human driving behavior from data. However, the predictions suffer from cascading errors, resulting in large inaccuracies over long time horizons. Furthermore, the learned models are black boxes, and thus it is often unclear how they arrive at their predictions. In contrast, rule-based models, which are informed by human experts, maintain long-term coherence in their predictions and are human-interpretable. However, such models often lack the sufficient expressiveness needed to capture complex real-world dynamics. In this work, we begin to close this gap by embedding the Intelligent Driver Model, a popular hand-crafted driver model, into deep neural networks. Our model's transparency can offer considerable advantages, e.g., in debugging the model and more easily interpreting its predictions. We evaluate our approach on a simulated merging scenario, showing that it yields a robust model that is end-to-end trainable and provides greater transparency at no cost to the model's predictive accuracy.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1576596113,
        "newsscientist":0.2046662373,
        "technologyreview":0.3638483093,
        "venturebeat":0.3308262351,
        "wired":0.2877272691,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00516v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659300762000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.12669v2",
        "predicted_newsworthiness":0.6220166024,
        "title":"EEG-Based Detection of Braking Intention During Simulated Driving",
        "summary":"Accurately detecting and identifying drivers' braking intention is the basis of man-machine driving. In this paper, we proposed an electroencephalographic (EEG)-based braking intention measurement strategy. We used the Car Learning to Act (Carla) platform to build the simulated driving environment. 11 subjects participated in our study, and each subject drove a simulated vehicle to complete emergency braking and normal braking tasks. We compared the EEG topographic maps in different braking situations and used three different classifiers to predict the subjects' braking intention through EEG signals. The experimental results showed that the average response time of subjects in emergency braking was 762 ms; emergency braking and no braking can be well distinguished, while normal braking and no braking were not easy to be classified; for the two different types of braking, emergency braking and normal braking had obvious differences in EEG topographic maps, and the classification results also showed that the two were highly distinguishable. This study provides a user-centered driver-assistance system and a good framework to combine with advanced shared control algorithms, which has the potential to be applied to achieve a more friendly interaction between the driver and vehicle in real driving environment.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1534864271,
        "newsscientist":0.20180478,
        "technologyreview":0.2815313687,
        "venturebeat":0.2478267953,
        "wired":0.2284777174,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12669v2",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1658816340000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.12200v1",
        "predicted_newsworthiness":0.6195867567,
        "title":"Aveiro Tech City Living Lab: A Communication, Sensing and Computing Platform for City Environments",
        "summary":"This article presents the deployment and experimentation architecture of the Aveiro Tech City Living Lab (ATCLL) in Aveiro, Portugal. This platform comprises a large number of Internet-of-Things devices with communication, sensing and computing capabilities. The communication infrastructure, built on fiber and Millimeter-wave (mmWave) links, integrates a communication network with radio terminals (WiFi, ITS-G5, C-V2X, 5G and LoRa(WAN)), multiprotocol, spread throughout 44 connected points of access in the city. Additionally, public transportation has also been equipped with communication and sensing units. All these points combine and interconnect a set of sensors, such as mobility (Radars, Lidars, video cameras) and environmental sensors. Combining edge computing and cloud management to deploy the services and manage the platform, and a data platform to gather and process the data, the living lab supports a wide range of services and applications: IoT, intelligent transportation systems and assisted driving, environmental monitoring, emergency and safety, among others. This article describes the architecture, implementation and deployment to make the overall platform to work and integrate researchers and citizens. Moreover, it showcases some examples of the performance metrics achieved in the city infrastructure, the data that can be collected, visualized and used to build services and applications to the cities, and, finally, different use cases in the mobility and safety scenarios.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2131557684,
        "newsscientist":0.2216667105,
        "technologyreview":0.3289703874,
        "venturebeat":0.3523933929,
        "wired":0.3193273768,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12200v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658756529000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.13842v1",
        "predicted_newsworthiness":0.6194139704,
        "title":"Dive into Machine Learning Algorithms for Influenza Virus Host Prediction with Hemagglutinin Sequences",
        "summary":"Influenza viruses mutate rapidly and can pose a threat to public health, especially to those in vulnerable groups. Throughout history, influenza A viruses have caused pandemics between different species. It is important to identify the origin of a virus in order to prevent the spread of an outbreak. Recently, there has been increasing interest in using machine learning algorithms to provide fast and accurate predictions for viral sequences. In this study, real testing data sets and a variety of evaluation metrics were used to evaluate machine learning algorithms at different taxonomic levels. As hemagglutinin is the major protein in the immune response, only hemagglutinin sequences were used and represented by position-specific scoring matrix and word embedding. The results suggest that the 5-grams-transformer neural network is the most effective algorithm for predicting viral sequence origins, with approximately 99.54% AUCPR, 98.01% F1 score and 96.60% MCC at a higher classification level, and approximately 94.74% AUCPR, 87.41% F1 score and 80.79% MCC at a lower classification level.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2413945152,
        "newsscientist":0.2642443926,
        "technologyreview":0.3019726223,
        "venturebeat":0.2672433371,
        "wired":0.1934682644,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13842v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658969694000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13382v2",
        "predicted_newsworthiness":0.619384507,
        "title":"Exploration and Application of AI in 6G Field",
        "summary":"The recent upsurge of diversified mobile applications, especially those supported by AI, is spurring heated discussions on the future evolution of wireless communications. While 5G is being deployed around the world, efforts from industry and academia have started to look beyond 5G and conceptualize 6G. We envision 6G to experience an unprecedented transformation that will make it completely different from the previous generations of wireless systems. In particular, 6G will go beyond mobile Internet and will be required to support AI services. Meanwhile, AI will play a critical role in designing and optimizing 6G architectures, protocols and operations. In this article, we discuss the features of 6G, and the difficulties of carrying out 6G, and AI-enabled methods for 6G network design and optimization.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.103492876,
        "newsscientist":0.1746755012,
        "technologyreview":0.2876421233,
        "venturebeat":0.2978678156,
        "wired":0.2422304015,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13382v2",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658913065000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.12935v1",
        "predicted_newsworthiness":0.6176413193,
        "title":"\"I Used To Carry A Wallet, Now I Just Need To Carry My Phone\": Understanding Current Banking Practices and Challenges Among Older Adults in China",
        "summary":"Managing finances is crucial for older adults who are retired and may rely on savings to ensure their life quality. As digital banking platforms (e.g., mobile apps, electronic payment) gradually replace physical ones, it is critical to understand how they adapt to digital banking and the potential frictions they experience. We conducted semi-structured interviews with 16 older adults in China, where the aging population is the largest and digital banking grows fast. We also interviewed bank employees to gain complementary perspectives of these help givers. Our findings show that older adults used both physical and digital platforms as an ecosystem based on perceived pros and cons. Perceived usefulness, self-confidence, and social influence were key motivators for learning digital banking. They experienced app-related (e.g., insufficient error-recovery support) and user-related challenges (e.g., trust, security and privacy concerns, low perceived self-efficacy) and developed coping strategies. We discuss design considerations to improve their banking experiences.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2418798507,
        "newsscientist":0.1967234975,
        "technologyreview":0.3188853161,
        "venturebeat":0.3114103957,
        "wired":0.3011475677,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12935v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.cy"
        ],
        "published":1658846649000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.12021v1",
        "predicted_newsworthiness":0.6157869541,
        "title":"Neural Generation Meets Real People: Building a Social, Informative Open-Domain Dialogue Agent",
        "summary":"We present Chirpy Cardinal, an open-domain social chatbot. Aiming to be both informative and conversational, our bot chats with users in an authentic, emotionally intelligent way. By integrating controlled neural generation with scaffolded, hand-written dialogue, we let both the user and bot take turns driving the conversation, producing an engaging and socially fluent experience. Deployed in the fourth iteration of the Alexa Prize Socialbot Grand Challenge, Chirpy Cardinal handled thousands of conversations per day, placing second out of nine bots with an average user rating of 3.58\/5.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1753650693,
        "newsscientist":0.2279155323,
        "technologyreview":0.367229987,
        "venturebeat":0.3622158377,
        "wired":0.3048123857,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12021v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658743043000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.02058v1",
        "predicted_newsworthiness":0.6126444709,
        "title":"Robots with Different Embodiments Can Express and Influence Carefulness in Object Manipulation",
        "summary":"Humans have an extraordinary ability to communicate and read the properties of objects by simply watching them being carried by someone else. This level of communicative skills and interpretation, available to humans, is essential for collaborative robots if they are to interact naturally and effectively. For example, suppose a robot is handing over a fragile object. In that case, the human who receives it should be informed of its fragility in advance, through an immediate and implicit message, i.e., by the direct modulation of the robot's action. This work investigates the perception of object manipulations performed with a communicative intent by two robots with different embodiments (an iCub humanoid robot and a Baxter robot). We designed the robots' movements to communicate carefulness or not during the transportation of objects. We found that not only this feature is correctly perceived by human observers, but it can elicit as well a form of motor adaptation in subsequent human object manipulations. In addition, we get an insight into which motion features may induce to manipulate an object more or less carefully.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1602035381,
        "newsscientist":0.2346764662,
        "technologyreview":0.3147115834,
        "venturebeat":0.2482707888,
        "wired":0.2416229545,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02058v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.lg"
        ],
        "published":1659533212000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.02052v1",
        "predicted_newsworthiness":0.6093986306,
        "title":"Large scale analysis of gender bias and sexism in song lyrics",
        "summary":"We employ Natural Language Processing techniques to analyse 377808 English song lyrics from the \"Two Million Song Database\" corpus, focusing on the expression of sexism across five decades (1960-2010) and the measurement of gender biases. Using a sexism classifier, we identify sexist lyrics at a larger scale than previous studies using small samples of manually annotated popular songs. Furthermore, we reveal gender biases by measuring associations in word embeddings learned on song lyrics. We find sexist content to increase across time, especially from male artists and for popular songs appearing in Billboard charts. Songs are also shown to contain different language biases depending on the gender of the performer, with male solo artist songs containing more and stronger biases. This is the first large scale analysis of this type, giving insights into language usage in such an influential part of popular culture.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2459197387,
        "newsscientist":0.1857582784,
        "technologyreview":0.2786261178,
        "venturebeat":0.2484277722,
        "wired":0.2865302123,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02052v1",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1659532722000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.12144v1",
        "predicted_newsworthiness":0.608243917,
        "title":"Personalised Robot Behaviour Modelling for Robot-Assisted Therapy in the Context of Autism Spectrum Disorder",
        "summary":"In robot-assisted therapy for individuals with Autism Spectrum Disorder, the workload of therapists during a therapeutic session is increased if they have to control the robot manually. To allow therapists to focus on the interaction with the person instead, the robot should be more autonomous, namely it should be able to interpret the person's state and continuously adapt its actions according to their behaviour. In this paper, we develop a personalised robot behaviour model that can be used in the robot decision-making process during an activity; this behaviour model is trained with the help of a user model that has been learned from real interaction data. We use Q-learning for this task, such that the results demonstrate that the policy requires about 10,000 iterations to converge. We thus investigate policy transfer for improving the convergence speed; we show that this is a feasible solution, but an inappropriate initial policy can lead to a suboptimal final return.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1482741575,
        "newsscientist":0.2074360228,
        "technologyreview":0.3312426326,
        "venturebeat":0.302432806,
        "wired":0.2405301197,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12144v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658753257000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00033v1",
        "predicted_newsworthiness":0.6080551433,
        "title":"Personalised recommendations of sleep behaviour with neural networks using sleep diaries captured in Sleepio",
        "summary":"SleepioTM is a digital mobile phone and web platform that uses techniques from cognitive behavioural therapy (CBT) to improve sleep in people with sleep difficulty. As part of this process, Sleepio captures data about the sleep behaviour of the users that have consented to such data being processed. For neural networks, the scale of the data is an opportunity to train meaningful models translatable to actual clinical practice. In collaboration with Big Health, the therapeutics company that created and utilizes Sleepio, we have analysed data from a random sample of 401,174 sleep diaries and built a neural network to model sleep behaviour and sleep quality of each individual in a personalised manner. We demonstrate that this neural network is more accurate than standard statistical methods in predicting the sleep quality of an individual based on his\/her behaviour from the last 10 days. We compare model performance in a wide range of hyperparameter settings representing various scenarios. We further show that the neural network can be used to produce personalised recommendations of what sleep habits users should follow to maximise sleep quality, and show that these recommendations are substantially better than the ones generated by standard methods. We finally show that the neural network can explain the recommendation given to each participant and calculate confidence intervals for each prediction, all of which are essential for clinicians to be able to adopt such a tool in clinical practice.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2143595986,
        "newsscientist":0.2572722749,
        "technologyreview":0.3232593099,
        "venturebeat":0.3148876242,
        "wired":0.2724662824,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00033v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659119345000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00496v1",
        "predicted_newsworthiness":0.6068030131,
        "title":"Wigglite: Low-cost Information Collection and Triage",
        "summary":"Consumers conducting comparison shopping, researchers making sense of competitive space, and developers looking for code snippets online all face the challenge of capturing the information they find for later use without interrupting their current flow. In addition, during many learning and exploration tasks, people need to externalize their mental context, such as estimating how urgent a topic is to follow up on, or rating a piece of evidence as a \"pro\" or \"con,\" which helps scaffold subsequent deeper exploration. However, current approaches incur a high cost, often requiring users to select, copy, context switch, paste, and annotate information in a separate document without offering specific affordances that capture their mental context. In this work, we explore a new interaction technique called \"wiggling,\" which can be used to fluidly collect, organize, and rate information during early sensemaking stages with a single gesture. Wiggling involves rapid back-and-forth movements of a pointer or up-and-down scrolling on a smartphone, which can indicate the information to be collected and its valence, using a single, light-weight gesture that does not interfere with other interactions that are already available. Through implementation and user evaluation, we found that wiggling helped participants accurately collect information and encode their mental context with a 58% reduction in operational cost while being 24% faster compared to a common baseline.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1964544102,
        "newsscientist":0.2385445399,
        "technologyreview":0.3040505829,
        "venturebeat":0.312840807,
        "wired":0.3260079223,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00496v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1659295439000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.14690v1",
        "predicted_newsworthiness":0.6067616132,
        "title":"Renting Edge Computing Resources for Service Hosting",
        "summary":"We consider the setting where a service is hosted on a third-party edge server deployed close to the users and a cloud server at a greater distance from the users. Due to the proximity of the edge servers to the users, requests can be served at the edge with low latency. However, as the computation resources at the edge are limited, some requests must be routed to the cloud for service and incur high latency. The system's overall performance depends on the rent cost incurred to use the edge server, the latency experienced by the users, and the cost incurred to change the amount of edge computation resources rented over time. The algorithmic challenge is to determine the amount of edge computation power to rent over time. We propose a deterministic online policy and characterize its performance for adversarial and stochastic i.i.d. request arrival processes. We also characterize a fundamental bound on the performance of any deterministic online policy. Further, we compare the performance of our policy with suitably modified versions of existing policies to conclude that our policy is robust to temporal changes in the intensity of request arrivals.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1624461451,
        "newsscientist":0.1888848714,
        "technologyreview":0.3075147891,
        "venturebeat":0.3475661612,
        "wired":0.284968784,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14690v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1659103217000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.12863v1",
        "predicted_newsworthiness":0.605905741,
        "title":"FRIB: Low-poisoning Rate Invisible Backdoor Attack based on Feature Repair",
        "summary":"During the generation of invisible backdoor attack poisoned data, the feature space transformation operation tends to cause the loss of some poisoned features and weakens the mapping relationship between source images with triggers and target labels, resulting in the need for a higher poisoning rate to achieve the corresponding backdoor attack success rate. To solve the above problems, we propose the idea of feature repair for the first time and introduce the blind watermark technique to repair the poisoned features lost during the generation of poisoned data. Under the premise of ensuring consistent labeling, we propose a low-poisoning rate invisible backdoor attack based on feature repair, named FRIB. Benefiting from the above design concept, the new method enhances the mapping relationship between the source images with triggers and the target labels, and increases the degree of misleading DNNs, thus achieving a high backdoor attack success rate with a very low poisoning rate. Ultimately, the detailed experimental results show that the goal of achieving a high success rate of backdoor attacks with a very low poisoning rate is achieved on all MNIST, CIFAR10, GTSRB, and ImageNet datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1231367408,
        "newsscientist":0.1883778522,
        "technologyreview":0.2845657532,
        "venturebeat":0.2381790069,
        "wired":0.2336900552,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12863v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658839737000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00824v1",
        "predicted_newsworthiness":0.6055895584,
        "title":"Safe Perception -- A Hierarchical Monitor Approach",
        "summary":"Our transportation world is rapidly transforming induced by an ever increasing level of autonomy. However, to obtain license of fully automated vehicles for widespread public use, it is necessary to assure safety of the entire system, which is still a challenge. This holds in particular for AI-based perception systems that have to handle a diversity of environmental conditions and road users, and at the same time should robustly detect all safety relevant objects (i.e no detection misses should occur). Yet, limited training and validation data make a proof of fault-free operation hardly achievable, as the perception system might be exposed to new, yet unknown objects or conditions on public roads. Hence, new safety approaches for AI-based perception systems are required. For this reason we propose in this paper a novel hierarchical monitoring approach that is able to validate the object list from a primary perception system, can reliably detect detection misses, and at the same time has a very low false alarm rate.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1321895354,
        "newsscientist":0.1985251529,
        "technologyreview":0.314154752,
        "venturebeat":0.2808762054,
        "wired":0.2373785715,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00824v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659359364000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01714v1",
        "predicted_newsworthiness":0.6047820737,
        "title":"An Open-Source Cultural Consensus Approach to Name-Based Gender Classification",
        "summary":"Name-based gender classification has enabled hundreds of otherwise infeasible scientific studies of gender. Yet, the lack of standardization, proliferation of ad hoc methods, reliance on paid services, understudied limitations, and conceptual debates cast a shadow over many applications. To address these problems we develop and evaluate an ensemble-based open-source method built on publicly available data of empirical name-gender associations. Our method integrates 36 distinct sources-spanning over 150 countries and more than a century-via a meta-learning algorithm inspired by Cultural Consensus Theory (CCT). We also construct a taxonomy with which names themselves can be classified. We find that our method's performance is competitive with paid services and that our method, and others, approach the upper limits of performance; we show that conditioning estimates on additional metadata (e.g. cultural context), further combining methods, or collecting additional name-gender association data is unlikely to meaningfully improve performance. This work definitively shows that name-based gender classification can be a reliable part of scientific research and provides a pair of tools, a classification method and a taxonomy of names, that realize this potential.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.264163919,
        "newsscientist":0.2314242762,
        "technologyreview":0.2911121585,
        "venturebeat":0.2550183604,
        "wired":0.2542435362,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01714v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1659470132000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2208.00913v1",
        "predicted_newsworthiness":0.6030387137,
        "title":"Effective Gesture Based Framework for Capturing User Input",
        "summary":"Computers today aren't just confined to laptops and desktops. Mobile gadgets like mobile phones and laptops also make use of it. However, one input device that hasn't changed in the last 50 years is the QWERTY keyboard. Users of virtual keyboards can type on any surface as if it were a keyboard thanks to sensor technology and artificial intelligence. In this research, we use the idea of image processing to create an application for seeing a computer keyboard using a novel framework which can detect hand gestures with precise accuracy while also being sustainable and financially viable. A camera is used to capture keyboard images and finger movements which subsequently acts as a virtual keyboard. In addition, a visible virtual mouse that accepts finger coordinates as input is also described in this study. This system has a direct benefit of reducing peripheral cost, reducing electronics waste generated due to external devices and providing accessibility to people who cannot use the traditional keyboard and mouse.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1419232839,
        "newsscientist":0.215694402,
        "technologyreview":0.2780867603,
        "venturebeat":0.2839675693,
        "wired":0.2792969329,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00913v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ai"
        ],
        "published":1659365897000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.00565v1",
        "predicted_newsworthiness":0.6028664419,
        "title":"Modeling Human Response to Robot Errors for Timely Error Detection",
        "summary":"In human-robot collaboration, robot errors are inevitable -- damaging user trust, willingness to work together, and task performance. Prior work has shown that people naturally respond to robot errors socially and that in social interactions it is possible to use human responses to detect errors. However, there is little exploration in the domain of non-social, physical human-robot collaboration such as assembly and tool retrieval. In this work, we investigate how people's organic, social responses to robot errors may be used to enable timely automatic detection of errors in physical human-robot interactions. We conducted a data collection study to obtain facial responses to train a real-time detection algorithm and a case study to explore the generalizability of our method with different task settings and errors. Our results show that natural social responses are effective signals for timely detection and localization of robot errors even in non-social contexts and that our method is robust across a variety of task contexts, robot errors, and user responses. This work contributes to robust error detection without detailed task specifications.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1607751771,
        "newsscientist":0.2337208123,
        "technologyreview":0.3418795494,
        "venturebeat":0.3022640648,
        "wired":0.2658101914,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00565v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659318931000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.10733v2",
        "predicted_newsworthiness":0.6015565061,
        "title":"GreenDB -- A Dataset and Benchmark for Extraction of Sustainability Information of Consumer Goods",
        "summary":"The production, shipping, usage, and disposal of consumer goods have a substantial impact on greenhouse gas emissions and the depletion of resources. Machine Learning (ML) can help to foster sustainable consumption patterns by accounting for sustainability aspects in product search or recommendations of modern retail platforms. However, the lack of large high quality publicly available product data with trustworthy sustainability information impedes the development of ML technology that can help to reach our sustainability goals. Here we present GreenDB, a database that collects products from European online shops on a weekly basis. As proxy for the products' sustainability, it relies on sustainability labels, which are evaluated by experts. The GreenDB schema extends the well-known schema.org Product definition and can be readily integrated into existing product catalogs. We present initial results demonstrating that ML models trained with our data can reliably (F1 score 96%) predict the sustainability label of products. These contributions can help to complement existing e-commerce experiences and ultimately encourage users to more sustainable consumption patterns.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2458293826,
        "newsscientist":0.2569610897,
        "technologyreview":0.3049504562,
        "venturebeat":0.318686359,
        "wired":0.2560869078,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.10733v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cy"
        ],
        "published":1658433582000,
        "published_hr":"Jul 21, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12850v1",
        "predicted_newsworthiness":0.6004034687,
        "title":"Towards Smart City Security: Violence and Weaponized Violence Detection using DCNN",
        "summary":"In this ever connected society, CCTVs have had a pivotal role in enforcing safety and security of the citizens by recording unlawful activities for the authorities to take actions. In a smart city context, using Deep Convolutional Neural Networks (DCNN) to detection violence and weaponized violence from CCTV videos will provide an additional layer of security by ensuring real-time detection around the clock. In this work, we introduced a new specialised dataset by gathering real CCTV footage of both weaponized and non-weaponized violence as well as non-violence videos from YouTube. We also proposed a novel approach in merging consecutive video frames into a single salient image which will then be the input to the DCNN. Results from multiple DCNN architectures have proven the effectiveness of our method by having the highest accuracy of 99\\%. We also take into consideration the efficiency of our methods through several parameter trade-offs to ensure smart city sustainability.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2056860105,
        "newsscientist":0.197733954,
        "technologyreview":0.3150183008,
        "venturebeat":0.2748524357,
        "wired":0.262664892,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12850v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658838661000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11774v1",
        "predicted_newsworthiness":0.5975414597,
        "title":"Towards a Sentiment-Aware Conversational Agent",
        "summary":"In this paper, we propose an end-to-end sentiment-aware conversational agent based on two models: a reply sentiment prediction model, which leverages the context of the dialogue to predict an appropriate sentiment for the agent to express in its reply; and a text generation model, which is conditioned on the predicted sentiment and the context of the dialogue, to produce a reply that is both context and sentiment appropriate. Additionally, we propose to use a sentiment classification model to evaluate the sentiment expressed by the agent during the development of the model. This allows us to evaluate the agent in an automatic way. Both automatic and human evaluation results show that explicitly guiding the text generation model with a pre-defined set of sentences leads to clear improvements, both regarding the expressed sentiment and the quality of the generated text.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1446073429,
        "newsscientist":0.1681107051,
        "technologyreview":0.289464755,
        "venturebeat":0.306238556,
        "wired":0.2274729189,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11774v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658681984000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14388v1",
        "predicted_newsworthiness":0.5974673005,
        "title":"Data Integrity Verification in Network Slicing using Oracles and Smart Contracts",
        "summary":"The fifth-generation (5G) wireless networks are expected to provide various services compared to the 4G and previous generations of networks. The Quality of Service requirements can be quite different in terms of latency, bandwidth, reliability, and availability. 5G technology allows the fragmentation of the network into small pieces, known as network slices. This network slicing is done by specific tools and the configuration must be protected from attacks that may be performed by malicious users. Thus in this paper, a solution to protect and prevent these failures from happening is addressed. For this solution to be carried out, a study was conducted on the Blockchain technology, as well as the use of Oracles in order to implement an integrity verification system, a system capable of assuring 5G network slices configuration integrity through a complete architecture involving Blockchain, Smart Contracts and Oracles.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1446568838,
        "newsscientist":0.1178001307,
        "technologyreview":0.2415191506,
        "venturebeat":0.2515291441,
        "wired":0.1979635043,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14388v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1659044926000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2208.00508v1",
        "predicted_newsworthiness":0.5973461984,
        "title":"Deep Active Learning with Budget Annotation",
        "summary":"Digital data collected over the decades and data currently being produced with use of information technology is vastly the unlabeled data or data without description. The unlabeled data is relatively easy to acquire but expensive to label even with use of domain experts. Most of the recent works focus on use of active learning with uncertainty metrics measure to address this problem. Although most uncertainty selection strategies are very effective, they fail to take informativeness of the unlabeled instances into account and are prone to querying outliers. In order to address these challenges we propose an hybrid approach of computing both the uncertainty and informativeness of an instance, then automaticaly label the computed instances using budget annotator. To reduce the annotation cost, we employ the state-of-the-art pre-trained models in order to avoid querying information already contained in those models. Our extensive experiments on different sets of datasets demonstrate the efficacy of the proposed approach.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1191720853,
        "newsscientist":0.1637060408,
        "technologyreview":0.282991263,
        "venturebeat":0.2849863817,
        "wired":0.1980208121,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00508v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cv"
        ],
        "published":1659298844000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12497v1",
        "predicted_newsworthiness":0.5964225606,
        "title":"Estimating and Controlling for Fairness via Sensitive Attribute Predictors",
        "summary":"Although machine learning classifiers have been increasingly used in high-stakes decision making (e.g., cancer diagnosis, criminal prosecution decisions), they have demonstrated biases against underrepresented groups. Standard definitions of fairness require access to sensitive attributes of interest (e.g., gender and race), which are often unavailable. In this work we demonstrate that in these settings where sensitive attributes are unknown, one can still reliably estimate and ultimately control for fairness by using proxy sensitive attributes derived from a sensitive attribute predictor. Specifically, we first show that with just a little knowledge of the complete data distribution, one may use a sensitive attribute predictor to obtain upper and lower bounds of the classifier's true fairness metric. Second, we demonstrate how one can provably control for fairness with respect to the true sensitive attributes by controlling for fairness with respect to the proxy sensitive attributes. Our results hold under assumptions that are significantly milder than previous works. We illustrate our results on a series of synthetic and real datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2240253472,
        "newsscientist":0.1975021406,
        "technologyreview":0.3082665465,
        "venturebeat":0.2731400945,
        "wired":0.2420186802,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12497v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cy"
        ],
        "published":1658778962000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11623v1",
        "predicted_newsworthiness":0.5937153887,
        "title":"A Simplistic and Cost-Effective Design for Real-World Development of an Ambient Assisted Living System for Fall Detection and Indoor Localization: Proof of Concept",
        "summary":"Falls, highly common in the constantly increasing global aging population, can have a variety of negative effects on their health, well-being, and quality of life, including restricting their capabilities to conduct Activities of Daily Living (ADLs), which are crucial for one's sustenance. Timely assistance during falls is highly necessary, which involves tracking the indoor location of the elderly during their diverse navigational patterns associated with ADLs to detect the precise location of a fall. With the decreasing caregiver population on a global scale, it is important that the future of intelligent living environments can detect falls during ADLs while being able to track the indoor location of the elderly in the real world. To address these challenges, this work proposes a cost-effective and simplistic design paradigm for an Ambient Assisted Living system that can capture multimodal components of user behaviors during ADLs that are necessary for performing fall detection and indoor localization in a simultaneous manner in the real world. Proof of concept results from real-world experiments are presented to uphold the effective working of the system. The findings from two comparison studies with prior works in this field are also presented to uphold the novelty of this work. The first comparison study shows how the proposed system outperforms prior works in the areas of indoor localization and fall detection in terms of the effectiveness of its software design and hardware design. The second comparison study shows that the cost for the development of this system is the least as compared to prior works in these fields, which involved real-world development of the underlining systems, thereby upholding its cost-effective nature.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1661219334,
        "newsscientist":0.2017007767,
        "technologyreview":0.2688229607,
        "venturebeat":0.2783546896,
        "wired":0.256433805,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11623v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ai",
            "cs.cv",
            "cs.lg"
        ],
        "published":1658621612000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.00953v1",
        "predicted_newsworthiness":0.5934128632,
        "title":"What do Deep Neural Networks Learn in Medical Images?",
        "summary":"Deep learning is increasingly gaining rapid adoption in healthcare to help improve patient outcomes. This is more so in medical image analysis which requires extensive training to gain the requisite expertise to become a trusted practitioner. However, while deep learning techniques have continued to provide state-of-the-art predictive performance, one of the primary challenges that stands to hinder this progress in healthcare is the opaque nature of the inference mechanism of these models. So, attribution has a vital role in building confidence in stakeholders for the predictions made by deep learning models to inform clinical decisions. This work seeks to answer the question: what do deep neural network models learn in medical images? In that light, we present a novel attribution framework using adaptive path-based gradient integration techniques. Results show a promising direction of building trust in domain experts to improve healthcare outcomes by allowing them to understand the input-prediction correlative structures, discover new bio-markers, and reveal potential model biases.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1876649119,
        "newsscientist":0.2336633841,
        "technologyreview":0.3660774932,
        "venturebeat":0.3247031814,
        "wired":0.2462888929,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00953v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659369914000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01099v1",
        "predicted_newsworthiness":0.592880969,
        "title":"Parsimonious Argument Annotations for Hate Speech Counter-narratives",
        "summary":"We present an enrichment of the Hateval corpus of hate speech tweets (Basile et. al 2019) aimed to facilitate automated counter-narrative generation. Comparably to previous work (Chung et. al. 2019), manually written counter-narratives are associated to tweets. However, this information alone seems insufficient to obtain satisfactory language models for counter-narrative generation. That is why we have also annotated tweets with argumentative information based on Wagemanns (2016), that we believe can help in building convincing and effective counter-narratives for hate speech against particular groups. We discuss adequacies and difficulties of this annotation process and present several baselines for automatic detection of the annotated elements. Preliminary results show that automatic annotators perform close to human annotators to detect some aspects of argumentation, while others only reach low or moderate level of inter-annotator agreement.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2504424789,
        "newsscientist":0.1831159708,
        "technologyreview":0.3155609052,
        "venturebeat":0.2597715799,
        "wired":0.3125025539,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01099v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659380312000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.01483v1",
        "predicted_newsworthiness":0.5927349226,
        "title":"Label Sleuth: From Unlabeled Text to a Classifier in a Few Hours",
        "summary":"Text classification can be useful in many real-world scenarios, saving a lot of time for end users. However, building a custom classifier typically requires coding skills and ML knowledge, which poses a significant barrier for many potential users. To lift this barrier, we introduce Label Sleuth, a free open source system for labeling and creating text classifiers. This system is unique for (a) being a no-code system, making NLP accessible to non-experts, (b) guiding users through the entire labeling process until they obtain a custom classifier, making the process efficient -- from cold start to classifier in a few hours, and (c) being open for configuration and extension by developers. By open sourcing Label Sleuth we hope to build a community of users and developers that will broaden the utilization of NLP models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1499730518,
        "newsscientist":0.185472672,
        "technologyreview":0.3079998955,
        "venturebeat":0.3255503459,
        "wired":0.2657030951,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01483v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.hc"
        ],
        "published":1659450703000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14355v1",
        "predicted_newsworthiness":0.5896754198,
        "title":"Multiple Attribute Fairness: Application to Fraud Detection",
        "summary":"We propose a fairness measure relaxing the equality conditions in the popular equal odds fairness regime for classification. We design an iterative, model-agnostic, grid-based heuristic that calibrates the outcomes per sensitive attribute value to conform to the measure. The heuristic is designed to handle high arity attribute values and performs a per attribute sanitization of outcomes across different protected attribute values. We also extend our heuristic for multiple attributes. Highlighting our motivating application, fraud detection, we show that the proposed heuristic is able to achieve fairness across multiple values of a single protected attribute, multiple protected attributes. When compared to current fairness techniques, that focus on two groups, we achieve comparable performance across several public data sets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1663546374,
        "newsscientist":0.1506001934,
        "technologyreview":0.2583538499,
        "venturebeat":0.2493049762,
        "wired":0.2063566349,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14355v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cy"
        ],
        "published":1659035985000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01103v1",
        "predicted_newsworthiness":0.5889485615,
        "title":"Safe and Efficient Exploration of Human Models During Human-Robot Interaction",
        "summary":"Many collaborative human-robot tasks require the robot to stay safe and work efficiently around humans. Since the robot can only stay safe with respect to its own model of the human, we want the robot to learn a good model of the human in order to act both safely and efficiently. This paper studies methods that enable a robot to safely explore the space of a human-robot system to improve the robot's model of the human, which will consequently allow the robot to access a larger state space and better work with the human. In particular, we introduce active exploration under the framework of energy-function based safe control, investigate the effect of different active exploration strategies, and finally analyze the effect of safe active exploration on both analytical and neural network human models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1151709931,
        "newsscientist":0.1764800853,
        "technologyreview":0.275015494,
        "venturebeat":0.2134338899,
        "wired":0.190918019,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01103v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659381058000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.00301v1",
        "predicted_newsworthiness":0.588201329,
        "title":"Towards Visualization of Time-Series Ecological Momentary Assessment (EMA) Data on Standalone Voice-First Virtual Assistants",
        "summary":"Population aging is an increasingly important consideration for health care in the 21th century, and continuing to have access and interact with digital health information is a key challenge for aging populations. Voice-based Intelligent Virtual Assistants (IVAs) are promising to improve the Quality of Life (QoL) of older adults, and coupled with Ecological Momentary Assessments (EMA) they can be effective to collect important health information from older adults, especially when it comes to repeated time-based events. However, this same EMA data is hard to access for the older adult: although the newest IVAs are equipped with a display, the effectiveness of visualizing time-series based EMA data on standalone IVAs has not been explored. To investigate the potential opportunities for visualizing time-series based EMA data on standalone IVAs, we designed a prototype system, where older adults are able to query and examine the time-series EMA data on Amazon Echo Show - a widely used commercially available standalone screen-based IVA. We conducted a preliminary semi-structured interview with a geriatrician and an older adult, and identified three findings that should be carefully considered when designing such visualizations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2377391899,
        "newsscientist":0.2583974141,
        "technologyreview":0.3361023905,
        "venturebeat":0.3428812562,
        "wired":0.3181354999,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00301v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.cy"
        ],
        "published":1659211395000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.00724v1",
        "predicted_newsworthiness":0.5859847897,
        "title":"Safe Policy Improvement Approaches and their Limitations",
        "summary":"Safe Policy Improvement (SPI) is an important technique for offline reinforcement learning in safety critical applications as it improves the behavior policy with a high probability. We classify various SPI approaches from the literature into two groups, based on how they utilize the uncertainty of state-action pairs. Focusing on the Soft-SPIBB (Safe Policy Improvement with Soft Baseline Bootstrapping) algorithms, we show that their claim of being provably safe does not hold. Based on this finding, we develop adaptations, the Adv-Soft-SPIBB algorithms, and show that they are provably safe. A heuristic adaptation, Lower-Approx-Soft-SPIBB, yields the best performance among all SPIBB algorithms in extensive experiments on two benchmarks. We also check the safety guarantees of the provably safe algorithms and show that huge amounts of data are necessary such that the safety bounds become useful in practice.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1200451694,
        "newsscientist":0.13982109,
        "technologyreview":0.2294574289,
        "venturebeat":0.2078918549,
        "wired":0.1745978055,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00724v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659348783000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12943v2",
        "predicted_newsworthiness":0.5857258439,
        "title":"Unique in what sense? Heterogeneous relationships between multiple types of uniqueness and popularity in music",
        "summary":"How does our society appreciate the uniqueness of cultural products? This fundamental puzzle has intrigued scholars in many fields, including psychology, sociology, anthropology, and marketing. It has been theorized that cultural products that balance familiarity and novelty are more likely to become popular. However, a cultural product's novelty is typically multifaceted. This paper uses songs as a case study to study the multiple facets of uniqueness and their relationship with success. We first unpack the multiple facets of a song's novelty or uniqueness and, next, measure its impact on a song's popularity. We employ a series of statistical models to study the relationship between a song's popularity and novelty associated with its lyrics, chord progressions, or audio properties. Our analyses performed on a dataset of over fifty thousand songs find a consistently negative association between all types of song novelty and popularity. Overall we found a song's lyrics uniqueness to have the most significant association with its popularity. However, audio uniqueness was the strongest predictor of a song's popularity, conditional on the song's genre. We further found the theme and repetitiveness of a song's lyrics to mediate the relationship between the song's popularity and novelty. Broadly, our results contradict the \"optimal distinctiveness theory\" (balance between novelty and familiarity) and call for an investigation into the multiple dimensions along which a cultural product's uniqueness could manifest.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2274462595,
        "newsscientist":0.1907814732,
        "technologyreview":0.2229907711,
        "venturebeat":0.2604363645,
        "wired":0.2645114727,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12943v2",
        "arxiv_primary_category":"cs.cy",
        "arxiv_all_categories":[
            "cs.cy"
        ],
        "published":1658846998000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computers and Society"
    },
    {
        "arxiv_id":"2207.12319v2",
        "predicted_newsworthiness":0.5846908507,
        "title":"OpenFilter: A Framework to Democratize Research Access to Social Media AR Filters",
        "summary":"Augmented Reality or AR filters on selfies have become very popular on social media platforms for a variety of applications, including marketing, entertainment and aesthetics. Given the wide adoption of AR face filters and the importance of faces in our social structures and relations, there is increased interest by the scientific community to analyze the impact of such filters from a psychological, artistic and sociological perspective. However, there are few quantitative analyses in this area mainly due to a lack of publicly available datasets of facial images with applied AR filters. The proprietary, close nature of most social media platforms does not allow users, scientists and practitioners to access the code and the details of the available AR face filters. Scraping faces from these platforms to collect data is ethically unacceptable and should, therefore, be avoided in research. In this paper, we present OpenFilter, a flexible framework to apply AR filters available in social media platforms on existing large collections of human faces. Moreover, we share FairBeauty and B-LFW, two beautified versions of the publicly available FairFace and LFW datasets and we outline insights derived from the analysis of these beautified datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2506021742,
        "newsscientist":0.2672009726,
        "technologyreview":0.3713647026,
        "venturebeat":0.3562708487,
        "wired":0.367514132,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12319v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658250325000,
        "published_hr":"Jul 19, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01366v1",
        "predicted_newsworthiness":0.5834891125,
        "title":"Detecting Individual Decision-Making Style: Exploring Behavioral Stylometry in Chess",
        "summary":"The advent of machine learning models that surpass human decision-making ability in complex domains has initiated a movement towards building AI systems that interact with humans. Many building blocks are essential for this activity, with a central one being the algorithmic characterization of human behavior. While much of the existing work focuses on aggregate human behavior, an important long-range goal is to develop behavioral models that specialize to individual people and can differentiate among them. To formalize this process, we study the problem of behavioral stylometry, in which the task is to identify a decision-maker from their decisions alone. We present a transformer-based approach to behavioral stylometry in the context of chess, where one attempts to identify the player who played a set of games. Our method operates in a few-shot classification framework, and can correctly identify a player from among thousands of candidate players with 98% accuracy given only 100 labeled games. Even when trained on amateur play, our method generalises to out-of-distribution samples of Grandmaster players, despite the dramatic differences between amateur and world-class players. Finally, we consider more broadly what our resulting embeddings reveal about human style in chess, as well as the potential ethical implications of powerful methods for identifying individuals from behavioral data.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1758649798,
        "newsscientist":0.2220722492,
        "technologyreview":0.3253884101,
        "venturebeat":0.2981005798,
        "wired":0.24363344,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01366v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.lg"
        ],
        "published":1659439096000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.12958v1",
        "predicted_newsworthiness":0.5824845815,
        "title":"From Interpretable Filters to Predictions of Convolutional Neural Networks with Explainable Artificial Intelligence",
        "summary":"Convolutional neural networks (CNN) are known for their excellent feature extraction capabilities to enable the learning of models from data, yet are used as black boxes. An interpretation of the convolutional filtres and associated features can help to establish an understanding of CNN to distinguish various classes. In this work, we focus on the explainability of a CNN model called as cnnexplain that is used for Covid-19 and non-Covid-19 classification with a focus on the interpretability of features by the convolutional filters, and how these features contribute to classification. Specifically, we have used various explainable artificial intelligence (XAI) methods, such as visualizations, SmoothGrad, Grad-CAM, and LIME to provide interpretation of convolutional filtres, and relevant features, and their role in classification. We have analyzed the explanation of these methods for Covid-19 detection using dry cough spectrograms. Explanation results obtained from the LIME, SmoothGrad, and Grad-CAM highlight important features of different spectrograms and their relevance to classification.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2164358101,
        "newsscientist":0.2462367896,
        "technologyreview":0.3308011065,
        "venturebeat":0.2930028271,
        "wired":0.2200277485,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12958v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658847925000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00085v1",
        "predicted_newsworthiness":0.5818427216,
        "title":"Machine Learning and Computer Vision Techniques in Bee Monitoring Applications",
        "summary":"Machine learning and computer vision are dynamically growing fields, which have proven to be able to solve very complex tasks. They could also be used for the monitoring of the honeybee colonies and for the inspection of their health state, which could identify potentially dangerous states before the situation is critical, or to better plan periodic bee colony inspections and therefore save significant costs. In this paper, we present an overview of the state-of-the-art computer vision and machine learning applications used for bee monitoring. We also demonstrate the potential of those methods as an example of an automated bee counter algorithm. The paper is aimed at veterinary and apidology professionals and experts, who might not be familiar with machine learning to introduce to them its possibilities, therefore each family of applications is opened by a brief theoretical introduction and motivation related to its base method. We hope that this paper will inspire other scientists to use the machine learning techniques for other applications in bee monitoring.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1559195681,
        "newsscientist":0.2307653817,
        "technologyreview":0.2932458172,
        "venturebeat":0.2690665232,
        "wired":0.223265959,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00085v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659131819000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01901v1",
        "predicted_newsworthiness":0.58167722,
        "title":"Asynchronous Federated Learning for Edge-assisted Vehicular Networks",
        "summary":"Vehicular networks enable vehicles support real-time vehicular applications through training data. Due to the limited computing capability, vehicles usually transmit data to a road side unit (RSU) at the network edge to process data. However, vehicles are usually reluctant to share data with each other due to the privacy issue. For the traditional federated learning (FL), vehicles train the data locally to obtain a local model and then upload the local model to the RSU to update the global model, thus the data privacy can be protected through sharing model parameters instead of data. The traditional FL updates the global model synchronously, i.e., the RSU needs to wait for all vehicles to upload their models for the global model updating. However, vehicles may usually drive out of the coverage of the RSU before they obtain their local models through training, which reduces the accuracy of the global model. It is necessary to propose an asynchronous federated learning (AFL) to solve this problem, where the RSU updates the global model once it receives a local model from a vehicle. However, the amount of data, computing capability and vehicle mobility may affect the accuracy of the global model. In this paper, we jointly consider the amount of data, computing capability and vehicle mobility to design an AFL scheme to improve the accuracy of the global model. Extensive simulation experiments have demonstrated that our scheme outperforms the FL scheme",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1177417816,
        "newsscientist":0.1359132424,
        "technologyreview":0.2616295667,
        "venturebeat":0.2686851282,
        "wired":0.2389197579,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01901v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ni",
            "cs.ro"
        ],
        "published":1659513902000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01211v1",
        "predicted_newsworthiness":0.5811409562,
        "title":"Gesture-aware Interactive Machine Teaching with In-situ Object Annotations",
        "summary":"Interactive Machine Teaching (IMT) systems allow non-experts to easily create Machine Learning (ML) models. However, existing vision-based IMT systems either ignore annotations on the objects of interest or require users to annotate in a post-hoc manner. Without the annotations on objects, the model may misinterpret the objects using unrelated features. Post-hoc annotations cause additional workload, which diminishes the usability of the overall model building process. In this paper, we develop LookHere, which integrates in-situ object annotations into vision-based IMT. LookHere exploits users' deictic gestures to segment the objects of interest in real time. This segmentation information can be additionally used for training. To achieve the reliable performance of this object segmentation, we utilize our custom dataset called HuTics, including 2040 front-facing images of deictic gestures toward various objects by 170 people. The quantitative results of our user study showed that participants were 16.3 times faster in creating a model with our system compared to a standard IMT system with a post-hoc annotation process while demonstrating comparable accuracies. Additionally, models created by our system showed a significant accuracy improvement ($\\Delta mIoU=0.466$) in segmenting the objects of interest compared to those without annotations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.129929023,
        "newsscientist":0.1996679028,
        "technologyreview":0.3112068637,
        "venturebeat":0.2962655206,
        "wired":0.2628251254,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01211v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1659407015000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.11609v1",
        "predicted_newsworthiness":0.5798205283,
        "title":"Exploring the Impact of Temporal Bias in Point-of-Interest Recommendation",
        "summary":"Recommending appropriate travel destinations to consumers based on contextual information such as their check-in time and location is a primary objective of Point-of-Interest (POI) recommender systems. However, the issue of contextual bias (i.e., how much consumers prefer one situation over another) has received little attention from the research community. This paper examines the effect of temporal bias, defined as the difference between users' check-in hours, leisure vs.~work hours, on the consumer-side fairness of context-aware recommendation algorithms. We believe that eliminating this type of temporal (and geographical) bias might contribute to a drop in traffic-related air pollution, noting that rush-hour traffic may be more congested. To surface effective POI recommendations, we evaluated the sensitivity of state-of-the-art context-aware models to the temporal bias contained in users' check-in activities on two POI datasets, namely Gowalla and Yelp. The findings show that the examined context-aware recommendation models prefer one group of users over another based on the time of check-in and that this preference persists even when users have the same amount of interactions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.194053263,
        "newsscientist":0.1754905219,
        "technologyreview":0.2579307449,
        "venturebeat":0.2836481412,
        "wired":0.2521613465,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11609v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1658611519000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.12574v1",
        "predicted_newsworthiness":0.5773601186,
        "title":"Enabling a Cooperative Driver Messenger System for Lane Change Assistance Application",
        "summary":"Sensor data and Vehicle-to-Everything (V2X) communication can greatly assist Connected and Autonomous Vehicles (CAVs) in situational awareness and provide a safer driving experience. While sensor data recorded from devices such as radar and camera can assist in local awareness in the close vicinity of the Host Vehicle (HV), the information obtained is useful solely for the HV itself. On the other hand, V2X communication can allow CAVs to communicate with each other and transceive basic and\/or advanced safety information, allowing each CAV to create a sophisticated local object map for situational awareness. This paper introduces a point-to-point Driver Messenger System (DMS) that regularly maintains a local object map of the HV and uses it to convey HV's Over-the-Air (OTA) Driver Intent Messages (DIMs) to nearby identified Target Vehicle(s) (TV(s)) based on a list of pre-defined common traffic applications. The focus of this paper is on the lane change application where DMS can use the local object map to automatically identify closest TV in adjacent lane in the direction of HV's intended lane change and inform the TV via a DIM. Within DMS, the paper proposes a TV recognition algorithm for lane change application that utilizes the HV's Path History (PH) to accurately determine the closest TV that could potentially benefit from receiving a DIM from HV. Finally, DMS is also shown to act as an advanced warning system by providing extra time and space headway measurements between the HV and TVs upon a number of simulated lane change scenarios.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1075626949,
        "newsscientist":0.1529995787,
        "technologyreview":0.2453977373,
        "venturebeat":0.2438772355,
        "wired":0.2258340174,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12574v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658792737000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2208.01230v1",
        "predicted_newsworthiness":0.5770929992,
        "title":"A Multifaceted Benchmarking of Synthetic Electronic Health Record Generation Models",
        "summary":"Synthetic health data have the potential to mitigate privacy concerns when sharing data to support biomedical research and the development of innovative healthcare applications. Modern approaches for data generation based on machine learning, generative adversarial networks (GAN) methods in particular, continue to evolve and demonstrate remarkable potential. Yet there is a lack of a systematic assessment framework to benchmark methods as they emerge and determine which methods are most appropriate for which use cases. In this work, we introduce a generalizable benchmarking framework to appraise key characteristics of synthetic health data with respect to utility and privacy metrics. We apply the framework to evaluate synthetic data generation methods for electronic health records (EHRs) data from two large academic medical centers with respect to several use cases. The results illustrate that there is a utility-privacy tradeoff for sharing synthetic EHR data. The results further indicate that no method is unequivocally the best on all criteria in each use case, which makes it evident why synthetic data generation methods need to be assessed in context.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1848411792,
        "newsscientist":0.2290353566,
        "technologyreview":0.3386027669,
        "venturebeat":0.3146391739,
        "wired":0.2584440518,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01230v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cy"
        ],
        "published":1659411885000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12283v1",
        "predicted_newsworthiness":0.5768518273,
        "title":"MedML: Fusing Medical Knowledge and Machine Learning Models for Early Pediatric COVID-19 Hospitalization and Severity Prediction",
        "summary":"The COVID-19 pandemic has caused devastating economic and social disruption, straining the resources of healthcare institutions worldwide. This has led to a nationwide call for models to predict hospitalization and severe illness in patients with COVID-19 to inform distribution of limited healthcare resources. We respond to one of these calls specific to the pediatric population. To address this challenge, we study two prediction tasks for the pediatric population using electronic health records: 1) predicting which children are more likely to be hospitalized, and 2) among hospitalized children, which individuals are more likely to develop severe symptoms. We respond to the national Pediatric COVID-19 data challenge with a novel machine learning model, MedML. MedML extracts the most predictive features based on medical knowledge and propensity scores from over 6 million medical concepts and incorporates the inter-feature relationships between heterogeneous medical features via graph neural networks (GNN). We evaluate MedML across 143,605 patients for the hospitalization prediction task and 11,465 patients for the severity prediction task using data from the National Cohort Collaborative (N3C) dataset. We also report detailed group-level and individual-level feature importance analyses to evaluate the model interpretability. MedML achieves up to a 7% higher AUROC score and up to a 14% higher AUPRC score compared to the best baseline machine learning models and performs well across all nine national geographic regions and over all three-month spans since the start of the pandemic. Our cross-disciplinary research team has developed a method of incorporating clinical domain knowledge as the framework for a new type of machine learning model that is more predictive and explainable than current state-of-the-art data-driven feature selection methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2855330402,
        "newsscientist":0.2747580262,
        "technologyreview":0.3391821469,
        "venturebeat":0.3031228672,
        "wired":0.2402361998,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12283v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658764574000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01721v1",
        "predicted_newsworthiness":0.5762970463,
        "title":"Rumor Stance Classification in Online Social Networks: A Survey on the State-of-the-Art, Prospects, and Future Challenges",
        "summary":"The emergence of the Internet as a ubiquitous technology has facilitated the rapid evolution of social media as the leading virtual platform for communication, content sharing, and information dissemination. In spite of revolutionizing the way news used to be delivered to people, this technology has also brought along with itself inevitable demerits. One such drawback is the spread of rumors facilitated by social media platforms which may provoke doubt and fear upon people. Therefore, the need to debunk rumors before their wide spread has become essential all the more. Over the years, many studies have been conducted to develop effective rumor verification systems. One aspect of such studies focuses on rumor stance classification, which concerns the task of utilizing users' viewpoints about a rumorous post to better predict the veracity of a rumor. Relying on users' stances in rumor verification task has gained great importance, for it has shown significant improvements in the model performances. In this paper, we conduct a comprehensive literature review on rumor stance classification in complex social networks. In particular, we present a thorough description of the approaches and mark the top performances. Moreover, we introduce multiple datasets available for this purpose and highlight their limitations. Finally, some challenges and future directions are discussed to stimulate further relevant research efforts.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1807007154,
        "newsscientist":0.1473778766,
        "technologyreview":0.2735242204,
        "venturebeat":0.2287610352,
        "wired":0.2756509532,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01721v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.ni"
        ],
        "published":1659470869000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.13596v1",
        "predicted_newsworthiness":0.5759903258,
        "title":"Fairness and Randomness in Machine Learning: Statistical Independence and Relativization",
        "summary":"Fair Machine Learning endeavors to prevent unfairness arising in the context of machine learning applications embedded in society. Despite the variety of definitions of fairness and proposed \"fair algorithms\", there remain unresolved conceptual problems regarding fairness. In this paper, we argue that randomness and fairness can be considered equivalent concepts in machine learning. We obtain a relativized notion of randomness expressed as statistical independence by appealing to Von Mises' century-old foundations for probability. Via fairness notions in machine learning, which are expressed as statistical independence as well, we then link the ante randomness assumptions about the data to the ex post requirements for fair predictions. This connection proves fruitful: we use it to argue that randomness and fairness are essentially relative and that randomness should reflect its nature as a modeling assumption in machine learning.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1995620497,
        "newsscientist":0.2138320223,
        "technologyreview":0.3318361545,
        "venturebeat":0.2807895334,
        "wired":0.2508386032,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13596v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cy"
        ],
        "published":1658937305000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13500v1",
        "predicted_newsworthiness":0.5757403526,
        "title":"Modelling Social Context for Fake News Detection: A Graph Neural Network Based Approach",
        "summary":"Detection of fake news is crucial to ensure the authenticity of information and maintain the news ecosystems reliability. Recently, there has been an increase in fake news content due to the recent proliferation of social media and fake content generation techniques such as Deep Fake. The majority of the existing modalities of fake news detection focus on content based approaches. However, most of these techniques fail to deal with ultra realistic synthesized media produced by generative models. Our recent studies find that the propagation characteristics of authentic and fake news are distinguishable, irrespective of their modalities. In this regard, we have investigated the auxiliary information based on social context to detect fake news. This paper has analyzed the social context of fake news detection with a hybrid graph neural network based approach. This hybrid model is based on integrating a graph neural network on the propagation of news and bi directional encoder representations from the transformers model on news content to learn the text features. Thus this proposed approach learns the content as well as the context features and hence able to outperform the baseline models with an f1 score of 0.91 on PolitiFact and 0.93 on the Gossipcop dataset, respectively",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1779667335,
        "newsscientist":0.1664850936,
        "technologyreview":0.3155684852,
        "venturebeat":0.2518061833,
        "wired":0.2731233693,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13500v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.cl",
            "cs.ir"
        ],
        "published":1658926713000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2208.01890v1",
        "predicted_newsworthiness":0.5749370546,
        "title":"High stable and accurate vehicle selection scheme based on federated edge learning in vehicular networks",
        "summary":"Federated edge learning (FEEL) technology for vehicular networks is considered as a promising technology to reduce the computation workload while keep the privacy of users. In the FEEL system, vehicles upload data to the edge servers, which train the vehicles' data to update local models and then return the result to vehicles to avoid sharing the original data. However, the cache queue in the edge is limited and the channel between edge server and each vehicle is a time varying wireless channel, which makes a challenge to select a suitable number of vehicles to upload data to keep a stable cache queue in edge server and maximize the learning accuracy. Moreover, selecting vehicles with different resource statuses to update data will affect the total amount of data involved in training, which further affects the model accuracy. In this paper, we propose a vehicle selection scheme, which maximizes the learning accuracy while ensuring the stability of the cache queue, where the statuses of all the vehicles in the coverage of edge server are taken into account. The performance of this scheme is evaluated through simulation experiments, which indicates that our proposed scheme can perform better than the known benchmark scheme.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.106740813,
        "newsscientist":0.1418873414,
        "technologyreview":0.2585772897,
        "venturebeat":0.2679240627,
        "wired":0.2408274963,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01890v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1659512377000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.12310v1",
        "predicted_newsworthiness":0.5743936543,
        "title":"Estimaci\u00f3n de \u00e1reas de cultivo mediante Deep Learning y programaci\u00f3n convencional",
        "summary":"Artificial Intelligence has enabled the implementation of more accurate and efficient solutions to problems in various areas. In the agricultural sector, one of the main needs is to know at all times the extent of land occupied or not by crops in order to improve production and profitability. The traditional methods of calculation demand the collection of data manually and in person in the field, causing high labor costs, execution times, and inaccuracy in the results. The present work proposes a new method based on Deep Learning techniques complemented with conventional programming for the determination of the area of populated and unpopulated crop areas. We have considered as a case study one of the most recognized companies in the planting and harvesting of sugar cane in Ecuador. The strategy combines a Generative Adversarial Neural Network (GAN) that is trained on a dataset of aerial photographs of natural and urban landscapes to improve image resolution; a Convolutional Neural Network (CNN) trained on a dataset of aerial photographs of sugar cane plots to distinguish populated or unpopulated crop areas; and a standard image processing module for the calculation of areas in a percentage manner. The experiments performed demonstrate a significant improvement in the quality of the aerial photographs as well as a remarkable differentiation between populated and unpopulated crop areas, consequently, a more accurate result of cultivated and uncultivated areas. The proposed method can be extended to the detection of possible pests, areas of weed vegetation, dynamic crop development, and both qualitative and quantitative quality control.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1767280157,
        "newsscientist":0.2094851058,
        "technologyreview":0.3000365382,
        "venturebeat":0.2559352802,
        "wired":0.1991353277,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12310v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658766175000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12052v1",
        "predicted_newsworthiness":0.5738538747,
        "title":"Designing an AI-Driven Talent Intelligence Solution: Exploring Big Data to extend the TOE Framework",
        "summary":"AI has the potential to improve approaches to talent management enabling dynamic provisions through implementing advanced automation. This study aims to identify the new requirements for developing AI-oriented artifacts to address talent management issues. Focusing on enhancing interactions between professional assessment and planning attributes, the design artifact is an intelligent employment automation solution for career guidance that is largely dependent on a talent intelligent module and an individuals growth needs. A design science method is adopted for conducting the experimental study with structured machine learning techniques which is the primary element of a comprehensive AI solution framework informed through a proposed moderation of the technology-organization-environment theory.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1988549684,
        "newsscientist":0.2021595418,
        "technologyreview":0.3577264057,
        "venturebeat":0.3476413625,
        "wired":0.2668891036,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12052v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1658745770000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.14145v1",
        "predicted_newsworthiness":0.5738049586,
        "title":"A Probabilistic Framework for Estimating the Risk of Pedestrian-Vehicle Conflicts at Intersections",
        "summary":"Pedestrian safety has become an important research topic among various studies due to the increased number of pedestrian-involved crashes. To evaluate pedestrian safety proactively, surrogate safety measures (SSMs) have been widely used in traffic conflict-based studies as they do not require historical crashes as inputs. However, most existing SSMs were developed based on the assumption that road users would maintain constant velocity and direction. Risk estimations based on this assumption are less unstable, more likely to be exaggerated, and unable to capture the evasive maneuvers of drivers. Considering the limitations among existing SSMs, this study proposes a probabilistic framework for estimating the risk of pedestrian-vehicle conflicts at intersections. The proposed framework loosen restrictions of constant speed by predicting trajectories using a Gaussian Process Regression and accounts for the different possible driver maneuvers with a Random Forest model. Real-world LiDAR data collected at an intersection was used to evaluate the performance of the proposed framework. The newly developed framework is able to identify all pedestrian-vehicle conflicts. Compared to the Time-to-Collision, the proposed framework provides a more stable risk estimation and captures the evasive maneuvers of vehicles. Moreover, the proposed framework does not require expensive computation resources, which makes it an ideal choice for real-time proactive pedestrian safety solutions at intersections.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1492288764,
        "newsscientist":0.1561689441,
        "technologyreview":0.2208128547,
        "venturebeat":0.1930273856,
        "wired":0.2027872132,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14145v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659020921000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11776v1",
        "predicted_newsworthiness":0.5730857232,
        "title":"Incorporating Heterogeneous User Behaviors and Social Influences for Predictive Analysis",
        "summary":"Behavior prediction based on historical behavioral data have practical real-world significance. It has been applied in recommendation, predicting academic performance, etc. With the refinement of user data description, the development of new functions, and the fusion of multiple data sources, heterogeneous behavioral data which contain multiple types of behaviors become more and more common. In this paper, we aim to incorporate heterogeneous user behaviors and social influences for behavior predictions. To this end, this paper proposes a variant of Long-Short Term Memory (LSTM) which can consider context information while modeling a behavior sequence, a projection mechanism which can model multi-faceted relationships among different types of behaviors, and a multi-faceted attention mechanism which can dynamically find out informative periods from different facets. Many kinds of behavioral data belong to spatio-temporal data. An unsupervised way to construct a social behavior graph based on spatio-temporal data and to model social influences is proposed. Moreover, a residual learning-based decoder is designed to automatically construct multiple high-order cross features based on social behavior representation and other types of behavior representations. Qualitative and quantitative experiments on real-world datasets have demonstrated the effectiveness of this model.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1687300913,
        "newsscientist":0.1953483795,
        "technologyreview":0.2739036613,
        "venturebeat":0.2760230858,
        "wired":0.2374062738,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11776v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658682337000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12105v1",
        "predicted_newsworthiness":0.5722125133,
        "title":"Ego-graph Replay based Continual Learning for Misinformation Engagement Prediction",
        "summary":"Online social network platforms have a problem with misinformation. One popular way of addressing this problem is via the use of machine learning based automated misinformation detection systems to classify if a post is misinformation. Instead of post hoc detection, we propose to predict if a user will engage with misinformation in advance and design an effective graph neural network classifier based on ego-graphs for this task. However, social networks are highly dynamic, reflecting continual changes in user behaviour, as well as the content being posted. This is problematic for machine learning models which are typically trained on a static training dataset, and can thus become outdated when the social network changes. Inspired by the success of continual learning on such problems, we propose an ego-graphs replay strategy in continual learning (EgoCL) using graph neural networks to effectively address this issue. We have evaluated the performance of our method on user engagement with misinformation on two Twitter datasets across nineteen misinformation and conspiracy topics. Our experimental results show that our approach EgoCL has better performance in terms of predictive accuracy and computational resources than the state of the art.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1952891291,
        "newsscientist":0.1955312745,
        "technologyreview":0.3346114168,
        "venturebeat":0.2938136773,
        "wired":0.291156387,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12105v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1658751345000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2208.01466v1",
        "predicted_newsworthiness":0.5720420511,
        "title":"A Secure Dynamic Edge Resource Federation Architecture for Cross-Domain IoT Systems",
        "summary":"The fast integration of 5G communication, Artificial Intelligence (AI), and Internet-of-Things (IoT) technologies is envisioned to enable Next Generation Networks (NGNs) for diverse smart services and user-defined applications for Smart Cities. However, it is still challenging to build a scalable and efficient infrastructure that satisfies the various performance, security, and management demands by heterogeneous IoT applications across multiple administrative domains. This paper presents a dynamic edge resource federation architecture, which integrates the concept of network slicing (NS) and blockchain to improve scalability, dynamicity, and security for multi-domain IoT applications. A NS-enabled dynamic edge resource federation framework adopts intelligent mechanisms to support efficient multi-domain service coordination that satisfies diverse Quality of Service (QoS) and security requirements. We propose a Hierarchical Integrated Federated Ledger (HIFL), which aims to guarantee decentralized security and privacy-preserving properties in multi-domain resource orchestration and service re-adjustment. As a secure-by-design solution, HIFL is promising to support efficient, trust and secured end-to-end IoT services. A preliminary proof-of-concept prototype has been implemented for comparing intra- and inter-domain performance expectations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1320575649,
        "newsscientist":0.1244537466,
        "technologyreview":0.2524679404,
        "venturebeat":0.2744376483,
        "wired":0.2073816101,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01466v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1659448826000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.13339v1",
        "predicted_newsworthiness":0.5714219476,
        "title":"ALBench: A Framework for Evaluating Active Learning in Object Detection",
        "summary":"Active learning is an important technology for automated machine learning systems. In contrast to Neural Architecture Search (NAS) which aims at automating neural network architecture design, active learning aims at automating training data selection. It is especially critical for training a long-tailed task, in which positive samples are sparsely distributed. Active learning alleviates the expensive data annotation issue through incrementally training models powered with efficient data selection. Instead of annotating all unlabeled samples, it iteratively selects and annotates the most valuable samples. Active learning has been popular in image classification, but has not been fully explored in object detection. Most of current approaches on object detection are evaluated with different settings, making it difficult to fairly compare their performance. To facilitate the research in this field, this paper contributes an active learning benchmark framework named as ALBench for evaluating active learning in object detection. Developed on an automatic deep model training system, this ALBench framework is easy-to-use, compatible with different active learning algorithms, and ensures the same training and testing protocols. We hope this automated benchmark system help researchers to easily reproduce literature's performance and have objective comparisons with prior arts. The code will be release through Github.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0988314075,
        "newsscientist":0.1625161691,
        "technologyreview":0.2654192838,
        "venturebeat":0.2435741611,
        "wired":0.1691026268,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13339v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658907983000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13552v1",
        "predicted_newsworthiness":0.5705581788,
        "title":"iCub Being Social: Exploiting Social Cues for Interactive Object Detection Learning",
        "summary":"Performing joint interaction requires constant mutual monitoring of own actions and their effects on the other's behaviour. Such an action-effect monitoring is boosted by social cues and might result in an increasing sense of agency. Joint actions and joint attention are strictly correlated and both of them contribute to the formation of a precise temporal coordination. In human-robot interaction, the robot's ability to establish joint attention with a human partner and exploit various social cues to react accordingly is a crucial step in creating communicative robots. Along the social component, an effective human-robot interaction can be seen as a new method to improve and make the robot's learning process more natural and robust for a given task. In this work we use different social skills, such as mutual gaze, gaze following, speech and human face recognition, to develop an effective teacher-learner scenario tailored to visual object learning in dynamic environments. Experiments on the iCub robot demonstrate that the system allows the robot to learn new objects through a natural interaction with a human teacher in presence of distractors.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.132677165,
        "newsscientist":0.2029043682,
        "technologyreview":0.2953850898,
        "venturebeat":0.2504985999,
        "wired":0.2229213952,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13552v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658933009000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.12148v1",
        "predicted_newsworthiness":0.5699676867,
        "title":"Applying Spatiotemporal Attention to Identify Distracted and Drowsy Driving with Vision Transformers",
        "summary":"A 20% rise in car crashes in 2021 compared to 2020 has been observed as a result of increased distraction and drowsiness. Drowsy and distracted driving are the cause of 45% of all car crashes. As a means to decrease drowsy and distracted driving, detection methods using computer vision can be designed to be low-cost, accurate, and minimally invasive. This work investigated the use of the vision transformer to outperform state-of-the-art accuracy from 3D-CNNs. Two separate transformers were trained for drowsiness and distractedness. The drowsy video transformer model was trained on the National Tsing-Hua University Drowsy Driving Dataset (NTHU-DDD) with a Video Swin Transformer model for 10 epochs on two classes -- drowsy and non-drowsy simulated over 10.5 hours. The distracted video transformer was trained on the Driver Monitoring Dataset (DMD) with Video Swin Transformer for 50 epochs over 9 distraction-related classes. The accuracy of the drowsiness model reached 44% and a high loss value on the test set, indicating overfitting and poor model performance. Overfitting indicates limited training data and applied model architecture lacked quantifiable parameters to learn. The distracted model outperformed state-of-the-art models on DMD reaching 97.5%, indicating that with sufficient data and a strong architecture, transformers are suitable for unfit driving detection. Future research should use newer and stronger models such as TokenLearner to achieve higher accuracy and efficiency, merge existing datasets to expand to detecting drunk driving and road rage to create a comprehensive solution to prevent traffic crashes, and deploying a functioning prototype to revolutionize the automotive safety industry.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.165713469,
        "newsscientist":0.2096404662,
        "technologyreview":0.2940672752,
        "venturebeat":0.2720138851,
        "wired":0.2394317776,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12148v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658507808000,
        "published_hr":"Jul 22, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11564v1",
        "predicted_newsworthiness":0.5699240111,
        "title":"A general-purpose method for applying Explainable AI for Anomaly Detection",
        "summary":"The need for explainable AI (XAI) is well established but relatively little has been published outside of the supervised learning paradigm. This paper focuses on a principled approach to applying explainability and interpretability to the task of unsupervised anomaly detection. We argue that explainability is principally an algorithmic task and interpretability is principally a cognitive task, and draw on insights from the cognitive sciences to propose a general-purpose method for practical diagnosis using explained anomalies. We define Attribution Error, and demonstrate, using real-world labeled datasets, that our method based on Integrated Gradients (IG) yields significantly lower attribution errors than alternative methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1434734694,
        "newsscientist":0.1995143407,
        "technologyreview":0.316126865,
        "venturebeat":0.2946707724,
        "wired":0.2159589651,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11564v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658598961000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12782v1",
        "predicted_newsworthiness":0.5690550535,
        "title":"An Explainable Decision Support System for Predictive Process Analytics",
        "summary":"Predictive Process Analytics is becoming an essential aid for organizations, providing online operational support of their processes. However, process stakeholders need to be provided with an explanation of the reasons why a given process execution is predicted to behave in a certain way. Otherwise, they will be unlikely to trust the predictive monitoring technology and, hence, adopt it. This paper proposes a predictive analytics framework that is also equipped with explanation capabilities based on the game theory of Shapley Values. The framework has been implemented in the IBM Process Mining suite and commercialized for business users. The framework has been tested on real-life event data to assess the quality of the predictions and the corresponding evaluations. In particular, a user evaluation has been performed in order to understand if the explanations provided by the system were intelligible to process stakeholders.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1268725485,
        "newsscientist":0.1454201309,
        "technologyreview":0.2464597606,
        "venturebeat":0.2789787838,
        "wired":0.1817415847,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12782v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658829349000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13835v1",
        "predicted_newsworthiness":0.5684459116,
        "title":"Impactful Robots: Evaluating Visual and Audio Warnings to Help Users Brace for Impact in Human Robot Interaction",
        "summary":"Wearable robotic devices have potential to assist and protect their users. Toward design of a Smart Helmet, this article examines the effectiveness of audio and visual warnings to help participants brace for impacts. A user study examines different warnings and impacts applied to users while running. Perturbation forces scaled to user mass are applied from different directions and user displacement is measured to characterize effectiveness of the warning. This is accomplished using the TreadPort Active Wind Tunnel adapted to deliver forward, rearward, right, or left perturbation forces at precise moments during the locomotor cycle. The article presents an overview of the system and demonstrates the ability to precisely deliver consistent warnings and perturbations during gait. User study results highlight effectiveness of visual and audio warnings to help users brace for impact, resulting in guidelines that will inform future human-robot warning systems.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.169746901,
        "newsscientist":0.2278436609,
        "technologyreview":0.2769896771,
        "venturebeat":0.2645925194,
        "wired":0.2694264529,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13835v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658966985000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.11822v1",
        "predicted_newsworthiness":0.5680197941,
        "title":"Efficient Embedding VNFs in 5G Network Slicing: A Deep Reinforcement Learning Approach",
        "summary":"5G radio access network (RAN) slicing aims to logically split an infrastructure into a set of self-contained programmable RAN slices, with each slice built on top of the underlying physical RAN (substrate) is a separate logical mobile network, which delivers a set of services with similar characteristics. Each RAN slice is constituted by various virtual network functions (VNFs) distributed geographically in numerous substrate nodes. A key challenge in building a robust RAN slicing is, therefore, designing a RAN slicing (RS)-configuration scheme that can utilize information such as resource availability in substrate networks as well as the interdependent relationships among slices to map (embed) VNFs onto live substrate nodes. With such motivation, we propose a machine-learning-powered RAN slicing scheme that aims to accommodate maximum numbers of slices (a set of connected Virtual Network Functions - VNFs) within a given request set. More specifically, we present a deep reinforcement scheme that is called Deep Allocation Agent (DAA). In short, DAA utilizes an empirically designed deep neural network that observes the current states of the substrate network and the requested slices to schedule the slices of which VNFs are then mapped to substrate nodes using an optimization algorithm. DAA is trained towards the goal of maximizing the number of accommodated slices in the given set by using an explicitly designed reward function. Our experiment study shows that, on average, DAA is able to maintain a rate of successfully routed slices above 80% in a resource-limited substrate network, and about 60% in extreme conditions, i.e., the available resources are much less than the demands.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1038594737,
        "newsscientist":0.13462696,
        "technologyreview":0.2446354158,
        "venturebeat":0.2639058164,
        "wired":0.1955236741,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11822v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658197463000,
        "published_hr":"Jul 18, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.12886v1",
        "predicted_newsworthiness":0.5679123412,
        "title":"Detection of road traffic crashes based on collision estimation",
        "summary":"This paper introduces a framework based on computer vision that can detect road traffic crashes (RCTs) by using the installed surveillance\/CCTV camera and report them to the emergency in real-time with the exact location and time of occurrence of the accident. The framework is built of five modules. We start with the detection of vehicles by using YOLO architecture; The second module is the tracking of vehicles using MOSSE tracker, Then the third module is a new approach to detect accidents based on collision estimation. Then the fourth module for each vehicle, we detect if there is a car accident or not based on the violent flow descriptor (ViF) followed by an SVM classifier for crash prediction. Finally, in the last stage, if there is a car accident, the system will send a notification to the emergency by using a GPS module that provides us with the location, time, and date of the accident to be sent to the emergency with the help of the GSM module. The main objective is to achieve higher accuracy with fewer false alarms and to implement a simple system based on pipelining technique.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1393922207,
        "newsscientist":0.163468399,
        "technologyreview":0.2272984495,
        "venturebeat":0.2144921,
        "wired":0.1997168467,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12886v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658841675000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01436v1",
        "predicted_newsworthiness":0.5675551202,
        "title":"Predicting Future Mosquito Habitats Using Time Series Climate Forecasting and Deep Learning",
        "summary":"Mosquito habitat ranges are projected to expand due to climate change. This investigation aims to identify future mosquito habitats by analyzing preferred ecological conditions of mosquito larvae. After assembling a data set with atmospheric records and larvae observations, a neural network is trained to predict larvae counts from ecological inputs. Time series forecasting is conducted on these variables and climate projections are passed into the initial deep learning model to generate location-specific larvae abundance predictions. The results support the notion of regional ecosystem-driven changes in mosquito spread, with high-elevation regions in particular experiencing an increase in susceptibility to mosquito infestation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2360368856,
        "newsscientist":0.251147745,
        "technologyreview":0.2622518101,
        "venturebeat":0.2162682061,
        "wired":0.1980024226,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01436v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659374709000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00461v1",
        "predicted_newsworthiness":0.5665182099,
        "title":"Adaptive Temperature Scaling for Robust Calibration of Deep Neural Networks",
        "summary":"In this paper, we study the post-hoc calibration of modern neural networks, a problem that has drawn a lot of attention in recent years. Many calibration methods of varying complexity have been proposed for the task, but there is no consensus about how expressive these should be. We focus on the task of confidence scaling, specifically on post-hoc methods that generalize Temperature Scaling, we call these the Adaptive Temperature Scaling family. We analyse expressive functions that improve calibration and propose interpretable methods. We show that when there is plenty of data complex models like neural networks yield better performance, but are prone to fail when the amount of data is limited, a common situation in certain post-hoc calibration applications like medical diagnosis. We study the functions that expressive methods learn under ideal conditions and design simpler methods but with a strong inductive bias towards these well-performing functions. Concretely, we propose Entropy-based Temperature Scaling, a simple method that scales the confidence of a prediction according to its entropy. Results show that our method obtains state-of-the-art performance when compared to others and, unlike complex models, it is robust against data scarcity. Moreover, our proposed model enables a deeper interpretation of the calibration process.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1307587497,
        "newsscientist":0.1891185083,
        "technologyreview":0.3031076934,
        "venturebeat":0.2674991211,
        "wired":0.1954768248,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00461v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659284406000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12515v1",
        "predicted_newsworthiness":0.565110834,
        "title":"A Survey on Trustworthy Recommender Systems",
        "summary":"Recommender systems (RS), serving at the forefront of Human-centered AI, are widely deployed in almost every corner of the web and facilitate the human decision-making process. However, despite their enormous capabilities and potential, RS may also lead to undesired counter-effects on users, items, producers, platforms, or even the society at large, such as compromised user trust due to non-transparency, unfair treatment of different consumers, or producers, privacy concerns due to extensive use of user's private data for personalization, just to name a few. All of these create an urgent need for Trustworthy Recommender Systems (TRS) so as to mitigate or avoid such adverse impacts and risks. In this survey, we will introduce techniques related to trustworthy and responsible recommendation, including but not limited to explainable recommendation, fairness in recommendation, privacy-aware recommendation, robustness in recommendation, user controllable recommendation, as well as the relationship between these different perspectives in terms of trustworthy and responsible recommendation. Through this survey, we hope to deliver readers with a comprehensive view of the research area and raise attention to the community about the importance, existing research achievements, and future research directions on trustworthy recommendation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1653796788,
        "newsscientist":0.1913070024,
        "technologyreview":0.3158984812,
        "venturebeat":0.3172595566,
        "wired":0.2675439448,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12515v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir"
        ],
        "published":1658780605000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.01876v1",
        "predicted_newsworthiness":0.5644298105,
        "title":"Leveraging Smartphone Sensors for Detecting Abnormal Gait for Smart Wearable Mobile Technologies",
        "summary":"Walking is one of the most common modes of terrestrial locomotion for humans. Walking is essential for humans to perform most kinds of daily activities. When a person walks, there is a pattern in it, and it is known as gait. Gait analysis is used in sports and healthcare. We can analyze this gait in different ways, like using video captured by the surveillance cameras or depth image cameras in the lab environment. It also can be recognized by wearable sensors. e.g., accelerometer, force sensors, gyroscope, flexible goniometer, magneto resistive sensors, electromagnetic tracking system, force sensors, and electromyography (EMG). Analysis through these sensors required a lab condition, or users must wear these sensors. For detecting abnormality in gait action of a human, we need to incorporate the sensors separately. We can know about one's health condition by abnormal human gait after detecting it. Understanding a regular gait vs. abnormal gait may give insights to the health condition of the subject using the smart wearable technologies. Therefore, in this paper, we proposed a way to analyze abnormal human gait through smartphone sensors. Though smart devices like smartphones and smartwatches are used by most of the person nowadays. So, we can track down their gait using sensors of these intelligent wearable devices.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1454689117,
        "newsscientist":0.1977642408,
        "technologyreview":0.2543879067,
        "venturebeat":0.2507090255,
        "wired":0.2501261856,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01876v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.cv",
            "cs.lg"
        ],
        "published":1659510016000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.11875v1",
        "predicted_newsworthiness":0.5644148599,
        "title":"Seeking Subjectivity in Visual Emotion Distribution Learning",
        "summary":"Visual Emotion Analysis (VEA), which aims to predict people's emotions towards different visual stimuli, has become an attractive research topic recently. Rather than a single label classification task, it is more rational to regard VEA as a Label Distribution Learning (LDL) problem by voting from different individuals. Existing methods often predict visual emotion distribution in a unified network, neglecting the inherent subjectivity in its crowd voting process. In psychology, the \\textit{Object-Appraisal-Emotion} model has demonstrated that each individual's emotion is affected by his\/her subjective appraisal, which is further formed by the affective memory. Inspired by this, we propose a novel \\textit{Subjectivity Appraise-and-Match Network (SAMNet)} to investigate the subjectivity in visual emotion distribution. To depict the diversity in crowd voting process, we first propose the \\textit{Subjectivity Appraising} with multiple branches, where each branch simulates the emotion evocation process of a specific individual. Specifically, we construct the affective memory with an attention-based mechanism to preserve each individual's unique emotional experience. A subjectivity loss is further proposed to guarantee the divergence between different individuals. Moreover, we propose the \\textit{Subjectivity Matching} with a matching loss, aiming at assigning unordered emotion labels to ordered individual predictions in a one-to-one correspondence with the Hungarian algorithm. Extensive experiments and comparisons are conducted on public visual emotion distribution datasets, and the results demonstrate that the proposed SAMNet consistently outperforms the state-of-the-art methods. Ablation study verifies the effectiveness of our method and visualization proves its interpretability.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1727630006,
        "newsscientist":0.2011515574,
        "technologyreview":0.2785784641,
        "venturebeat":0.2537963688,
        "wired":0.2054693882,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11875v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658715603000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13700v1",
        "predicted_newsworthiness":0.5636334555,
        "title":"Remote Medication Status Prediction for Individuals with Parkinson's Disease using Time-series Data from Smartphones",
        "summary":"Medication for neurological diseases such as the Parkinson's disease usually happens remotely at home, away from hospitals. Such out-of-lab environments pose challenges in collecting timely and accurate health status data using the limited professional care devices for health condition analysis, medication adherence measurement and future dose or treatment planning. Individual differences in behavioral signals collected from wearable sensors also lead to difficulties in adopting current general machine learning analysis pipelines. To address these challenges, we present a method for predicting medication status of Parkinson's disease patients using the public mPower dataset, which contains 62,182 remote multi-modal test records collected on smartphones from 487 patients. The proposed method shows promising results in predicting three medication status objectively: Before Medication (AUC=0.95), After Medication (AUC=0.958), and Another Time (AUC=0.976) by examining patient-wise historical records with the attention weights learned through a Transformer model. We believe our method provides an innovative way for personalized remote health sensing in a timely and objective fashion which could benefit a broad range of similar applications.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1863711666,
        "newsscientist":0.2332546817,
        "technologyreview":0.3149114477,
        "venturebeat":0.3084201604,
        "wired":0.2655134837,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13700v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658801288000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11808v1",
        "predicted_newsworthiness":0.5615667596,
        "title":"ArmanEmo: A Persian Dataset for Text-based Emotion Detection",
        "summary":"With the recent proliferation of open textual data on social media platforms, Emotion Detection (ED) from Text has received more attention over the past years. It has many applications, especially for businesses and online service providers, where emotion detection techniques can help them make informed commercial decisions by analyzing customers\/users' feelings towards their products and services. In this study, we introduce ArmanEmo, a human-labeled emotion dataset of more than 7000 Persian sentences labeled for seven categories. The dataset has been collected from different resources, including Twitter, Instagram, and Digikala (an Iranian e-commerce company) comments. Labels are based on Ekman's six basic emotions (Anger, Fear, Happiness, Hatred, Sadness, Wonder) and another category (Other) to consider any other emotion not included in Ekman's model. Along with the dataset, we have provided several baseline models for emotion classification focusing on the state-of-the-art transformer-based language models. Our best model achieves a macro-averaged F1 score of 75.39 percent across our test dataset. Moreover, we also conduct transfer learning experiments to compare our proposed dataset's generalization against other Persian emotion datasets. Results of these experiments suggest that our dataset has superior generalizability among the existing Persian emotion datasets. ArmanEmo is publicly available for non-commercial use at https:\/\/github.com\/Arman-Rayan-Sharif\/arman-text-emotion.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1900751871,
        "newsscientist":0.1795067125,
        "technologyreview":0.2727123782,
        "venturebeat":0.2660559015,
        "wired":0.2351077865,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11808v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1658694923000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00974v1",
        "predicted_newsworthiness":0.5614381448,
        "title":"Information Gain Sampling for Active Learning in Medical Image Classification",
        "summary":"Large, annotated datasets are not widely available in medical image analysis due to the prohibitive time, costs, and challenges associated with labelling large datasets. Unlabelled datasets are easier to obtain, and in many contexts, it would be feasible for an expert to provide labels for a small subset of images. This work presents an information-theoretic active learning framework that guides the optimal selection of images from the unlabelled pool to be labeled based on maximizing the expected information gain (EIG) on an evaluation dataset. Experiments are performed on two different medical image classification datasets: multi-class diabetic retinopathy disease scale classification and multi-class skin lesion classification. Results indicate that by adapting EIG to account for class-imbalances, our proposed Adapted Expected Information Gain (AEIG) outperforms several popular baselines including the diversity based CoreSet and uncertainty based maximum entropy sampling. Specifically, AEIG achieves ~95% of overall performance with only 19% of the training data, while other active learning approaches require around 25%. We show that, by careful design choices, our model can be integrated into existing deep learning classifiers.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.107952673,
        "newsscientist":0.1456406275,
        "technologyreview":0.2409283456,
        "venturebeat":0.222159047,
        "wired":0.1453413888,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00974v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659371153000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12043v1",
        "predicted_newsworthiness":0.5610392527,
        "title":"Representational Ethical Model Calibration",
        "summary":"Equity is widely held to be fundamental to the ethics of healthcare. In the context of clinical decision-making, it rests on the comparative fidelity of the intelligence -- evidence-based or intuitive -- guiding the management of each individual patient. Though brought to recent attention by the individuating power of contemporary machine learning, such epistemic equity arises in the context of any decision guidance, whether traditional or innovative. Yet no general framework for its quantification, let alone assurance, currently exists. Here we formulate epistemic equity in terms of model fidelity evaluated over learnt multi-dimensional representations of identity crafted to maximise the captured diversity of the population, introducing a comprehensive framework for Representational Ethical Model Calibration. We demonstrate use of the framework on large-scale multimodal data from UK Biobank to derive diverse representations of the population, quantify model performance, and institute responsive remediation. We offer our approach as a principled solution to quantifying and assuring epistemic equity in healthcare, with applications across the research, clinical, and regulatory domains.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2600238927,
        "newsscientist":0.2749077776,
        "technologyreview":0.3701465754,
        "venturebeat":0.3102627374,
        "wired":0.2769257615,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12043v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cy"
        ],
        "published":1658745219000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01705v1",
        "predicted_newsworthiness":0.5609611996,
        "title":"Success of Uncertainty-Aware Deep Models Depends on Data Manifold Geometry",
        "summary":"For responsible decision making in safety-critical settings, machine learning models must effectively detect and process edge-case data. Although existing works show that predictive uncertainty is useful for these tasks, it is not evident from literature which uncertainty-aware models are best suited for a given dataset. Thus, we compare six uncertainty-aware deep learning models on a set of edge-case tasks: robustness to adversarial attacks as well as out-of-distribution and adversarial detection. We find that the geometry of the data sub-manifold is an important factor in determining the success of various models. Our finding suggests an interesting direction in the study of uncertainty-aware deep learning models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1413547711,
        "newsscientist":0.1873353083,
        "technologyreview":0.314081221,
        "venturebeat":0.2847836536,
        "wired":0.2300510398,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01705v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659468979000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00636v1",
        "predicted_newsworthiness":0.5604310907,
        "title":"Studying writer-suggestion interaction: A qualitative study to understand writer interaction with aligned\/misaligned next-phrase suggestion",
        "summary":"We present an exploratory qualitative study to understand how writers interact with next-phrase suggestions. While there has been some quantitative research on the effects of suggestion systems on writing, there has been little qualitative work to understand how writers interact with suggestion systems and how it affects their writing process - specifically for a non-native but English writer. We conducted a study where amateur writers were asked to write two movie reviews each, one without suggestions and one with. We found writers interact with next-phrase suggestions in various complex ways - writers are able to abstract multiple parts of the suggestions and incorporate them within their writing - even when they disagree with the suggestion as a whole. The suggestion system also had various effects on the writing processes - contributing to different aspects of the writing process in unique ways. We propose a model of writer-suggestion interaction for writing with GPT-2 for a movie review writing task, followed by ways in which the model can be used for future research, along with outlining opportunities for research and design.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.179824659,
        "newsscientist":0.1699551235,
        "technologyreview":0.2235123778,
        "venturebeat":0.207978288,
        "wired":0.2481070439,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00636v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ai"
        ],
        "published":1659336547000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.12944v1",
        "predicted_newsworthiness":0.5582300476,
        "title":"AMF: Adaptable Weighting Fusion with Multiple Fine-tuning for Image Classification",
        "summary":"Fine-tuning is widely applied in image classification tasks as a transfer learning approach. It re-uses the knowledge from a source task to learn and obtain a high performance in target tasks. Fine-tuning is able to alleviate the challenge of insufficient training data and expensive labelling of new data. However, standard fine-tuning has limited performance in complex data distributions. To address this issue, we propose the Adaptable Multi-tuning method, which adaptively determines each data sample's fine-tuning strategy. In this framework, multiple fine-tuning settings and one policy network are defined. The policy network in Adaptable Multi-tuning can dynamically adjust to an optimal weighting to feed different samples into models that are trained using different fine-tuning strategies. Our method outperforms the standard fine-tuning approach by 1.69%, 2.79% on the datasets FGVC-Aircraft, and Describable Texture, yielding comparable performance on the datasets Stanford Cars, CIFAR-10, and Fashion-MNIST.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0824486656,
        "newsscientist":0.1197744169,
        "technologyreview":0.2246593302,
        "venturebeat":0.1994738186,
        "wired":0.1430334486,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12944v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658847003000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13280v1",
        "predicted_newsworthiness":0.5574856891,
        "title":"On-Device CPU Scheduling for Sense-React Systems",
        "summary":"Sense-react systems (e.g. robotics and AR\/VR) have to take highly responsive real-time actions, driven by complex decisions involving a pipeline of sensing, perception, planning, and reaction tasks. These tasks must be scheduled on resource-constrained devices such that the performance goals and the requirements of the application are met. This is a difficult scheduling problem that requires handling multiple scheduling dimensions, and variations in resource usage and availability. In practice, system designers manually tune parameters for their specific hardware and application, which results in poor generalization and increases the development burden. In this work, we highlight the emerging need for scheduling CPU resources at runtime in sense-react systems. We study three canonical applications (face tracking, robot navigation, and VR) to first understand the key scheduling requirements for such systems. Armed with this understanding, we develop a scheduling framework, Catan, that dynamically schedules compute resources across different components of an app so as to meet the specified application requirements. Through experiments with a prototype implemented on a widely-used robotics framework (ROS) and an open-source AR\/VR platform, we show the impact of system scheduling on meeting the performance goals for the three applications, how Catan is able to achieve better application performance than hand-tuned configurations, and how it dynamically adapts to runtime variations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1297753982,
        "newsscientist":0.1734414875,
        "technologyreview":0.2764280567,
        "venturebeat":0.3258267532,
        "wired":0.2803182125,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13280v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658894736000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01815v1",
        "predicted_newsworthiness":0.557050806,
        "title":"Effidit: Your AI Writing Assistant",
        "summary":"In this technical report, we introduce Effidit (Efficient and Intelligent Editing), a digital writing assistant that facilitates users to write higher-quality text more efficiently by using artificial intelligence (AI) technologies. Previous writing assistants typically provide the function of error checking (to detect and correct spelling and grammatical errors) and limited text-rewriting functionality. With the emergence of large-scale neural language models, some systems support automatically completing a sentence or a paragraph. In Effidit, we significantly expand the capacities of a writing assistant by providing functions in five categories: text completion, error checking, text polishing, keywords to sentences (K2S), and cloud input methods (cloud IME). In the text completion category, Effidit supports generation-based sentence completion, retrieval-based sentence completion, and phrase completion. In contrast, many other writing assistants so far only provide one or two of the three functions. For text polishing, we have three functions: (context-aware) phrase polishing, sentence paraphrasing, and sentence expansion, whereas many other writing assistants often support one or two functions in this category. The main contents of this report include major modules of Effidit, methods for implementing these modules, and evaluation results of some key methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.138715517,
        "newsscientist":0.1901662097,
        "technologyreview":0.3214061399,
        "venturebeat":0.3281696706,
        "wired":0.2459892932,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01815v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659493485000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.01844v1",
        "predicted_newsworthiness":0.5559257271,
        "title":"Multiclass ASMA vs Targeted PGD Attack in Image Segmentation",
        "summary":"Deep learning networks have demonstrated high performance in a large variety of applications, such as image classification, speech recognition, and natural language processing. However, there exists a major vulnerability exploited by the use of adversarial attacks. An adversarial attack imputes images by altering the input image very slightly, making it nearly undetectable to the naked eye, but results in a very different classification by the network. This paper explores the projected gradient descent (PGD) attack and the Adaptive Mask Segmentation Attack (ASMA) on the image segmentation DeepLabV3 model using two types of architectures: MobileNetV3 and ResNet50, It was found that PGD was very consistent in changing the segmentation to be its target while the generalization of ASMA to a multiclass target was not as effective. The existence of such attack however puts all of image classification deep learning networks in danger of exploitation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1098043665,
        "newsscientist":0.1498093862,
        "technologyreview":0.2773074604,
        "venturebeat":0.2344209822,
        "wired":0.1788754192,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01844v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659503130000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14694v1",
        "predicted_newsworthiness":0.5554076578,
        "title":"Design Methodology for Deep Out-of-Distribution Detectors in Real-Time Cyber-Physical Systems",
        "summary":"When machine learning (ML) models are supplied with data outside their training distribution, they are more likely to make inaccurate predictions; in a cyber-physical system (CPS), this could lead to catastrophic system failure. To mitigate this risk, an out-of-distribution (OOD) detector can run in parallel with an ML model and flag inputs that could lead to undesirable outcomes. Although OOD detectors have been well studied in terms of accuracy, there has been less focus on deployment to resource constrained CPSs. In this study, a design methodology is proposed to tune deep OOD detectors to meet the accuracy and response time requirements of embedded applications. The methodology uses genetic algorithms to optimize the detector's preprocessing pipeline and selects a quantization method that balances robustness and response time. It also identifies several candidate task graphs under the Robot Operating System (ROS) for deployment of the selected design. The methodology is demonstrated on two variational autoencoder based OOD detectors from the literature on two embedded platforms. Insights into the trade-offs that occur during the design process are provided, and it is shown that this design methodology can lead to a drastic reduction in response time in relation to an unoptimized OOD detector while maintaining comparable accuracy.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1010304559,
        "newsscientist":0.1811899461,
        "technologyreview":0.30586691,
        "venturebeat":0.298061432,
        "wired":0.2405717138,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14694v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659103587000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.02043v1",
        "predicted_newsworthiness":0.5552532291,
        "title":"SmartControllerJS: A JavaScript library to turn smartphones into controllers for web-based interactive experiments",
        "summary":"We introduce SmartControllerJS, a new JavaScript library for fast, cost-effective designing of web applications controlled via everyday smartphones. At its core, SmartControllerJS establishes a connection between two webpages, one page running on a desktop browser and the other on the user's smartphone. The smartphone webpage loads a controller interface allowing users to control a web application running on their computer's browser. The SmartControllerJS framework enables fast iteration loops when designing interactive user experiments because it has minimal friction and allows for scaling, while having no running costs. We first describe how this library is built, how it can be used, and provide interactive examples. We then present two games designed for public screens along with results from user studies evaluating acceptability and ease of use. Finally, we implement a custom controller based on user feedback and introduce connection monitoring tools. We believe SmartControllerJS can accelerate the design of interactive experiments for researchers in Human-Computer Interaction, and be a useful tool for educational projects. To experience the various demos, we recommend reading this work on a desktop computer with your smartphone in hand. The library and the demos are available at https:\/\/github.com\/SmartControllerJS",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1927552918,
        "newsscientist":0.2419836362,
        "technologyreview":0.3185509387,
        "venturebeat":0.3517254379,
        "wired":0.3485027441,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02043v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1659532302000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.01514v1",
        "predicted_newsworthiness":0.5549465232,
        "title":"MBSE analysis for energy sustainability improvement in manufacturing industry",
        "summary":"With the ever increasing complexity of Industry 4.0 systems, plant energy management systems developed to improve energy sustainability become equally complex. Based on a Model-Based Systems Engineering analysis, this paper aims to provide a general approach to perform holistic development of an autonomous energy management system for manufacturing industries. This Energy Management System (EMS) will be capable of continuously improving its ability to assess, predict, and act, in order to improve by monitoring and controlling the energy sustainability of manufacturing systems. The approach was implemented with the System Modeling Language (SysML).",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1437681662,
        "newsscientist":0.1462115032,
        "technologyreview":0.2108468098,
        "venturebeat":0.1875767174,
        "wired":0.1545666831,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01514v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659452413000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14024v2",
        "predicted_newsworthiness":0.5544726374,
        "title":"Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer",
        "summary":"Large-scale deployment of autonomous vehicles has been continually delayed due to safety concerns. On the one hand, comprehensive scene understanding is indispensable, a lack of which would result in vulnerability to rare but complex traffic situations, such as the sudden emergence of unknown objects. However, reasoning from a global context requires access to sensors of multiple types and adequate fusion of multi-modal sensor signals, which is difficult to achieve. On the other hand, the lack of interpretability in learning models also hampers the safety with unverifiable failure causes. In this paper, we propose a safety-enhanced autonomous driving framework, named Interpretable Sensor Fusion Transformer(InterFuser), to fully process and fuse information from multi-modal multi-view sensors for achieving comprehensive scene understanding and adversarial event detection. Besides, intermediate interpretable features are generated from our framework, which provide more semantics and are exploited to better constrain actions to be within the safe sets. We conducted extensive experiments on CARLA benchmarks, where our model outperforms prior methods, ranking the first on the public CARLA Leaderboard. Our code will be made available at https:\/\/github.com\/opendilab\/InterFuser",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1438069832,
        "newsscientist":0.1849308145,
        "technologyreview":0.3193662743,
        "venturebeat":0.298663878,
        "wired":0.2481376776,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14024v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg",
            "cs.ro"
        ],
        "published":1659008181000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14218v1",
        "predicted_newsworthiness":0.5540606806,
        "title":"Gender In Gender Out: A Closer Look at User Attributes in Context-Aware Recommendation",
        "summary":"This paper studies user attributes in light of current concerns in the recommender system community: diversity, coverage, calibration, and data minimization. In experiments with a conventional context-aware recommender system that leverages side information, we show that user attributes do not always improve recommendation. Then, we demonstrate that user attributes can negatively impact diversity and coverage. Finally, we investigate the amount of information about users that ``survives'' from the training data into the recommendation lists produced by the recommender. This information is a weak signal that could in the future be exploited for calibration or studied further as a privacy leak.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2120512009,
        "newsscientist":0.2019230937,
        "technologyreview":0.3266766265,
        "venturebeat":0.3212983457,
        "wired":0.2965747674,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14218v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659026270000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13165v2",
        "predicted_newsworthiness":0.553731019,
        "title":"YOLO and Mask R-CNN for Vehicle Number Plate Identification",
        "summary":"License plate scanners have grown in popularity in parking lots during the past few years. In order to quickly identify license plates, traditional plate recognition devices used in parking lots employ a fixed source of light and shooting angles. For skewed angles, such as license plate images taken with ultra-wide angle or fisheye lenses, deformation of the license plate recognition plate can also be quite severe, impairing the ability of standard license plate recognition systems to identify the plate. Mask RCNN gadget that may be utilised for oblique pictures and various shooting angles. The results of the experiments show that the suggested design will be capable of classifying license plates with bevel angles larger than 0\/60. Character recognition using the suggested Mask R-CNN approach has advanced significantly as well. The proposed Mask R-CNN method has also achieved significant progress in character recognition, which is tilted more than 45 degrees as compared to the strategy of employing the YOLOv2 model. Experiment results also suggest that the methodology presented in the open data plate collecting is better than other techniques (known as the AOLP dataset).",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1054255415,
        "newsscientist":0.146258994,
        "technologyreview":0.2376843545,
        "venturebeat":0.2112087758,
        "wired":0.1868320208,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13165v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658864519000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13381v2",
        "predicted_newsworthiness":0.5529711526,
        "title":"Look Closer to Your Enemy: Learning to Attack via Teacher-student Mimicking",
        "summary":"This paper aims to generate realistic attack samples of person re-identification, ReID, by reading the enemy's mind (VM). In this paper, we propose a novel inconspicuous and controllable ReID attack baseline, LCYE, to generate adversarial query images. Concretely, LCYE first distills VM's knowledge via teacher-student memory mimicking in the proxy task. Then this knowledge prior acts as an explicit cipher conveying what is essential and realistic, believed by VM, for accurate adversarial misleading. Besides, benefiting from the multiple opposing task framework of LCYE, we further investigate the interpretability and generalization of ReID models from the view of the adversarial attack, including cross-domain adaption, cross-model consensus, and online learning process. Extensive experiments on four ReID benchmarks show that our method outperforms other state-of-the-art attackers with a large margin in white-box, black-box, and target attacks. Our code is now available at https:\/\/gitfront.io\/r\/user-3704489\/mKXusqDT4ffr\/LCYE\/.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1690120927,
        "newsscientist":0.226616155,
        "technologyreview":0.3383552174,
        "venturebeat":0.2834496737,
        "wired":0.2558294303,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13381v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658912988000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12236v1",
        "predicted_newsworthiness":0.5522349943,
        "title":"Personality-Driven Social Multimedia Content Recommendation",
        "summary":"Social media marketing plays a vital role in promoting brand and product values to wide audiences. In order to boost their advertising revenues, global media buying platforms such as Facebook Ads constantly reduce the reach of branded organic posts, pushing brands to spend more on paid media ads. In order to run organic and paid social media marketing efficiently, it is necessary to understand the audience, tailoring the content to fit their interests and online behaviours, which is impossible to do manually at a large scale. At the same time, various personality type categorization schemes such as the Myers-Briggs Personality Type indicator make it possible to reveal the dependencies between personality traits and user content preferences on a wider scale by categorizing audience behaviours in a unified and structured manner. This problem is yet to be studied in depth by the research community, while the level of impact of different personality traits on content recommendation accuracy has not been widely utilised and comprehensively evaluated so far. Specifically, in this work we investigate the impact of human personality traits on the content recommendation model by applying a novel personality-driven multi-view content recommender system called Personality Content Marketing Recommender Engine, or PersiC. Our experimental results and real-world case study demonstrate not just PersiC's ability to perform efficient human personality-driven multi-view content recommendation, but also allow for actionable digital ad strategy recommendations, which when deployed are able to improve digital advertising efficiency by over 420% as compared to the original human-guided approach.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1742798991,
        "newsscientist":0.1853434925,
        "technologyreview":0.2748295136,
        "venturebeat":0.3038574237,
        "wired":0.2653671835,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12236v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.ai"
        ],
        "published":1658759838000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.13243v2",
        "predicted_newsworthiness":0.5518195396,
        "title":"Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks",
        "summary":"The last decade of machine learning has seen drastic increases in scale and capabilities, and deep neural networks (DNNs) are increasingly being deployed across a wide range of domains. However, the inner workings of DNNs are generally difficult to understand, raising concerns about the safety of using these systems without a rigorous understanding of how they function. In this survey, we review literature on techniques for interpreting the inner components of DNNs, which we call \"inner\" interpretability methods. Specifically, we review methods for interpreting weights, neurons, subnetworks, and latent representations with a focus on how these techniques relate to the goal of designing safer, more trustworthy AI systems. We also highlight connections between interpretability and work in modularity, adversarial robustness, continual learning, network compression, and studying the human visual system. Finally, we discuss key challenges and argue for future work in interpretability for AI safety that focuses on diagnostics, benchmarking, and robustness.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1556291384,
        "newsscientist":0.2238895659,
        "technologyreview":0.3773251493,
        "venturebeat":0.3309627973,
        "wired":0.2618272271,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13243v2",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cl",
            "cs.cv"
        ],
        "published":1658887153000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13970v1",
        "predicted_newsworthiness":0.5516949712,
        "title":"PHEMEPlus: Enriching Social Media Rumour Verification with External Evidence",
        "summary":"Work on social media rumour verification utilises signals from posts, their propagation and users involved. Other lines of work target identifying and fact-checking claims based on information from Wikipedia, or trustworthy news articles without considering social media context. However works combining the information from social media with external evidence from the wider web are lacking. To facilitate research in this direction, we release a novel dataset, PHEMEPlus, an extension of the PHEME benchmark, which contains social media conversations as well as relevant external evidence for each rumour. We demonstrate the effectiveness of incorporating such evidence in improving rumour verification models. Additionally, as part of the evidence collection, we evaluate various ways of query formulation to identify the most effective method.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1933028425,
        "newsscientist":0.1698409802,
        "technologyreview":0.2611404484,
        "venturebeat":0.228314755,
        "wired":0.2726625024,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13970v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai",
            "cs.cy",
            "cs.lg"
        ],
        "published":1659000065000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11511v1",
        "predicted_newsworthiness":0.5513101036,
        "title":"SSBNet: Improving Visual Recognition Efficiency by Adaptive Sampling",
        "summary":"Downsampling is widely adopted to achieve a good trade-off between accuracy and latency for visual recognition. Unfortunately, the commonly used pooling layers are not learned, and thus cannot preserve important information. As another dimension reduction method, adaptive sampling weights and processes regions that are relevant to the task, and is thus able to better preserve useful information. However, the use of adaptive sampling has been limited to certain layers. In this paper, we show that using adaptive sampling in the building blocks of a deep neural network can improve its efficiency. In particular, we propose SSBNet which is built by inserting sampling layers repeatedly into existing networks like ResNet. Experiment results show that the proposed SSBNet can achieve competitive image classification and object detection performance on ImageNet and COCO datasets. For example, the SSB-ResNet-RS-200 achieved 82.6% accuracy on ImageNet dataset, which is 0.6% higher than the baseline ResNet-RS-152 with a similar complexity. Visualization shows the advantage of SSBNet in allowing different layers to focus on different positions, and ablation studies further validate the advantage of adaptive sampling over uniform methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1018265043,
        "newsscientist":0.1487614024,
        "technologyreview":0.2630983089,
        "venturebeat":0.2367037228,
        "wired":0.1642822536,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11511v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1658581315000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01468v1",
        "predicted_newsworthiness":0.5508639309,
        "title":"Unravelling Interlanguage Facts via Explainable Machine Learning",
        "summary":"Native language identification (NLI) is the task of training (via supervised machine learning) a classifier that guesses the native language of the author of a text. This task has been extensively researched in the last decade, and the performance of NLI systems has steadily improved over the years. We focus on a different facet of the NLI task, i.e., that of analysing the internals of an NLI classifier trained by an \\emph{explainable} machine learning algorithm, in order to obtain explanations of its classification decisions, with the ultimate goal of gaining insight into which linguistic phenomena ``give a speaker's native language away''. We use this perspective in order to tackle both NLI and a (much less researched) companion task, i.e., guessing whether a text has been written by a native or a non-native speaker. Using three datasets of different provenance (two datasets of English learners' essays and a dataset of social media posts), we investigate which kind of linguistic traits (lexical, morphological, syntactic, and statistical) are most effective for solving our two tasks, namely, are most indicative of a speaker's L1. We also present two case studies, one on Spanish and one on Italian learners of English, in which we analyse individual linguistic traits that the classifiers have singled out as most important for spotting these L1s. Overall, our study shows that the use of explainable machine learning can be a valuable tool for th",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1535505662,
        "newsscientist":0.1750299723,
        "technologyreview":0.2789111194,
        "venturebeat":0.259855201,
        "wired":0.1930790047,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01468v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1659449115000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.12763v1",
        "predicted_newsworthiness":0.5505483161,
        "title":"Using Abstraction for Interpretable Robot Programs in Stochastic Domains",
        "summary":"A robot's actions are inherently stochastic, as its sensors are noisy and its actions do not always have the intended effects. For this reason, the agent language Golog has been extended to models with degrees of belief and stochastic actions. While this allows more precise robot models, the resulting programs are much harder to comprehend, because they need to deal with the noise, e.g., by looping until some desired state has been reached with certainty, and because the resulting action traces consist of a large number of actions cluttered with sensor noise. To alleviate these issues, we propose to use abstraction. We define a high-level and nonstochastic model of the robot and then map the high-level model into the lower-level stochastic model. The resulting programs are much easier to understand, often do not require belief operators or loops, and produce much shorter action traces.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1002437657,
        "newsscientist":0.1640508549,
        "technologyreview":0.2832305058,
        "venturebeat":0.2330448801,
        "wired":0.215071436,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12763v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1658826937000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2208.00870v1",
        "predicted_newsworthiness":0.5477766909,
        "title":"Suggestion Lists vs. Continuous Generation: Interaction Design for Writing with Generative Models on Mobile Devices Affect Text Length, Wording and Perceived Authorship",
        "summary":"Neural language models have the potential to support human writing. However, questions remain on their integration and influence on writing and output. To address this, we designed and compared two user interfaces for writing with AI on mobile devices, which manipulate levels of initiative and control: 1) Writing with continuously generated text, the AI adds text word-by-word and user steers. 2) Writing with suggestions, the AI suggests phrases and user selects from a list. In a supervised online study (N=18), participants used these prototypes and a baseline without AI. We collected touch interactions, ratings on inspiration and authorship, and interview data. With AI suggestions, people wrote less actively, yet felt they were the author. Continuously generated text reduced this perceived authorship, yet increased editing behavior. In both designs, AI increased text length and was perceived to influence wording. Our findings add new empirical evidence on the impact of UI design decisions on user experience and output with co-creative systems.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1851254056,
        "newsscientist":0.2203224163,
        "technologyreview":0.3118962705,
        "venturebeat":0.3038881473,
        "wired":0.3033404133,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00870v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ai"
        ],
        "published":1659362231000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.02031v1",
        "predicted_newsworthiness":0.5463075475,
        "title":"Cross-lingual Approaches for the Detection of Adverse Drug Reactions in German from a Patient's Perspective",
        "summary":"In this work, we present the first corpus for German Adverse Drug Reaction (ADR) detection in patient-generated content. The data consists of 4,169 binary annotated documents from a German patient forum, where users talk about health issues and get advice from medical doctors. As is common in social media data in this domain, the class labels of the corpus are very imbalanced. This and a high topic imbalance make it a very challenging dataset, since often, the same symptom can have several causes and is not always related to a medication intake. We aim to encourage further multi-lingual efforts in the domain of ADR detection and provide preliminary experiments for binary classification using different methods of zero- and few-shot learning based on a multi-lingual model. When fine-tuning XLM-RoBERTa first on English patient forum data and then on the new German data, we achieve an F1-score of 37.52 for the positive class. We make the dataset and models publicly available for the community.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1860993123,
        "newsscientist":0.2017674958,
        "technologyreview":0.2397600007,
        "venturebeat":0.2264281508,
        "wired":0.1920507213,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02031v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.lg"
        ],
        "published":1659531121000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14535v1",
        "predicted_newsworthiness":0.5454988573,
        "title":"SERCNN: Stacked Embedding Recurrent Convolutional Neural Network in Detecting Depression on Twitter",
        "summary":"Conventional approaches to identify depression are not scalable, and the public has limited awareness of mental health, especially in developing countries. As evident by recent studies, social media has the potential to complement mental health screening on a greater scale. The vast amount of first-person narrative posts in chronological order can provide insights into one's thoughts, feelings, behavior, or mood for some time, enabling a better understanding of depression symptoms reflected in the online space. In this paper, we propose SERCNN, which improves the user representation by (1) stacking two pretrained embeddings from different domains and (2) reintroducing the embedding context to the MLP classifier. Our SERCNN shows great performance over state-of-the-art and other baselines, achieving 93.7% accuracy in a 5-fold cross-validation setting. Since not all users share the same level of online activity, we introduced the concept of a fixed observation window that quantifies the observation period in a predefined number of posts. With as minimal as 10 posts per user, SERCNN performed exceptionally well with an 87% accuracy, which is on par with the BERT model, while having 98% less in the number of parameters. Our findings open up a promising direction for detecting depression on social media with a smaller number of posts for inference, towards creating solutions for a cost-effective and timely intervention. We hope that our work can bring this research area closer to real-world adoption in existing clinical practice.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2034351166,
        "newsscientist":0.2181764012,
        "technologyreview":0.3096434655,
        "venturebeat":0.2950245641,
        "wired":0.2726951038,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14535v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.cl",
            "cs.si"
        ],
        "published":1659082095000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2208.02121v1",
        "predicted_newsworthiness":0.5453956245,
        "title":"Pedestrian-Robot Interactions on Autonomous Crowd Navigation: Reactive Control Methods and Evaluation Metrics",
        "summary":"Autonomous navigation in highly populated areas remains a challenging task for robots because of the difficulty in guaranteeing safe interactions with pedestrians in unstructured situations. In this work, we present a crowd navigation control framework that delivers continuous obstacle avoidance and post-contact control evaluated on an autonomous personal mobility vehicle. We propose evaluation metrics for accounting efficiency, controller response and crowd interactions in natural crowds. We report the results of over 110 trials in different crowd types: sparse, flows, and mixed traffic, with low- (< 0.15 ppsm), mid- (< 0.65 ppsm), and high- (< 1 ppsm) pedestrian densities. We present comparative results between two low-level obstacle avoidance methods and a baseline of shared control. Results show a 10% drop in relative time to goal on the highest density tests, and no other efficiency metric decrease. Moreover, autonomous navigation showed to be comparable to shared-control navigation with a lower relative jerk and significantly higher fluency in commands indicating high compatibility with the crowd. We conclude that the reactive controller fulfils a necessary task of fast and continuous adaptation to crowd navigation, and it should be coupled with high-level planners for environmental and situational awareness.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1412962903,
        "newsscientist":0.1663028745,
        "technologyreview":0.2556067526,
        "venturebeat":0.2226286988,
        "wired":0.2225192948,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02121v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv",
            "cs.hc"
        ],
        "published":1659538563000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01076v1",
        "predicted_newsworthiness":0.5430570256,
        "title":"Rethinking Quality of Experience for Metaverse Services: A Consumer-based Economics Perspective",
        "summary":"The Metaverse is considered to be one prototype of the next-generation Internet, which contains people's expectations for the future world. However, the academic discussion of the Metaverse still mainly focused on the system technical design, and few research studied Metaverse challenges from the perspective of consumers, i.e., Metaverse users. One difficulty is that the analysis from the consumer's perspective requires interdisciplinary theoretical framework and quantifiable Quality of Experience (QoE) measurements. In this article, pioneering from consumers' point of view, we explore an interaction between Metaverse system design and consumer behaviors. Specifically, we rethink the QoE and propose an interdisciplinary framework that encompasses both the Metaverse service providers (MSPs) and consumer considerations. From the macro perspective, we introduce a joint optimization scheme that simultaneously considers the Metaverse system design, consumers' utility, and profitability of the MSPs. From the micro perspective, we advocate the Willingness-to-Pay (WTP) as an easy-to-implement QoE measurement for future Metaverse system studies. To illustrate the usability of the proposed integrated framework, a use case of Metaverse, i.e., virtual traveling, is presented. We show that our framework can benefit the MSPs in offering competitive and economical service design to consumers while maximizing the profit.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2200468846,
        "newsscientist":0.2031914381,
        "technologyreview":0.3033589388,
        "venturebeat":0.3665177363,
        "wired":0.3130784166,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01076v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1659377884000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.14227v1",
        "predicted_newsworthiness":0.5427373011,
        "title":"Visual Recognition by Request",
        "summary":"In this paper, we present a novel protocol of annotation and evaluation for visual recognition. Different from traditional settings, the protocol does not require the labeler\/algorithm to annotate\/recognize all targets (objects, parts, etc.) at once, but instead raises a number of recognition instructions and the algorithm recognizes targets by request. This mechanism brings two beneficial properties to reduce the burden of annotation, namely, (i) variable granularity: different scenarios can have different levels of annotation, in particular, object parts can be labeled only in large and clear instances, (ii) being open-domain: new concepts can be added to the database in minimal costs. To deal with the proposed setting, we maintain a knowledge base and design a query-based visual recognition framework that constructs queries on-the-fly based on the requests. We evaluate the recognition system on two mixed-annotated datasets, CPP and ADE20K, and demonstrate its promising ability of learning from partially labeled data as well as adapting to new concepts with only text labels.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0980291645,
        "newsscientist":0.1619076057,
        "technologyreview":0.2473719295,
        "venturebeat":0.2423442756,
        "wired":0.199075672,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14227v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659027311000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01136v1",
        "predicted_newsworthiness":0.5412973884,
        "title":"Exploring the GLIDE model for Human Action-effect Prediction",
        "summary":"We address the following action-effect prediction task. Given an image depicting an initial state of the world and an action expressed in text, predict an image depicting the state of the world following the action. The prediction should have the same scene context as the input image. We explore the use of the recently proposed GLIDE model for performing this task. GLIDE is a generative neural network that can synthesize (inpaint) masked areas of an image, conditioned on a short piece of text. Our idea is to mask-out a region of the input image where the effect of the action is expected to occur. GLIDE is then used to inpaint the masked region conditioned on the required action. In this way, the resulting image has the same background context as the input image, updated to show the effect of the action. We give qualitative results from experiments using the EPIC dataset of ego-centric videos labelled with actions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1466068276,
        "newsscientist":0.1975967602,
        "technologyreview":0.2724058143,
        "venturebeat":0.2306405358,
        "wired":0.2111268195,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01136v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659387099000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14447v1",
        "predicted_newsworthiness":0.5412245033,
        "title":"Dataset and Evaluation algorithm design for GOALS Challenge",
        "summary":"Glaucoma causes irreversible vision loss due to damage to the optic nerve, and there is no cure for glaucoma.OCT imaging modality is an essential technique for assessing glaucomatous damage since it aids in quantifying fundus structures. To promote the research of AI technology in the field of OCT-assisted diagnosis of glaucoma, we held a Glaucoma OCT Analysis and Layer Segmentation (GOALS) Challenge in conjunction with the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) 2022 to provide data and corresponding annotations for researchers studying layer segmentation from OCT images and the classification of glaucoma. This paper describes the released 300 circumpapillary OCT images, the baselines of the two sub-tasks, and the evaluation methodology. The GOALS Challenge is accessible at https:\/\/aistudio.baidu.com\/aistudio\/competition\/detail\/230.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1097079794,
        "newsscientist":0.1795701142,
        "technologyreview":0.2726644694,
        "venturebeat":0.2607292412,
        "wired":0.192480254,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14447v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659063086000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11637v1",
        "predicted_newsworthiness":0.541067721,
        "title":"Explored An Effective Methodology for Fine-Grained Snake Recognition",
        "summary":"Fine-Grained Visual Classification (FGVC) is a longstanding and fundamental problem in computer vision and pattern recognition, and underpins a diverse set of real-world applications. This paper describes our contribution at SnakeCLEF2022 with FGVC. Firstly, we design a strong multimodal backbone to utilize various meta-information to assist in fine-grained identification. Secondly, we provide new loss functions to solve the long tail distribution with dataset. Then, in order to take full advantage of unlabeled datasets, we use self-supervised learning and supervised learning joint training to provide pre-trained model. Moreover, some effective data process tricks also are considered in our experiments. Last but not least, fine-tuned in downstream task with hard mining, ensambled kinds of model performance. Extensive experiments demonstrate that our method can effectively improve the performance of fine-grained recognition. Our method can achieve a macro f1 score 92.7% and 89.4% on private and public dataset, respectively, which is the 1st place among the participators on private leaderboard.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0981301814,
        "newsscientist":0.1654162022,
        "technologyreview":0.2332973755,
        "venturebeat":0.2028008334,
        "wired":0.1658966362,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11637v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658629155000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00296v1",
        "predicted_newsworthiness":0.5404862744,
        "title":"ANOVA-based Automatic Attribute Selection and a Predictive Model for Heart Disease Prognosis",
        "summary":"Studies show that Studies that cardiovascular diseases (CVDs) are malignant for human health. Thus, it is important to have an efficient way of CVD prognosis. In response to this, the healthcare industry has adopted machine learning-based smart solutions to alleviate the manual process of CVD prognosis. Thus, this work proposes an information fusion technique that combines key attributes of a person through analysis of variance (ANOVA) and domain experts' knowledge. It also introduces a new collection of CVD data samples for emerging research. There are thirty-eight experiments conducted exhaustively to verify the performance of the proposed framework on four publicly available benchmark datasets and the newly created dataset in this work. The ablation study shows that the proposed approach can achieve a competitive mean average accuracy (mAA) of 99.2% and a mean average AUC of 97.9%.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1406075277,
        "newsscientist":0.1905893776,
        "technologyreview":0.2945475532,
        "venturebeat":0.2913497674,
        "wired":0.1797019458,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00296v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659209358000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13505v1",
        "predicted_newsworthiness":0.5401588569,
        "title":"Multi-Forgery Detection Challenge 2022: Push the Frontier of Unconstrained and Diverse Forgery Detection",
        "summary":"In this paper, we present the Multi-Forgery Detection Challenge held concurrently with the IEEE Computer Society Workshop on Biometrics at CVPR 2022. Our Multi-Forgery Detection Challenge aims to detect automatic image manipulations including but not limited to image editing, image synthesis, image generation, image photoshop, etc. Our challenge has attracted 674 teams from all over the world, with about 2000 valid result submission counts. We invited the Top 10 teams to present their solutions to the challenge, from which three teams are awarded prizes in the grand finale. In this paper, we present the solutions from the Top 3 teams, in order to boost the research work in the field of image forgery detection.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1234560077,
        "newsscientist":0.1812070137,
        "technologyreview":0.2508988135,
        "venturebeat":0.215432558,
        "wired":0.1966902275,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13505v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658927754000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13958v2",
        "predicted_newsworthiness":0.5400142489,
        "title":"Learning Based High-Level Decision Making for Abortable Overtaking in Autonomous Vehicles",
        "summary":"Autonomous vehicles are a growing technology that aims to enhance safety, accessibility, efficiency, and convenience through autonomous maneuvers ranging from lane change to overtaking. Overtaking is one of the most challenging maneuvers for autonomous vehicles, and current techniques for autonomous overtaking are limited to simple situations. This paper studies how to increase safety in autonomous overtaking by allowing the maneuver to be aborted. We propose a decision-making process based on a deep Q-Network to determine if and when the overtaking maneuver needs to be aborted. The proposed algorithm is empirically evaluated in simulation with varying traffic situations, indicating that the proposed method improves safety during overtaking maneuvers. Furthermore, the approach is demonstrated in real-world experiments using the autonomous shuttle iseAuto.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1144505333,
        "newsscientist":0.1548599824,
        "technologyreview":0.2818338893,
        "venturebeat":0.2436271937,
        "wired":0.2201176196,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13958v2",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658997988000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13976v1",
        "predicted_newsworthiness":0.5395035028,
        "title":"Federated Learning for IoUT: Concepts, Applications, Challenges and Opportunities",
        "summary":"Internet of Underwater Things (IoUT) have gained rapid momentum over the past decade with applications spanning from environmental monitoring and exploration, defence applications, etc. The traditional IoUT systems use machine learning (ML) approaches which cater the needs of reliability, efficiency and timeliness. However, an extensive review of the various studies conducted highlight the significance of data privacy and security in IoUT frameworks as a predominant factor in achieving desired outcomes in mission critical applications. Federated learning (FL) is a secured, decentralized framework which is a recent development in machine learning, that will help in fulfilling the challenges faced by conventional ML approaches in IoUT. This paper presents an overview of the various applications of FL in IoUT, its challenges, open issues and indicates direction of future research prospects.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1475204939,
        "newsscientist":0.1964857483,
        "technologyreview":0.2857755767,
        "venturebeat":0.2937138378,
        "wired":0.2464283098,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13976v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659001225000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12186v1",
        "predicted_newsworthiness":0.5394643523,
        "title":"On Binding Objects to Symbols: Learning Physical Concepts to Understand Real from Fake",
        "summary":"We revisit the classic signal-to-symbol barrier in light of the remarkable ability of deep neural networks to generate realistic synthetic data. DeepFakes and spoofing highlight the feebleness of the link between physical reality and its abstract representation, whether learned by a digital computer or a biological agent. Starting from a widely applicable definition of abstract concept, we show that standard feed-forward architectures cannot capture but trivial concepts, regardless of the number of weights and the amount of training data, despite being extremely effective classifiers. On the other hand, architectures that incorporate recursion can represent a significantly larger class of concepts, but may still be unable to learn them from a finite dataset. We qualitatively describe the class of concepts that can be \"understood\" by modern architectures trained with variants of stochastic gradient descent, using a (free energy) Lagrangian to measure information complexity. Even if a concept has been understood, however, a network has no means of communicating its understanding to an external agent, except through continuous interaction and validation. We then characterize physical objects as abstract concepts and use the previous analysis to show that physical objects can be encoded by finite architectures. However, to understand physical concepts, sensors must provide persistently exciting observations, for which the ability to control the data acquisition process is essential (active perception). The importance of control depends on the modality, benefiting visual more than acoustic or chemical perception. Finally, we conclude that binding physical entities to digital identities is possible in finite time with finite resources, solving in principle the signal-to-symbol barrier problem, but we highlight the need for continuous validation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1619593998,
        "newsscientist":0.2702454629,
        "technologyreview":0.3858641898,
        "venturebeat":0.316388102,
        "wired":0.2742350954,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12186v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cv"
        ],
        "published":1658769719000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00752v1",
        "predicted_newsworthiness":0.538827056,
        "title":"Data Collection and Analysis of French Dialects",
        "summary":"This paper discusses creating and analysing a new dataset for data mining and text analytics research, contributing to a joint Leeds University research project for the Corpus of National Dialects. This report investigates machine learning classifiers to classify samples of French dialect text across various French-speaking countries. Following the steps of the CRISP-DM methodology, this report explores the data collection process, data quality issues and data conversion for text analysis. Finally, after applying suitable data mining techniques, the evaluation methods, best overall features and classifiers and conclusions are discussed.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1801236575,
        "newsscientist":0.1364431853,
        "technologyreview":0.2130411357,
        "venturebeat":0.2205770938,
        "wired":0.1642751537,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00752v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1659352877000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.13247v1",
        "predicted_newsworthiness":0.5387754336,
        "title":"Concurrent Subsidiary Supervision for Unsupervised Source-Free Domain Adaptation",
        "summary":"The prime challenge in unsupervised domain adaptation (DA) is to mitigate the domain shift between the source and target domains. Prior DA works show that pretext tasks could be used to mitigate this domain shift by learning domain invariant representations. However, in practice, we find that most existing pretext tasks are ineffective against other established techniques. Thus, we theoretically analyze how and when a subsidiary pretext task could be leveraged to assist the goal task of a given DA problem and develop objective subsidiary task suitability criteria. Based on this criteria, we devise a novel process of sticker intervention and cast sticker classification as a supervised subsidiary DA problem concurrent to the goal task unsupervised DA. Our approach not only improves goal task adaptation performance, but also facilitates privacy-oriented source-free DA i.e. without concurrent source-target access. Experiments on the standard Office-31, Office-Home, DomainNet, and VisDA benchmarks demonstrate our superiority for both single-source and multi-source source-free DA. Our approach also complements existing non-source-free works, achieving leading performance.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1293591102,
        "newsscientist":0.1708506606,
        "technologyreview":0.2843204679,
        "venturebeat":0.2810349672,
        "wired":0.2235664606,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13247v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658888709000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13055v1",
        "predicted_newsworthiness":0.5386935601,
        "title":"Contextualizing Online Conversational Networks",
        "summary":"Online social connections occur within a specific conversational context. Prior work in network analysis of social media data attempts to contextualize data through filtering. We propose a method of contextualizing online conversational connections automatically and illustrate this method with Twitter data. Specifically, we detail a graph neural network model capable of representing tweets in a vector space based on their text, hashtags, URLs, and neighboring tweets. Once tweets are represented, clusters of tweets uncover conversational contexts. We apply our method to a dataset with 4.5 million tweets discussing the 2020 US election. We find that even filtered data contains many different conversational contexts, with users engaging in multiple contexts. Central users in the contextualized networks differ significantly from central users in the overall network. This result implies that standard network analysis on social media data can be unreliable in the face of multiple conversational contexts. We further demonstrate that dynamic analysis of conversational contexts gives a qualitative understanding of conversational flow.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1968428309,
        "newsscientist":0.1736489635,
        "technologyreview":0.3063584347,
        "venturebeat":0.2887419156,
        "wired":0.3075319917,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13055v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1658856308000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.14766v1",
        "predicted_newsworthiness":0.5386401478,
        "title":"Deep Reinforcement Learning for End-to-End Network Slicing: Challenges and Solutions",
        "summary":"5G and beyond is expected to enable various emerging use cases with diverse performance requirements from vertical industries. To serve these use cases cost-effectively, network slicing plays a key role in dynamically creating virtual end-to-end networks according to specific resource demands. A network slice may have hundreds of configurable parameters over multiple technical domains that define the performance of the network slice, which makes it impossible to use traditional model-based solutions to orchestrate resources for network slices. In this article, we discuss how to design and deploy deep reinforcement learning (DRL), a model-free approach, to address the network slicing problem. First, we analyze the network slicing problem and present a standard-compliant system architecture that enables DRL-based solutions in 5G and beyond networks. Second, we provide an in-depth analysis of the challenges in designing and deploying DRL in network slicing systems. Third, we explore multiple promising techniques, i.e., safety and distributed DRL, and imitation learning, for automating end-to-end network slicing.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1028696905,
        "newsscientist":0.1383916223,
        "technologyreview":0.2550053984,
        "venturebeat":0.2730721708,
        "wired":0.2064681285,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14766v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1659111426000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.11802v1",
        "predicted_newsworthiness":0.5385221275,
        "title":"On The Convexity Of The Effective Reproduction Number",
        "summary":"In this study we analyze the evolution of the effective reproduction number, $R$, through a SIR spreading process in heterogeneous networks; Characterizing its decay process allows to analytically study the effects of countermeasures on the progress of the virus under heterogeneity, and to optimize their policies. A striking result of recent studies has shown that heterogeneity across nodes\/individuals (or, super-spreading) may have a drastic effect on the spreading process progression, which may cause a non-linear decrease of $R$ in the number of infected individuals. We account for heterogeneity and analyze the stochastic progression of the spreading process. We show that the decrease of $R$ is, in fact, convex in the number of infected individuals, where this convexity stems from heterogeneity. The analysis is based on establishing stochastic monotonic relations between the susceptible populations in varying times of the spread. We demonstrate that the convex behavior of the effective reproduction number affects the performance of countermeasures used to fight a spread of a virus. The results are applicable to the control of virus and malware spreading in computer networks as well. We examine numerically the sensitivity of the Herd Immunity Threshold (HIT) to the heterogeneity level and to the chosen policy.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2143427653,
        "newsscientist":0.2003012337,
        "technologyreview":0.2229904875,
        "venturebeat":0.1532087712,
        "wired":0.1533733143,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11802v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1658693721000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.13438v1",
        "predicted_newsworthiness":0.5381213206,
        "title":"A Contact-Safe Reinforcement Learning Framework for Contact-Rich Robot Manipulation",
        "summary":"Reinforcement learning shows great potential to solve complex contact-rich robot manipulation tasks. However, the safety of using RL in the real world is a crucial problem, since unexpected dangerous collisions might happen when the RL policy is imperfect during training or in unseen scenarios. In this paper, we propose a contact-safe reinforcement learning framework for contact-rich robot manipulation, which maintains safety in both the task space and joint space. When the RL policy causes unexpected collisions between the robot arm and the environment, our framework is able to immediately detect the collision and ensure the contact force to be small. Furthermore, the end-effector is enforced to perform contact-rich tasks compliantly, while keeping robust to external disturbances. We train the RL policy in simulation and transfer it to the real robot. Real world experiments on robot wiping tasks show that our method is able to keep the contact force small both in task space and joint space even when the policy is under unseen scenario with unexpected collision, while rejecting the disturbances on the main task.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1008672683,
        "newsscientist":0.1468253646,
        "technologyreview":0.2417686888,
        "venturebeat":0.1948346911,
        "wired":0.1635544726,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13438v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai"
        ],
        "published":1658918144000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01234v1",
        "predicted_newsworthiness":0.5379968783,
        "title":"Flood Prediction Using Machine Learning Models",
        "summary":"Floods are one of nature's most catastrophic calamities which cause irreversible and immense damage to human life, agriculture, infrastructure and socio-economic system. Several studies on flood catastrophe management and flood forecasting systems have been conducted. The accurate prediction of the onset and progression of floods in real time is challenging. To estimate water levels and velocities across a large area, it is necessary to combine data with computationally demanding flood propagation models. This paper aims to reduce the extreme risks of this natural disaster and also contributes to policy suggestions by providing a prediction for floods using different machine learning models. This research will use Binary Logistic Regression, K-Nearest Neighbor (KNN), Support Vector Classifier (SVC) and Decision tree Classifier to provide an accurate prediction. With the outcome, a comparative analysis will be conducted to understand which model delivers a better accuracy.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1770167738,
        "newsscientist":0.1911458339,
        "technologyreview":0.2513171392,
        "venturebeat":0.243594665,
        "wired":0.1807147224,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01234v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659412783000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01248v1",
        "predicted_newsworthiness":0.537416847,
        "title":"Can Gaze Beat Touch? A Fitts' Law Evaluation of Gaze, Touch, and Mouse Inputs",
        "summary":"Gaze input has been a promising substitute for mouse input for point and select interactions. Individuals with severe motor and speech disabilities primarily rely on gaze input for communication. Gaze input also serves as a hands-free input modality in the scenarios of situationally-induced impairments and disabilities (SIIDs). Hence, the performance of gaze input has often been compared to mouse input through standardized performance evaluation procedure like the Fitts' Law. With the proliferation of touch-enabled devices such as smartphones, tablet PCs, or any computing device with a touch surface, it is also important to compare the performance of gaze input to touch input. In this study, we conducted ISO 9241-9 Fitts' Law evaluation to compare the performance of multimodal gaze and foot-based input to touch input in a standard desktop environment, while using mouse input as the baseline. From a study involving 12 participants, we found that the gaze input has the lowest throughput (2.55 bits\/s), and the highest movement time (1.04 s) of the three inputs. In addition, though touch input involves maximum physical movements, it achieved the highest throughput (6.67 bits\/s), the least movement time (0.5 s), and was the most preferred input. While there are similarities in how quickly pointing can be moved from source to target location when using both gaze and touch inputs, target selection consumes maximum time with gaze input. Hence, with a throughput that is over 160% higher than gaze, touch proves to be a superior input modality.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1338331177,
        "newsscientist":0.1785945828,
        "technologyreview":0.2328431058,
        "venturebeat":0.2448855048,
        "wired":0.2393780942,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01248v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1659415531000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.14243v1",
        "predicted_newsworthiness":0.5370718057,
        "title":"Combining human parsing with analytical feature extraction and ranking schemes for high-generalization person reidentification",
        "summary":"Person reidentification (re-ID) has been receiving increasing attention in recent years due to its importance for both science and society. Machine learning and particularly Deep Learning (DL) has become the main re-id tool that allowed researches to achieve unprecedented accuracy levels on benchmark datasets. However, there is a known problem of poor generalization of DL models. That is, models trained to achieve high accuracy on one dataset perform poorly on other ones and require re-training. To address this issue, we present a model without trainable parameters which shows great potential for high generalization. It combines a fully analytical feature extraction and similarity ranking scheme with DL-based human parsing used to obtain the initial subregion classification. We show that such combination to a high extent eliminates the drawbacks of existing analytical methods. We use interpretable color and texture features which have human-readable similarity measures associated with them. To verify the proposed method we conduct experiments on Market1501 and CUHK03 datasets achieving competitive rank-1 accuracy comparable with that of DL-models. Most importantly we show that our method achieves 63.9% and 93.5% rank-1 cross-domain accuracy when applied to transfer learning tasks. It is significantly higher than previously reported 30-50% transfer accuracy. We discuss the potential ways of adding new features to further improve the model. We also show the advantage of interpretable features for constructing human-generated queries from verbal description to conduct search without a query image.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1227750329,
        "newsscientist":0.1712681456,
        "technologyreview":0.2805816726,
        "venturebeat":0.2551919336,
        "wired":0.180295788,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14243v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659028968000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01575v1",
        "predicted_newsworthiness":0.5368123445,
        "title":"ferret: a Framework for Benchmarking Explainers on Transformers",
        "summary":"Many interpretability tools allow practitioners and researchers to explain Natural Language Processing systems. However, each tool requires different configurations and provides explanations in different forms, hindering the possibility of assessing and comparing them. A principled, unified evaluation benchmark will guide the users through the central question: which explanation method is more reliable for my use case? We introduce ferret, an easy-to-use, extensible Python library to explain Transformer-based models integrated with the Hugging Face Hub. It offers a unified benchmarking suite to test and compare a wide range of state-of-the-art explainers on any text or interpretability corpora. In addition, ferret provides convenient programming abstractions to foster the introduction of new explanation methods, datasets, or evaluation metrics.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1524510236,
        "newsscientist":0.1878532044,
        "technologyreview":0.2980190962,
        "venturebeat":0.2991559387,
        "wired":0.2357686107,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01575v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659457302000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2208.00005v1",
        "predicted_newsworthiness":0.5360182169,
        "title":"Testing Relational Understanding in Text-Guided Image Generation",
        "summary":"Relations are basic building blocks of human cognition. Classic and recent work suggests that many relations are early developing, and quickly perceived. Machine models that aspire to human-level perception and reasoning should reflect the ability to recognize and reason generatively about relations. We report a systematic empirical examination of a recent text-guided image generation model (DALL-E 2), using a set of 15 basic physical and social relations studied or proposed in the literature, and judgements from human participants (N = 169). Overall, we find that only ~22% of images matched basic relation prompts. Based on a quantitative examination of people's judgments, we suggest that current image generation models do not yet have a grasp of even basic relations involving simple objects and agents. We examine reasons for model successes and failures, and suggest possible improvements based on computations observed in biological intelligence.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1741496224,
        "newsscientist":0.2321016776,
        "technologyreview":0.3090374397,
        "venturebeat":0.2655688343,
        "wired":0.237774681,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00005v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai",
            "cs.lg"
        ],
        "published":1659063578000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12194v2",
        "predicted_newsworthiness":0.5352404855,
        "title":"Domain Decorrelation with Potential Energy Ranking",
        "summary":"Machine learning systems, especially the methods based on deep learning, enjoy great success in modern computer vision tasks under experimental settings. Generally, these classic deep learning methods are built on the \\emph{i.i.d.} assumption, supposing the training and test data are drawn from a similar distribution independently and identically. However, the aforementioned \\emph{i.i.d.} assumption is in general unavailable in the real-world scenario, and as a result, leads to sharp performance decay of deep learning algorithms. Behind this, domain shift is one of the primary factors to be blamed. In order to tackle this problem, we propose using \\textbf{Po}tential \\textbf{E}nergy \\textbf{R}anking (PoER) to decouple the object feature and the domain feature (\\emph{i.e.,} appearance feature) in given images, promoting the learning of label-discriminative features while filtering out the irrelevant correlations between the objects and the background. PoER helps the neural networks to capture label-related features which contain the domain information first in shallow layers and then distills the label-discriminative representations out progressively, enforcing the neural networks to be aware of the characteristic of objects and background which is vital to the generation of domain-invariant features. PoER reports superior performance on domain generalization benchmarks, improving the average top-1 accuracy by at least 1.20\\% compared to the existing methods. Moreover, we use PoER in the ECCV 2022 NICO Challenge\\footnote{https:\/\/nicochallenge.com}, achieving top place with only a vanilla ResNet-18. The code has been made available at https:\/\/github.com\/ForeverPs\/PoER.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1169123749,
        "newsscientist":0.1751541402,
        "technologyreview":0.2983387727,
        "venturebeat":0.2671398417,
        "wired":0.1945805747,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12194v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658756033000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00386v1",
        "predicted_newsworthiness":0.5349449483,
        "title":"Robotic Dough Shaping",
        "summary":"We address the problem of shaping a piece of dough-like deformable material into a 2D target shape presented upfront. We use a 6 degree-of-freedom WidowX-250 Robot Arm equipped with a rolling pin and information collected from an RGB-D camera and a tactile sensor. We present and compare several control policies, including a dough shrinking action, in extensive experiments across three kinds of deformable materials and across three target dough shape sizes, achieving the intersection over union (IoU) of 0.90. Our results show that: i) rolling dough from the highest dough point is more efficient than from the 2D\/3D dough centroid; ii) it might be better to stop the roll movement at the current dough boundary as opposed to the target shape outline; iii) the shrink action might be beneficial only if properly tuned with respect to the exapand action; and iv) the Play-Doh material is easier to shape to a target shape as compared to Plasticine or Kinetic sand. Video demonstrations of our work are available at https:\/\/youtu.be\/ZzLMxuITdt4",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1030381065,
        "newsscientist":0.1862799168,
        "technologyreview":0.230410908,
        "venturebeat":0.1727787556,
        "wired":0.1849097593,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00386v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai",
            "cs.cv"
        ],
        "published":1659253854000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.11717v1",
        "predicted_newsworthiness":0.5342347766,
        "title":"A Priority Map for Vision-and-Language Navigation with Trajectory Plans and Feature-Location Cues",
        "summary":"In a busy city street, a pedestrian surrounded by distractions can pick out a single sign if it is relevant to their route. Artificial agents in outdoor Vision-and-Language Navigation (VLN) are also confronted with detecting supervisory signal on environment features and location in inputs. To boost the prominence of relevant features in transformer-based architectures without costly preprocessing and pretraining, we take inspiration from priority maps - a mechanism described in neuropsychological studies. We implement a novel priority map module and pretrain on auxiliary tasks using low-sample datasets with high-level representations of routes and environment-related references to urban features. A hierarchical process of trajectory planning - with subsequent parameterised visual boost filtering on visual inputs and prediction of corresponding textual spans - addresses the core challenges of cross-modal alignment and feature-level localisation. The priority map module is integrated into a feature-location framework that doubles the task completion rates of standalone transformers and attains state-of-the-art performance on the Touchdown benchmark for VLN. Code and data are referenced in Appendix C.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1377103677,
        "newsscientist":0.1978486379,
        "technologyreview":0.3005913357,
        "venturebeat":0.2864182961,
        "wired":0.2549107893,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11717v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658660985000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.00147v1",
        "predicted_newsworthiness":0.5340256213,
        "title":"Few-Shot Class-Incremental Learning from an Open-Set Perspective",
        "summary":"The continual appearance of new objects in the visual world poses considerable challenges for current deep learning methods in real-world deployments. The challenge of new task learning is often exacerbated by the scarcity of data for the new categories due to rarity or cost. Here we explore the important task of Few-Shot Class-Incremental Learning (FSCIL) and its extreme data scarcity condition of one-shot. An ideal FSCIL model needs to perform well on all classes, regardless of their presentation order or paucity of data. It also needs to be robust to open-set real-world conditions and be easily adapted to the new tasks that always arise in the field. In this paper, we first reevaluate the current task setting and propose a more comprehensive and practical setting for the FSCIL task. Then, inspired by the similarity of the goals for FSCIL and modern face recognition systems, we propose our method -- Augmented Angular Loss Incremental Classification or ALICE. In ALICE, instead of the commonly used cross-entropy loss, we propose to use the angular penalty loss to obtain well-clustered features. As the obtained features not only need to be compactly clustered but also diverse enough to maintain generalization for future incremental classes, we further discuss how class augmentation, data augmentation, and data balancing affect classification performance. Experiments on benchmark datasets, including CIFAR100, miniImageNet, and CUB200, demonstrate the improved performance of ALICE over the state-of-the-art FSCIL methods.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.109305852,
        "newsscientist":0.1557837973,
        "technologyreview":0.2715791232,
        "venturebeat":0.2529669867,
        "wired":0.1952311677,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00147v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659159768000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14221v1",
        "predicted_newsworthiness":0.5336230313,
        "title":"Humans disagree with the IoU for measuring object detector localization error",
        "summary":"The localization quality of automatic object detectors is typically evaluated by the Intersection over Union (IoU) score. In this work, we show that humans have a different view on localization quality. To evaluate this, we conduct a survey with more than 70 participants. Results show that for localization errors with the exact same IoU score, humans might not consider that these errors are equal, and express a preference. Our work is the first to evaluate IoU with humans and makes it clear that relying on IoU scores alone to evaluate localization errors might not be sufficient.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.157319511,
        "newsscientist":0.2199185728,
        "technologyreview":0.2817947087,
        "venturebeat":0.24101285,
        "wired":0.2353469243,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14221v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.hc"
        ],
        "published":1659026611000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14116v1",
        "predicted_newsworthiness":0.5331645609,
        "title":"Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction",
        "summary":"We present Claim-Dissector: a novel latent variable model for fact-checking and fact-analysis, which given a claim and a set of retrieved provenances allows learning jointly: (i) what are the relevant provenances to this claim (ii) what is the veracity of this claim. We propose to disentangle the per-provenance relevance probability and its contribution to the final veracity probability in an interpretable way - the final veracity probability is proportional to a linear ensemble of per-provenance relevance probabilities. This way, it can be clearly identified the relevance of which sources contributes to what extent towards the final probability. We show that our system achieves state-of-the-art results on FEVER dataset comparable to two-stage systems typically used in traditional fact-checking pipelines, while it often uses significantly less parameters and computation. Our analysis shows that proposed approach further allows to learn not just which provenances are relevant, but also which provenances lead to supporting and which toward denying the claim, without direct supervision. This not only adds interpretability, but also allows to detect claims with conflicting evidence automatically. Furthermore, we study whether our model can learn fine-grained relevance cues while using coarse-grained supervision. We show that our model can achieve competitive sentence-recall while using only paragraph-level relevance supervision. Finally, traversing towards the finest granularity of relevance, we show that our framework is capable of identifying relevance at the token-level. To do this, we present a new benchmark focusing on token-level interpretability - humans annotate tokens in relevant provenances they considered essential when making their judgement. Then we measure how similar are these annotations to tokens our model is focusing on. Our code, and dataset will be released online.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1599738958,
        "newsscientist":0.1585935808,
        "technologyreview":0.2464367741,
        "venturebeat":0.2247809776,
        "wired":0.2072964741,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14116v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai"
        ],
        "published":1659018606000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11837v2",
        "predicted_newsworthiness":0.5330239503,
        "title":"Inter-model Interpretability: Self-supervised Models as a Case Study",
        "summary":"Since early machine learning models, metrics such as accuracy and precision have been the de facto way to evaluate and compare trained models. However, a single metric number doesn't fully capture the similarities and differences between models, especially in the computer vision domain. A model with high accuracy on a certain dataset might provide a lower accuracy on another dataset, without any further insights. To address this problem we build on a recent interpretability technique called Dissect to introduce \\textit{inter-model interpretability}, which determines how models relate or complement each other based on the visual concepts they have learned (such as objects and materials). Towards this goal, we project 13 top-performing self-supervised models into a Learned Concepts Embedding (LCE) space that reveals proximities among models from the perspective of learned concepts. We further crossed this information with the performance of these models on four computer vision tasks and 15 datasets. The experiment allowed us to categorize the models into three categories and revealed for the first time the type of visual concepts different tasks requires. This is a step forward for designing cross-task learning algorithms.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1138763181,
        "newsscientist":0.1795912049,
        "technologyreview":0.3058263536,
        "venturebeat":0.2720432272,
        "wired":0.1841155891,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11837v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658703018000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12086v1",
        "predicted_newsworthiness":0.5319657324,
        "title":"Efficient Classification with Counterfactual Reasoning and Active Learning",
        "summary":"Data augmentation is one of the most successful techniques to improve the classification accuracy of machine learning models in computer vision. However, applying data augmentation to tabular data is a challenging problem since it is hard to generate synthetic samples with labels. In this paper, we propose an efficient classifier with a novel data augmentation technique for tabular data. Our method called CCRAL combines causal reasoning to learn counterfactual samples for the original training samples and active learning to select useful counterfactual samples based on a region of uncertainty. By doing this, our method can maximize our model's generalization on the unseen testing data. We validate our method analytically, and compare with the standard baselines. Our experimental results highlight that CCRAL achieves significantly better performance than those of the baselines across several real-world tabular datasets in terms of accuracy and AUC. Data and source code are available at: https:\/\/github.com\/nphdang\/CCRAL.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.119543718,
        "newsscientist":0.1791865109,
        "technologyreview":0.3143217285,
        "venturebeat":0.2870647259,
        "wired":0.1904888025,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12086v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658750620000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11838v1",
        "predicted_newsworthiness":0.5313058745,
        "title":"SAVCHOI: Detecting Suspicious Activities using Dense Video Captioning with Human Object Interactions",
        "summary":"Detecting suspicious activities in surveillance videos has been a longstanding problem, which can further lead to difficulties in detecting crimes. The authors propose a novel approach for detecting and summarizing the suspicious activities going on in the surveillance videos. They also create ground truth summaries for the UCF-Crime video dataset. Further, the authors test existing state-of-the-art algorithms for Dense Video Captioning for a subset of this dataset and propose a model for this task by leveraging Human-Object Interaction models for the Visual features. They observe that this formulation for Dense Captioning achieves large gains over earlier approaches by a significant margin. The authors also perform an ablative analysis of the dataset and the model and report their findings.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1400864952,
        "newsscientist":0.1555815213,
        "technologyreview":0.2438026851,
        "venturebeat":0.2213726692,
        "wired":0.2077076802,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11838v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658703203000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13799v1",
        "predicted_newsworthiness":0.5305552126,
        "title":"Network polarization, filter bubbles, and echo chambers: An annotated review of measures, models, and case studies",
        "summary":"Polarization arises when the underlying network connecting the members of a society is formed by highly connected groups with weak intergroup connectivity. The increasing polarization, the strengthening of echo chambers, and the isolation caused by information filters in social networks are increasingly attracting the attention of researchers from different areas of knowledge such as computer science, economics, social and political sciences. Despite hundreds of publications in this area, there was little effort to systematize or present the knowledge developed in the field in an organized way. This study presents an annotated review of network polarization measures, models used to handle existing polarization, their applications, and case studies. Altogether, 405 scientific articles and conference papers were examined, with 74 filtered for this review. Several approaches for measuring polarization in graphs and networks were identified, including those based on homophily, modularity, random walks, and balance theory. The models used for reducing polarization included methods that propose edge or node editions (including edge insertions or deletions, and edge weight modifications), changes in social network design, or changes in the recommendation systems embedded in these networks. This review will be helpful to researchers investigating polarized social networks from a theoretical and applied perspective.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2469384286,
        "newsscientist":0.2012241634,
        "technologyreview":0.3157900296,
        "venturebeat":0.2684110941,
        "wired":0.2962557388,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13799v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.cy"
        ],
        "published":1658957007000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.11786v1",
        "predicted_newsworthiness":0.5303388267,
        "title":"Physics-Informed Learning of Aerosol Microphysics",
        "summary":"Aerosol particles play an important role in the climate system by absorbing and scattering radiation and influencing cloud properties. They are also one of the biggest sources of uncertainty for climate modeling. Many climate models do not include aerosols in sufficient detail due to computational constraints. In order to represent key processes, aerosol microphysical properties and processes have to be accounted for. This is done in the ECHAM-HAM global climate aerosol model using the M7 microphysics, but high computational costs make it very expensive to run with finer resolution or for a longer time. We aim to use machine learning to emulate the microphysics model at sufficient accuracy and reduce the computational cost by being fast at inference time. The original M7 model is used to generate data of input-output pairs to train a neural network on it. We are able to learn the variables' tendencies achieving an average $R^2$ score of $77.1\\% $. We further explore methods to inform and constrain the neural network with physical knowledge to reduce mass violation and enforce mass positivity. On a GPU we achieve a speed-up of up to over 64x compared to the original model.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1587723629,
        "newsscientist":0.2150090327,
        "technologyreview":0.2488033154,
        "venturebeat":0.2094920586,
        "wired":0.1854052834,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11786v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658687212000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13626v1",
        "predicted_newsworthiness":0.5294693827,
        "title":"Towards Mapping and Assessing Sidewalk Accessibility Across Sociocultural and Geographic Contexts",
        "summary":"Despite the important role of sidewalks in supporting mobility, accessibility, and public health, there is a lack of high-quality datasets and corresponding analyses on sidewalk existence and condition. Our work explores a twofold vision: first, to develop scalable mechanisms to locate and assess sidewalks in cities across the world, and second, to use this data to support new urban analyses and mobility tools. We report on two preliminary urban science explorations enabled by our approach: exploring geo-spatial patterns and key correlates of sidewalk accessibility and examining differences in sidewalk infrastructure across regions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2488858404,
        "newsscientist":0.2093069901,
        "technologyreview":0.2519050034,
        "venturebeat":0.201090755,
        "wired":0.2367150839,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13626v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.cy"
        ],
        "published":1658939881000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.01329v1",
        "predicted_newsworthiness":0.5289096092,
        "title":"Self-Supervised Traversability Prediction by Learning to Reconstruct Safe Terrain",
        "summary":"Navigating off-road with a fast autonomous vehicle depends on a robust perception system that differentiates traversable from non-traversable terrain. Typically, this depends on a semantic understanding which is based on supervised learning from images annotated by a human expert. This requires a significant investment in human time, assumes correct expert classification, and small details can lead to misclassification. To address these challenges, we propose a method for predicting high- and low-risk terrains from only past vehicle experience in a self-supervised fashion. First, we develop a tool that projects the vehicle trajectory into the front camera image. Second, occlusions in the 3D representation of the terrain are filtered out. Third, an autoencoder trained on masked vehicle trajectory regions identifies low- and high-risk terrains based on the reconstruction error. We evaluated our approach with two models and different bottleneck sizes with two different training and testing sites with a fourwheeled off-road vehicle. Comparison with two independent test sets of semantic labels from similar terrain as training sites demonstrates the ability to separate the ground as low-risk and the vegetation as high-risk with 81.1% and 85.1% accuracy.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.121458074,
        "newsscientist":0.1757337167,
        "technologyreview":0.2554926553,
        "venturebeat":0.2260229451,
        "wired":0.2093012442,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01329v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv"
        ],
        "published":1659432279000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01708v1",
        "predicted_newsworthiness":0.5267667371,
        "title":"Autonomous Agriculture Robot for Smart Farming",
        "summary":"This project aims to develop and demonstrate a ground robot with intelligence capable of conducting semi-autonomous farm operations for different low-heights vegetable crops referred as Agriculture Application Robot(AAR). AAR is a lightweight, solar-electric powered robot that uses intelligent perception for conducting detection and classification of plants and their characteristics. The system also has a robotic arm for the autonomous weed cutting process. The robot can deliver fertilizer spraying, insecticide, herbicide, and other fluids to the targets such as crops, weeds, and other pests. Besides, it provides information for future research into higher-level tasks such as yield estimation, crop, and soil health monitoring. We present the design of robot and the associated experiments which show the promising results in real world environments.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1348881287,
        "newsscientist":0.2153413928,
        "technologyreview":0.2640037751,
        "venturebeat":0.2075399384,
        "wired":0.2141137837,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01708v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai",
            "cs.cv"
        ],
        "published":1659469128000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.02019v1",
        "predicted_newsworthiness":0.5263815389,
        "title":"YOLO-FaceV2: A Scale and Occlusion Aware Face Detector",
        "summary":"In recent years, face detection algorithms based on deep learning have made great progress. These algorithms can be generally divided into two categories, i.e. two-stage detector like Faster R-CNN and one-stage detector like YOLO. Because of the better balance between accuracy and speed, one-stage detectors have been widely used in many applications. In this paper, we propose a real-time face detector based on the one-stage detector YOLOv5, named YOLO-FaceV2. We design a Receptive Field Enhancement module called RFE to enhance receptive field of small face, and use NWD Loss to make up for the sensitivity of IoU to the location deviation of tiny objects. For face occlusion, we present an attention module named SEAM and introduce Repulsion Loss to solve it. Moreover, we use a weight function Slide to solve the imbalance between easy and hard samples and use the information of the effective receptive field to design the anchor. The experimental results on WiderFace dataset show that our face detector outperforms YOLO and its variants can be find in all easy, medium and hard subsets. Source code in https:\/\/github.com\/Krasjet-Yu\/YOLO-FaceV2",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1024859234,
        "newsscientist":0.1590754814,
        "technologyreview":0.2593807631,
        "venturebeat":0.2355415642,
        "wired":0.1857638663,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02019v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659530400000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00598v1",
        "predicted_newsworthiness":0.5260902355,
        "title":"A Real-time Edge-AI System for Reef Surveys",
        "summary":"Crown-of-Thorn Starfish (COTS) outbreaks are a major cause of coral loss on the Great Barrier Reef (GBR) and substantial surveillance and control programs are ongoing to manage COTS populations to ecologically sustainable levels. In this paper, we present a comprehensive real-time machine learning-based underwater data collection and curation system on edge devices for COTS monitoring. In particular, we leverage the power of deep learning-based object detection techniques, and propose a resource-efficient COTS detector that performs detection inferences on the edge device to assist marine experts with COTS identification during the data collection phase. The preliminary results show that several strategies for improving computational efficiency (e.g., batch-wise processing, frame skipping, model input size) can be combined to run the proposed detection model on edge hardware with low resource consumption and low information loss.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1610885717,
        "newsscientist":0.2408517172,
        "technologyreview":0.30852214,
        "venturebeat":0.3021755512,
        "wired":0.2317684393,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00598v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659326774000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01534v1",
        "predicted_newsworthiness":0.5258483517,
        "title":"Towards Psychologically-Grounded Dynamic Preference Models",
        "summary":"Designing recommendation systems that serve content aligned with time varying preferences requires proper accounting of the feedback effects of recommendations on human behavior and psychological condition. We argue that modeling the influence of recommendations on people's preferences must be grounded in psychologically plausible models. We contribute a methodology for developing grounded dynamic preference models. We demonstrate this method with models that capture three classic effects from the psychology literature: Mere-Exposure, Operant Conditioning, and Hedonic Adaptation. We conduct simulation-based studies to show that the psychological models manifest distinct behaviors that can inform system design. Our study has two direct implications for dynamic user modeling in recommendation systems. First, the methodology we outline is broadly applicable for psychologically grounding dynamic preference models. It allows us to critique recent contributions based on their limited discussion of psychological foundation and their implausible predictions. Second, we discuss implications of dynamic preference models for recommendation systems evaluation and design. In an example, we show that engagement and diversity metrics may be unable to capture desirable recommendation system performance.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1962011325,
        "newsscientist":0.1934225644,
        "technologyreview":0.2498364857,
        "venturebeat":0.2671843924,
        "wired":0.2496870627,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01534v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.ai",
            "cs.hc"
        ],
        "published":1659372838000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2207.13859v1",
        "predicted_newsworthiness":0.5256825629,
        "title":"Caching Scalable Videos in the Edge of Wireless Cellular Networks",
        "summary":"By pre-fetching popular videos into the local caches of edge nodes, wireless edge caching provides an effective means of reducing repeated content deliveries. To meet the various viewing quality requirements of multimedia users, scalable video coding (SVC) is integrated with edge caching, where the constituent layers of scalable videos are flexibly cached and transmitted to users. In this article, we discuss the challenges arising from the different content popularity and various viewing requirements of scalable videos, and present the diverse types of cached contents as well as the corresponding transmission schemes. We provide an overview of the existing caching schemes, and summarize the criteria of making caching decisions. A case study is then presented, where the transmission delay is quantified and used as the performance metric. Simulation results confirm that giving cognizance to the realistic requirements of end users is capable of significantly reducing the content transmission delay, compared to the existing caching schemes operating without SVC. The results also verify that the transmission delay of the proposed random caching scheme is lower than that of the caching scheme which only provides local caching gain.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1023475285,
        "newsscientist":0.1128821739,
        "technologyreview":0.1651791497,
        "venturebeat":0.2188429505,
        "wired":0.1955896001,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13859v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658974907000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.14380v1",
        "predicted_newsworthiness":0.5254774091,
        "title":"Webcam Eye Tracking: Study Conduction and Acceptance of Remote Tests with Gaze Analysis",
        "summary":"Webcam eye tracking for the collection of gaze data in the context of user studies is convenient - it can be used in remote tests where participants do not need special hardware. The approach has strong limitations, especially regarding the motion-free nature of the test persons during data recording and the quality of the gaze data obtained. Our study with 52 participants shows that usable eye tracking data can be obtained with commercially available webcams in a remote setting. However, a high drop off rate must be considered, which is why we recommend a high over-recruitment of 150%. We also show that the acceptance of the approach by the study participants is high despite the given limitations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1658846231,
        "newsscientist":0.2123433084,
        "technologyreview":0.2570652559,
        "venturebeat":0.2547240504,
        "wired":0.2307161677,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14380v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1659042522000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.13016v1",
        "predicted_newsworthiness":0.5253751419,
        "title":"Modeling the Social Influence of COVID-19 via Personalized Propagation with Deep Learning",
        "summary":"Social influence prediction has permeated many domains, including marketing, behavior prediction, recommendation systems, and more. However, traditional methods of predicting social influence not only require domain expertise,they also rely on extracting user features, which can be very tedious. Additionally, graph convolutional networks (GCNs), which deals with graph data in non-Euclidean space, are not directly applicable to Euclidean space. To overcome these problems, we extended DeepInf such that it can predict the social influence of COVID-19 via the transition probability of the page rank domain. Furthermore, our implementation gives rise to a deep learning-based personalized propagation algorithm, called DeepPP. The resulting algorithm combines the personalized propagation of a neural prediction model with the approximate personalized propagation of a neural prediction model from page rank analysis. Four social networks from different domains as well as two COVID-19 datasets were used to demonstrate the efficiency and effectiveness of the proposed algorithm. Compared to other baseline methods, DeepPP provides more accurate social influence predictions. Further, experiments demonstrate that DeepPP can be applied to real-world prediction data for COVID-19.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.263046666,
        "newsscientist":0.2670095642,
        "technologyreview":0.3578813393,
        "venturebeat":0.3302020208,
        "wired":0.295511151,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13016v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.lg"
        ],
        "published":1658852657000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.13857v1",
        "predicted_newsworthiness":0.5247746398,
        "title":"Measuring Difficulty of Novelty Reaction",
        "summary":"Current AI systems are designed to solve close-world problems with the assumption that the underlying world is remaining more or less the same. However, when dealing with real-world problems such assumptions can be invalid as sudden and unexpected changes can occur. To effectively deploy AI-powered systems in the real world, AI systems should be able to deal with open-world novelty quickly. Inevitably, dealing with open-world novelty raises an important question of novelty difficulty. Knowing whether one novelty is harder to deal with than another, can help researchers to train their systems systematically. In addition, it can also serve as a measurement of the performance of novelty robust AI systems. In this paper, we propose to define the novelty reaction difficulty as a relative difficulty of performing the known task after the introduction of the novelty. We propose a universal method that can be applied to approximate the difficulty. We present the approximations of the difficulty using our method and show how it aligns with the results of the evaluation of AI agents designed to deal with novelty.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1579555983,
        "newsscientist":0.2286103937,
        "technologyreview":0.3733674759,
        "venturebeat":0.331959113,
        "wired":0.2672934612,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13857v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1658974567000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.14205v1",
        "predicted_newsworthiness":0.5247590374,
        "title":"DoRO: Disambiguation of referred object for embodied agents",
        "summary":"Robotic task instructions often involve a referred object that the robot must locate (ground) within the environment. While task intent understanding is an essential part of natural language understanding, less effort is made to resolve ambiguity that may arise while grounding the task. Existing works use vision-based task grounding and ambiguity detection, suitable for a fixed view and a static robot. However, the problem magnifies for a mobile robot, where the ideal view is not known beforehand. Moreover, a single view may not be sufficient to locate all the object instances in the given area, which leads to inaccurate ambiguity detection. Human intervention is helpful only if the robot can convey the kind of ambiguity it is facing. In this article, we present DoRO (Disambiguation of Referred Object), a system that can help an embodied agent to disambiguate the referred object by raising a suitable query whenever required. Given an area where the intended object is, DoRO finds all the instances of the object by aggregating observations from multiple views while exploring & scanning the area. It then raises a suitable query using the information from the grounded object instances. Experiments conducted with the AI2Thor simulator show that DoRO not only detects the ambiguity more accurately but also raises verbose queries with more accurate information from the visual-language grounding.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1103279189,
        "newsscientist":0.1803137908,
        "technologyreview":0.2871290846,
        "venturebeat":0.2657341265,
        "wired":0.2240736056,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14205v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.ai"
        ],
        "published":1659025279000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.14192v1",
        "predicted_newsworthiness":0.5243896573,
        "title":"Mining Cross-Person Cues for Body-Part Interactiveness Learning in HOI Detection",
        "summary":"Human-Object Interaction (HOI) detection plays a crucial role in activity understanding. Though significant progress has been made, interactiveness learning remains a challenging problem in HOI detection: existing methods usually generate redundant negative H-O pair proposals and fail to effectively extract interactive pairs. Though interactiveness has been studied in both whole body- and part- level and facilitates the H-O pairing, previous works only focus on the target person once (i.e., in a local perspective) and overlook the information of the other persons. In this paper, we argue that comparing body-parts of multi-person simultaneously can afford us more useful and supplementary interactiveness cues. That said, to learn body-part interactiveness from a global perspective: when classifying a target person's body-part interactiveness, visual cues are explored not only from herself\/himself but also from other persons in the image. We construct body-part saliency maps based on self-attention to mine cross-person informative cues and learn the holistic relationships between all the body-parts. We evaluate the proposed method on widely-used benchmarks HICO-DET and V-COCO. With our new perspective, the holistic global-local body-part interactiveness learning achieves significant improvements over state-of-the-art. Our code is available at https:\/\/github.com\/enlighten0707\/Body-Part-Map-for-Interactiveness.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1061386389,
        "newsscientist":0.1472196238,
        "technologyreview":0.1940695444,
        "venturebeat":0.1803050436,
        "wired":0.1603501189,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14192v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659023871000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00031v1",
        "predicted_newsworthiness":0.5241805018,
        "title":"Paddy Leaf diseases identification on Infrared Images based on Convolutional Neural Networks",
        "summary":"Agriculture is the mainstay of human society because it is an essential need for every organism. Paddy cultivation is very significant so far as humans are concerned, largely in the Asian continent, and it is one of the staple foods. However, plant diseases in agriculture lead to depletion in productivity. Plant diseases are generally caused by pests, insects, and pathogens that decrease productivity to a large scale if not controlled within a particular time. Eventually, one cannot see an increase in paddy yield. Accurate and timely identification of plant diseases can help farmers mitigate losses due to pests and diseases. Recently, deep learning techniques have been used to identify paddy diseases and overcome these problems. This paper implements a convolutional neural network (CNN) based on a model and tests a public dataset consisting of 636 infrared image samples with five paddy disease classes and one healthy class. The proposed model proficiently identified and classified paddy diseases of five different types and achieved an accuracy of 88.28%",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1567038161,
        "newsscientist":0.1929568435,
        "technologyreview":0.2602771354,
        "venturebeat":0.2218560244,
        "wired":0.1533679934,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00031v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659119069000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13417v1",
        "predicted_newsworthiness":0.5233870059,
        "title":"Hardly Perceptible Trojan Attack against Neural Networks with Bit Flips",
        "summary":"The security of deep neural networks (DNNs) has attracted increasing attention due to their widespread use in various applications. Recently, the deployed DNNs have been demonstrated to be vulnerable to Trojan attacks, which manipulate model parameters with bit flips to inject a hidden behavior and activate it by a specific trigger pattern. However, all existing Trojan attacks adopt noticeable patch-based triggers (e.g., a square pattern), making them perceptible to humans and easy to be spotted by machines. In this paper, we present a novel attack, namely hardly perceptible Trojan attack (HPT). HPT crafts hardly perceptible Trojan images by utilizing the additive noise and per pixel flow field to tweak the pixel values and positions of the original images, respectively. To achieve superior attack performance, we propose to jointly optimize bit flips, additive noise, and flow field. Since the weight bits of the DNNs are binary, this problem is very hard to be solved. We handle the binary constraint with equivalent replacement and provide an effective optimization algorithm. Extensive experiments on CIFAR-10, SVHN, and ImageNet datasets show that the proposed HPT can generate hardly perceptible Trojan images, while achieving comparable or better attack performance compared to the state-of-the-art methods. The code is available at: https:\/\/github.com\/jiawangbai\/HPT.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1107124705,
        "newsscientist":0.1896669229,
        "technologyreview":0.2960693873,
        "venturebeat":0.2417754277,
        "wired":0.2348214278,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13417v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658915777000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13441v1",
        "predicted_newsworthiness":0.521592461,
        "title":"Time Series Forecasting Models Copy the Past: How to Mitigate",
        "summary":"Time series forecasting is at the core of important application domains posing significant challenges to machine learning algorithms. Recently neural network architectures have been widely applied to the problem of time series forecasting. Most of these models are trained by minimizing a loss function that measures predictions' deviation from the real values. Typical loss functions include mean squared error (MSE) and mean absolute error (MAE). In the presence of noise and uncertainty, neural network models tend to replicate the last observed value of the time series, thus limiting their applicability to real-world data. In this paper, we provide a formal definition of the above problem and we also give some examples of forecasts where the problem is observed. We also propose a regularization term penalizing the replication of previously seen values. We evaluate the proposed regularization term both on synthetic and real-world datasets. Our results indicate that the regularization term mitigates to some extent the aforementioned problem and gives rise to more robust models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1531580303,
        "newsscientist":0.1805913938,
        "technologyreview":0.2773448023,
        "venturebeat":0.2552703595,
        "wired":0.2012764878,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13441v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658918340000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.11553v1",
        "predicted_newsworthiness":0.5214799246,
        "title":"High-Resolution Swin Transformer for Automatic Medical Image Segmentation",
        "summary":"The Resolution of feature maps is critical for medical image segmentation. Most of the existing Transformer-based networks for medical image segmentation are U-Net-like architecture that contains an encoder that utilizes a sequence of Transformer blocks to convert the input medical image from high-resolution representation into low-resolution feature maps and a decoder that gradually recovers the high-resolution representation from low-resolution feature maps. Unlike previous studies, in this paper, we utilize the network design style from the High-Resolution Network (HRNet), replace the convolutional layers with Transformer blocks, and continuously exchange information from the different resolution feature maps that are generated by Transformer blocks. The newly Transformer-based network presented in this paper is denoted as High-Resolution Swin Transformer Network (HRSTNet). Extensive experiments illustrate that HRSTNet can achieve comparable performance with the state-of-the-art Transformer-based U-Net-like architecture on Brain Tumor Segmentation(BraTS) 2021 and the liver dataset from Medical Segmentation Decathlon. The code of HRSTNet will be publicly available at https:\/\/github.com\/auroua\/HRSTNet.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0904488024,
        "newsscientist":0.1514538614,
        "technologyreview":0.22908144,
        "venturebeat":0.1984008393,
        "wired":0.1519388839,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11553v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658595337000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13267v1",
        "predicted_newsworthiness":0.520431287,
        "title":"Fault Detection and Classification of Aerospace Sensors using a VGG16-based Deep Neural Network",
        "summary":"Compared with traditional model-based fault detection and classification (FDC) methods, deep neural networks (DNN) prove to be effective for the aerospace sensors FDC problems. However, time being consumed in training the DNN is excessive, and explainability analysis for the FDC neural network is still underwhelming. A concept known as imagefication-based intelligent FDC has been studied in recent years. This concept advocates to stack the sensors measurement data into an image format, the sensors FDC issue is then transformed to abnormal regions detection problem on the stacked image, which may well borrow the recent advances in the machine vision vision realm. Although promising results have been claimed in the imagefication-based intelligent FDC researches, due to the low size of the stacked image, small convolutional kernels and shallow DNN layers were used, which hinders the FDC performance. In this paper, we first propose a data augmentation method which inflates the stacked image to a larger size (correspondent to the VGG16 net developed in the machine vision realm). The FDC neural network is then trained via fine-tuning the VGG16 directly. To truncate and compress the FDC net size (hence its running time), we perform model pruning on the fine-tuned net. Class activation mapping (CAM) method is also adopted for explainability analysis of the FDC net to verify its internal operations. Via data augmentation, fine-tuning from VGG16, and model pruning, the FDC net developed in this paper claims an FDC accuracy 98.90% across 4 aircraft at 5 flight conditions (running time 26 ms). The CAM results also verify the FDC net w.r.t. its internal operations.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0853383347,
        "newsscientist":0.1605495832,
        "technologyreview":0.2650605805,
        "venturebeat":0.2412130901,
        "wired":0.177369817,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13267v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658891657000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01127v1",
        "predicted_newsworthiness":0.5202827041,
        "title":"Disparate Censorship & Undertesting: A Source of Label Bias in Clinical Machine Learning",
        "summary":"As machine learning (ML) models gain traction in clinical applications, understanding the impact of clinician and societal biases on ML models is increasingly important. While biases can arise in the labels used for model training, the many sources from which these biases arise are not yet well-studied. In this paper, we highlight disparate censorship (i.e., differences in testing rates across patient groups) as a source of label bias that clinical ML models may amplify, potentially causing harm. Many patient risk-stratification models are trained using the results of clinician-ordered diagnostic and laboratory tests of labels. Patients without test results are often assigned a negative label, which assumes that untested patients do not experience the outcome. Since orders are affected by clinical and resource considerations, testing may not be uniform in patient populations, giving rise to disparate censorship. Disparate censorship in patients of equivalent risk leads to undertesting in certain groups, and in turn, more biased labels for such groups. Using such biased labels in standard ML pipelines could contribute to gaps in model performance across patient groups. Here, we theoretically and empirically characterize conditions in which disparate censorship or undertesting affect model performance across subgroups. Our findings call attention to disparate censorship as a source of label bias in clinical ML models.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2392597572,
        "newsscientist":0.2375458411,
        "technologyreview":0.3286281548,
        "venturebeat":0.2766207529,
        "wired":0.2470393215,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01127v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.cy"
        ],
        "published":1659384931000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13991v1",
        "predicted_newsworthiness":0.5202451828,
        "title":"CoNet: Borderless and decentralized server cooperation in edge computing",
        "summary":"In edge computing (EC), by offloading tasks to edge server or remote cloud, the system performance can be improved greatly. However, since the traffic distribution in EC is heterogeneous and dynamic, it is difficult for an individual edge server to provide satisfactory computation service anytime and anywhere. This issue motivated the researchers to study the cooperation between edge servers. The previous server cooperation algorithms have disadvantages since the cooperated region is limited within one-hop. However, the performance of EC can be improved further by releasing the restriction of cooperation region. Even some works have extended the cooperated region to multi-hops, they fail to support the task offloading which is one of the core issues of edge computing. Therefore, we propose a new decentralized and borderless server cooperation algorithm for edge computing which takes task offloading strategy into account, named CoNet. In CoNet, the cooperation region is not limited. Each server forms its own basic cooperation unit (BCU) and calculates its announced capability based on BCU. The server's capability, the processing delay, the task and calculation result forwarding delay are considered during the calculation. The task division strategy bases on the real capability of host-server and the announced capability of cooperation-servers. This cooperation process is recursive and will be terminated once the terminal condition is satisfied. The simulation results demonstrate the advantages of CoNet over previous works.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0920490446,
        "newsscientist":0.1034299005,
        "technologyreview":0.1711513207,
        "venturebeat":0.2191993526,
        "wired":0.142072419,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13991v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1659003257000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.13050v1",
        "predicted_newsworthiness":0.5202071382,
        "title":"Efficient High-Resolution Deep Learning: A Survey",
        "summary":"Cameras in modern devices such as smartphones, satellites and medical equipment are capable of capturing very high resolution images and videos. Such high-resolution data often need to be processed by deep learning models for cancer detection, automated road navigation, weather prediction, surveillance, optimizing agricultural processes and many other applications. Using high-resolution images and videos as direct inputs for deep learning models creates many challenges due to their high number of parameters, computation cost, inference latency and GPU memory consumption. Simple approaches such as resizing the images to a lower resolution are common in the literature, however, they typically significantly decrease accuracy. Several works in the literature propose better alternatives in order to deal with the challenges of high-resolution data and improve accuracy and speed while complying with hardware limitations and time restrictions. This survey describes such efficient high-resolution deep learning methods, summarizes real-world applications of high-resolution deep learning, and provides comprehensive information about available high-resolution datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1185437183,
        "newsscientist":0.1640515165,
        "technologyreview":0.2649927491,
        "venturebeat":0.2459544078,
        "wired":0.198477145,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13050v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658855633000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01862v1",
        "predicted_newsworthiness":0.5198119342,
        "title":"MixNet: Structured Deep Neural Motion Prediction for Autonomous Racing",
        "summary":"Reliably predicting the motion of contestant vehicles surrounding an autonomous racecar is crucial for effective and performant planning. Although highly expressive, deep neural networks are black-box models, making their usage challenging in safety-critical applications, such as autonomous driving. In this paper, we introduce a structured way of forecasting the movement of opposing racecars with deep neural networks. The resulting set of possible output trajectories is constrained. Hence quality guarantees about the prediction can be given. We report the performance of the model by evaluating it together with an LSTM-based encoder-decoder architecture on data acquired from high-fidelity Hardware-in-the-Loop simulations. The proposed approach outperforms the baseline regarding the prediction accuracy but still fulfills the quality guarantees. Thus, a robust real-world application of the model is proven. The presented model was deployed on the racecar of the Technical University of Munich for the Indy Autonomous Challenge 2021. The code used in this research is available as open-source software at www.github.com\/TUMFTM\/MixNet.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1117210774,
        "newsscientist":0.1570461865,
        "technologyreview":0.2788813974,
        "venturebeat":0.2663964906,
        "wired":0.2344733161,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01862v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659507368000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.13254v1",
        "predicted_newsworthiness":0.5194647781,
        "title":"Contextual Information and Commonsense Based Prompt for Emotion Recognition in Conversation",
        "summary":"Emotion recognition in conversation (ERC) aims to detect the emotion for each utterance in a given conversation. The newly proposed ERC models have leveraged pre-trained language models (PLMs) with the paradigm of pre-training and fine-tuning to obtain good performance. However, these models seldom exploit PLMs' advantages thoroughly, and perform poorly for the conversations lacking explicit emotional expressions. In order to fully leverage the latent knowledge related to the emotional expressions in utterances, we propose a novel ERC model CISPER with the new paradigm of prompt and language model (LM) tuning. Specifically, CISPER is equipped with the prompt blending the contextual information and commonsense related to the interlocutor's utterances, to achieve ERC more effectively. Our extensive experiments demonstrate CISPER's superior performance over the state-of-the-art ERC models, and the effectiveness of leveraging these two kinds of significant prompt information for performance gains. To reproduce our experimental results conveniently, CISPER's sourcecode and the datasets have been shared at https:\/\/github.com\/DeqingYang\/CISPER.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1235363032,
        "newsscientist":0.1361984969,
        "technologyreview":0.223885445,
        "venturebeat":0.2474899699,
        "wired":0.1898253687,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13254v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658889245000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.12065v1",
        "predicted_newsworthiness":0.5181265682,
        "title":"Dynamic Channel Selection in Self-Supervised Learning",
        "summary":"Whilst computer vision models built using self-supervised approaches are now commonplace, some important questions remain. Do self-supervised models learn highly redundant channel features? What if a self-supervised network could dynamically select the important channels and get rid of the unnecessary ones? Currently, convnets pre-trained with self-supervision have obtained comparable performance on downstream tasks in comparison to their supervised counterparts in computer vision. However, there are drawbacks to self-supervised models including their large numbers of parameters, computationally expensive training strategies and a clear need for faster inference on downstream tasks. In this work, our goal is to address the latter by studying how a standard channel selection method developed for supervised learning can be applied to networks trained with self-supervision. We validate our findings on a range of target budgets $t_{d}$ for channel computation on image classification task across different datasets, specifically CIFAR-10, CIFAR-100, and ImageNet-100, obtaining comparable performance to that of the original network when selecting all channels but at a significant reduction in computation reported in terms of FLOPs.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0900905998,
        "newsscientist":0.144102893,
        "technologyreview":0.2553196931,
        "venturebeat":0.2310981308,
        "wired":0.1791047431,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12065v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658747928000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01149v1",
        "predicted_newsworthiness":0.5178345615,
        "title":"A Feasibility Study on Image Inpainting for Non-cleft Lip Generation from Patients with Cleft Lip",
        "summary":"A Cleft lip is a congenital abnormality requiring surgical repair by a specialist. The surgeon must have extensive experience and theoretical knowledge to perform surgery, and Artificial Intelligence (AI) method has been proposed to guide surgeons in improving surgical outcomes. If AI can be used to predict what a repaired cleft lip would look like, surgeons could use it as an adjunct to adjust their surgical technique and improve results. To explore the feasibility of this idea while protecting patient privacy, we propose a deep learning-based image inpainting method that is capable of covering a cleft lip and generating a lip and nose without a cleft. Our experiments are conducted on two real-world cleft lip datasets and are assessed by expert cleft lip surgeons to demonstrate the feasibility of the proposed method.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1288660552,
        "newsscientist":0.1947324938,
        "technologyreview":0.2998737647,
        "venturebeat":0.2568026259,
        "wired":0.2004227425,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01149v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659390289000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12381v1",
        "predicted_newsworthiness":0.5176582483,
        "title":"LightX3ECG: A Lightweight and eXplainable Deep Learning System for 3-lead Electrocardiogram Classification",
        "summary":"Cardiovascular diseases (CVDs) are a group of heart and blood vessel disorders that is one of the most serious dangers to human health, and the number of such patients is still growing. Early and accurate detection plays a key role in successful treatment and intervention. Electrocardiogram (ECG) is the gold standard for identifying a variety of cardiovascular abnormalities. In clinical practices and most of the current research, standard 12-lead ECG is mainly used. However, using a lower number of leads can make ECG more prevalent as it can be conveniently recorded by portable or wearable devices. In this research, we develop a novel deep learning system to accurately identify multiple cardiovascular abnormalities by using only three ECG leads.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1160808863,
        "newsscientist":0.168705104,
        "technologyreview":0.2497101392,
        "venturebeat":0.2431838922,
        "wired":0.1731612755,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12381v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1658771369000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01152v1",
        "predicted_newsworthiness":0.5174169259,
        "title":"Interpretable Time Series Clustering Using Local Explanations",
        "summary":"This study focuses on exploring the use of local interpretability methods for explaining time series clustering models. Many of the state-of-the-art clustering models are not directly explainable. To provide explanations for these clustering algorithms, we train classification models to estimate the cluster labels. Then, we use interpretability methods to explain the decisions of the classification models. The explanations are used to obtain insights into the clustering models. We perform a detailed numerical study to test the proposed approach on multiple datasets, clustering models, and classification models. The analysis of the results shows that the proposed approach can be used to explain time series clustering models, specifically when the underlying classification model is accurate. Lastly, we provide a detailed analysis of the results, discussing how our approach can be used in a real-life scenario.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1337459934,
        "newsscientist":0.1560056714,
        "technologyreview":0.2116808521,
        "venturebeat":0.2032838979,
        "wired":0.1644591147,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01152v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659390676000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13453v1",
        "predicted_newsworthiness":0.5171824491,
        "title":"Safe and Robust Experience Sharing for Deterministic Policy Gradient Algorithms",
        "summary":"Learning in high dimensional continuous tasks is challenging, mainly when the experience replay memory is very limited. We introduce a simple yet effective experience sharing mechanism for deterministic policies in continuous action domains for the future off-policy deep reinforcement learning applications in which the allocated memory for the experience replay buffer is limited. To overcome the extrapolation error induced by learning from other agents' experiences, we facilitate our algorithm with a novel off-policy correction technique without any action probability estimates. We test the effectiveness of our method in challenging OpenAI Gym continuous control tasks and conclude that it can achieve a safe experience sharing across multiple agents and exhibits a robust performance when the replay memory is strictly limited.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1020511441,
        "newsscientist":0.1394834708,
        "technologyreview":0.2387517368,
        "venturebeat":0.2106776919,
        "wired":0.1578056951,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13453v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1658920250000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13430v1",
        "predicted_newsworthiness":0.5171385226,
        "title":"Concept Drift Challenge in Multimedia Anomaly Detection: A Case Study with Facial Datasets",
        "summary":"Anomaly detection in multimedia datasets is a widely studied area. Yet, the concept drift challenge in data has been ignored or poorly handled by the majority of the anomaly detection frameworks. The state-of-the-art approaches assume that the data distribution at training and deployment time will be the same. However, due to various real-life environmental factors, the data may encounter drift in its distribution or can drift from one class to another in the late future. Thus, a one-time trained model might not perform adequately. In this paper, we systematically investigate the effect of concept drift on various detection models and propose a modified Adaptive Gaussian Mixture Model (AGMM) based framework for anomaly detection in multimedia data. In contrast to the baseline AGMM, the proposed extension of AGMM remembers the past for a longer period in order to handle the drift better. Extensive experimental analysis shows that the proposed model better handles the drift in data as compared with the baseline AGMM. Further, to facilitate research and comparison with the proposed framework, we contribute three multimedia datasets constituting faces as samples. The face samples of individuals correspond to the age difference of more than ten years to incorporate a longer temporal context.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1096932123,
        "newsscientist":0.1526851711,
        "technologyreview":0.2305403063,
        "venturebeat":0.2100074326,
        "wired":0.1704110971,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13430v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658917104000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00881v1",
        "predicted_newsworthiness":0.5169789931,
        "title":"Computer vision-based analysis of buildings and built environments: A systematic review of current approaches",
        "summary":"Analysing 88 sources published from 2011 to 2021, this paper presents a first systematic review of the computer vision-based analysis of buildings and the built environments to assess its value to architectural and urban design studies. Following a multi-stage selection process, the types of algorithms and data sources used are discussed in respect to architectural applications such as a building classification, detail classification, qualitative environmental analysis, building condition survey, and building value estimation. This reveals current research gaps and trends, and highlights two main categories of research aims. First, to use or optimise computer vision methods for architectural image data, which can then help automate time-consuming, labour-intensive, or complex tasks of visual analysis. Second, to explore the methodological benefits of machine learning approaches to investigate new questions about the built environment by finding patterns and relationships between visual, statistical, and qualitative data, which can overcome limitations of conventional manual analysis. The growing body of research offers new methods to architectural and design studies, with the paper identifying future challenges and directions of research.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1921081473,
        "newsscientist":0.1922639758,
        "technologyreview":0.2530908756,
        "venturebeat":0.2335370096,
        "wired":0.2085745602,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00881v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659363471000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00403v1",
        "predicted_newsworthiness":0.5169459889,
        "title":"SG-Based Analysis of LEO Satellite-Relayed Communication Systems",
        "summary":"Due to their low latency, high capacity, and seamless worldwide coverage, low Earth orbit (LEO) satellites are essential to the equal access network. Stochastic geometry (SG) is an appropriate method for such a large and irregular system. The SG model can effectively assess and estimate the performance of the network as well as handle the growing network scale. In this article, a number of common satellite distribution models are examined. In the non-technical description, system-level metrics such as coverage probability are introduced. The impact of gateway density, as well as the quantity and height of satellites, on latency and likelihood of coverage, is then researched. This essay concludes by outlining potential uses for SG in the future.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0765814065,
        "newsscientist":0.1083936331,
        "technologyreview":0.1326261869,
        "venturebeat":0.1242892593,
        "wired":0.1087578142,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00403v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1659259274000,
        "published_hr":"Jul 31, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2208.01755v1",
        "predicted_newsworthiness":0.5158990277,
        "title":"Exploring Gender Bias in Retrieval Models",
        "summary":"Biases in culture, gender, ethnicity, etc. have existed for decades and have affected many areas of human social interaction. These biases have been shown to impact machine learning (ML) models, and for natural language processing (NLP), this can have severe consequences for downstream tasks. Mitigating gender bias in information retrieval (IR) is important to avoid propagating stereotypes. In this work, we employ a dataset consisting of two components: (1) relevance of a document to a query and (2) \"gender\" of a document, in which pronouns are replaced by male, female, and neutral conjugations. We definitively show that pre-trained models for IR do not perform well in zero-shot retrieval tasks when full fine-tuning of a large pre-trained BERT encoder is performed and that lightweight fine-tuning performed with adapter networks improves zero-shot retrieval performance almost by 20% over baseline. We also illustrate that pre-trained models have gender biases that result in retrieved articles tending to be more often male than female. We overcome this by introducing a debiasing technique that penalizes the model when it prefers males over females, resulting in an effective model that retrieves articles in a balanced fashion across genders.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2086284928,
        "newsscientist":0.1936892655,
        "technologyreview":0.3018410277,
        "venturebeat":0.286696036,
        "wired":0.256233734,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01755v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ir"
        ],
        "published":1659474725000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14131v1",
        "predicted_newsworthiness":0.5153862625,
        "title":"PencilNet: Zero-Shot Sim-to-Real Transfer Learning for Robust Gate Perception in Autonomous Drone Racing",
        "summary":"In autonomous and mobile robotics, one of the main challenges is the robust on-the-fly perception of the environment, which is often unknown and dynamic, like in autonomous drone racing. In this work, we propose a novel deep neural network-based perception method for racing gate detection -- PencilNet -- which relies on a lightweight neural network backbone on top of a pencil filter. This approach unifies predictions of the gates' 2D position, distance, and orientation in a single pose tuple. We show that our method is effective for zero-shot sim-to-real transfer learning that does not need any real-world training samples. Moreover, our framework is highly robust to illumination changes commonly seen under rapid flight compared to state-of-art methods. A thorough set of experiments demonstrates the effectiveness of this approach in multiple challenging scenarios, where the drone completes various tracks under different lighting conditions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0929617376,
        "newsscientist":0.1758755569,
        "technologyreview":0.2895331912,
        "venturebeat":0.2644726113,
        "wired":0.2424581066,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14131v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659019949000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01093v1",
        "predicted_newsworthiness":0.5151669081,
        "title":"EBOCA: Evidences for BiOmedical Concepts Association Ontology",
        "summary":"There is a large number of online documents data sources available nowadays. The lack of structure and the differences between formats are the main difficulties to automatically extract information from them, which also has a negative impact on its use and reuse. In the biomedical domain, the DISNET platform emerged to provide researchers with a resource to obtain information in the scope of human disease networks by means of large-scale heterogeneous sources. Specifically in this domain, it is critical to offer not only the information extracted from different sources, but also the evidence that supports it. This paper proposes EBOCA, an ontology that describes (i) biomedical domain concepts and associations between them, and (ii) evidences supporting these associations; with the objective of providing an schema to improve the publication and description of evidences and biomedical associations in this domain. The ontology has been successfully evaluated to ensure there are no errors, modelling pitfalls and that it meets the previously defined functional requirements. Test data coming from a subset of DISNET and automatic association extractions from texts has been transformed according to the proposed ontology to create a Knowledge Graph that can be used in real scenarios, and which has also been used for the evaluation of the presented ontology.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1988427053,
        "newsscientist":0.2110057481,
        "technologyreview":0.2466931626,
        "venturebeat":0.2227287996,
        "wired":0.1797410793,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01093v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai"
        ],
        "published":1659379623000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.12035v1",
        "predicted_newsworthiness":0.5150423784,
        "title":"What makes you change your mind? An empirical investigation in online group decision-making conversations",
        "summary":"People leverage group discussions to collaborate in order to solve complex tasks, e.g. in project meetings or hiring panels. By doing so, they engage in a variety of conversational strategies where they try to convince each other of the best approach and ultimately reach a decision. In this work, we investigate methods for detecting what makes someone change their mind. To this end, we leverage a recently introduced dataset containing group discussions of people collaborating to solve a task. To find out what makes someone change their mind, we incorporate various techniques such as neural text classification and language-agnostic change point detection. Evaluation of these methods shows that while the task is not trivial, the best way to approach it is using a language-aware model with learning-to-rank training. Finally, we examine the cues that the models develop as indicative of the cause of a change of mind.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1753222953,
        "newsscientist":0.1911707535,
        "technologyreview":0.2788366145,
        "venturebeat":0.2858753674,
        "wired":0.23869537,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12035v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658744371000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11941v1",
        "predicted_newsworthiness":0.5142900823,
        "title":"GE-Grasp: Efficient Target-Oriented Grasping in Dense Clutter",
        "summary":"Grasping in dense clutter is a fundamental skill for autonomous robots. However, the crowdedness and occlusions in the cluttered scenario cause significant difficulties to generate valid grasp poses without collisions, which results in low efficiency and high failure rates. To address these, we present a generic framework called GE-Grasp for robotic motion planning in dense clutter, where we leverage diverse action primitives for occluded object removal and present the generator-evaluator architecture to avoid spatial collisions. Therefore, our GE-Grasp is capable of grasping objects in dense clutter efficiently with promising success rates. Specifically, we define three action primitives: target-oriented grasping for target capturing, pushing, and nontarget-oriented grasping to reduce the crowdedness and occlusions. The generators effectively provide various action candidates referring to the spatial information. Meanwhile, the evaluators assess the selected action primitive candidates, where the optimal action is implemented by the robot. Extensive experiments in simulated and real-world environments show that our approach outperforms the state-of-the-art methods of grasping in clutter with respect to motion efficiency and success rates. Moreover, we achieve comparable performance in the real world as that in the simulation environment, which indicates the strong generalization ability of our GE-Grasp. Supplementary material is available at: https:\/\/github.com\/CaptainWuDaoKou\/GE-Grasp.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0823164586,
        "newsscientist":0.137942259,
        "technologyreview":0.2108712462,
        "venturebeat":0.1758170202,
        "wired":0.1657646427,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11941v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1658733522000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.12360v1",
        "predicted_newsworthiness":0.5140331631,
        "title":"Exploiting High Quality Tactile Sensors for Simplified Grasping",
        "summary":"Robots are expected to grasp a wide range of objects varying in shape, weight or material type. Providing robots with tactile capabilities similar to humans is thus essential for applications involving human-to-robot or robot-to-robot interactions, particularly in those situations where a robot is expected to grasp and manipulate complex objects not previously encountered. A critical aspect for successful object grasp and manipulation is the use of high-quality fingertips equipped with multiple high-performance sensors, distributed appropriately across a specific contact surface. In this paper, we present a detailed analysis of the use of two different types of commercially available robotic fingertips (BioTac and WTS-FT), each of which is equipped with multiple sensors distributed across the fingertips' contact surface. We further demonstrate that, due to the high performance of the fingertips, a complex adaptive grasping algorithm is not required for grasping of everyday objects. We conclude that a simple algorithm based on a proportional controller will suffice for many grasping applications, provided the relevant fingertips exhibit high sensitivity. In a quantified assessment, we also demonstrate that, due in part to the sensor distribution, the BioTac-based fingertip performs better than the WTS-FT device, in enabling lifting of loads up to 850g, and that the simple proportional controller can adapt the grasp even when the object is exposed to significant external vibrational challenges.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.071771771,
        "newsscientist":0.16071541,
        "technologyreview":0.185549658,
        "venturebeat":0.1410606988,
        "wired":0.1367506756,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12360v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv"
        ],
        "published":1658769577000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01252v1",
        "predicted_newsworthiness":0.5139418801,
        "title":"A Novel Transformer Network with Shifted Window Cross-Attention for Spatiotemporal Weather Forecasting",
        "summary":"Earth Observatory is a growing research area that can capitalize on the powers of AI for short time forecasting, a Now-casting scenario. In this work, we tackle the challenge of weather forecasting using a video transformer network. Vision transformer architectures have been explored in various applications, with major constraints being the computational complexity of Attention and the data hungry training. To address these issues, we propose the use of Video Swin-Transformer, coupled with a dedicated augmentation scheme. Moreover, we employ gradual spatial reduction on the encoder side and cross-attention on the decoder. The proposed approach is tested on the Weather4Cast2021 weather forecasting challenge data, which requires the prediction of 8 hours ahead future frames (4 per hour) from an hourly weather product sequence. The dataset was normalized to 0-1 to facilitate using the evaluation metrics across different datasets. The model results in an MSE score of 0.4750 when provided with training data, and 0.4420 during transfer learning without using training data, respectively.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.140679512,
        "newsscientist":0.1881992208,
        "technologyreview":0.2522688148,
        "venturebeat":0.2379658006,
        "wired":0.2085575497,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01252v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659416693000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12496v1",
        "predicted_newsworthiness":0.5136791795,
        "title":"NeuriCam: Video Super-Resolution and Colorization Using Key Frames",
        "summary":"We present NeuriCam, a key-frame video super-resolution and colorization based system, to achieve low-power video capture from dual-mode IOT cameras. Our idea is to design a dual-mode camera system where the first mode is low power (1.1~mW) but only outputs gray-scale, low resolution and noisy video and the second mode consumes much higher power (100~mW) but outputs color and higher resolution images. To reduce total energy consumption, we heavily duty cycle the high power mode to output an image only once every second. The data from this camera system is then wirelessly streamed to a nearby plugged-in gateway, where we run our real-time neural network decoder to reconstruct a higher resolution color video. To achieve this, we introduce an attention feature filter mechanism that assigns different weights to different features, based on the correlation between the feature map and contents of the input frame at each spatial location. We design a wireless hardware prototype using off-the-shelf cameras and address practical issues including packet loss and perspective mismatch. Our evaluation shows that our dual-camera hardware reduces camera energy consumption while achieving an average gray-scale PSNR gain of 3.7~dB over prior video super resolution methods and 5.6~dB RGB gain over existing color propagation methods. Open-source code: https:\/\/github.com\/vb000\/NeuriCam.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0949673441,
        "newsscientist":0.1526122736,
        "technologyreview":0.226149273,
        "venturebeat":0.252448602,
        "wired":0.2348476298,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12496v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658778897000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13771v1",
        "predicted_newsworthiness":0.5135834861,
        "title":"CompText: Visualizing, Comparing & Understanding Text Corpus",
        "summary":"A common practice in Natural Language Processing (NLP) is to visualize the text corpus without reading through the entire literature, still grasping the central idea and key points described. For a long time, researchers focused on extracting topics from the text and visualizing them based on their relative significance in the corpus. However, recently, researchers started coming up with more complex systems that not only expose the topics of the corpus but also word closely related to the topic to give users a holistic view. These detailed visualizations spawned research on comparing text corpora based on their visualization. Topics are often compared to idealize the difference between corpora. However, to capture greater semantics from different corpora, researchers have started to compare texts based on the sentiment of the topics related to the text. Comparing the words carrying the most weightage, we can get an idea about the important topics for corpus. There are multiple existing texts comparing methods present that compare topics rather than sentiments but we feel that focusing on sentiment-carrying words would better compare the two corpora. Since only sentiments can explain the real feeling of the text and not just the topic, topics without sentiments are just nouns. We aim to differentiate the corpus with a focus on sentiment, as opposed to comparing all the words appearing in the two corpora. The rationale behind this is, that the two corpora do not many have identical words for side-by-side comparison, so comparing the sentiment words gives us an idea of how the corpora are appealing to the emotions of the reader. We can argue that the entropy or the unexpectedness and divergence of topics should also be of importance and help us to identify key pivot points and the importance of certain topics in the corpus alongside relative sentiment.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2083368822,
        "newsscientist":0.1872051461,
        "technologyreview":0.2443802853,
        "venturebeat":0.2305586809,
        "wired":0.2421536311,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13771v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658952271000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11679v1",
        "predicted_newsworthiness":0.5130413474,
        "title":"Affective Behaviour Analysis Using Pretrained Model with Facial Priori",
        "summary":"Affective behaviour analysis has aroused researchers' attention due to its broad applications. However, it is labor exhaustive to obtain accurate annotations for massive face images. Thus, we propose to utilize the prior facial information via Masked Auto-Encoder (MAE) pretrained on unlabeled face images. Furthermore, we combine MAE pretrained Vision Transformer (ViT) and AffectNet pretrained CNN to perform multi-task emotion recognition. We notice that expression and action unit (AU) scores are pure and intact features for valence-arousal (VA) regression. As a result, we utilize AffectNet pretrained CNN to extract expression scores concatenating with expression and AU scores from ViT to obtain the final VA features. Moreover, we also propose a co-training framework with two parallel MAE pretrained ViT for expression recognition tasks. In order to make the two views independent, we random mask most patches during the training process. Then, JS divergence is performed to make the predictions of the two views as consistent as possible. The results on ABAW4 show that our methods are effective.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.131740986,
        "newsscientist":0.1583954677,
        "technologyreview":0.2398651421,
        "venturebeat":0.2295376908,
        "wired":0.1759585561,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11679v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658647688000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12558v1",
        "predicted_newsworthiness":0.5129932411,
        "title":"A Pilot Study on The Impact of Stereoscopic Display Type on User Interactions Within A Immersive Analytics Environment",
        "summary":"Immersive Analytics (IA) and consumer adoption of augmented reality (AR) and virtual reality (VR) head-mounted displays (HMDs) are both rapidly growing. When used in conjunction, stereoscopic IA environments can offer improved user understanding and engagement; however, it is unclear how the choice of stereoscopic display impacts user interactions within an IA environment. This paper presents a pilot study that examines the impact of stereoscopic display type on object manipulation and environmental navigation using consumer-available AR and VR displays. This work finds that the display type can impact how users manipulate virtual content, how they navigate the environment, and how able they are to answer questions about the represented data.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1654458589,
        "newsscientist":0.1850078154,
        "technologyreview":0.2669494047,
        "venturebeat":0.3573921503,
        "wired":0.2804465324,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12558v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1658787833000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.13313v1",
        "predicted_newsworthiness":0.5129501125,
        "title":"Social Live-Streaming Use & Well-being: Examining Participation, Financial Commitment, Social Capital, and Psychological Well-being on Twitch.tv",
        "summary":"This study examines how active participation, financial commitment, and passive participation in the leading social live-streaming service, Twitch.tv, relate to individuals' psychological well-being. The three dimensions of social capital-structural, relational, and cognitive-as well as parasocial relationship are explored as mediators. Cross-sectional survey data from 396 respondents was analyzed by comparing two fully saturated structural equation models. Findings indicate actively participating in a favorite streamers' Chat is positively associated with increased well-being. Structural social capital, or having more social interaction ties, positively mediates the relationship between active participation and well-being, as well as financial commitment and well-being. Greater cognitive social capital, or shared values and goals with a favorite streamer, is related to decreased well-being. Parasocial relationship does not significantly mediate the relationship between use and well-being. Our results demonstrate the importance of tangible social ties over the perceived relationships or identification with a favorite streamer.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2735497946,
        "newsscientist":0.2010433447,
        "technologyreview":0.3040207314,
        "venturebeat":0.3341923051,
        "wired":0.319646331,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13313v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si",
            "cs.hc"
        ],
        "published":1658902209000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.13801v1",
        "predicted_newsworthiness":0.5129383694,
        "title":"Towards Sleep Scoring Generalization Through Self-Supervised Meta-Learning",
        "summary":"In this work we introduce a novel meta-learning method for sleep scoring based on self-supervised learning. Our approach aims at building models for sleep scoring that can generalize across different patients and recording facilities, but do not require a further adaptation step to the target data. Towards this goal, we build our method on top of the Model Agnostic Meta-Learning (MAML) framework by incorporating a self-supervised learning (SSL) stage, and call it S2MAML. We show that S2MAML can significantly outperform MAML. The gain in performance comes from the SSL stage, which we base on a general purpose pseudo-task that limits the overfitting to the subject-specific patterns present in the training dataset. We show that S2MAML outperforms standard supervised learning and MAML on the SC, ST, ISRUC, UCD and CAP datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1247591825,
        "newsscientist":0.177549287,
        "technologyreview":0.2306316924,
        "venturebeat":0.2204864388,
        "wired":0.1620668745,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13801v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658957244000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13814v1",
        "predicted_newsworthiness":0.5128109014,
        "title":"Data Driven Modeling Social Media Influence using Differential Equations",
        "summary":"Individuals modify their opinions towards a topic based on their social interactions. Opinion evolution models conceptualize the change of opinion as a uni-dimensional continuum, and the effect of influence is built by the group size, the network structures, or the relations among opinions within the group. However, how to model the personal opinion evolution process under the effect of the online social influence as a function remains unclear. Here, we show that the uni-dimensional continuous user opinions can be represented by compressed high-dimensional word embeddings, and its evolution can be accurately modelled by an ordinary differential equation (ODE) that reflects the social network influencer interactions. Our three major contributions are: (1) introduce a data-driven pipeline representing the personal evolution of opinions with a time kernel, (2) based on previous psychology models, we model the opinion evolution process as a function of online social influence using an ordinary differential equation, and (3) applied Our opinion evolution model to the real-time Twitter data. We perform our analysis on 87 active users with corresponding influencers on the COVID-19 topic from 2020 to 2022. The regression results demonstrate that 99% of the variation in the quantified opinions can be explained by the way we model the connected opinions from their influencers. Our research on the COVID-19 topic and for the account analysed shows that social media users primarily shift their opinion based on influencers they follow (e.g., model explains for 99% variation) and self-evolution of opinion over a long time scale is limited.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2214376348,
        "newsscientist":0.2108330636,
        "technologyreview":0.3102021036,
        "venturebeat":0.2824540012,
        "wired":0.2908621952,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13814v1",
        "arxiv_primary_category":"cs.si",
        "arxiv_all_categories":[
            "cs.si"
        ],
        "published":1658960662000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Social and Information Networks"
    },
    {
        "arxiv_id":"2207.14798v1",
        "predicted_newsworthiness":0.5126456074,
        "title":"Personalized Promotion Decision Making Based on Direct and Enduring Effect Predictions",
        "summary":"Promotions have been trending in the e-commerce marketplace to build up customer relationships and guide customers towards the desired actions. Since incentives are effective to engage customers and customers have different preferences for different types of incentives, the demand for personalized promotion decision making is increasing over time. However, research on promotion decision making has focused specifically on purchase conversion during the promotion period (the direct effect), while generally disregarding the enduring effect in the post promotion period. To achieve a better lift return on investment (lift ROI) on the enduring effect of the promotion and improve customer retention and loyalty, we propose a framework of multiple treatment promotion decision making by modeling each customer's direct and enduring response. First, we propose a customer direct and enduring effect (CDEE) model which predicts the customer direct and enduring response. With the help of the predictions of the CDEE, we personalize incentive allocation to optimize the enduring effect while keeping the cost under the budget. To estimate the effect of decision making, we apply an unbiased evaluation approach of business metrics with randomized control trial (RCT) data. We compare our method with benchmarks using two promotions in Mercari and achieve significantly better results.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1668100658,
        "newsscientist":0.1508793091,
        "technologyreview":0.2216382102,
        "venturebeat":0.2993651441,
        "wired":0.1898735059,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14798v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.lg"
        ],
        "published":1658560437000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    },
    {
        "arxiv_id":"2208.01963v1",
        "predicted_newsworthiness":0.5125098253,
        "title":"Localization and Classification of Parasitic Eggs in Microscopic Images Using an EfficientDet Detector",
        "summary":"IPIs caused by protozoan and helminth parasites are among the most common infections in humans in LMICs. They are regarded as a severe public health concern, as they cause a wide array of potentially detrimental health conditions. Researchers have been developing pattern recognition techniques for the automatic identification of parasite eggs in microscopic images. Existing solutions still need improvements to reduce diagnostic errors and generate fast, efficient, and accurate results. Our paper addresses this and proposes a multi-modal learning detector to localize parasitic eggs and categorize them into 11 categories. The experiments were conducted on the novel Chula-ParasiteEgg-11 dataset that was used to train both EfficientDet model with EfficientNet-v2 backbone and EfficientNet-B7+SVM. The dataset has 11,000 microscopic training images from 11 categories. Our results show robust performance with an accuracy of 92%, and an F1 score of 93%. Additionally, the IOU distribution illustrates the high localization capability of the detector.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1315140855,
        "newsscientist":0.1829487415,
        "technologyreview":0.1923283673,
        "venturebeat":0.1772685626,
        "wired":0.132456473,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01963v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659522498000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13315v1",
        "predicted_newsworthiness":0.5123693732,
        "title":"Portrait Interpretation and a Benchmark",
        "summary":"We propose a task we name Portrait Interpretation and construct a dataset named Portrait250K for it. Current researches on portraits such as human attribute recognition and person re-identification have achieved many successes, but generally, they: 1) may lack mining the interrelationship between various tasks and the possible benefits it may bring; 2) design deep models specifically for each task, which is inefficient; 3) may be unable to cope with the needs of a unified model and comprehensive perception in actual scenes. In this paper, the proposed portrait interpretation recognizes the perception of humans from a new systematic perspective. We divide the perception of portraits into three aspects, namely Appearance, Posture, and Emotion, and design corresponding sub-tasks for each aspect. Based on the framework of multi-task learning, portrait interpretation requires a comprehensive description of static attributes and dynamic states of portraits. To invigorate research on this new task, we construct a new dataset that contains 250,000 images labeled with identity, gender, age, physique, height, expression, and posture of the whole body and arms. Our dataset is collected from 51 movies, hence covering extensive diversity. Furthermore, we focus on representation learning for portrait interpretation and propose a baseline that reflects our systematic perspective. We also propose an appropriate metric for this task. Our experimental results demonstrate that combining the tasks related to portrait interpretation can yield benefits. Code and dataset will be made public.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.12882212,
        "newsscientist":0.1549788579,
        "technologyreview":0.2496037636,
        "venturebeat":0.2169230622,
        "wired":0.1885691983,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13315v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658903109000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12571v1",
        "predicted_newsworthiness":0.5117052956,
        "title":"Innovations in Neural Data-to-text Generation",
        "summary":"The neural boom that has sparked natural language processing (NLP) research through the last decade has similarly led to significant innovations in data-to-text generation (DTG). This survey offers a consolidated view into the neural DTG paradigm with a structured examination of the approaches, benchmark datasets, and evaluation protocols. This survey draws boundaries separating DTG from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella. With this holistic view, we highlight promising avenues for DTG research that not only focus on the design of linguistically capable systems but also systems that exhibit fairness and accountability.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.130636573,
        "newsscientist":0.1627560626,
        "technologyreview":0.2845436837,
        "venturebeat":0.2824860868,
        "wired":0.2231290891,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12571v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1658791308000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14017v1",
        "predicted_newsworthiness":0.51154977,
        "title":"Unsupervised Frequent Pattern Mining for CEP",
        "summary":"Complex Event Processing (CEP) is a set of methods that allow efficient knowledge extraction from massive data streams using complex and highly descriptive patterns. Numerous applications, such as online finance, healthcare monitoring and fraud detection use CEP technologies to capture critical alerts, potential threats, or vital notifications in real time. As of today, in many fields, patterns are manually defined by human experts. However, desired patterns often contain convoluted relations that are difficult for humans to detect, and human expertise is scarce in many domains. We present REDEEMER (REinforcement baseD cEp pattErn MinER), a novel reinforcement and active learning approach aimed at mining CEP patterns that allow expansion of the knowledge extracted while reducing the human effort required. This approach includes a novel policy gradient method for vast multivariate spaces and a new way to combine reinforcement and active learning for CEP rule learning while minimizing the number of labels needed for training. REDEEMER aims to enable CEP integration in domains that could not utilize it before. To the best of our knowledge, REDEEMER is the first system that suggests new CEP rules that were not observed beforehand, and is the first method aimed for increasing pattern knowledge in fields where experts do not possess sufficient information required for CEP tools. Our experiments on diverse data-sets demonstrate that REDEEMER is able to extend pattern knowledge while outperforming several state-of-the-art reinforcement learning methods for pattern mining.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1376550709,
        "newsscientist":0.1703173469,
        "technologyreview":0.2747903689,
        "venturebeat":0.3002170839,
        "wired":0.2104566648,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14017v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659007445000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12471v1",
        "predicted_newsworthiness":0.5105566879,
        "title":"Secure Service Implementation with Slice Isolation and WireGuard",
        "summary":"Network slicing enables the provision of services for different verticals over a shared infrastructure. Nevertheless, security is still one of the main challenges when sharing resources. In this paper, we study how WireGuard can provide an encrypted Virtual Private Network (VPN) tunnel as a service between network functions in 5G setting. The open source management and orchestration entity deploys and orchestrates the network functions into network services and slices. We create multiple scenarios emulating a real-life cellular network deploying VPN-as-a-Service between the different network functions to secure and isolate network slices. The performance measurements demonstrate from 0.8 Gbps to 2.5 Gbps throughput and below 1ms delay between network functions using WireGuard. The performance evaluation results are aligned with 5G key performance indicators, making WireGuard suited to provide security in slice isolation in future generations of cellular networks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1363959659,
        "newsscientist":0.124084879,
        "technologyreview":0.2212880144,
        "venturebeat":0.2846002373,
        "wired":0.2259848507,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12471v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658774893000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2208.02205v1",
        "predicted_newsworthiness":0.5105206768,
        "title":"DAHiTrA: Damage Assessment Using a Novel Hierarchical Transformer Architecture",
        "summary":"This paper presents DAHiTrA, a novel deep-learning model with hierarchical transformers to classify building damages based on satellite images in the aftermath of hurricanes. An automated building damage assessment provides critical information for decision making and resource allocation for rapid emergency response. Satellite imagery provides real-time, high-coverage information and offers opportunities to inform large-scale post-disaster building damage assessment. In addition, deep-learning methods have shown to be promising in classifying building damage. In this work, a novel transformer-based network is proposed for assessing building damage. This network leverages hierarchical spatial features of multiple resolutions and captures temporal difference in the feature domain after applying a transformer encoder on the spatial features. The proposed network achieves state-of-the-art-performance when tested on a large-scale disaster damage dataset (xBD) for building localization and damage classification, as well as on LEVIR-CD dataset for change detection tasks. In addition, we introduce a new high-resolution satellite imagery dataset, Ida-BD (related to the 2021 Hurricane Ida in Louisiana in 2021, for domain adaptation to further evaluate the capability of the model to be applied to newly damaged areas with scarce data. The domain adaptation results indicate that the proposed model can be adapted to a new event with only limited fine-tuning. Hence, the proposed model advances the current state of the art through better performance and domain adaptation. Also, Ida-BD provides a higher-resolution annotated dataset for future studies in this field.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.180115397,
        "newsscientist":0.1990865383,
        "technologyreview":0.2602415796,
        "venturebeat":0.2360261227,
        "wired":0.2186781733,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.02205v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659544899000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00775v1",
        "predicted_newsworthiness":0.5104688599,
        "title":"Pavementscapes: a large-scale hierarchical image dataset for asphalt pavement damage segmentation",
        "summary":"Pavement damage segmentation has benefited enormously from deep learning. % and large-scale datasets. However, few current public datasets limit the potential exploration of deep learning in the application of pavement damage segmentation. To address this problem, this study has proposed Pavementscapes, a large-scale dataset to develop and evaluate methods for pavement damage segmentation. Pavementscapes is comprised of 4,000 images with a resolution of $1024 \\times 2048$, which have been recorded in the real-world pavement inspection projects with 15 different pavements. A total of 8,680 damage instances are manually labeled with six damage classes at the pixel level. The statistical study gives a thorough investigation and analysis of the proposed dataset. The numeral experiments propose the top-performing deep neural networks capable of segmenting pavement damages, which provides the baselines of the open challenge for pavement inspection. The experiment results also indicate the existing problems for damage segmentation using deep learning, and this study provides potential solutions.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1187800705,
        "newsscientist":0.1630575986,
        "technologyreview":0.2478408055,
        "venturebeat":0.2274898048,
        "wired":0.1915409748,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00775v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658634027000,
        "published_hr":"Jul 23, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13975v1",
        "predicted_newsworthiness":0.5095939873,
        "title":"On the Effects of Different Types of Label Noise in Multi-Label Remote Sensing Image Classification",
        "summary":"The development of accurate methods for multi-label classification (MLC) of remote sensing (RS) images is one of the most important research topics in RS. To address MLC problems, the use of deep neural networks that require a high number of reliable training images annotated by multiple land-cover class labels (multi-labels) have been found popular in RS. However, collecting such annotations is time-consuming and costly. A common procedure to obtain annotations at zero labeling cost is to rely on thematic products or crowdsourced labels. As a drawback, these procedures come with the risk of label noise that can distort the learning process of the MLC algorithms. In the literature, most label noise robust methods are designed for single label classification (SLC) problems in computer vision (CV), where each image is annotated by a single label. Unlike SLC, label noise in MLC can be associated with: 1) subtractive label-noise (a land cover class label is not assigned to an image while that class is present in the image); 2) additive label-noise (a land cover class label is assigned to an image although that class is not present in the given image); and 3) mixed label-noise (a combination of both). In this paper, we investigate three different noise robust CV SLC methods and adapt them to be robust for multi-label noise scenarios in RS. During experiments we study the effects of different types of multi-label noise and evaluate the adapted methods rigorously. To this end, we also introduce a synthetic multi-label noise injection strategy that is more adequate to simulate operational scenarios compared to the uniform label noise injection strategy, in which the labels of absent and present classes are flipped at uniform probability. Further, we study the relevance of different evaluation metrics in MLC problems under noisy multi-labels.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.124487012,
        "newsscientist":0.1622368029,
        "technologyreview":0.2156710121,
        "venturebeat":0.1860865701,
        "wired":0.1642964181,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13975v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659001110000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14686v1",
        "predicted_newsworthiness":0.509586423,
        "title":"Forensic License Plate Recognition with Compression-Informed Transformers",
        "summary":"Forensic license plate recognition (FLPR) remains an open challenge in legal contexts such as criminal investigations, where unreadable license plates (LPs) need to be deciphered from highly compressed and\/or low resolution footage, e.g., from surveillance cameras. In this work, we propose a side-informed Transformer architecture that embeds knowledge on the input compression level to improve recognition under strong compression. We show the effectiveness of Transformers for license plate recognition (LPR) on a low-quality real-world dataset. We also provide a synthetic dataset that includes strongly degraded, illegible LP images and analyze the impact of knowledge embedding on it. The network outperforms existing FLPR methods and standard state-of-the art image recognition models while requiring less parameters. For the severest degraded images, we can improve recognition by up to 8.9 percent points.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1123341638,
        "newsscientist":0.1538601404,
        "technologyreview":0.2587795132,
        "venturebeat":0.2245435379,
        "wired":0.182898168,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14686v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659103104000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13343v1",
        "predicted_newsworthiness":0.5094683424,
        "title":"Towards Soft Fairness in Restless Multi-Armed Bandits",
        "summary":"Restless multi-armed bandits (RMAB) is a framework for allocating limited resources under uncertainty. It is an extremely useful model for monitoring beneficiaries and executing timely interventions to ensure maximum benefit in public health settings (e.g., ensuring patients take medicines in tuberculosis settings, ensuring pregnant mothers listen to automated calls about good pregnancy practices). Due to the limited resources, typically certain communities or regions are starved of interventions that can have follow-on effects. To avoid starvation in the executed interventions across individuals\/regions\/communities, we first provide a soft fairness constraint and then provide an approach to enforce the soft fairness constraint in RMABs. The soft fairness constraint requires that an algorithm never probabilistically favor one arm over another if the long-term cumulative reward of choosing the latter arm is higher. Our approach incorporates softmax based value iteration method in the RMAB setting to design selection algorithms that manage to satisfy the proposed fairness constraint. Our method, referred to as SoftFair, also provides theoretical performance guarantees and is asymptotically optimal. Finally, we demonstrate the utility of our approaches on simulated benchmarks and show that the soft fairness constraint can be handled without a significant sacrifice on value.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1777799978,
        "newsscientist":0.1663407018,
        "technologyreview":0.216453469,
        "venturebeat":0.2016760197,
        "wired":0.1660216299,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13343v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai",
            "cs.cy"
        ],
        "published":1658908592000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13791v1",
        "predicted_newsworthiness":0.5092620868,
        "title":"Learning to Assess Danger from Movies for Cooperative Escape Planning in Hazardous Environments",
        "summary":"There has been a plethora of work towards improving robot perception and navigation, yet their application in hazardous environments, like during a fire or an earthquake, is still at a nascent stage. We hypothesize two key challenges here: first, it is difficult to replicate such scenarios in the real world, which is necessary for training and testing purposes. Second, current systems are not fully able to take advantage of the rich multi-modal data available in such hazardous environments. To address the first challenge, we propose to harness the enormous amount of visual content available in the form of movies and TV shows, and develop a dataset that can represent hazardous environments encountered in the real world. The data is annotated with high-level danger ratings for realistic disaster images, and corresponding keywords are provided that summarize the content of the scene. In response to the second challenge, we propose a multi-modal danger estimation pipeline for collaborative human-robot escape scenarios. Our Bayesian framework improves danger estimation by fusing information from robot's camera sensor and language inputs from the human. Furthermore, we augment the estimation module with a risk-aware planner that helps in identifying safer paths out of the dangerous environment. Through extensive simulations, we exhibit the advantages of our multi-modal perception framework that gets translated into tangible benefits such as higher success rate in a collaborative human-robot mission.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1731658809,
        "newsscientist":0.2128647533,
        "technologyreview":0.2766727407,
        "venturebeat":0.2535707246,
        "wired":0.2477685787,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13791v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.cv",
            "cs.hc",
            "cs.lg"
        ],
        "published":1658956035000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.12544v1",
        "predicted_newsworthiness":0.5092567966,
        "title":"End-User Puppeteering of Expressive Movements",
        "summary":"The end-user programming of social robot behavior is usually limited by a predefined set of movements. We are proposing a puppeteering robotic interface that provides a more intuitive method of programming robot expressive movements. As the user manipulates the puppet of a robot, the actual robot replicates the movements, providing real-time visual feedback. Through this proposed interface, even with limited training, a novice user can design and program expressive movements efficiently. We present our preliminary user study results in this extended abstract.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1373422117,
        "newsscientist":0.2076588488,
        "technologyreview":0.2845459625,
        "venturebeat":0.2620222145,
        "wired":0.2388949757,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12544v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.hc"
        ],
        "published":1658785146000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01368v1",
        "predicted_newsworthiness":0.5090979892,
        "title":"PyABSA: Open Framework for Aspect-based Sentiment Analysis",
        "summary":"Aspect-based sentiment analysis (ABSA) has become a prevalent task in recent years. However, the absence of a unified framework in the present ABSA research makes it challenging to compare different models' performance fairly. Therefore, we created an open-source ABSA framework, namely PYABSA. Besides, previous efforts usually neglect the precursor aspect term extraction (ASC) subtask and focus on the aspect sentiment classification (ATE) subtask. Compared to previous works, PYABSA includes the features of aspect term extraction, aspect sentiment classification, and text classification, while multiple ABSA subtasks can be adapted to PYABSA owing to its modular architecture. To facilitate ABSA applications, PYABSAseamless integrates multilingual modelling, automated dataset annotation, etc., which are helpful in deploying ABSA services. In ASC and ATE, PYABSA provides up to 33 and 7 built-in models, respectively, while all the models provide quick training and instant inference. Besides, PYABSA contains 180K+ ABSA instances from 21 augmented ABSA datasets for applications and studies. PyABSA is available at https:\/\/github.com\/yangheng95\/PyABSA",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1412473523,
        "newsscientist":0.1310567671,
        "technologyreview":0.2180526106,
        "venturebeat":0.2363001881,
        "wired":0.1992751088,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01368v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659439656000,
        "published_hr":"Aug 02, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.11936v1",
        "predicted_newsworthiness":0.5088753881,
        "title":"Cloud-native 5G experimental platform with over-the-air transmissions and end-to-end monitoring",
        "summary":"5G represents a revolutionary shift with respect to previous generations given its design centered on network softwarization. Within such a change of paradigm, cloud-native solutions are widely regarded as the future of vertical application development because of their enhanced flexibility and adaptability to complex and dynamic scenarios. In this context, we present an experimental framework with over-the-air transmissions that tackles two critical aspects for enhancing the lifecycle management of 5G and beyond networks: cloud-native deployments of 5G core network functions (NFs) and end-to-end monitoring. First, we deploy Open5GS and Prometheus-based monitoring as containerized network functions (CNFs) in a Kubernetes cluster spanning a multi-tier network with a multi-access edge computing (MEC) host. We then demonstrate the end-to-end monitoring system by showcasing via Grafana dashboards both infrastructure resources and radio metrics of two scenarios; one devoted to user plane function (UPF) re-selection and the other to user mobility.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1103500795,
        "newsscientist":0.131547968,
        "technologyreview":0.2077998377,
        "venturebeat":0.2951505188,
        "wired":0.221157593,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11936v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658732465000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.13131v1",
        "predicted_newsworthiness":0.5087998164,
        "title":"Semi-analytical Industrial Cooling System Model for Reinforcement Learning",
        "summary":"We present a hybrid industrial cooling system model that embeds analytical solutions within a multi-physics simulation. This model is designed for reinforcement learning (RL) applications and balances simplicity with simulation fidelity and interpretability. The model's fidelity is evaluated against real world data from a large scale cooling system. This is followed by a case study illustrating how the model can be used for RL research. For this, we develop an industrial task suite that allows specifying different problem settings and levels of complexity, and use it to evaluate the performance of different RL algorithms.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1275516826,
        "newsscientist":0.1670056027,
        "technologyreview":0.2613134546,
        "venturebeat":0.2427068561,
        "wired":0.1823463268,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13131v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.lg",
            "cs.ro"
        ],
        "published":1658859557000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.12267v1",
        "predicted_newsworthiness":0.5079661877,
        "title":"Continuous ErrP detections during multimodal human-robot interaction",
        "summary":"Human-in-the-loop approaches are of great importance for robot applications. In the presented study, we implemented a multimodal human-robot interaction (HRI) scenario, in which a simulated robot communicates with its human partner through speech and gestures. The robot announces its intention verbally and selects the appropriate action using pointing gestures. The human partner, in turn, evaluates whether the robot's verbal announcement (intention) matches the action (pointing gesture) chosen by the robot. For cases where the verbal announcement of the robot does not match the corresponding action choice of the robot, we expect error-related potentials (ErrPs) in the human electroencephalogram (EEG). These intrinsic evaluations of robot actions by humans, evident in the EEG, were recorded in real time, continuously segmented online and classified asynchronously. For feature selection, we propose an approach that allows the combinations of forward and backward sliding windows to train a classifier. We achieved an average classification performance of 91% across 9 subjects. As expected, we also observed a relatively high variability between the subjects. In the future, the proposed feature selection approach will be extended to allow for customization of feature selection. To this end, the best combinations of forward and backward sliding windows will be automatically selected to account for inter-subject variability in classification performance. In addition, we plan to use the intrinsic human error evaluation evident in the error case by the ErrP in interactive reinforcement learning to improve multimodal human-robot interaction.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.12368053,
        "newsscientist":0.2008340435,
        "technologyreview":0.2695984032,
        "venturebeat":0.242006659,
        "wired":0.2051761619,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12267v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro",
            "cs.hc",
            "cs.lg"
        ],
        "published":1658763572000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2207.11754v1",
        "predicted_newsworthiness":0.5078442625,
        "title":"Virtual Reality Therapy for the Psychological Well-being of Palliative Care Patients in Hong Kong",
        "summary":"In this paper we introduce novel Virtual Reality (VR) and Augmented Reality (AR) treatments to improve the psychological well being of patients in palliative care, based on interviews with a clinical psychologist who has successfully implemented VR assisted interventions on palliative care patients in the Hong Kong hospital system. Our VR and AR assisted interventions are adaptations of traditional palliative care therapies which simultaneously facilitate patients communication with family and friends while isolated in hospital due to physical weakness and COVID-19 related restrictions. The first system we propose is a networked, metaverse platform for palliative care patients to create customized virtual environments with therapists, family and friends which function as immersive and collaborative versions of 'life review' and 'reminiscence therapy'. The second proposed system will investigate the use of Mixed Reality telepresence and haptic touch in an AR environment, which will allow palliative care patients to physically feel friends and family in a virtual space, adding to the sense of presence and immersion in that environment.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.2384737461,
        "newsscientist":0.2542823617,
        "technologyreview":0.2990903702,
        "venturebeat":0.3733186473,
        "wired":0.3010346386,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11754v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1658673112000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.12114v1",
        "predicted_newsworthiness":0.5077495458,
        "title":"When Virtual Reality Meets Rate Splitting Multiple Access: A Joint Communication and Computation Approach",
        "summary":"Rate Splitting Multiple Access (RSMA) has emerged as an effective interference management scheme for applications that require high data rates. Although RSMA has shown advantages in rate enhancement and spectral efficiency, it has yet not to be ready for latency-sensitive applications such as virtual reality streaming, which is an essential building block of future 6G networks. Unlike conventional High-Definition streaming applications, streaming virtual reality applications requires not only stringent latency requirements but also the computation capability of the transmitter to quickly respond to dynamic users' demands. Thus, conventional RSMA approaches usually fail to address the challenges caused by computational demands at the transmitter, let alone the dynamic nature of the virtual reality streaming applications. To overcome the aforementioned challenges, we first formulate the virtual reality streaming problem assisted by RSMA as a joint communication and computation optimization problem. A novel multicast approach is then proposed to cluster users into different groups based on a Field-of-View metric and transmit multicast streams in a hierarchical manner. After that, we propose a deep reinforcement learning approach to obtain the solution for the optimization problem. Extensive simulations show that our framework can achieve the millisecond-latency requirement, which is much lower than other baseline schemes.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0886115882,
        "newsscientist":0.1343077829,
        "technologyreview":0.2076965095,
        "venturebeat":0.2943850229,
        "wired":0.2043308761,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12114v1",
        "arxiv_primary_category":"cs.ni",
        "arxiv_all_categories":[
            "cs.ni"
        ],
        "published":1658751906000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Networking and Internet Architecture"
    },
    {
        "arxiv_id":"2207.12615v1",
        "predicted_newsworthiness":0.5076654999,
        "title":"Exploring the Design of Adaptation Protocols for Improved Generalization and Machine Learning Safety",
        "summary":"While directly fine-tuning (FT) large-scale, pretrained models on task-specific data is well-known to induce strong in-distribution task performance, recent works have demonstrated that different adaptation protocols, such as linear probing (LP) prior to FT, can improve out-of-distribution generalization. However, the design space of such adaptation protocols remains under-explored and the evaluation of such protocols has primarily focused on distribution shifts. Therefore, in this work, we evaluate common adaptation protocols across distributions shifts and machine learning safety metrics (e.g., anomaly detection, calibration, robustness to corruptions). We find that protocols induce disparate trade-offs that were not apparent from prior evaluation. Further, we demonstrate that appropriate pairing of data augmentation and protocol can substantially mitigate this trade-off. Finally, we hypothesize and empirically see that using hardness-promoting augmentations during LP and then FT with augmentations may be particularly effective for trade-off mitigation.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1167557379,
        "newsscientist":0.1716535758,
        "technologyreview":0.3136871296,
        "venturebeat":0.2871525122,
        "wired":0.2083230214,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12615v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658802784000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12259v1",
        "predicted_newsworthiness":0.5074911001,
        "title":"Surrogate Modeling of Melt Pool Thermal Field using Deep Learning",
        "summary":"Powder-based additive manufacturing has transformed the manufacturing industry over the last decade. In Laser Powder Bed Fusion, a specific part is built in an iterative manner in which two-dimensional cross-sections are formed on top of each other by melting and fusing the proper areas of the powder bed. In this process, the behavior of the melt pool and its thermal field has a very important role in predicting the quality of the manufactured part and its possible defects. However, the simulation of such a complex phenomenon is usually very time-consuming and requires huge computational resources. Flow-3D is one of the software packages capable of executing such simulations using iterative numerical solvers. In this work, we create three datasets of single-trail processes using Flow-3D and use them to train a convolutional neural network capable of predicting the behavior of the three-dimensional thermal field of the melt pool solely by taking three parameters as input: laser power, laser velocity, and time step. The CNN achieves a relative Root Mean Squared Error of 2% to 3% for the temperature field and an average Intersection over Union score of 80% to 90% in predicting the melt pool area. Moreover, since time is included as one of the inputs of the model, the thermal field can be instantly obtained for any arbitrary time step without the need to iterate and compute all the steps",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0796687541,
        "newsscientist":0.1465325791,
        "technologyreview":0.2380804786,
        "venturebeat":0.214044609,
        "wired":0.1523019636,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12259v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658762836000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12817v1",
        "predicted_newsworthiness":0.5074496672,
        "title":"Bodily Behaviors in Social Interaction: Novel Annotations and State-of-the-Art Evaluation",
        "summary":"Body language is an eye-catching social signal and its automatic analysis can significantly advance artificial intelligence systems to understand and actively participate in social interactions. While computer vision has made impressive progress in low-level tasks like head and body pose estimation, the detection of more subtle behaviors such as gesturing, grooming, or fumbling is not well explored. In this paper we present BBSI, the first set of annotations of complex Bodily Behaviors embedded in continuous Social Interactions in a group setting. Based on previous work in psychology, we manually annotated 26 hours of spontaneous human behavior in the MPIIGroupInteraction dataset with 15 distinct body language classes. We present comprehensive descriptive statistics on the resulting dataset as well as results of annotation quality evaluations. For automatic detection of these behaviors, we adapt the Pyramid Dilated Attention Network (PDAN), a state-of-the-art approach for human action detection. We perform experiments using four variants of spatial-temporal features as input to PDAN: Two-Stream Inflated 3D CNN, Temporal Segment Networks, Temporal Shift Module and Swin Transformer. Results are promising and indicate a great room for improvement in this difficult task. Representing a key piece in the puzzle towards automatic understanding of social behavior, BBSI is fully available to the research community.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1507807136,
        "newsscientist":0.1888199572,
        "technologyreview":0.2399696645,
        "venturebeat":0.2240989476,
        "wired":0.2007354337,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12817v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658834640000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12576v1",
        "predicted_newsworthiness":0.5070097774,
        "title":"WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models",
        "summary":"While vision-and-language models perform well on tasks such as visual question answering, they struggle when it comes to basic human commonsense reasoning skills. In this work, we introduce WinoGAViL: an online game to collect vision-and-language associations, (e.g., werewolves to a full moon), used as a dynamic benchmark to evaluate state-of-the-art models. Inspired by the popular card game Codenames, a spymaster gives a textual cue related to several visual candidates, and another player has to identify them. Human players are rewarded for creating associations that are challenging for a rival AI model but still solvable by other human players. We use the game to collect 3.5K instances, finding that they are intuitive for humans (>90% Jaccard index) but challenging for state-of-the-art AI models, where the best model (ViLT) achieves a score of 52%, succeeding mostly where the cue is visually salient. Our analysis as well as the feedback we collect from players indicate that the collected associations require diverse reasoning skills, including general knowledge, common sense, abstraction, and more. We release the dataset, the code and the interactive game, aiming to allow future data collection that can be used to develop models with better association abilities.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1689371934,
        "newsscientist":0.2403265841,
        "technologyreview":0.3609131936,
        "venturebeat":0.3394299081,
        "wired":0.280492619,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12576v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl",
            "cs.ai",
            "cs.cv",
            "cs.hc"
        ],
        "published":1658793464000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.14723v1",
        "predicted_newsworthiness":0.5064149622,
        "title":"Meta Reinforcement Learning with Successor Feature Based Context",
        "summary":"Most reinforcement learning (RL) methods only focus on learning a single task from scratch and are not able to use prior knowledge to learn other tasks more effectively. Context-based meta RL techniques are recently proposed as a possible solution to tackle this. However, they are usually less efficient than conventional RL and may require many trial-and-errors during training. To address this, we propose a novel meta-RL approach that achieves competitive performance comparing to existing meta-RL algorithms, while requires significantly fewer environmental interactions. By combining context variables with the idea of decomposing reward in successor feature framework, our method does not only learn high-quality policies for multiple tasks simultaneously but also can quickly adapt to new tasks with a small amount of training. Compared with state-of-the-art meta-RL baselines, we empirically show the effectiveness and data efficiency of our method on several continuous control tasks.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1005921355,
        "newsscientist":0.1394935514,
        "technologyreview":0.250343718,
        "venturebeat":0.2288813407,
        "wired":0.1644699259,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14723v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ai"
        ],
        "published":1659106367000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.13398v1",
        "predicted_newsworthiness":0.5063116064,
        "title":"Emergent social NPC interactions in the Social NPCs Skyrim mod and beyond",
        "summary":"This work presents an implementation of a social architecture model for authoring Non-Player Character (NPC) in open world games inspired in academic research on agentbased modeling. Believable NPC authoring is burdensome in terms of rich dialogue and responsive behaviors. We briefly present the characteristics and advantages of using a social agent architecture for this task and describe an implementation of a social agent architecture CiF-CK released as a mod Social NPCs for The Elder Scrolls V: Skyrim",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.181415313,
        "newsscientist":0.1943178989,
        "technologyreview":0.2767890024,
        "venturebeat":0.3004958508,
        "wired":0.2815830041,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13398v1",
        "arxiv_primary_category":"cs.ai",
        "arxiv_all_categories":[
            "cs.ai",
            "cs.hc"
        ],
        "published":1658914223000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Artificial Intelligence"
    },
    {
        "arxiv_id":"2207.12112v1",
        "predicted_newsworthiness":0.5059738604,
        "title":"Active Learning Strategies for Weakly-supervised Object Detection",
        "summary":"Object detectors trained with weak annotations are affordable alternatives to fully-supervised counterparts. However, there is still a significant performance gap between them. We propose to narrow this gap by fine-tuning a base pre-trained weakly-supervised detector with a few fully-annotated samples automatically selected from the training set using ``box-in-box'' (BiB), a novel active learning strategy designed specifically to address the well-documented failure modes of weakly-supervised detectors. Experiments on the VOC07 and COCO benchmarks show that BiB outperforms other active learning techniques and significantly improves the base weakly-supervised detector's performance with only a few fully-annotated images per class. BiB reaches 97% of the performance of fully-supervised Fast RCNN with only 10% of fully-annotated images on VOC07. On COCO, using on average 10 fully-annotated images per class, or equivalently 1% of the training set, BiB also reduces the performance gap (in AP) between the weakly-supervised detector and the fully-supervised Fast RCNN by over 70%, showing a good trade-off between performance and data efficiency. Our code is publicly available at https:\/\/github.com\/huyvvo\/BiB.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0894791468,
        "newsscientist":0.1404847196,
        "technologyreview":0.2192275854,
        "venturebeat":0.189742361,
        "wired":0.1465656323,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12112v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658751721000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11720v1",
        "predicted_newsworthiness":0.5053991418,
        "title":"Progressive Feature Learning for Realistic Cloth-Changing Gait Recognition",
        "summary":"Gait recognition is instrumental in crime prevention and social security, for it can be conducted at a long distance without the cooperation of subjects. However, existing datasets and methods cannot deal with the most challenging problem in realistic gait recognition effectively: walking in different clothes (CL). In order to tackle this problem, we propose two benchmarks: CASIA-BN-RCC and OUMVLP-RCC, to simulate the cloth-changing condition in practice. The two benchmarks can force the algorithm to realize cross-view and cross-cloth with two sub-datasets. Furthermore, we propose a new framework that can be applied with off-the-shelf backbones to improve its performance in the Realistic Cloth-Changing problem with Progressive Feature Learning. Specifically, in our framework, we design Progressive Mapping and Progressive Uncertainty to extract the cross-view features and then extract cross-cloth features on the basis. In this way, the features from the cross-view sub-dataset can first dominate the feature space and relieve the uneven distribution caused by the adverse effect from the cross-cloth sub-dataset. The experiments on our benchmarks show that our framework can effectively improve the recognition performance in CL conditions. Our codes and datasets will be released after accepted.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1114397153,
        "newsscientist":0.1505566664,
        "technologyreview":0.2079549165,
        "venturebeat":0.1920810299,
        "wired":0.170617697,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11720v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1658662013000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12062v1",
        "predicted_newsworthiness":0.5053189787,
        "title":"Meta Neural Ordinary Differential Equations For Adaptive Asynchronous Control",
        "summary":"Model-based Reinforcement Learning and Control have demonstrated great potential in various sequential decision making problem domains, including in robotics settings. However, real-world robotics systems often present challenges that limit the applicability of those methods. In particular, we note two problems that jointly happen in many industrial systems: 1) Irregular\/asynchronous observations and actions and 2) Dramatic changes in environment dynamics from an episode to another (e.g. varying payload inertial properties). We propose a general framework that overcomes those difficulties by meta-learning adaptive dynamics models for continuous-time prediction and control. We evaluate the proposed approach on a simulated industrial robot. Evaluations on real robotic systems will be added in future iterations of this pre-print.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1035619786,
        "newsscientist":0.1541085167,
        "technologyreview":0.2603095715,
        "venturebeat":0.2182621635,
        "wired":0.1918280374,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12062v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg",
            "cs.ro"
        ],
        "published":1658747249000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.14096v2",
        "predicted_newsworthiness":0.5052457696,
        "title":"Towards Large-Scale Small Object Detection: Survey and Benchmarks",
        "summary":"With the rise of deep convolutional neural networks, object detection has achieved prominent advances in past years. However, such prosperity could not camouflage the unsatisfactory situation of Small Object Detection (SOD), one of the notoriously challenging tasks in computer vision, owing to the poor visual appearance and noisy representation caused by the intrinsic structure of small targets. In addition, large-scale dataset for benchmarking small object detection methods remains a bottleneck. In this paper, we first conduct a thorough review of small object detection. Then, to catalyze the development of SOD, we construct two large-scale Small Object Detection dAtasets (SODA), SODA-D and SODA-A, which focus on the Driving and Aerial scenarios respectively. SODA-D includes 24704 high-quality traffic images and 277596 instances of 9 categories. For SODA-A, we harvest 2510 high-resolution aerial images and annotate 800203 instances over 9 classes. The proposed datasets, as we know, are the first-ever attempt to large-scale benchmarks with a vast collection of exhaustively annotated instances tailored for multi-category SOD. Finally, we evaluate the performance of mainstream methods on SODA. We expect the released benchmarks could facilitate the development of SOD and spawn more breakthroughs in this field. Datasets and codes will be available soon at: \\url{https:\/\/shaunyuan22.github.io\/SODA}.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1111033019,
        "newsscientist":0.1810206868,
        "technologyreview":0.2614363397,
        "venturebeat":0.2426001522,
        "wired":0.2171824317,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14096v2",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659016938000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.01166v1",
        "predicted_newsworthiness":0.5052124475,
        "title":"Ithaca365: Dataset and Driving Perception under Repeated and Challenging Weather Conditions",
        "summary":"Advances in perception for self-driving cars have accelerated in recent years due to the availability of large-scale datasets, typically collected at specific locations and under nice weather conditions. Yet, to achieve the high safety requirement, these perceptual systems must operate robustly under a wide variety of weather conditions including snow and rain. In this paper, we present a new dataset to enable robust autonomous driving via a novel data collection process - data is repeatedly recorded along a 15 km route under diverse scene (urban, highway, rural, campus), weather (snow, rain, sun), time (day\/night), and traffic conditions (pedestrians, cyclists and cars). The dataset includes images and point clouds from cameras and LiDAR sensors, along with high-precision GPS\/INS to establish correspondence across routes. The dataset includes road and object annotations using amodal masks to capture partial occlusions and 3D bounding boxes. We demonstrate the uniqueness of this dataset by analyzing the performance of baselines in amodal segmentation of road and objects, depth estimation, and 3D object detection. The repeated routes opens new research directions in object discovery, continual learning, and anomaly detection. Link to Ithaca365: https:\/\/ithaca365.mae.cornell.edu\/",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1370173594,
        "newsscientist":0.1961252331,
        "technologyreview":0.2993014572,
        "venturebeat":0.283778556,
        "wired":0.2557012645,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01166v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659394532000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.11769v1",
        "predicted_newsworthiness":0.5046702354,
        "title":"CODiT: Conformal Out-of-Distribution Detection in Time-Series Data",
        "summary":"Machine learning models are prone to making incorrect predictions on inputs that are far from the training distribution. This hinders their deployment in safety-critical applications such as autonomous vehicles and healthcare. The detection of a shift from the training distribution of individual datapoints has gained attention. A number of techniques have been proposed for such out-of-distribution (OOD) detection. But in many applications, the inputs to a machine learning model form a temporal sequence. Existing techniques for OOD detection in time-series data either do not exploit temporal relationships in the sequence or do not provide any guarantees on detection. We propose using deviation from the in-distribution temporal equivariance as the non-conformity measure in conformal anomaly detection framework for OOD detection in time-series data.Computing independent predictions from multiple conformal detectors based on the proposed measure and combining these predictions by Fisher's method leads to the proposed detector CODiT with guarantees on false detection in time-series data. We illustrate the efficacy of CODiT by achieving state-of-the-art results on computer vision datasets in autonomous driving. We also show that CODiT can be used for OOD detection in non-vision datasets by performing experiments on the physiological GAIT sensory dataset. Code, data, and trained models are available at https:\/\/github.com\/kaustubhsridhar\/time-series-OOD.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1130337519,
        "newsscientist":0.1757153079,
        "technologyreview":0.2538479104,
        "venturebeat":0.2417850349,
        "wired":0.1933009103,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.11769v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658680874000,
        "published_hr":"Jul 24, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2207.12165v1",
        "predicted_newsworthiness":0.504548075,
        "title":"dCAM: Dimension-wise Class Activation Map for Explaining Multivariate Data Series Classification",
        "summary":"Data series classification is an important and challenging problem in data science. Explaining the classification decisions by finding the discriminant parts of the input that led the algorithm to some decisions is a real need in many applications. Convolutional neural networks perform well for the data series classification task; though, the explanations provided by this type of algorithm are poor for the specific case of multivariate data series. Addressing this important limitation is a significant challenge. In this paper, we propose a novel method that solves this problem by highlighting both the temporal and dimensional discriminant information. Our contribution is two-fold: we first describe a convolutional architecture that enables the comparison of dimensions; then, we propose a method that returns dCAM, a Dimension-wise Class Activation Map specifically designed for multivariate time series (and CNN-based models). Experiments with several synthetic and real datasets demonstrate that dCAM is not only more accurate than previous approaches, but the only viable solution for discriminant feature discovery and classification explanation in multivariate time series. This paper has appeared in SIGMOD'22.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.128170612,
        "newsscientist":0.1712743861,
        "technologyreview":0.2632004913,
        "venturebeat":0.2567300006,
        "wired":0.1792194091,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12165v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1658754245000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01084v1",
        "predicted_newsworthiness":0.5033644779,
        "title":"Robotic Interestingness via Human-Informed Few-Shot Object Detection",
        "summary":"Interestingness recognition is crucial for decision making in autonomous exploration for mobile robots. Previous methods proposed an unsupervised online learning approach that can adapt to environments and detect interesting scenes quickly, but lack the ability to adapt to human-informed interesting objects. To solve this problem, we introduce a human-interactive framework, AirInteraction, that can detect human-informed objects via few-shot online learning. To reduce the communication bandwidth, we first apply an online unsupervised learning algorithm on the unmanned vehicle for interestingness recognition and then only send the potential interesting scenes to a base-station for human inspection. The human operator is able to draw and provide bounding box annotations for particular interesting objects, which are sent back to the robot to detect similar objects via few-shot learning. Only using few human-labeled examples, the robot can learn novel interesting object categories during the mission and detect interesting scenes that contain the objects. We evaluate our method on various interesting scene recognition datasets. To the best of our knowledge, it is the first human-informed few-shot object detection framework for autonomous exploration.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1241544361,
        "newsscientist":0.2246804613,
        "technologyreview":0.3045650446,
        "venturebeat":0.2804564244,
        "wired":0.2623137821,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01084v1",
        "arxiv_primary_category":"cs.ro",
        "arxiv_all_categories":[
            "cs.ro"
        ],
        "published":1659378981000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Robotics"
    },
    {
        "arxiv_id":"2208.01910v1",
        "predicted_newsworthiness":0.5030101396,
        "title":"Multimodal Generation of Novel Action Appearances for Synthetic-to-Real Recognition of Activities of Daily Living",
        "summary":"Domain shifts, such as appearance changes, are a key challenge in real-world applications of activity recognition models, which range from assistive robotics and smart homes to driver observation in intelligent vehicles. For example, while simulations are an excellent way of economical data collection, a Synthetic-to-Real domain shift leads to a > 60% drop in accuracy when recognizing activities of Daily Living (ADLs). We tackle this challenge and introduce an activity domain generation framework which creates novel ADL appearances (novel domains) from different existing activity modalities (source domains) inferred from video training data. Our framework computes human poses, heatmaps of body joints, and optical flow maps and uses them alongside the original RGB videos to learn the essence of source domains in order to generate completely new ADL domains. The model is optimized by maximizing the distance between the existing source appearances and the generated novel appearances while ensuring that the semantics of an activity is preserved through an additional classification loss. While source data multimodality is an important concept in this design, our setup does not rely on multi-sensor setups, (i.e., all source modalities are inferred from a single video only.) The newly created activity domains are then integrated in the training of the ADL classification networks, resulting in models far less susceptible to changes in data distributions. Extensive experiments on the Synthetic-to-Real benchmark Sims4Action demonstrate the potential of the domain generation paradigm for cross-domain ADL recognition, setting new state-of-the-art results. Our code is publicly available at https:\/\/github.com\/Zrrr1997\/syn2real_DG",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1192711568,
        "newsscientist":0.1749133126,
        "technologyreview":0.2478732547,
        "venturebeat":0.2339122669,
        "wired":0.2035922627,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01910v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659515313000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00344v1",
        "predicted_newsworthiness":0.5024508613,
        "title":"Towards Intercultural Affect Recognition: Audio-Visual Affect Recognition in the Wild Across Six Cultures",
        "summary":"In our multicultural world, affect-aware AI systems that support humans need the ability to perceive affect across variations in emotion expression patterns across cultures. These models must perform well in cultural contexts on which they have not been trained. A standard assumption in affective computing is that affect recognition models trained and used within the same culture (intracultural) will perform better than models trained on one culture and used on different cultures (intercultural). We test this assumption and present the first systematic study of intercultural affect recognition models using videos of real-world dyadic interactions from six cultures. We develop an attention-based feature selection approach under temporal causal discovery to identify behavioral cues that can be leveraged in intercultural affect recognition models. Across all six cultures, our findings demonstrate that intercultural affect recognition models were as effective or more effective than intracultural models. We identify and contribute useful behavioral features for intercultural affect recognition; facial features from the visual modality were more useful than the audio modality in this study's context. Our paper presents a proof-of-concept and motivation for the future development of intercultural affect recognition systems.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1849243161,
        "newsscientist":0.206333183,
        "technologyreview":0.2825213408,
        "venturebeat":0.2699810044,
        "wired":0.2324288074,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00344v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.hc",
            "cs.lg"
        ],
        "published":1659235157000,
        "published_hr":"Jul 30, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12720v1",
        "predicted_newsworthiness":0.5021041186,
        "title":"Convolutional neural networks and multi-threshold analysis for contamination detection in the apparel industry",
        "summary":"Quality control of apparel items is mandatory in modern textile industry, as consumer's awareness and expectations about the highest possible standard is constantly increasing in favor of sustainable and ethical textile products. Such a level of quality is achieved by checking the product throughout its life cycle, from raw materials to boxed stock. Checks may include color shading tests, fasteners fatigue tests, fabric weigh tests, contamination tests, etc. This work deals specifically with the automatic detection of contaminations given by small parts in the finished product such as raw material like little stones and plastic bits or materials from the construction process, like a whole needle or a clip. Identification is performed by a two-level processing of X-ray images of the items: in the first, a multi-threshold analysis recognizes the contaminations by gray level and shape attributes; the second level consists of a deep learning classifier that has been trained to distinguish between true positives and false positives. The automatic detector was successfully deployed in an actual production plant, since the results satisfy the technical specification of the process, namely a number of false negatives smaller than 3% and a number of false positives smaller than 15%.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1549100991,
        "newsscientist":0.195151778,
        "technologyreview":0.250476699,
        "venturebeat":0.2247521406,
        "wired":0.1694311781,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12720v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658823701000,
        "published_hr":"Jul 26, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.13362v1",
        "predicted_newsworthiness":0.5018770957,
        "title":"Camouflaged Object Detection via Context-aware Cross-level Fusion",
        "summary":"Camouflaged object detection (COD) aims to identify the objects that conceal themselves in natural scenes. Accurate COD suffers from a number of challenges associated with low boundary contrast and the large variation of object appearances, e.g., object size and shape. To address these challenges, we propose a novel Context-aware Cross-level Fusion Network (C2F-Net), which fuses context-aware cross-level features for accurately identifying camouflaged objects. Specifically, we compute informative attention coefficients from multi-level features with our Attention-induced Cross-level Fusion Module (ACFM), which further integrates the features under the guidance of attention coefficients. We then propose a Dual-branch Global Context Module (DGCM) to refine the fused features for informative feature representations by exploiting rich global context information. Multiple ACFMs and DGCMs are integrated in a cascaded manner for generating a coarse prediction from high-level features. The coarse prediction acts as an attention map to refine the low-level features before passing them to our Camouflage Inference Module (CIM) to generate the final prediction. We perform extensive experiments on three widely used benchmark datasets and compare C2F-Net with state-of-the-art (SOTA) models. The results show that C2F-Net is an effective COD model and outperforms SOTA models remarkably. Further, an evaluation on polyp segmentation datasets demonstrates the promising potentials of our C2F-Net in COD downstream applications. Our code is publicly available at: https:\/\/github.com\/Ben57882\/C2FNet-TSCVT.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0923098315,
        "newsscientist":0.1518222044,
        "technologyreview":0.2360991238,
        "venturebeat":0.2066477458,
        "wired":0.1675864055,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13362v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658910856000,
        "published_hr":"Jul 27, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.12080v1",
        "predicted_newsworthiness":0.5016476869,
        "title":"Intention-Conditioned Long-Term Human Egocentric Action Forecasting @ EGO4D Challenge 2022",
        "summary":"To anticipate how a human would act in the future, it is essential to understand the human intention since it guides the human towards a certain goal. In this paper, we propose a hierarchical architecture which assumes a sequence of human action (low-level) can be driven from the human intention (high-level). Based on this, we deal with Long-Term Action Anticipation task in egocentric videos. Our framework first extracts two level of human information over the N observed videos human actions through a Hierarchical Multi-task MLP Mixer (H3M). Then, we condition the uncertainty of the future through an Intention-Conditioned Variational Auto-Encoder (I-CVAE) that generates K stable predictions of the next Z=20 actions that the observed human might perform. By leveraging human intention as high-level information, we claim that our model is able to anticipate more time-consistent actions in the long-term, thus improving the results over baseline methods in EGO4D Challenge. This work ranked first in the EGO4D LTA Challenge by providing more plausible anticipated sequences, improving the anticipation of nouns and overall actions. The code is available at https:\/\/github.com\/Evm7\/ego4dlta-icvae.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1198288235,
        "newsscientist":0.1830629884,
        "technologyreview":0.2451412582,
        "venturebeat":0.2272465362,
        "wired":0.2052798394,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.12080v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1658750221000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14315v1",
        "predicted_newsworthiness":0.5012964665,
        "title":"SPot-the-Difference Self-Supervised Pre-training for Anomaly Detection and Segmentation",
        "summary":"Visual anomaly detection is commonly used in industrial quality inspection. In this paper, we present a new dataset as well as a new self-supervised learning method for ImageNet pre-training to improve anomaly detection and segmentation in 1-class and 2-class 5\/10\/high-shot training setups. We release the Visual Anomaly (VisA) Dataset consisting of 10,821 high-resolution color images (9,621 normal and 1,200 anomalous samples) covering 12 objects in 3 domains, making it the largest industrial anomaly detection dataset to date. Both image and pixel-level labels are provided. We also propose a new self-supervised framework - SPot-the-difference (SPD) - which can regularize contrastive self-supervised pre-training, such as SimSiam, MoCo and SimCLR, to be more suitable for anomaly detection tasks. Our experiments on VisA and MVTec-AD dataset show that SPD consistently improves these contrastive pre-training baselines and even the supervised pre-training. For example, SPD improves Area Under the Precision-Recall curve (AU-PR) for anomaly segmentation by 5.9% and 6.8% over SimSiam and supervised pre-training respectively in the 2-class high-shot regime. We open-source the project at http:\/\/github.com\/amazon-research\/spot-diff .",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1022035577,
        "newsscientist":0.1622593234,
        "technologyreview":0.2526241709,
        "venturebeat":0.2325645297,
        "wired":0.1684471474,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14315v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659031203000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00102v1",
        "predicted_newsworthiness":0.5011323806,
        "title":"An Open Source Interactive Visual Analytics Tool for Comparative Programming Comprehension",
        "summary":"This paper proposes an open source visual analytics tool consisting of several views and perspectives on eye movement data collected during code reading tasks when writing computer programs. Hence the focus of this work is on code and program comprehension. The source code is shown as a visual stimulus. It can be inspected in combination with overlaid scanpaths in which the saccades can be visually encoded in several forms, including straight, curved, and orthogonal lines, modifiable by interaction techniques. The tool supports interaction techniques like filter functions, aggregations, data sampling, and many more. We illustrate the usefulness of our tool by applying it to the eye movements of 216 programmers of multiple expertise levels that were collected during two code comprehension tasks. Our tool helped to analyze the difference between the strategic program comprehension of programmers based on their demographic background, time taken to complete the task, choice of programming task, and expertise.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1347708655,
        "newsscientist":0.152568272,
        "technologyreview":0.2292103362,
        "venturebeat":0.2305655818,
        "wired":0.1998808949,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00102v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc",
            "cs.ir"
        ],
        "published":1659137028000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2208.00848v1",
        "predicted_newsworthiness":0.5011202827,
        "title":"DeFL: Decentralized Weight Aggregation for Cross-silo Federated Learning",
        "summary":"Federated learning (FL) is an emerging promising paradigm of privacy-preserving machine learning (ML). An important type of FL is cross-silo FL, which enables a small scale of organizations to cooperatively train a shared model by keeping confidential data locally and aggregating weights on a central parameter server. However, the central server may be vulnerable to malicious attacks or software failures in practice. To address this issue, in this paper, we propose DeFL, a novel decentralized weight aggregation framework for cross-silo FL. DeFL eliminates the central server by aggregating weights on each participating node and weights of only the current training round are maintained and synchronized among all nodes. We use Multi-Krum to enable aggregating correct weights from honest nodes and use HotStuff to ensure the consistency of the training round number and weights among all nodes. Besides, we theoretically analyze the Byzantine fault tolerance, convergence, and complexity of DeFL. We conduct extensive experiments over two widely-adopted public datasets, i.e. CIFAR-10 and Sentiment140, to evaluate the performance of DeFL. Results show that DeFL defends against common threat models with minimal accuracy loss, and achieves up to 100x reduction in storage overhead and up to 12x reduction in network overhead, compared to state-of-the-art decentralized FL approaches.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1477998749,
        "newsscientist":0.1710535235,
        "technologyreview":0.2936651762,
        "venturebeat":0.3019206577,
        "wired":0.2492941872,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00848v1",
        "arxiv_primary_category":"cs.lg",
        "arxiv_all_categories":[
            "cs.lg"
        ],
        "published":1659361009000,
        "published_hr":"Aug 01, 2022",
        "arxiv_primary_category_hr":"Machine Learning"
    },
    {
        "arxiv_id":"2208.01996v1",
        "predicted_newsworthiness":0.5010574692,
        "title":"Adaptive Domain Generalization via Online Disagreement Minimization",
        "summary":"Deep neural networks suffer from significant performance deterioration when there exists distribution shift between deployment and training. Domain Generalization (DG) aims to safely transfer a model to unseen target domains by only relying on a set of source domains. Although various DG approaches have been proposed, a recent study named DomainBed, reveals that most of them do not beat the simple Empirical Risk Minimization (ERM). To this end, we propose a general framework that is orthogonal to existing DG algorithms and could improve their performance consistently. Unlike previous DG works that stake on a static source model to be hopefully a universal one, our proposed AdaODM adaptively modifies the source model at test time for different target domains. Specifically, we create multiple domain-specific classifiers upon a shared domain-generic feature extractor. The feature extractor and classifiers are trained in an adversarial way, where the feature extractor embeds the input samples into a domain-invariant space, and the multiple classifiers capture the distinct decision boundaries that each of them relates to a specific source domain. During testing, distribution differences between target and source domains could be effectively measured by leveraging prediction disagreement among source classifiers. By fine-tuning source models to minimize the disagreement at test time, target domain features are well aligned to the invariant feature space. We verify AdaODM on two popular DG methods, namely ERM and CORAL, and four DG benchmarks, namely VLCS, PACS, OfficeHome, and TerraIncognita. The results show AdaODM stably improves the generalization capacity on unseen domains and achieves state-of-the-art performance.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1028014484,
        "newsscientist":0.1469453542,
        "technologyreview":0.2998480435,
        "venturebeat":0.2755838522,
        "wired":0.1874162419,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.01996v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.lg"
        ],
        "published":1659527471000,
        "published_hr":"Aug 03, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14083v1",
        "predicted_newsworthiness":0.500585364,
        "title":"Weakly-Supervised Camouflaged Object Detection with Scribble Annotations",
        "summary":"Existing camouflaged object detection (COD) methods rely heavily on large-scale datasets with pixel-wise annotations. However, due to the ambiguous boundary, it is very time-consuming and labor-intensive to annotate camouflage objects pixel-wisely (which takes ~ 60 minutes per image). In this paper, we propose the first weakly-supervised camouflaged object detection (COD) method, using scribble annotations as supervision. To achieve this, we first construct a scribble-based camouflaged object dataset with 4,040 images and corresponding scribble annotations. It is worth noting that annotating the scribbles used in our dataset takes only ~ 10 seconds per image, which is 360 times faster than per-pixel annotations. However, the network directly using scribble annotations for supervision will fail to localize the boundary of camouflaged objects and tend to have inconsistent predictions since scribble annotations only describe the primary structure of objects without details. To tackle this problem, we propose a novel consistency loss composed of two parts: a reliable cross-view loss to attain reliable consistency over different images, and a soft inside-view loss to maintain consistency inside a single prediction map. Besides, we observe that humans use semantic information to segment regions near boundaries of camouflaged objects. Therefore, we design a feature-guided loss, which includes visual features directly extracted from images and semantically significant features captured by models. Moreover, we propose a novel network that detects camouflaged objects by scribble learning on structural information and semantic relations. Experimental results show that our model outperforms relevant state-of-the-art methods on three COD benchmarks with an average improvement of 11.0% on MAE, 3.2% on S-measure, 2.5% on E-measure and 4.4% on weighted F-measure.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.0992731509,
        "newsscientist":0.1550351597,
        "technologyreview":0.2313495334,
        "venturebeat":0.1943752524,
        "wired":0.1628568754,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14083v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv"
        ],
        "published":1659015607000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2208.00788v1",
        "predicted_newsworthiness":0.5004448056,
        "title":"A Hybrid CNN-LSTM model for Video Deepfake Detection by Leveraging Optical Flow Features",
        "summary":"Deepfakes are the synthesized digital media in order to create ultra-realistic fake videos to trick the spectator. Deep generative algorithms, such as, Generative Adversarial Networks(GAN) are widely used to accomplish such tasks. This approach synthesizes pseudo-realistic contents that are very difficult to distinguish by traditional detection methods. In most cases, Convolutional Neural Network(CNN) based discriminators are being used for detecting such synthesized media. However, it emphasise primarily on the spatial attributes of individual video frames, thereby fail to learn the temporal information from their inter-frame relations. In this paper, we leveraged an optical flow based feature extraction approach to extract the temporal features, which are then fed to a hybrid model for classification. This hybrid model is based on the combination of CNN and recurrent neural network (RNN) architectures. The hybrid model provides effective performance on open source data-sets such as, DFDC, FF++ and Celeb-DF. This proposed method shows an accuracy of 66.26%, 91.21% and 79.49% in DFDC, FF++, and Celeb-DF respectively with a very reduced No of sample size of approx 100 samples(frames). This promises early detection of fake contents compared to existing modalities.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1236057586,
        "newsscientist":0.1766247627,
        "technologyreview":0.2492137337,
        "venturebeat":0.2297704111,
        "wired":0.2094972608,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2208.00788v1",
        "arxiv_primary_category":"cs.cv",
        "arxiv_all_categories":[
            "cs.cv",
            "cs.ai"
        ],
        "published":1659001089000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computer Vision and Pattern Recognition"
    },
    {
        "arxiv_id":"2207.14403v1",
        "predicted_newsworthiness":0.5004033732,
        "title":"Interactive Evaluation of Dialog Track at DSTC9",
        "summary":"The ultimate goal of dialog research is to develop systems that can be effectively used in interactive settings by real users. To this end, we introduced the Interactive Evaluation of Dialog Track at the 9th Dialog System Technology Challenge. This track consisted of two sub-tasks. The first sub-task involved building knowledge-grounded response generation models. The second sub-task aimed to extend dialog models beyond static datasets by assessing them in an interactive setting with real users. Our track challenges participants to develop strong response generation models and explore strategies that extend them to back-and-forth interactions with real users. The progression from static corpora to interactive evaluation introduces unique challenges and facilitates a more thorough assessment of open-domain dialog systems. This paper provides an overview of the track, including the methodology and results. Furthermore, it provides insights into how to best evaluate open-domain dialog models",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1322595085,
        "newsscientist":0.1533361829,
        "technologyreview":0.2613064436,
        "venturebeat":0.2871194555,
        "wired":0.2515716392,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14403v1",
        "arxiv_primary_category":"cs.cl",
        "arxiv_all_categories":[
            "cs.cl"
        ],
        "published":1659048844000,
        "published_hr":"Jul 28, 2022",
        "arxiv_primary_category_hr":"Computation and Language"
    },
    {
        "arxiv_id":"2207.13664v1",
        "predicted_newsworthiness":0.5003637881,
        "title":"Generic Approach to Visualization of Time Series Data",
        "summary":"Time series is a collection of data instances that are ordered according to a time stamp. Stock prices, temperature, etc are examples of time series data in real life. Time series data are used for forecasting sales, predicting trends. Visualization is the process of visually representing data or the relationship between features of a data either in a two-dimensional plot or a three-dimensional plot. Visualizing the time series data constitutes an important part of the process for working with a time series dataset. Visualizing the data not only helps in the modelling process but it can also be used to identify trends and features that cause those trends. In this work, we take a real-life time series dataset and analyse how the target feature relates to other features of the dataset through visualization. From the work that has been carried out, we present an effective method of visualization for time series data which will be much useful for machine learning modelling with such datasets.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.1495452562,
        "newsscientist":0.1853017017,
        "technologyreview":0.2477189694,
        "venturebeat":0.2501728975,
        "wired":0.192022073,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.13664v1",
        "arxiv_primary_category":"cs.hc",
        "arxiv_all_categories":[
            "cs.hc"
        ],
        "published":1658732903000,
        "published_hr":"Jul 25, 2022",
        "arxiv_primary_category_hr":"Human-Computer Interaction"
    },
    {
        "arxiv_id":"2207.14704v1",
        "predicted_newsworthiness":0.5001643732,
        "title":"Understanding the Relation of User and News Representations in Content-Based Neural News Recommendation",
        "summary":"A number of models for neural content-based news recommendation have been proposed. However, there is limited understanding of the relative importances of the three main components of such systems (news encoder, user encoder, and scoring function) and the trade-offs involved. In this paper, we assess the hypothesis that the most widely used means of matching user and candidate news representations is not expressive enough. We allow our system to model more complex relations between the two by assessing more expressive scoring functions. Across a wide range of baseline and established systems this results in consistent improvements of around 6 points in AUC. Our results also indicate a trade-off between the complexity of news encoder and scoring function: A fairly simple baseline model scores well above 68% AUC on the MIND dataset and comes within 2 points of the published state-of-the-art, while requiring a fraction of the computational costs.",
        "completion1":"This is a test completion for latency checking.",
        "completion2":"This is a test completion for latency checking.",
        "completion3":"This is a test completion for latency checking.",
        "theconversation":0.147293423,
        "newsscientist":0.1525243891,
        "technologyreview":0.2568997959,
        "venturebeat":0.2563295572,
        "wired":0.2422920993,
        "arxiv_url":"http:\/\/arxiv.org\/abs\/2207.14704v1",
        "arxiv_primary_category":"cs.ir",
        "arxiv_all_categories":[
            "cs.ir",
            "cs.cl"
        ],
        "published":1659104665000,
        "published_hr":"Jul 29, 2022",
        "arxiv_primary_category_hr":"Information Retrieval"
    }
]